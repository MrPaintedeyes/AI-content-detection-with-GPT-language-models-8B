,Unnamed: 0,blog_post,topic_category,AI_or_human,classification_result_zeroshot,classification_result_fewshot
0,0,"September 2024There's some debate about whether it's a good idea to ""follow your
passion."" In fact the question is impossible to answer with a simple
yes or no. Sometimes you should and sometimes you shouldn't, but
the border between should and shouldn't is very complicated. The
only way to give a general answer is to trace it.When people talk about this question, there's always an implicit
""instead of."" All other things being equal, why wouldn't you work
on what interests you the most? So even raising the question implies
that all other things aren't equal, and that you have to choose
between working on what interests you the most and something else,
like what pays the best.And indeed if your main goal is to make money, you can't usually
afford to work on what interests you the most. People pay you for
doing what they want, not what you want. But there's an obvious
exception: when you both want the same thing. For example, if you
love football, and you're good enough at it, you can get paid a lot
to play it.Of course the odds are against you in a case like football, because
so many other people like playing it too. This is not to say you
shouldn't try though. It depends how much ability you have and how
hard you're willing to work.The odds are better when you have strange tastes: when you like
something that pays well and that few other people like. For example,
it's clear that Bill Gates truly loved running a software company.
He didn't just love programming, which a lot of people do. He loved
writing software for customers. That is a very strange taste indeed,
but if you have it, you can make a lot by indulging it.There are even some people who have a genuine intellectual interest
in making money. This is distinct from mere greed. They just can't
help noticing when something is mispriced, and can't help doing
something about it. It's like a puzzle for them.
[1]In fact there's an edge case here so spectacular that it turns all
the preceding advice on its head. If you want to make a really 
huge
amount of money — hundreds of millions or even billions of dollars
— it turns out to be very useful to work on what interests you the
most. The reason is not the extra motivation you get from doing
this, but that the way to make a really large amount of money is
to start a startup, and working on what interests you is an excellent
way to discover startup ideas.Many if not most of the biggest startups began as projects the
founders were doing for fun. Apple, Google, and Facebook all began
that way. Why is this pattern so common? Because the best ideas
tend to be such outliers that you'd overlook them if you were
consciously looking for ways to make money. Whereas if you're young
and good at technology, your unconscious instincts about what would
be interesting to work on are very well aligned with what needs to
be built.So there's something like a midwit peak for making money. If you
don't need to make much, you can work on whatever you're most
interested in; if you want to become moderately rich, you can't
usually afford to; but if you want to become super rich, and you're
young and good at technology, working on what you're most interested
in becomes a good idea again.What if you're not sure what you want? What if you're attracted to
the idea of making money and more attracted to some kinds of work
than others, but neither attraction predominates? How do you break
ties?The key here is to understand that such ties are only apparent.
When you have trouble choosing between following your interests and
making money, it's never because you have complete knowledge of
yourself and of the types of work you're choosing between, and the
options are perfectly balanced. When you can't decide which path
to take, it's almost always due to ignorance. In fact you're usually
suffering from three kinds of ignorance simultaneously: you don't
know what makes you happy, what the various kinds of work are really
like, or how well you could do them. 
[2]In a way this ignorance is excusable. It's often hard to predict
these things, and no one even tells you that you need to. If you're
ambitious you're told you should go to college, and this is good
advice so far as it goes, but that's where it usually ends. No one
tells you how to figure out what to work on, or how hard this can
be.What do you do in the face of uncertainty? Get more certainty. And
probably the best way to do that is to try working on things you're
interested in. That will get you more information about how interested
you are in them, how good you are at them, and how much scope they
offer for ambition.Don't wait. Don't wait till the end of college to figure out what
to work on. Don't even wait for internships during college. You
don't necessarily need a job doing x in order to work on x; often
you can just start doing it in some form yourself. And since figuring
out what to work on is a problem that could take years to solve,
the sooner you start, the better.One useful trick for judging different kinds of work is to look at
who your colleagues will be. You'll become like whoever you work
with. Do you want to become like these people?Indeed, the difference in character between different kinds of work
is magnified by the fact that everyone else is facing the same
decisions as you. If you choose a kind of work mainly for how well
it pays, you'll be surrounded by other people who chose it for the
same reason, and that will make it even more soul-sucking than it
seems from the outside. Whereas if you choose work you're genuinely
interested in, you'll be surrounded mostly by other people who are
genuinely interested in it, and that will make it extra inspiring.
[3]The other thing you do in the face of uncertainty is to make choices
that are uncertainty-proof. The less sure you are about what to do,
the more important it is to choose options that give you more options
in the future. I call this ""staying upwind."" If you're unsure whether
to major in math or economics, for example, choose math; math is
upwind of economics in the sense that it will be easier to switch
later from math to economics than from economics to math.There's one case, though, where it's easy to say whether you should
work on what interests you the most: if you want to do 
great work.
This is not a sufficient condition for doing great work, but it is
a necessary one.There's a lot of selection bias in advice about whether to ""follow
your passion,"" and this is the reason. Most such advice comes from
people who are famously successful, and if you ask someone who's
famously successful how to do what they did, most will tell you
that you have to work on what you're most interested in. And this
is in fact true.That doesn't mean it's the right advice for everyone. Not everyone
can do great work, or wants to. But if you do want to, the complicated
question of whether or not to work on what interests you the most
becomes simple. The answer is yes. The root of great work is a sort
of ambitious curiosity, and you can't manufacture that.Notes[1]
These examples show why it's a mistake to assume that economic
inequality must be evidence of some kind of brokenness or unfairness.
It's obvious that different people have different interests, and
that some interests yield far more money than others, so how can
it not be obvious that some people will end up much richer than
others? In a world where some people like to write enterprise
software and others like to make studio pottery, economic inequality
is the natural outcome.[2]
Difficulty choosing between interests is a different matter.
That's not always due to ignorance. It's often intrinsically
difficult. I still have trouble doing it.[3]
You can't always take people at their word on this. Since
it's more prestigious to work on things you're interested in than
to be driven by money, people who are driven mainly by money will
often claim to be more interested in their work than they actually
are. One way to test such claims is by doing the following thought
experiment: if their work didn't pay well, would they take day jobs
doing something else in order to do it in their spare time? Lots
of mathematicians and scientists and engineers would. Historically
lots have. But I don't think as many investment bankers would.This thought experiment is also useful for distinguishing between
university departments.Thanks to Trevor Blackwell, Paul Buchheit, 
Jessica Livingston,
Robert Morris, Harj Taggar, and Garry Tan for reading drafts of
this.","career advice
",human,"human
","human
"
1,1,"

Want to start a startup?  Get funded by
Y Combinator.





March 2005(Parts of this essay began as replies to students who wrote to
me with questions.)Recently I've had several emails from computer science
undergrads asking what to do in college. I might not
be the best source of advice, because I was a philosophy major in
college.  But I took so many CS classes that most CS majors thought
I was one.  I was certainly a hacker, at least.HackingWhat should you do in college to become a 
good hacker?  There are two
main things you can do: become very good at programming, and learn
a lot about specific, cool problems.  These turn out to be equivalent,
because each drives you to do the other.The way to be good at programming is to work (a) a lot (b) on hard
problems.  And the way to make yourself work on hard problems is
to work on some very engaging project.
Odds are this project won't be a class assignment.  My friend Robert
learned a lot by writing network software when he was an
undergrad. One of his projects was to connect Harvard to the
Arpanet; it had been one of the original nodes, but by 1984 the
connection had died. [1]  Not only was this
work not for a class, but because he spent all his time on it
and neglected his studies, he was kicked out of
school for a year. [2]  It all evened out in the end, and now he's
a professor at MIT.  But you'll probably be happier if you don't
go to that extreme; it caused him a lot of worry at the time.Another way to be good at programming is to find other people who
are good at it, and learn what they know.  Programmers tend to sort
themselves into tribes according to the type of work they do and
the tools they use, and some tribes are 
smarter than others.  Look
around you and see what the smart people seem to be working on;
there's usually a reason.Some of the smartest people around you are professors.  So one way
to find interesting work is to volunteer as a research assistant.
Professors are especially interested in people who can solve tedious
system-administration type problems for them, so that is a way to
get a foot in the door.  What they fear are
flakes and resume padders.  It's all too
common for an assistant to result in a net increase in work.  So
you have to make it clear you'll mean a net decrease.Don't be put off if they say no.  Rejection is almost always less
personal than the rejectee imagines.  Just move on to the next.
(This applies to dating too.)Beware, because although most professors are smart, not all of them
work on interesting stuff.  Professors have to publish novel results
to advance their careers, but there is more competition in more
interesting areas of research.  So what less ambitious professors
do is turn out a series of papers whose conclusions are novel because
no one else cares about them.  You're better off avoiding these.I never worked as a research assistant, so I feel a bit dishonest
recommending that route.  I learned to program by writing stuff of
my own, particularly by trying to reverse-engineer Winograd's
SHRDLU.  I was as obsessed with that program as a mother with a new baby.Whatever the disadvantages of working by yourself, the advantage
is that the project is all your own.  You never have to compromise
or ask anyone's permission, and if you have a new idea you can just
sit down and start implementing it.In your own projects you don't have to worry about novelty (as
professors do) or profitability (as businesses do).  All that matters
is how hard the project is technically, and that has no correlation
to the nature of the application.  ""Serious"" applications like   
databases are often trivial and dull technically (if you ever suffer
from insomnia, try reading the technical literature about databases)
while ""frivolous"" applications like games are often very sophisticated.
I'm sure there are game companies out there working on products
with more intellectual content than the research at the
bottom nine tenths of university CS departments.If I were in college now I'd probably work on
graphics: a network game, for example, or a tool for 3D animation.
When I was an undergrad there weren't enough cycles around to make
graphics interesting, but it's hard to imagine anything more fun
to work on now.MathWhen I was in college, a lot of the professors believed (or at least
wished) that 
computer science was a branch of math.  This idea was
strongest at Harvard, where there wasn't even a CS major till the
1980s; till then one had to major in applied math.  But it was
nearly as bad at Cornell.  When I told the fearsome Professor Conway
that I was interested in AI (a hot topic then), he told me I should
major in math.  I'm still not sure whether he thought AI required
math, or whether he thought AI was nonsense and that majoring in
something rigorous would cure me of such stupid ambitions.In fact, the amount of math you need as a hacker is a lot less   
than most university departments like to admit.  I don't think you  
need much more than high school math plus a few concepts from the
theory of computation.  (You have to know what an n^2 algorithm is
if you want to avoid writing them.) Unless you're planning to write
math applications, of course.  Robotics, for example, is all math.But while you don't literally need math for most kinds of hacking,
in the sense of knowing 1001 tricks for differentiating formulas, 
math is very much worth studying for its own sake.  It's a 
valuable source of metaphors for almost any kind of work.[3] I wish 
I'd studied more math in college for that reason.Like a lot of people, I was mathematically abused as a child.  I   
learned to think of math as a collection of formulas that were
neither beautiful nor had any relation to my life (despite attempts
to translate them into ""word problems""), but had to be memorized 
in order to do well on tests.One of the most valuable things you could do in college would be
to learn what math is really about.  This may not be easy, because
a lot of good mathematicians are bad teachers.  And while there are
many popular books on math, few seem good.  The best I can think
of are W. W. Sawyer's.  And of course Euclid. [4]EverythingThomas Huxley said ""Try to learn something about everything and 
everything about something.""  Most universities aim at this
ideal.But what's everything?  To me it means, all that people
learn in the course of working honestly on hard problems.  All such 
work tends to be related, in that ideas and techniques from one   
field can often be transplanted successfully to others.  Even others
that seem quite distant.  For example, I write 
essays the same way
I write software: I sit down and blow out a lame version 1 as fast
as I can type, then spend several weeks rewriting it.Working on hard problems is not, by itself, enough.  Medieval   
alchemists were working on a hard problem, but their approach was 
so bogus that there was little
to learn from studying it, except possibly about people's ability  
to delude themselves.  Unfortunately the sort of AI I was trying    
to learn in college had the same flaw: a very hard problem, blithely
approached with hopelessly inadequate techniques.  Bold?  Closer 
to fraudulent.
The social sciences are also fairly bogus, because they're so much 
influenced by intellectual fashions.  If a 
physicist met a colleague
from 100 years ago, he could teach him some new things; if a psychologist
met a colleague from 100 years ago, they'd just get into an
ideological argument.
Yes, of course, you'll learn something by taking a
psychology class.  The point is, you'll learn more by taking
a class in another department.The worthwhile departments, in my opinion, are math, the hard
sciences, engineering, history (especially economic and social  
history, and the history of science), architecture, and the classics.
A survey course in art history may be worthwhile.  Modern literature
is important, but the way to learn about it is just to read.  I
don't know enough about music to say.You can skip the social sciences, philosophy, and the various
departments created recently in response to political pressures.
Many of these fields talk about important problems, certainly.  But
the way they talk about them is useless.  For example, philosophy   
talks, among other things, about our obligations to one another;    
but you can learn more about this from a wise grandmother or E. B.
White than from an academic philosopher.I speak here from experience.  I should probably have been offended 
when people laughed at Clinton for saying ""It depends on what the  
meaning of the word 'is' is.""  I took about five classes in college
on what the meaning of ""is"" is.Another way to figure out which fields are worth studying is to  
create the  dropout graph. For example, I know many people  
who switched from math to computer science because they found math  
too hard, and no one who did the opposite.  People don't do hard
things gratuitously; no one will work on a harder problem unless  
it is proportionately (or at least log(n)) more rewarding.  So
probably math is more worth studying than computer science.  By
similar comparisons you can make a graph of all the departments in
a university.  At the bottom you'll find the subjects with least 
intellectual content.If you use this method, you'll get roughly the same answer I just 
gave.Language courses are an anomaly.  I think they're better considered
as extracurricular activities, like pottery classes.  They'd be far
more useful when combined with some time living in a country where 
the language is spoken.  On a whim I studied Arabic as a freshman.
It was a lot of work, and the only lasting benefits were a weird  
ability to identify semitic roots and some insights into how people
recognize words.Studio art and creative writing courses are wildcards.  Usually 
you don't get taught much:  you just work (or don't work) on whatever
you want, and then sit around offering ""crits"" of one another's
creations under the vague supervision of the teacher.  But writing and
art are both very hard problems that (some) people work honestly
at, so they're worth doing, especially if you can find a good
teacher.JobsOf course college students have to think about more than just
learning.  There are also two practical problems to consider: jobs,
and graduate school.In theory a liberal education is not supposed to supply job training.
But everyone knows this is a bit of a fib.  Hackers at every college
learn practical skills, and not by accident.What you should learn to get a job depends on the kind you want.
If you want to work in a big company, learn how to hack 
Blub on
Windows.  If you want to work at a cool little company or research  
lab, you'll do better to learn Ruby on Linux.  And if you want to  
start your own company, which I think will be more and more common,
master the most powerful tools you can find, because you're going
to be in a race against your competitors, and they'll be your horse.There is not a direct correlation between the skills you should    
learn in college and those you'll use in a job.  You should aim     
slightly high in college.In workouts a football player may bench press 300 pounds, even
though he may never have to exert anything like that much force in
the course of a game.  Likewise, if your professors try to make you
learn stuff that's more advanced than you'll need in a job, it may
not just be because they're academics, detached from the real world.
They may be trying to make you lift weights with your brain.The programs you write in classes differ in three critical ways
from the ones you'll write in the real world: they're small; you
get to start from scratch; and the problem is usually artificial   
and predetermined.  In the real world, programs are bigger, tend   
to involve existing code, and often require you to figure out what 
the problem is before you can solve it.You don't have to wait to leave (or even enter) college to learn   
these skills.  If you want to learn how to deal with existing code,
for example, you can contribute to open-source projects.  The sort
of employer you want to work for will be as impressed by that as 
good grades on class assignments.In existing open-source projects you don't get much practice at
the third skill, deciding what problems to solve.  But there's 
nothing to stop you starting new projects of your own.  And  good
employers will be even more impressed
with that.What sort of problem should you try to solve?  One way to answer
that is to ask what you need as a user.  For example, I stumbled
on a good algorithm for spam filtering because I wanted to stop  
getting spam.  Now what I wish I had was a mail reader that somehow
prevented my inbox from filling up.  I tend to use my inbox as a
todo list.  But that's like using a screwdriver to open
bottles; what one really wants is a bottle opener.Grad SchoolWhat about grad school?  Should you go?  And how do you get into a  
good one?In principle, grad school is professional training in research, and
you shouldn't go unless you want to do research as a career.  And   
yet half the people who get PhDs in CS don't go into research.
I didn't go to grad school to become a professor.  I went because   
I wanted to learn more.So if you're mainly interested in hacking and you go to grad school,
you'll find a lot of other people who are similarly out of their  
element.  And if half the people around you are out of their element in the
same way you are, are you really out of your element?There's a fundamental problem in ""computer science,"" and it surfaces
in situations like this.  No one is sure what ""research"" is supposed to be.  
A lot
of research is hacking that had to be crammed into the form of an
academic paper to yield one more quantum of publication.So it's kind of misleading to ask whether you'll be at home in grad
school, because very few people are quite at home in computer
science.  The whole field is uncomfortable in its own skin.  So
the fact that you're mainly interested in hacking shouldn't deter  
you from going to grad school.  Just be warned you'll have to do a lot of stuff   
you don't like.Number one will be your dissertation.  Almost everyone hates their
dissertation by the time they're done with it.  The
process inherently tends to produce an unpleasant result, like a cake made out
of whole wheat flour and baked for twelve hours.  Few dissertations 
are read with pleasure, especially by their authors.But thousands before you have suffered through writing a dissertation.
And aside from that, grad school is close to paradise.  Many people
remember it as the happiest time of their lives.  And nearly all
the rest, including me, remember it as a period that would have 
been, if they hadn't had to write a dissertation. [5]The danger with grad school is that you don't see the scary part
upfront.  PhD programs start out as college part 2, with several
years of classes.  So by the time you face the horror of writing a  
dissertation, you're already several years in.  If you quit now,
you'll be a grad-school dropout, and you probably won't like that
idea.  When Robert got kicked out of grad school for writing the
Internet worm of 1988, I envied him enormously for finding a way out
without the stigma of failure. On the whole, grad school is probably better than most alternatives.  You meet a 
lot of smart people, and your glum procrastination will at least    
be a powerful common bond.  And of course you have a PhD at the
end.  I forgot about that. I suppose that's worth something.The greatest advantage of a PhD (besides being the union card of
academia, of course) may be that it gives you some baseline confidence.
For example, the Honeywell thermostats in my house have the most
atrocious UI.  My mother, who has the same model, diligently spent
a day reading the user's manual to learn how to operate hers.  She
assumed the problem was with her.  But I can think to myself ""If
someone with a PhD in computer science can't understand this
thermostat, it must be badly 
designed.""If you still want to go to grad school after this equivocal
recommendation, I can give you solid advice about how to get in. 
A lot of my friends are CS professors now, so I have the inside
story about admissions.  It's quite different from college.  At
most colleges, admissions officers decide who gets in.  For PhD
programs, the professors do.  And they try to do
it well, because the people they admit are going to be working for
them.Apparently only recommendations really matter at the best schools.
Standardized tests count for nothing, and grades for little.  The
essay is mostly an opportunity to disqualify yourself by saying   
something stupid.  The only thing professors
trust is recommendations, preferably from people they know. [6]So if you want to get into a PhD program, the key is to impress
your professors.  And from my friends who are professors I know 
what impresses them: not merely trying to impress them.  They're
not impressed by students who get good grades or want to be their
research assistants so they can get into grad school.  They're
impressed by students who get good grades and want to be their  
research assistants because they're genuinely interested in the 
topic.So the best thing you can do in college, whether you want to get
into grad school or just be good at hacking, is figure out what you
truly like.  It's hard to trick professors into letting you into
grad school, and impossible to trick problems into letting you solve
them.  College is where faking stops working.  From this point,
unless you want to go work for a big company, which is like reverting
to high school, the only way forward is through doing what you 
love.Notes
[1] No one seems to have minded, which shows how unimportant
the Arpanet (which became the Internet) was as late as
1984.[2] This is why, when I became an employer, I didn't care
about GPAs.  In fact, we actively sought out people   
who'd failed out of school.  We once put up posters around Harvard
saying ""Did you just get kicked out for doing badly in your classes
because you spent all your time working on some project of your   
own?  Come work for us!""  We managed to find a kid who had been, 
and he was a great hacker.When Harvard kicks undergrads out for a year, they have to get jobs.
The idea is to show them how awful the real world is, so they'll    
understand how lucky they are to be in college.  This plan backfired
with the guy who came to work for us, because he had more fun than
he'd had in school, and made more that year from stock options than
any of his professors did in salary.  So instead of crawling back
repentant at the end of the year, he took another year off and went
to Europe.  He did eventually graduate at about 26.[3] Eric Raymond says the best metaphors for hackers are
in set theory, combinatorics, and graph theory.Trevor Blackwell reminds you to take math classes intended for math majors.
""'Math for engineers' classes sucked mightily. In fact any 'x for
engineers' sucks, where x includes math, law, writing and visual
design.""[4] Other highly recommended books: What is Mathematics?, by
Courant and Robbins;  Geometry and the Imagination by Hilbert and 
Cohn-Vossen.
And for those interested in graphic design,
Byrne's Euclid.
[5] If you wanted to have the perfect life, the thing to do would
be to go to grad school, secretly write your dissertation in the
first year or two, and then just enjoy yourself for the next three
years, dribbling out a chapter at a time.  This prospect will make
grad students' mouths water, but I know of no one who's had the
discipline to pull it off.[6] One professor friend says that 15-20% of the grad students they
admit each year are ""long shots.""  But what he means by long shots
are people whose applications are perfect in every way, except
that no one on the admissions committee knows the professors who
wrote the recommendations.So if you want to get into
grad school in the sciences, you need to go to college somewhere with
real research professors.  Otherwise you'll seem a risky bet
to admissions committees, no matter how good you are.Which implies
a surprising but apparently inevitable consequence:
little liberal arts colleges are doomed.
 Most smart
high school kids at least consider going into the sciences, even
if they ultimately choose not to.
Why go to a college that limits their options?Thanks to Trevor Blackwell, Alex Lewin, Jessica Livingston,
Robert Morris, Eric
Raymond, and several 
anonymous CS professors 
for reading drafts of this, and to the students whose questions
began it.More Advice for UndergradsJoel Spolsky: Advice for Computer Science College StudentsEric Raymond: How to Become a Hacker","career advice
",human,"human
","human
"
2,2,"October 2020One of the biggest things holding people back from doing great work
is the fear of making something lame. And this fear is not an
irrational one. Many great projects go through a stage early on
where they don't seem very impressive, even to their creators. You
have to push through this stage to reach the great work that lies
beyond. But many people don't. Most people don't even reach the
stage of making something they're embarrassed by, let alone continue
past it. They're too frightened even to start.Imagine if we could turn off the fear of making something lame.
Imagine how much more we'd do.Is there any hope of turning it off? I think so. I think the habits
at work here are not very deeply rooted.Making new things is itself a new thing for us as a species. It has
always happened, but till the last few centuries it happened so
slowly as to be invisible to individual humans. And since we didn't
need customs for dealing with new ideas, we didn't develop any.We just don't have enough experience with early versions of ambitious
projects to know how to respond to them. We judge them as we would
judge more finished work, or less ambitious projects. We don't
realize they're a special case.Or at least, most of us don't. One reason I'm confident we can do
better is that it's already starting to happen. There are already
a few places that are living in the future in this respect. Silicon
Valley is one of them: an unknown person working on a strange-sounding
idea won't automatically be dismissed the way they would back home.
In Silicon Valley, people have learned how dangerous that is.The right way to deal with new ideas is to treat them as a challenge
to your imagination — not just to have lower standards, but to
switch polarity entirely, from listing 
the reasons an idea won't
work to trying to think of ways it could. That's what I do when I
meet people with new ideas. I've become quite good at it, but I've
had a lot of practice. Being a partner at Y Combinator means being
practically immersed in strange-sounding ideas proposed by unknown
people. Every six months you get thousands of new ones thrown at
you and have to sort through them, knowing that in a world with a
power-law distribution of outcomes, it will be painfully obvious
if you miss the needle in this haystack. Optimism becomes
urgent.But I'm hopeful that, with time, this kind of optimism can become
widespread enough that it becomes a social custom, not just a trick
used by a few specialists. It is after all an extremely lucrative
trick, and those tend to spread quickly.Of course, inexperience is not the only reason people are too harsh
on early versions of ambitious projects. They also do it to seem
clever. And in a field where the new ideas are risky, like startups,
those who dismiss them are in fact more likely to be right. Just
not when their predictions are 
weighted by outcome.But there is another more sinister reason people dismiss new ideas.
If you try something ambitious, many of those around you will hope,
consciously or unconsciously, that you'll fail. They worry that if
you try something ambitious and succeed, it will put you above them.
In some countries this is not just an individual failing but part
of the national culture.I wouldn't claim that people in Silicon Valley overcome these
impulses because they're morally better. 
[1]
The reason many hope
you'll succeed is that they hope to rise with you. For investors
this incentive is particularly explicit. They want you to succeed
because they hope you'll make them rich in the process. But many
other people you meet can hope to benefit in some way from your
success. At the very least they'll be able to say, when you're
famous, that they've known you since way back.But even if Silicon Valley's encouraging attitude
is rooted in self-interest, it has over time actually grown into a
sort of benevolence. Encouraging startups has been practiced for
so long that it has become a custom. Now it just seems that that's
what one does with startups.Maybe Silicon Valley is too optimistic. Maybe it's too easily fooled
by impostors. Many less optimistic journalists want to believe that.
But the lists of impostors they cite are suspiciously short, and
plagued with asterisks. 
[2] If you use revenue as the test, Silicon
Valley's optimism seems better tuned than the rest of the world's.
And because it works, it will spread.There's a lot more to new ideas than new startup ideas, of course.
The fear of making something lame holds people back in every field.
But Silicon Valley shows how quickly customs can evolve to support
new ideas. And that in turn proves that dismissing new ideas is not
so deeply rooted in human nature that it can't be unlearnt.
___________
Unfortunately, if you want to do new things, you'll face a force
more powerful than other people's skepticism: your own skepticism.
You too will judge your early work too harshly. How do you avoid
that?This is a difficult problem, because you don't want to completely
eliminate your horror of making something lame. That's what steers
you toward doing good work. You just want to turn it off temporarily,
the way a painkiller temporarily turns off pain.People have already discovered several techniques that work. Hardy
mentions two in A Mathematician's Apology:

  Good work is not done by ""humble"" men. It is one of the first
  duties of a professor, for example, in any subject, to exaggerate
  a little both the importance of his subject and his importance
  in it.

If you overestimate the importance of what you're working on, that
will compensate for your mistakenly harsh judgment of your initial
results. If you look at something that's 20% of the way to a goal
worth 100 and conclude that it's 10% of the way to a goal worth
200, your estimate of its expected value is correct even though
both components are wrong.It also helps, as Hardy suggests, to be slightly overconfident.
I've noticed in many fields that the most successful people are
slightly overconfident. On the face of it this seems implausible.
Surely it would be optimal to have exactly the right estimate of
one's abilities. How could it be an advantage to be mistaken?
Because this error compensates for other sources of error in the
opposite direction: being slightly overconfident armors you against
both other people's skepticism and your own.Ignorance has a similar effect. It's safe to make the mistake of
judging early work as finished work if you're a sufficiently lax
judge of finished work. I doubt it's possible to cultivate this
kind of ignorance, but empirically it's a real advantage, especially
for the young.Another way to get through the lame phase of ambitious projects is
to surround yourself with the right people — to create an eddy in
the social headwind. But it's not enough to collect people who are
always encouraging. You'd learn to discount that. You need colleagues
who can actually tell an ugly duckling from a baby swan. The people
best able to do this are those working on similar projects of their
own, which is why university departments and research labs work so
well. You don't need institutions to collect colleagues. They
naturally coalesce, given the chance. But it's very much worth
accelerating this process by seeking out other people trying to do
new things.Teachers are in effect a special case of colleagues. It's a teacher's
job both to see the promise of early work and to encourage you to
continue. But teachers who are good at this are unfortunately quite
rare, so if you have the opportunity to learn from one, take it.
[3]For some it might work to rely on sheer discipline: to tell yourself
that you just have to press on through the initial crap phase and
not get discouraged. But like a lot of ""just tell yourself"" advice,
this is harder than it sounds. And it gets still harder as you get
older, because your standards rise. The old do have one compensating
advantage though: they've been through this before.It can help if you focus less on where you are and more on the rate
of change. You won't worry so much about doing bad work if you can
see it improving. Obviously the faster it improves, the easier this
is. So when you start something new, it's good if you can spend a
lot of time on it. That's another advantage of being young: you
tend to have bigger blocks of time.Another common trick is to start by considering new work to be of
a different, less exacting type. To start a painting saying that
it's just a sketch, or a new piece of software saying that it's
just a quick hack. Then you judge your initial results by a lower
standard. Once the project is rolling you can sneakily convert it
to something more.
[4]This will be easier if you use a medium that lets you work fast and
doesn't require too much commitment up front. It's easier to convince
yourself that something is just a sketch when you're drawing in a
notebook than when you're carving stone. Plus you get initial results
faster. 
[5]
[6]It will be easier to try out a risky project if you think of it as
a way to learn and not just as a way to make something. Then even
if the project truly is a failure, you'll still have gained by it.
If the problem is sharply enough defined, failure itself is
knowledge: if the theorem you're trying to prove turns out to
be false, or you use a structural member of a certain size and
it fails under stress, you've learned something, even if it
isn't what you wanted to learn.
[7]One motivation that works particularly well for me is curiosity.
I like to try new things just to see how they'll turn out. We started
Y Combinator in this spirit, and it was one of main things that
kept me going while I was working on 
Bel. Having worked for so long
with various dialects of Lisp, I was very curious to see what its
inherent shape was: what you'd end up with if you followed the
axiomatic approach all the way.But it's a bit strange that you have to play mind games with yourself
to avoid being discouraged by lame-looking early efforts. The thing
you're trying to trick yourself into believing is in fact the truth.
A lame-looking early version of an ambitious project truly is more
valuable than it seems. So the ultimate solution may be to teach
yourself that.One way to do it is to study the histories of people who've
done great work. What were they thinking early on? What was the
very first thing they did? It can sometimes be hard to get an
accurate answer to this question, because people are often embarrassed
by their earliest work and make little effort to publish it. (They
too misjudge it.) But when you can get an accurate picture of the
first steps someone made on the path to some great work, they're
often pretty feeble.
[8]Perhaps if you study enough such cases, you can teach yourself to
be a better judge of early work. Then you'll be immune both to other
people's skepticism and your own fear of making something lame.
You'll see early work for what it is.Curiously enough, the solution to the problem of judging early work
too harshly is to realize that our attitudes toward it are themselves
early work. Holding everything to the same standard is a crude
version 1. We're already evolving better customs, and we can already
see signs of how big the payoff will be.
Notes[1]
This assumption may be too conservative. There is some evidence
that historically the Bay Area has attracted a 
different sort of person than, 
say, New York City.[2]
One of their great favorites is Theranos. But the most conspicuous
feature of Theranos's cap table is the absence of Silicon Valley
firms. Journalists were fooled by Theranos, but Silicon Valley
investors weren't.[3]
I made two mistakes about teachers when I was younger.  I
cared more about professors' research than their reputations as
teachers, and I was also wrong about what it meant to be a good
teacher. I thought it simply meant to be good at explaining things.[4]
Patrick Collison points out that you can go past treating
something as a hack in the sense of a prototype and onward to the
sense of the word that means something closer to a practical joke:

  I think there may be something related to being a hack that can
  be powerful — the idea of making the tenuousness and implausibility
  a feature. ""Yes, it's a bit ridiculous, right?  I'm just trying
  to see how far such a naive approach can get."" YC seemed to me
  to have this characteristic.

[5]
Much of the advantage of switching from physical to digital
media is not the software per se but that it lets you start something
new with little upfront commitment.[6]
John Carmack adds:

  The value of a medium without a vast gulf between the early work
  and the final work is exemplified in game mods. The original
  Quake game was a golden age for mods, because everything was very
  flexible, but so crude due to technical limitations, that quick
  hacks to try out a gameplay idea weren't all that far from the
  official game. Many careers were born from that, but as the
  commercial game quality improved over the years, it became almost
  a full time job to make a successful mod that would be appreciated
  by the community. This was dramatically reversed with Minecraft
  and later Roblox, where the entire esthetic of the experience was
  so explicitly crude that innovative gameplay concepts became the
  overriding value. These ""crude"" game mods by single authors are
  now often bigger deals than massive professional teams' work.

[7]
Lisa Randall suggests that we

  treat new things as experiments. That way there's no such thing
  as failing, since you learn something no matter what. You treat
  it like an experiment in the sense that if it really rules something
  out, you give up and move on, but if there's some way to vary it
  to make it work better, go ahead and do that

[8]
Michael Nielsen points out that the internet has made this
easier, because you can see programmers' first commits, musicians'
first videos, and so on.Thanks to Trevor Blackwell, John Carmack, Patrick Collison, Jessica
Livingston, Michael Nielsen, and Lisa Randall for reading drafts
of this.","career advice
",human,"human
","human
"
3,3,"




""...the mere consciousness of an engagement will sometimes worry a whole day.""– Charles Dickens




July 2009One reason programmers dislike meetings so much is that they're on
a different type of schedule from other people.  Meetings cost them
more.There are two types of schedule, which I'll call the manager's
schedule and the maker's schedule.  The manager's schedule is for
bosses.  It's embodied in the traditional appointment book, with
each day cut into one hour intervals.  You can block off several
hours for a single task if you need to, but by default you change
what you're doing every hour.When you use time that way, it's merely a practical problem to meet
with someone.  Find an open slot in your schedule, book them, and
you're done.Most powerful people are on the manager's schedule.  It's the
schedule of command.  But there's another way of using time that's
common among people who make things, like programmers and writers.
They generally prefer to use time in units of half a day at least.
You can't write or program well in units of an hour.  That's barely
enough time to get started.When you're operating on the maker's schedule, meetings are a
disaster.  A single meeting can blow a whole afternoon, by breaking
it into two pieces each too small to do anything hard in.  Plus you
have to remember to go to the meeting.  That's no problem for someone
on the manager's schedule.  There's always something coming on the
next hour; the only question is what.  But when someone on the
maker's schedule has a meeting, they have to think about it.For someone on the maker's schedule, having a meeting is like
throwing an exception.  It doesn't merely cause you to switch from
one task to another; it changes the mode in which you work.I find one meeting can sometimes affect a whole day.   A meeting
commonly blows at least half a day, by breaking up a morning or
afternoon.  But in addition there's sometimes a cascading effect.
If I know the afternoon is going to be broken up, I'm slightly less
likely to start something ambitious in the morning.  I know this
may sound oversensitive, but if you're a maker, think of your own
case.  Don't your spirits rise at the thought of having an entire
day free to work, with no appointments at all?  Well, that means
your spirits are correspondingly depressed when you don't.  And
ambitious projects are by definition close to the limits of your
capacity.  A small decrease in morale is enough to kill them off.Each type of schedule works fine by itself.  Problems arise when
they meet.  Since most powerful people operate on the manager's
schedule, they're in a position to make everyone resonate at their
frequency if they want to.  But the smarter ones restrain themselves,
if they know that some of the people working for them need long
chunks of time to work in.Our case is an unusual one.  Nearly all investors, including all
VCs I know, operate on the manager's schedule.  But 
Y Combinator
runs on the maker's schedule.  Rtm and Trevor and I do because we
always have, and Jessica does too, mostly, because she's gotten
into sync with us.I wouldn't be surprised if there start to be more companies like
us.  I suspect founders may increasingly be able to resist, or at
least postpone, turning into managers, just as a few decades ago
they started to be able to resist switching from jeans
to suits.How do we manage to advise so many startups on the maker's schedule?
By using the classic device for simulating the manager's schedule
within the maker's: office hours.  Several times a week I set aside
a chunk of time to meet founders we've funded.  These chunks of
time are at the end of my working day, and I wrote a signup program
that ensures all the appointments within a given set of office hours
are clustered at the end.  Because they come at the end of my day
these meetings are never an interruption.  (Unless their working
day ends at the same time as mine, the meeting presumably interrupts
theirs, but since they made the appointment it must be worth it to
them.)  During busy periods, office hours sometimes get long enough
that they compress the day, but they never interrupt it.
When we were working on our own startup, back in the 90s, I evolved
another trick for partitioning the day.  I used to program from
dinner till about 3 am every day, because at night no one could
interrupt me.  Then I'd sleep till about 11 am, and come in and
work until dinner on what I called ""business stuff.""  I never thought
of it in these terms, but in effect I had two workdays each day,
one on the manager's schedule and one on the maker's.When you're operating on the manager's schedule you can do something
you'd never want to do on the maker's: you can have speculative
meetings.  You can meet someone just to get to know one another.
If you have an empty slot in your schedule, why not?  Maybe it will
turn out you can help one another in some way.Business people in Silicon Valley (and the whole world, for that
matter) have speculative meetings all the time.  They're effectively
free if you're on the manager's schedule.  They're so common that
there's distinctive language for proposing them: saying that you
want to ""grab coffee,"" for example.Speculative meetings are terribly costly if you're on the maker's
schedule, though.  Which puts us in something of a bind.  Everyone
assumes that, like other investors, we run on the manager's schedule.
So they introduce us to someone they think we ought to meet, or
send us an email proposing we grab coffee.  At this point we have
two options, neither of them good: we can meet with them, and lose
half a day's work; or we can try to avoid meeting them, and probably
offend them.Till recently we weren't clear in our own minds about the source
of the problem.  We just took it for granted that we had to either
blow our schedules or offend people.  But now that I've realized
what's going on, perhaps there's a third option: to write something
explaining the two types of schedule.  Maybe eventually, if the
conflict between the manager's schedule and the maker's schedule
starts to be more widely understood, it will become less of a
problem.Those of us on the maker's schedule are willing to compromise.   We
know we have to have some number of meetings.  All we ask from those
on the manager's schedule is that they understand the cost.
Thanks to Sam Altman, Trevor Blackwell, Paul Buchheit, Jessica Livingston,
and Robert Morris for reading drafts of this.Related:How to Do What You LoveGood and Bad ProcrastinationTurkish TranslationFrench TranslationKorean TranslationGerman Translation","career advice
",human,"human
","human
"
4,4,"December 2019I've seen the same pattern in many different fields: even though
lots of people have worked hard in the field, only a small fraction
of the space of possibilities has been explored, because they've
all worked on similar things.Even the smartest, most imaginative people are surprisingly
conservative when deciding what to work on. People who would never
dream of being fashionable in any other way get sucked into working
on fashionable problems.If you want to try working on unfashionable problems, one of the
best places to look is in fields that people think have already been
fully explored: essays, Lisp, venture funding — you may notice a
pattern here. If you can find a new approach into a big but apparently
played out field, the value of whatever you discover will be
multiplied by its enormous surface area.The best protection against getting drawn into working on the same
things as everyone else may be to genuinely 
love what you're doing.
Then you'll continue to work on it even if you make the same mistake
as other people and think that it's too marginal to matter.Japanese TranslationArabic TranslationFrench Translation","career advice
",human,"human
","human
"
5,5,"May 2001

(These are some notes I made
for a panel discussion on programming language design
at MIT on May 10, 2001.)1. Programming Languages Are for People.Programming languages
are how people talk to computers.  The computer would be just as
happy speaking any language that was unambiguous.  The reason we
have high level languages is because people can't deal with
machine language.  The point of programming
languages is to prevent our poor frail human brains from being 
overwhelmed by a mass of detail.Architects know that some kinds of design problems are more personal
than others.  One of the cleanest, most abstract design problems
is designing bridges.  There your job is largely a matter of spanning
a given distance with the least material.  The other end of the
spectrum is designing chairs.  Chair designers have to spend their
time thinking about human butts.Software varies in the same way. Designing algorithms for routing
data through a network is a nice, abstract problem, like designing
bridges.  Whereas designing programming languages is like designing
chairs: it's all about dealing with human weaknesses.Most of us hate to acknowledge this.  Designing systems of great
mathematical elegance sounds a lot more appealing to most of us
than pandering to human weaknesses.  And there is a role for mathematical
elegance: some kinds of elegance make programs easier to understand.
But elegance is not an end in itself.And when I say languages have to be designed to suit human weaknesses,
I don't mean that languages have to be designed for bad programmers.
In fact I think you ought to design for the 
best programmers, but
even the best programmers have limitations.  I don't think anyone
would like programming in a language where all the variables were
the letter x with integer subscripts.2. Design for Yourself and Your Friends.If you look at the history of programming languages, a lot of the best
ones were languages designed for their own authors to use, and a
lot of the worst ones were designed for other people to use.When languages are designed for other people, it's always a specific
group of other people: people not as smart as the language designer.
So you get a language that talks down to you.  Cobol is the most
extreme case, but a lot of languages are pervaded by this spirit.It has nothing to do with how abstract the language is.  C is pretty
low-level, but it was designed for its authors to use, and that's
why hackers like it.The argument for designing languages for bad programmers is that
there are more bad programmers than good programmers.  That may be
so.  But those few good programmers write a disproportionately
large percentage of the software.I'm interested in the question, how do you design a language that
the very best hackers will like?  I happen to think this is
identical to the question, how do you design a good programming
language?, but even if it isn't, it is at least an interesting
question.3. Give the Programmer as Much Control as Possible.Many languages
(especially the ones designed for other people) have the attitude
of a governess: they try to prevent you from
doing things that they think aren't good for you.  I like the   
opposite approach: give the programmer as much
control as you can.When I first learned Lisp, what I liked most about it was
that it considered me an equal partner.  In the other languages
I had learned up till then, there was the language and there was my   
program, written in the language, and the two were very separate.
But in Lisp the functions and macros I wrote were just like those
that made up the language itself.  I could rewrite the language
if I wanted.  It had the same appeal as open-source software.4. Aim for Brevity.Brevity is underestimated and even scorned.
But if you look into the hearts of hackers, you'll see that they
really love it.  How many times have you heard hackers speak fondly
of how in, say, APL, they could do amazing things with just a couple
lines of code?  I think anything that really smart people really
love is worth paying attention to.I think almost anything
you can do to make programs shorter is good.  There should be lots
of library functions; anything that can be implicit should be;
the syntax should be terse to a fault; even the names of things
should be short.And it's not only programs that should be short.  The manual should
be thin as well.  A good part of manuals is taken up with clarifications
and reservations and warnings and special cases.  If you force  
yourself to shorten the manual, in the best case you do it by fixing
the things in the language that required so much explanation.5. Admit What Hacking Is.A lot of people wish that hacking was
mathematics, or at least something like a natural science.  I think
hacking is more like architecture.  Architecture is
related to physics, in the sense that architects have to design
buildings that don't fall down, but the actual goal of architects
is to make great buildings, not to make discoveries about statics.What hackers like to do is make great programs.
And I think, at least in our own minds, we have to remember that it's
an admirable thing to write great programs, even when this work 
doesn't translate easily into the conventional intellectual
currency of research papers.  Intellectually, it is just as
worthwhile to design a language programmers will love as it is to design a
horrible one that embodies some idea you can publish a paper
about.1. How to Organize Big Libraries?Libraries are becoming an
increasingly important component of programming languages.  They're
also getting bigger, and this can be dangerous.  If it takes longer
to find the library function that will do what you want than it
would take to write it yourself, then all that code is doing nothing
but make your manual thick.  (The Symbolics manuals were a case in 
point.)  So I think we will have to work on ways to organize
libraries.  The ideal would be to design them so that the programmer
could guess what library call would do the right thing.2. Are People Really Scared of Prefix Syntax?This is an open
problem in the sense that I have wondered about it for years and
still don't know the answer.  Prefix syntax seems perfectly natural
to me, except possibly for math.  But it could be that a lot of 
Lisp's unpopularity is simply due to having an unfamiliar syntax.   
Whether to do anything about it, if it is true, is another question. 

3. What Do You Need for Server-Based Software?

I think a lot of the most exciting new applications that get written
in the next twenty years will be Web-based applications, meaning
programs that sit on the server and talk to you through a Web
browser.  And to write these kinds of programs we may need some
new things.One thing we'll need is support for the new way that server-based 
apps get released.  Instead of having one or two big releases a
year, like desktop software, server-based apps get released as a
series of small changes.  You may have as many as five or ten
releases a day.  And as a rule everyone will always use the latest
version.You know how you can design programs to be debuggable?
Well, server-based software likewise has to be designed to be
changeable.  You have to be able to change it easily, or at least
to know what is a small change and what is a momentous one.Another thing that might turn out to be useful for server based
software, surprisingly, is continuations.  In Web-based software
you can use something like continuation-passing style to get the
effect of subroutines in the inherently 
stateless world of a Web
session.  Maybe it would be worthwhile having actual continuations,
if it was not too expensive.4. What New Abstractions Are Left to Discover?I'm not sure how
reasonable a hope this is, but one thing I would really love to    
do, personally, is discover a new abstraction-- something that would
make as much of a difference as having first class functions or
recursion or even keyword parameters.  This may be an impossible
dream.  These things don't get discovered that often.  But I am always
looking.1. You Can Use Whatever Language You Want.Writing application
programs used to mean writing desktop software.  And in desktop
software there is a big bias toward writing the application in the
same language as the operating system.  And so ten years ago,
writing software pretty much meant writing software in C.
Eventually a tradition evolved:
application programs must not be written in unusual languages.  
And this tradition had so long to develop that nontechnical people
like managers and venture capitalists also learned it.Server-based software blows away this whole model.  With server-based
software you can use any language you want.  Almost nobody understands
this yet (especially not managers and venture capitalists).
A few hackers understand it, and that's why we even hear
about new, indy languages like Perl and Python.  We're not hearing
about Perl and Python because people are using them to write Windows
apps.What this means for us, as people interested in designing programming
languages, is that there is now potentially an actual audience for
our work.2. Speed Comes from Profilers.Language designers, or at least
language implementors, like to write compilers that generate fast
code.  But I don't think this is what makes languages fast for users.
Knuth pointed out long ago that speed only matters in a few critical
bottlenecks.  And anyone who's tried it knows that you can't guess
where these bottlenecks are.  Profilers are the answer.Language designers are solving the wrong problem.  Users don't need
benchmarks to run fast.  What they need is a language that can show
them what parts of their own programs need to be rewritten.  That's
where speed comes from in practice.  So maybe it would be a net 
win if language implementors took half the time they would
have spent doing compiler optimizations and spent it writing a
good profiler instead.3. You Need an Application to Drive the Design of a Language.This may not be an absolute rule, but it seems like the best languages
all evolved together with some application they were being used to
write.  C was written by people who needed it for systems programming.
Lisp was developed partly to do symbolic differentiation, and
McCarthy was so eager to get started that he was writing differentiation
programs even in the first paper on Lisp, in 1960.It's especially good if your application solves some new problem.
That will tend to drive your language to have new features that   
programmers need.  I personally am interested in writing
a language that will be good for writing server-based applications.[During the panel, Guy Steele also made this point, with the
additional suggestion that the application should not consist of
writing the compiler for your language, unless your language
happens to be intended for writing compilers.]4. A Language Has to Be Good for Writing Throwaway Programs.You know what a throwaway program is: something you write quickly for
some limited task.  I think if you looked around you'd find that  
a lot of big, serious programs started as throwaway programs.  I
would not be surprised if most programs started as throwaway
programs.  And so if you want to make a language that's good for
writing software in general, it has to be good for writing throwaway
programs, because that is the larval stage of most software.5. Syntax Is Connected to Semantics.It's traditional to think of
syntax and semantics as being completely separate.  This will
sound shocking, but it may be that they aren't.
I think that what you want in your language may be related
to how you express it.I was talking recently to Robert Morris, and he pointed out that
operator overloading is a bigger win in languages with infix
syntax.  In a language with prefix syntax, any function you define
is effectively an operator.  If you want to define a plus for a
new type of number you've made up, you can just define a new function
to add them.  If you do that in a language with infix syntax,
there's a big difference in appearance between the use of an
overloaded operator and a function call.1. New Programming Languages.Back in the 1970s
it was fashionable to design new programming languages.  Recently
it hasn't been.  But I think server-based software will make new  
languages fashionable again.  With server-based software, you can
use any language you want, so if someone does design a language that
actually seems better than others that are available, there will be
people who take a risk and use it.2. Time-Sharing.Richard Kelsey gave this as an idea whose time
has come again in the last panel, and I completely agree with him.
My guess (and Microsoft's guess, it seems) is that much computing
will move from the desktop onto remote servers.  In other words,  
time-sharing is back.  And I think there will need to be support
for it at the language level.  For example, I know that Richard
and Jonathan Rees have done a lot of work implementing process  
scheduling within Scheme 48.3. Efficiency.Recently it was starting to seem that computers
were finally fast enough.  More and more we were starting to hear
about byte code, which implies to me at least that we feel we have
cycles to spare.  But I don't think we will, with server-based
software.   Someone is going to have to pay for the servers that
the software runs on, and the number of users they can support per
machine will be the divisor of their capital cost.So I think efficiency will matter, at least in computational
bottlenecks.  It will be especially important to do i/o fast,
because server-based applications do a lot of i/o.It may turn out that byte code is not a win, in the end.  Sun and
Microsoft seem to be facing off in a kind of a battle of the byte
codes at the moment.  But they're doing it because byte code is a
convenient place to insert themselves into the process, not because
byte code is in itself a good idea.  It may turn out that this
whole battleground gets bypassed.  That would be kind of amusing.1. Clients.This is just a guess, but my guess is that
the winning model for most applications will be purely server-based.
Designing software that works on the assumption that everyone will 
have your client is like designing a society on the assumption that
everyone will just be honest.  It would certainly be convenient, but
you have to assume it will never happen.I think there will be a proliferation of devices that have some
kind of Web access, and all you'll be able to assume about them is
that they can support simple html and forms.  Will you have a
browser on your cell phone?  Will there be a phone in your palm  
pilot?  Will your blackberry get a bigger screen? Will you be able
to browse the Web on your gameboy?  Your watch?  I don't know.  
And I don't have to know if I bet on
everything just being on the server.  It's
just so much more robust to have all the 
brains on the server.2. Object-Oriented Programming.I realize this is a
controversial one, but I don't think object-oriented programming
is such a big deal.  I think it is a fine model for certain kinds
of applications that need that specific kind of data structure,   
like window systems, simulations, and cad programs.  But I don't
see why it ought to be the model for all programming.I think part of the reason people in big companies like object-oriented
programming is because it yields a lot of what looks like work.
Something that might naturally be represented as, say, a list of
integers, can now be represented as a class with all kinds of
scaffolding and hustle and bustle.Another attraction of
object-oriented programming is that methods give you some of the
effect of first class functions.  But this is old news to Lisp
programmers.  When you have actual first class functions, you can
just use them in whatever way is appropriate to the task at hand,
instead of forcing everything into a mold of classes and methods.What this means for language design, I think, is that you shouldn't
build object-oriented programming in too deeply.  Maybe the
answer is to offer more general, underlying stuff, and let people design
whatever object systems they want as libraries.3. Design by Committee.Having your language designed by a committee is a big pitfall,  
and not just for the reasons everyone knows about.  Everyone
knows that committees tend to yield lumpy, inconsistent designs.  
But I think a greater danger is that they won't take risks.
When one person is in charge he can take risks
that a committee would never agree on.Is it necessary to take risks to design a good language though?
Many people might suspect
that language design is something where you should stick fairly
close to the conventional wisdom.  I bet this isn't true.
In everything else people do, reward is proportionate to risk.
Why should language design be any different?Japanese Translation","programming advice
",human,"human
","human
"
6,6,"May 2001(This article was written as a kind of business plan for a
new language.
So it is missing (because it takes for granted) the most important
feature of a good programming language: very powerful abstractions.)A friend of mine once told an eminent operating systems
expert that he wanted to design a really good
programming language.  The expert told him that it would be a
waste of time, that programming languages don't become popular
or unpopular based on their merits, and so no matter how
good his language was, no one would use it.  At least, that
was what had happened to the language he had designed.What does make a language popular?  Do popular
languages deserve their popularity?  Is it worth trying to
define a good programming language?  How would you do it?I think the answers to these questions can be found by looking 
at hackers, and learning what they want.  Programming
languages are for hackers, and a programming language
is good as a programming language (rather than, say, an
exercise in denotational semantics or compiler design)
if and only if hackers like it.1 The Mechanics of PopularityIt's true, certainly, that most people don't choose programming
languages simply based on their merits.  Most programmers are told
what language to use by someone else.  And yet I think the effect
of such external factors on the popularity of programming languages
is not as great as it's sometimes thought to be. I think a bigger
problem is that a hacker's idea of a good programming language is
not the same as most language designers'.Between the two, the hacker's opinion is the one that matters.
Programming languages are not theorems. They're tools, designed
for people, and they have to be designed to suit human strengths
and weaknesses as much as shoes have to be designed for human feet.
If a shoe pinches when you put it on, it's a bad shoe, however
elegant it may be as a piece of sculpture.It may be that the majority of programmers can't tell a good language
from a bad one. But that's no different with any other tool. It
doesn't mean that it's a waste of time to try designing a good
language. Expert hackers 
can tell a good language when they see
one, and they'll use it. Expert hackers are a tiny minority,
admittedly, but that tiny minority write all the good software,
and their influence is such that the rest of the programmers will
tend to use whatever language they use. Often, indeed, it is not
merely influence but command: often the expert hackers are the very
people who, as their bosses or faculty advisors, tell the other
programmers what language to use.The opinion of expert hackers is not the only force that determines
the relative popularity of programming languages — legacy software
(Cobol) and hype (Ada, Java) also play a role — but I think it is
the most powerful force over the long term. Given an initial critical
mass and enough time, a programming language probably becomes about
as popular as it deserves to be. And popularity further separates
good languages from bad ones, because feedback from real live users
always leads to improvements. Look at how much any popular language
has changed during its life. Perl and Fortran are extreme cases,
but even Lisp has changed a lot. Lisp 1.5 didn't have macros, for
example; these evolved later, after hackers at MIT had spent a
couple years using Lisp to write real programs. [1]So whether or not a language has to be good to be popular, I think
a language has to be popular to be good. And it has to stay popular
to stay good. The state of the art in programming languages doesn't
stand still. And yet the Lisps we have today are still pretty much
what they had at MIT in the mid-1980s, because that's the last time
Lisp had a sufficiently large and demanding user base.Of course, hackers have to know about a language before they can
use it. How are they to hear? From other hackers. But there has to
be some initial group of hackers using the language for others even
to hear about it. I wonder how large this group has to be; how many
users make a critical mass? Off the top of my head, I'd say twenty.
If a language had twenty separate users, meaning twenty users who
decided on their own to use it, I'd consider it to be real.Getting there can't be easy. I would not be surprised if it is
harder to get from zero to twenty than from twenty to a thousand.
The best way to get those initial twenty users is probably to use
a trojan horse: to give people an application they want, which
happens to be written in the new language.2 External FactorsLet's start by acknowledging one external factor that does affect
the popularity of a programming language. To become popular, a
programming language has to be the scripting language of a popular
system. Fortran and Cobol were the scripting languages of early
IBM mainframes. C was the scripting language of Unix, and so, later,
was Perl. Tcl is the scripting language of Tk. Java and Javascript
are intended to be the scripting languages of web browsers.Lisp is not a massively popular language because it is not the
scripting language of a massively popular system. What popularity
it retains dates back to the 1960s and 1970s, when it was the
scripting language of MIT. A lot of the great programmers of the
day were associated with MIT at some point. And in the early 1970s,
before C, MIT's dialect of Lisp, called MacLisp, was one of the
only programming languages a serious hacker would want to use.Today Lisp is the scripting language of two moderately popular
systems, Emacs and Autocad, and for that reason I suspect that most
of the Lisp programming done today is done in Emacs Lisp or AutoLisp.Programming languages don't exist in isolation. To hack is a
transitive verb — hackers are usually hacking something — and in
practice languages are judged relative to whatever they're used to
hack. So if you want to design a popular language, you either have
to supply more than a language, or you have to design your language
to replace the scripting language of some existing system.Common Lisp is unpopular partly because it's an orphan. It did
originally come with a system to hack: the Lisp Machine. But Lisp
Machines (along with parallel computers) were steamrollered by the
increasing power of general purpose processors in the 1980s. Common
Lisp might have remained popular if it had been a good scripting
language for Unix. It is, alas, an atrociously bad one.One way to describe this situation is to say that a language isn't
judged on its own merits. Another view is that a programming language
really isn't a programming language unless it's also the scripting
language of something. This only seems unfair if it comes as a
surprise. I think it's no more unfair than expecting a programming
language to have, say, an implementation. It's just part of what
a programming language is.A programming language does need a good implementation, of course,
and this must be free. Companies will pay for software, but individual
hackers won't, and it's the hackers you need to attract.A language also needs to have a book about it. The book should be
thin, well-written, and full of good examples. K&R is the ideal
here. At the moment I'd almost say that a language has to have a
book published by O'Reilly. That's becoming the test of mattering
to hackers.There should be online documentation as well. In fact, the book
can start as online documentation. But I don't think that physical
books are outmoded yet. Their format is convenient, and the de
facto censorship imposed by publishers is a useful if imperfect
filter. Bookstores are one of the most important places for learning
about new languages.3 BrevityGiven that you can supply the three things any language needs — a
free implementation, a book, and something to hack — how do you
make a language that hackers will like?One thing hackers like is brevity. Hackers are lazy, in the same
way that mathematicians and modernist architects are lazy: they
hate anything extraneous. It would not be far from the truth to
say that a hacker about to write a program decides what language
to use, at least subconsciously, based on the total number of
characters he'll have to type. If this isn't precisely how hackers
think, a language designer would do well to act as if it were.It is a mistake to try to baby the user with long-winded expressions
that are meant to resemble English. Cobol is notorious for this
flaw. A hacker would consider being asked to writeadd x to y giving zinstead ofz = x+yas something between an insult to his intelligence and a sin against
God.It has sometimes been said that Lisp should use first and rest
instead of car and cdr, because it would make programs easier to
read. Maybe for the first couple hours. But a hacker can learn
quickly enough that car means the first element of a list and cdr
means the rest. Using first and rest means 50% more typing. And
they are also different lengths, meaning that the arguments won't
line up when they're called, as car and cdr often are, in successive
lines. I've found that it matters a lot how code lines up on the
page. I can barely read Lisp code when it is set in a variable-width
font, and friends say this is true for other languages too.Brevity is one place where strongly typed languages lose. All other
things being equal, no one wants to begin a program with a bunch
of declarations. Anything that can be implicit, should be.The individual tokens should be short as well. Perl and Common Lisp
occupy opposite poles on this question. Perl programs can be almost
cryptically dense, while the names of built-in Common Lisp operators
are comically long. The designers of Common Lisp probably expected
users to have text editors that would type these long names for
them. But the cost of a long name is not just the cost of typing
it. There is also the cost of reading it, and the cost of the space
it takes up on your screen.4 HackabilityThere is one thing more important than brevity to a hacker: being
able to do what you want. In the history of programming languages
a surprising amount of effort has gone into preventing programmers
from doing things considered to be improper. This is a dangerously
presumptuous plan. How can the language designer know what the
programmer is going to need to do? I think language designers would
do better to consider their target user to be a genius who will
need to do things they never anticipated, rather than a bumbler
who needs to be protected from himself. The bumbler will shoot
himself in the foot anyway. You may save him from referring to
variables in another package, but you can't save him from writing
a badly designed program to solve the wrong problem, and taking
forever to do it.Good programmers often want to do dangerous and unsavory things.
By unsavory I mean things that go behind whatever semantic facade
the language is trying to present: getting hold of the internal
representation of some high-level abstraction, for example. Hackers
like to hack, and hacking means getting inside things and second
guessing the original designer.Let yourself be second guessed. When you make any tool, people use
it in ways you didn't intend, and this is especially true of a
highly articulated tool like a programming language. Many a hacker
will want to tweak your semantic model in a way that you never
imagined. I say, let them; give the programmer access to as much
internal stuff as you can without endangering runtime systems like
the garbage collector.In Common Lisp I have often wanted to iterate through the fields
of a struct — to comb out references to a deleted object, for example,
or find fields that are uninitialized. I know the structs are just
vectors underneath. And yet I can't write a general purpose function
that I can call on any struct. I can only access the fields by
name, because that's what a struct is supposed to mean.A hacker may only want to subvert the intended model of things once
or twice in a big program. But what a difference it makes to be
able to. And it may be more than a question of just solving a
problem. There is a kind of pleasure here too. Hackers share the
surgeon's secret pleasure in poking about in gross innards, the
teenager's secret pleasure in popping zits. [2] For boys, at least,
certain kinds of horrors are fascinating. Maxim magazine publishes
an annual volume of photographs, containing a mix of pin-ups and
grisly accidents. They know their audience.Historically, Lisp has been good at letting hackers have their way.
The political correctness of Common Lisp is an aberration. Early
Lisps let you get your hands on everything. A good deal of that
spirit is, fortunately, preserved in macros. What a wonderful thing,
to be able to make arbitrary transformations on the source code.Classic macros are a real hacker's tool — simple, powerful, and
dangerous. It's so easy to understand what they do: you call a
function on the macro's arguments, and whatever it returns gets
inserted in place of the macro call. Hygienic macros embody the
opposite principle. They try to protect you from understanding what
they're doing. I have never heard hygienic macros explained in one
sentence. And they are a classic example of the dangers of deciding
what programmers are allowed to want. Hygienic macros are intended
to protect me from variable capture, among other things, but variable
capture is exactly what I want in some macros.A really good language should be both clean and dirty: cleanly
designed, with a small core of well understood and highly orthogonal
operators, but dirty in the sense that it lets hackers have their
way with it. C is like this. So were the early Lisps. A real hacker's
language will always have a slightly raffish character.A good programming language should have features that make the kind
of people who use the phrase ""software engineering"" shake their
heads disapprovingly. At the other end of the continuum are languages
like Ada and Pascal, models of propriety that are good for teaching
and not much else.5 Throwaway ProgramsTo be attractive to hackers, a language must be good for writing
the kinds of programs they want to write. And that means, perhaps
surprisingly, that it has to be good for writing throwaway programs.A throwaway program is a program you write quickly for some limited
task: a program to automate some system administration task, or
generate test data for a simulation, or convert data from one format
to another. The surprising thing about throwaway programs is that,
like the ""temporary"" buildings built at so many American universities
during World War II, they often don't get thrown away. Many evolve
into real programs, with real features and real users.I have a hunch that the best big programs begin life this way,
rather than being designed big from the start, like the Hoover Dam.
It's terrifying to build something big from scratch. When people
take on a project that's too big, they become overwhelmed. The
project either gets bogged down, or the result is sterile and
wooden: a shopping mall rather than a real downtown, Brasilia rather
than Rome, Ada rather than C.Another way to get a big program is to start with a throwaway
program and keep improving it. This approach is less daunting, and
the design of the program benefits from evolution. I think, if one
looked, that this would turn out to be the way most big programs
were developed. And those that did evolve this way are probably
still written in whatever language they were first written in,
because it's rare for a program to be ported, except for political
reasons. And so, paradoxically, if you want to make a language that
is used for big systems, you have to make it good for writing
throwaway programs, because that's where big systems come from.Perl is a striking example of this idea. It was not only designed
for writing throwaway programs, but was pretty much a throwaway
program itself. Perl began life as a collection of utilities for
generating reports, and only evolved into a programming language
as the throwaway programs people wrote in it grew larger. It was
not until Perl 5 (if then) that the language was suitable for
writing serious programs, and yet it was already massively popular.What makes a language good for throwaway programs? To start with,
it must be readily available. A throwaway program is something that
you expect to write in an hour. So the language probably must
already be installed on the computer you're using. It can't be
something you have to install before you use it. It has to be there.
C was there because it came with the operating system. Perl was
there because it was originally a tool for system administrators,
and yours had already installed it.Being available means more than being installed, though. An
interactive language, with a command-line interface, is more
available than one that you have to compile and run separately. A
popular programming language should be interactive, and start up
fast.Another thing you want in a throwaway program is brevity. Brevity
is always attractive to hackers, and never more so than in a program
they expect to turn out in an hour.6 LibrariesOf course the ultimate in brevity is to have the program already
written for you, and merely to call it. And this brings us to what
I think will be an increasingly important feature of programming
languages: library functions. Perl wins because it has large
libraries for manipulating strings. This class of library functions
are especially important for throwaway programs, which are often
originally written for converting or extracting data.  Many Perl
programs probably begin as just a couple library calls stuck
together.I think a lot of the advances that happen in programming languages
in the next fifty years will have to do with library functions. I
think future programming languages will have libraries that are as
carefully designed as the core language. Programming language design
will not be about whether to make your language strongly or weakly
typed, or object oriented, or functional, or whatever, but about
how to design great libraries. The kind of language designers who
like to think about how to design type systems may shudder at this.
It's almost like writing applications! Too bad. Languages are for
programmers, and libraries are what programmers need.It's hard to design good libraries. It's not simply a matter of
writing a lot of code. Once the libraries get too big, it can
sometimes take longer to find the function you need than to write
the code yourself. Libraries need to be designed using a small set
of orthogonal operators, just like the core language. It ought to
be possible for the programmer to guess what library call will do
what he needs.Libraries are one place Common Lisp falls short. There are only
rudimentary libraries for manipulating strings, and almost none
for talking to the operating system. For historical reasons, Common
Lisp tries to pretend that the OS doesn't exist. And because you
can't talk to the OS, you're unlikely to be able to write a serious
program using only the built-in operators in Common Lisp. You have
to use some implementation-specific hacks as well, and in practice
these tend not to give you everything you want. Hackers would think
a lot more highly of Lisp if Common Lisp had powerful string
libraries and good OS support.7 SyntaxCould a language with Lisp's syntax, or more precisely, lack of
syntax, ever become popular? I don't know the answer to this
question. I do think that syntax is not the main reason Lisp isn't
currently popular. Common Lisp has worse problems than unfamiliar
syntax. I know several programmers who are comfortable with prefix
syntax and yet use Perl by default, because it has powerful string
libraries and can talk to the os.There are two possible problems with prefix notation: that it is
unfamiliar to programmers, and that it is not dense enough. The
conventional wisdom in the Lisp world is that the first problem is
the real one. I'm not so sure. Yes, prefix notation makes ordinary
programmers panic. But I don't think ordinary programmers' opinions
matter. Languages become popular or unpopular based on what expert
hackers think of them, and I think expert hackers might be able to
deal with prefix notation. Perl syntax can be pretty incomprehensible,
but that has not stood in the way of Perl's popularity. If anything
it may have helped foster a Perl cult.A more serious problem is the diffuseness of prefix notation. For
expert hackers, that really is a problem. No one wants to write
(aref a x y) when they could write a[x,y].In this particular case there is a way to finesse our way out of
the problem. If we treat data structures as if they were functions
on indexes, we could write (a x y) instead, which is even shorter
than the Perl form. Similar tricks may shorten other types of
expressions.We can get rid of (or make optional) a lot of parentheses by making
indentation significant. That's how programmers read code anyway:
when indentation says one thing and delimiters say another, we go
by the indentation. Treating indentation as significant would
eliminate this common source of bugs as well as making programs
shorter.Sometimes infix syntax is easier to read. This is especially true
for math expressions. I've used Lisp my whole programming life and
I still don't find prefix math expressions natural. And yet it is
convenient, especially when you're generating code, to have operators
that take any number of arguments. So if we do have infix syntax,
it should probably be implemented as some kind of read-macro.I don't think we should be religiously opposed to introducing syntax
into Lisp, as long as it translates in a well-understood way into
underlying s-expressions. There is already a good deal of syntax
in Lisp. It's not necessarily bad to introduce more, as long as no
one is forced to use it. In Common Lisp, some delimiters are reserved
for the language, suggesting that at least some of the designers
intended to have more syntax in the future.One of the most egregiously unlispy pieces of syntax in Common Lisp
occurs in format strings; format is a language in its own right,
and that language is not Lisp. If there were a plan for introducing
more syntax into Lisp, format specifiers might be able to be included
in it. It would be a good thing if macros could generate format
specifiers the way they generate any other kind of code.An eminent Lisp hacker told me that his copy of CLTL falls open to
the section format. Mine too. This probably indicates room for
improvement. It may also mean that programs do a lot of I/O.8 EfficiencyA good language, as everyone knows, should generate fast code. But
in practice I don't think fast code comes primarily from things
you do in the design of the language. As Knuth pointed out long
ago, speed only matters in certain critical bottlenecks.  And as
many programmers have observed since, one is very often mistaken
about where these bottlenecks are.So, in practice, the way to get fast code is to have a very good
profiler, rather than by, say, making the language strongly typed.
You don't need to know the type of every argument in every call in
the program. You do need to be able to declare the types of arguments
in the bottlenecks. And even more, you need to be able to find out
where the bottlenecks are.One complaint people have had with Lisp is that it's hard to tell
what's expensive. This might be true. It might also be inevitable,
if you want to have a very abstract language. And in any case I
think good profiling would go a long way toward fixing the problem:
you'd soon learn what was expensive.Part of the problem here is social. Language designers like to
write fast compilers. That's how they measure their skill. They
think of the profiler as an add-on, at best. But in practice a good
profiler may do more to improve the speed of actual programs written
in the language than a compiler that generates fast code. Here,
again, language designers are somewhat out of touch with their
users. They do a really good job of solving slightly the wrong
problem.It might be a good idea to have an active profiler — to push
performance data to the programmer instead of waiting for him to
come asking for it. For example, the editor could display bottlenecks
in red when the programmer edits the source code. Another approach
would be to somehow represent what's happening in running programs.
This would be an especially big win in server-based applications,
where you have lots of running programs to look at. An active
profiler could show graphically what's happening in memory as a
program's running, or even make sounds that tell what's happening.Sound is a good cue to problems. In one place I worked, we had a
big board of dials showing what was happening to our web servers.
The hands were moved by little servomotors that made a slight noise
when they turned. I couldn't see the board from my desk, but I
found that I could tell immediately, by the sound, when there was
a problem with a server.It might even be possible to write a profiler that would automatically
detect inefficient algorithms. I would not be surprised if certain
patterns of memory access turned out to be sure signs of bad
algorithms. If there were a little guy running around inside the
computer executing our programs, he would probably have as long
and plaintive a tale to tell about his job as a federal government
employee. I often have a feeling that I'm sending the processor on
a lot of wild goose chases, but I've never had a good way to look
at what it's doing.A number of Lisps now compile into byte code, which is then executed
by an interpreter. This is usually done to make the implementation
easier to port, but it could be a useful language feature. It might
be a good idea to make the byte code an official part of the
language, and to allow programmers to use inline byte code in
bottlenecks. Then such optimizations would be portable too.The nature of speed, as perceived by the end-user, may be changing.
With the rise of server-based applications, more and more programs
may turn out to be i/o-bound. It will be worth making i/o fast.
The language can help with straightforward measures like simple,
fast, formatted output functions, and also with deep structural
changes like caching and persistent objects.Users are interested in response time. But another kind of efficiency
will be increasingly important: the number of simultaneous users
you can support per processor. Many of the interesting applications
written in the near future will be server-based, and the number of
users per server is the critical question for anyone hosting such
applications. In the capital cost of a business offering a server-based
application, this is the divisor.For years, efficiency hasn't mattered much in most end-user
applications. Developers have been able to assume that each user
would have an increasingly powerful processor sitting on their
desk. And by Parkinson's Law, software has expanded to use the
resources available. That will change with server-based applications.
In that world, the hardware and software will be supplied together.
For companies that offer server-based applications, it will make
a very big difference to the bottom line how many users they can
support per server.In some applications, the processor will be the limiting factor,
and execution speed will be the most important thing to optimize.
But often memory will be the limit; the number of simultaneous
users will be determined by the amount of memory you need for each
user's data. The language can help here too. Good support for
threads will enable all the users to share a single heap. It may
also help to have persistent objects and/or language level support
for lazy loading.9 TimeThe last ingredient a popular language needs is time. No one wants
to write programs in a language that might go away, as so many
programming languages do. So most hackers will tend to wait until
a language has been around for a couple years before even considering
using it.Inventors of wonderful new things are often surprised to discover
this, but you need time to get any message through to people. A
friend of mine rarely does anything the first time someone asks
him. He knows that people sometimes ask for things that they turn
out not to want. To avoid wasting his time, he waits till the third
or fourth time he's asked to do something; by then, whoever's asking
him may be fairly annoyed, but at least they probably really do
want whatever they're asking for.Most people have learned to do a similar sort of filtering on new
things they hear about. They don't even start paying attention
until they've heard about something ten times. They're perfectly
justified: the majority of hot new whatevers do turn out to be a
waste of time, and eventually go away. By delaying learning VRML,
I avoided having to learn it at all.So anyone who invents something new has to expect to keep repeating
their message for years before people will start to get it. We
wrote what was, as far as I know, the first web-server based
application, and it took us years to get it through to people that
it didn't have to be downloaded. It wasn't that they were stupid.
They just had us tuned out.The good news is, simple repetition solves the problem. All you
have to do is keep telling your story, and eventually people will
start to hear. It's not when people notice you're there that they
pay attention; it's when they notice you're still there.It's just as well that it usually takes a while to gain momentum.
Most technologies evolve a good deal even after they're first
launched — programming languages especially. Nothing could be better,
for a new techology, than a few years of being used only by a small
number of early adopters. Early adopters are sophisticated and
demanding, and quickly flush out whatever flaws remain in your
technology. When you only have a few users you can be in close
contact with all of them. And early adopters are forgiving when
you improve your system, even if this causes some breakage.There are two ways new technology gets introduced: the organic
growth method, and the big bang method. The organic growth method
is exemplified by the classic seat-of-the-pants underfunded garage
startup. A couple guys, working in obscurity, develop some new
technology. They launch it with no marketing and initially have
only a few (fanatically devoted) users. They continue to improve
the technology, and meanwhile their user base grows by word of
mouth. Before they know it, they're big.The other approach, the big bang method, is exemplified by the
VC-backed, heavily marketed startup. They rush to develop a product,
launch it with great publicity, and immediately (they hope) have
a large user base.Generally, the garage guys envy the big bang guys. The big bang
guys are smooth and confident and respected by the VCs. They can
afford the best of everything, and the PR campaign surrounding the
launch has the side effect of making them celebrities. The organic
growth guys, sitting in their garage, feel poor and unloved. And
yet I think they are often mistaken to feel sorry for themselves.
Organic growth seems to yield better technology and richer founders
than the big bang method. If you look at the dominant technologies
today, you'll find that most of them grew organically.This pattern doesn't only apply to companies. You see it in sponsored
research too. Multics and Common Lisp were big-bang projects, and
Unix and MacLisp were organic growth projects.10 Redesign""The best writing is rewriting,"" wrote E. B. White.  Every good
writer knows this, and it's true for software too. The most important
part of design is redesign. Programming languages, especially,
don't get redesigned enough.To write good software you must simultaneously keep two opposing
ideas in your head. You need the young hacker's naive faith in
his abilities, and at the same time the veteran's skepticism. You
have to be able to think 
how hard can it be? with one half of
your brain while thinking 
it will never work with the other.The trick is to realize that there's no real contradiction here.
You want to be optimistic and skeptical about two different things.
You have to be optimistic about the possibility of solving the
problem, but skeptical about the value of whatever solution you've
got so far.People who do good work often think that whatever they're working
on is no good. Others see what they've done and are full of wonder,
but the creator is full of worry. This pattern is no coincidence:
it is the worry that made the work good.If you can keep hope and worry balanced, they will drive a project
forward the same way your two legs drive a bicycle forward. In the
first phase of the two-cycle innovation engine, you work furiously
on some problem, inspired by your confidence that you'll be able
to solve it. In the second phase, you look at what you've done in
the cold light of morning, and see all its flaws very clearly. But
as long as your critical spirit doesn't outweigh your hope, you'll
be able to look at your admittedly incomplete system, and think,
how hard can it be to get the rest of the way?, thereby continuing
the cycle.It's tricky to keep the two forces balanced. In young hackers,
optimism predominates. They produce something, are convinced it's
great, and never improve it. In old hackers, skepticism predominates,
and they won't even dare to take on ambitious projects.Anything you can do to keep the redesign cycle going is good. Prose
can be rewritten over and over until you're happy with it. But
software, as a rule, doesn't get redesigned enough. Prose has
readers, but software has users. If a writer rewrites an essay,
people who read the old version are unlikely to complain that their
thoughts have been broken by some newly introduced incompatibility.Users are a double-edged sword. They can help you improve your
language, but they can also deter you from improving it. So choose
your users carefully, and be slow to grow their number. Having
users is like optimization: the wise course is to delay it. Also,
as a general rule, you can at any given time get away with changing
more than you think. Introducing change is like pulling off a
bandage: the pain is a memory almost as soon as you feel it.Everyone knows that it's not a good idea to have a language designed
by a committee. Committees yield bad design. But I think the worst
danger of committees is that they interfere with redesign. It is
so much work to introduce changes that no one wants to bother.
Whatever a committee decides tends to stay that way, even if most
of the members don't like it.Even a committee of two gets in the way of redesign. This happens
particularly in the interfaces between pieces of software written
by two different people. To change the interface both have to agree
to change it at once. And so interfaces tend not to change at all,
which is a problem because they tend to be one of the most ad hoc
parts of any system.One solution here might be to design systems so that interfaces
are horizontal instead of vertical — so that modules are always
vertically stacked strata of abstraction. Then the interface will
tend to be owned by one of them. The lower of two levels will either
be a language in which the upper is written, in which case the
lower level will own the interface, or it will be a slave, in which
case the interface can be dictated by the upper level.11 LispWhat all this implies is that there is hope for a new Lisp.  There
is hope for any language that gives hackers what they want, including
Lisp. I think we may have made a mistake in thinking that hackers
are turned off by Lisp's strangeness. This comforting illusion may
have prevented us from seeing the real problem with Lisp, or at
least Common Lisp, which is that it sucks for doing what hackers
want to do. A hacker's language needs powerful libraries and
something to hack. Common Lisp has neither. A hacker's language is
terse and hackable. Common Lisp is not.The good news is, it's not Lisp that sucks, but Common Lisp. If we
can develop a new Lisp that is a real hacker's language, I think
hackers will use it. They will use whatever language does the job.
All we have to do is make sure this new Lisp does some important
job better than other languages.History offers some encouragement. Over time, successive new
programming languages have taken more and more features from Lisp.
There is no longer much left to copy before the language you've
made is Lisp. The latest hot language, Python, is a watered-down
Lisp with infix syntax and no macros. A new Lisp would be a natural
step in this progression.I sometimes think that it would be a good marketing trick to call
it an improved version of Python. That sounds hipper than Lisp. To
many people, Lisp is a slow AI language with a lot of parentheses.
Fritz Kunze's official biography carefully avoids mentioning the
L-word.  But my guess is that we shouldn't be afraid to call the
new Lisp Lisp. Lisp still has a lot of latent respect among the
very best hackers — the ones who took 6.001 and understood it, for
example. And those are the users you need to win.In ""How to Become a Hacker,"" Eric Raymond describes Lisp as something
like Latin or Greek — a language you should learn as an intellectual
exercise, even though you won't actually use it:

  Lisp is worth learning for the profound enlightenment experience
  you will have when you finally get it; that experience will make
  you a better programmer for the rest of your days, even if you
  never actually use Lisp itself a lot.

If I didn't know Lisp, reading this would set me asking questions.
A language that would make me a better programmer, if it means
anything at all, means a language that would be better for programming.
And that is in fact the implication of what Eric is saying.As long as that idea is still floating around, I think hackers will
be receptive enough to a new Lisp, even if it is called Lisp. But
this Lisp must be a hacker's language, like the classic Lisps of
the 1970s. It must be terse, simple, and hackable. And it must have
powerful libraries for doing what hackers want to do now.In the matter of libraries I think there is room to beat languages
like Perl and Python at their own game. A lot of the new applications
that will need to be written in the coming years will be 
server-based
applications. There's no reason a new Lisp shouldn't have string
libraries as good as Perl, and if this new Lisp also had powerful
libraries for server-based applications, it could be very popular.
Real hackers won't turn up their noses at a new tool that will let
them solve hard problems with a few library calls. Remember, hackers
are lazy.It could be an even bigger win to have core language support for
server-based applications. For example, explicit support for programs
with multiple users, or data ownership at the level of type tags.Server-based applications also give us the answer to the question
of what this new Lisp will be used to hack. It would not hurt to
make Lisp better as a scripting language for Unix. (It would be
hard to make it worse.) But I think there are areas where existing
languages would be easier to beat. I think it might be better to
follow the model of Tcl, and supply the Lisp together with a complete
system for supporting server-based applications. Lisp is a natural
fit for server-based applications. Lexical closures provide a way
to get the effect of subroutines when the ui is just a series of
web pages. S-expressions map nicely onto html, and macros are good
at generating it. There need to be better tools for writing
server-based applications, and there needs to be a new Lisp, and
the two would work very well together.12 The Dream LanguageBy way of summary, let's try describing the hacker's dream language.
The dream language is 
beautiful, clean, and terse. It has an
interactive toplevel that starts up fast. You can write programs
to solve common problems with very little code.  Nearly all the
code in any program you write is code that's specific to your
application. Everything else has been done for you.The syntax of the language is brief to a fault. You never have to
type an unnecessary character, or even to use the shift key much.Using big abstractions you can write the first version of a program
very quickly. Later, when you want to optimize, there's a really
good profiler that tells you where to focus your attention. You
can make inner loops blindingly fast, even writing inline byte code
if you need to.There are lots of good examples to learn from, and the language is
intuitive enough that you can learn how to use it from examples in
a couple minutes. You don't need to look in the manual much. The
manual is thin, and has few warnings and qualifications.The language has a small core, and powerful, highly orthogonal
libraries that are as carefully designed as the core language. The
libraries all work well together; everything in the language fits
together like the parts in a fine camera. Nothing is deprecated,
or retained for compatibility. The source code of all the libraries
is readily available. It's easy to talk to the operating system
and to applications written in other languages.The language is built in layers. The higher-level abstractions are
built in a very transparent way out of lower-level abstractions,
which you can get hold of if you want.Nothing is hidden from you that doesn't absolutely have to be. The
language offers abstractions only as a way of saving you work,
rather than as a way of telling you what to do. In fact, the language
encourages you to be an equal participant in its design. You can
change everything about it, including even its syntax, and anything
you write has, as much as possible, the same status as what comes
predefined.Notes[1]  Macros very close to the modern idea were proposed by Timothy
Hart in 1964, two years after Lisp 1.5 was released. What was
missing, initially, were ways to avoid variable capture and multiple
evaluation; Hart's examples are subject to both.[2]  In When the Air Hits Your Brain, neurosurgeon Frank Vertosick
recounts a conversation in which his chief resident, Gary, talks
about the difference between surgeons and internists (""fleas""):

  Gary and I ordered a large pizza and found an open booth. The
  chief lit a cigarette. ""Look at those goddamn fleas, jabbering
  about some disease they'll see once in their lifetimes. That's
  the trouble with fleas, they only like the bizarre stuff. They
  hate their bread and butter cases. That's the difference between
  us and the fucking fleas. See, we love big juicy lumbar disc
  herniations, but they hate hypertension....""

It's hard to think of a lumbar disc herniation as juicy (except
literally). And yet I think I know what they mean. I've often had
a juicy bug to track down. Someone who's not a programmer would
find it hard to imagine that there could be pleasure in a bug.
Surely it's better if everything just works. In one way, it is.
And yet there is undeniably a grim satisfaction in hunting down
certain sorts of bugs.Postscript VersionArcFive Questions about Language DesignHow to Become a HackerJapanese Translation","programming advice
",human,"human
","human
"
7,7,"
There is a kind of mania for object-oriented programming at the moment, but

some of the smartest programmers I know are some of the least excited about it.My own feeling is that object-oriented
programming is a useful technique in some
cases, but it isn't something that has to pervade every program you
write.  You should be able to define new types,
but you shouldn't have to express every program as the
definition of new types.I think there are five reasons people like object-oriented   
programming, and three and a half of them are bad:
 Object-oriented programming is exciting   
if you have a statically-typed language without 
lexical closures or macros.  To some degree, it offers a way around these
limitations.  (See Greenspun's Tenth Rule.) Object-oriented programming is popular in big companies,
because it suits the way they write software.  At big companies,
software tends to be written by large (and frequently changing)  
teams of
mediocre programmers.  Object-oriented programming imposes a
discipline on these programmers that prevents any one of them
from doing too much damage.  The price is that the resulting
code is bloated with protocols and full of duplication.  
This is not too high a price for big companies, because their
software is probably going to be bloated and full of 
duplication anyway. Object-oriented
programming generates a lot of what looks like work.
Back in the days of fanfold, there was a type of programmer who
would only put five or ten lines of code on a page, preceded
by twenty lines of elaborately formatted comments. 
Object-oriented programming is like crack for these people: it lets
you incorporate all this scaffolding right into your source
code.  Something that a Lisp hacker might handle by pushing
a symbol onto a list becomes a whole file of classes and
methods.  So it is a good tool if you want to convince yourself,
or someone else, that you are doing a lot of work. If a language is itself an object-oriented program, it can
be extended by users.  Well, maybe.  Or maybe you can do
even better by offering the sub-concepts
of object-oriented programming a la carte.  Overloading, 
for example, is not intrinsically tied to classes.  We'll see. Object-oriented abstractions map neatly onto the domains
of  certain specific kinds of programs, like simulations and CAD
systems.                                        


I personally have never needed object-oriented abstractions.
Common Lisp has an enormously powerful object system and I've
never used it once.  I've done a lot of things (e.g. making         
hash tables full of closures) that would have required 
object-oriented techniques to do in wimpier languages, but
I have never had to use CLOS.Maybe I'm just stupid, or have worked on some limited subset
of applications.  There is a danger in designing a language
based on one's own experience of programming.  But it seems
more dangerous to put stuff in that you've never needed 
because it's thought to be a good idea.Rees Re: OOSpanish Translation","programming advice
",human,"human
","human
"
8,8,"May 2003If Lisp is so great, why don't more people use it?  I was    
asked this question by a student in the audience at a 
talk I gave recently.  Not for the first time, either.In languages, as in so many things, there's not much     
correlation between popularity and quality.  Why does   
John Grisham (King of Torts sales rank, 44) outsell
Jane Austen (Pride and Prejudice sales rank, 6191)?
Would even Grisham claim that it's because he's a better
writer?Here's the first sentence of Pride and Prejudice:

It is a truth universally acknowledged, that a single man 
in possession of a good fortune must be in want of a
wife.

""It is a truth universally acknowledged?""  Long words for
the first sentence of a love story.Like Jane Austen, Lisp looks hard.  Its syntax, or lack
of syntax, makes it look completely unlike 
the languages
most people are used to.  Before I learned Lisp, I was afraid
of it too.  I recently came across a notebook from 1983
in which I'd written:

I suppose I should learn Lisp, but it seems so foreign.

Fortunately, I was 19 at the time and not too resistant to learning
new things.  I was so ignorant that learning
almost anything meant learning new things.People frightened by Lisp make up other reasons for not
using it.  The standard
excuse, back when C was the default language, was that Lisp
was too slow.  Now that Lisp dialects are among
the faster
languages available, that excuse has gone away.
Now the standard excuse is openly circular: that other languages
are more popular.(Beware of such reasoning.  It gets you Windows.)Popularity is always self-perpetuating, but it's especially
so in programming languages. More libraries
get written for popular languages, which makes them still
more popular.  Programs often have to work with existing programs,
and this is easier if they're written in the same language,
so languages spread from program to program like a virus.
And managers prefer popular languages, because they give them 
more leverage over developers, who can more easily be replaced.Indeed, if programming languages were all more or less equivalent,
there would be little justification for using any but the most
popular.  But they aren't all equivalent, not by a long
shot.  And that's why less popular languages, like Jane Austen's 
novels, continue to survive at all.  When everyone else is reading 
the latest John Grisham novel, there will always be a few people 
reading Jane Austen instead.Japanese TranslationRomanian TranslationSpanish Translation","programming advice
",human,"human
","human
"
9,9,"January 2003(This article was given as a talk at the 2003 Spam Conference.
It describes the work I've done to improve the performance of
the algorithm described in A Plan for Spam,
and what I plan to do in the future.)The first discovery I'd like to present here is an algorithm for
lazy evaluation of research papers.  Just
write whatever you want and don't cite any previous work, and
indignant readers will send you references to all the papers you
should have cited.   I discovered this algorithm
after ``A Plan for Spam'' [1] was on Slashdot.Spam filtering is a subset of text classification,
which is a well established field, but the first papers about
Bayesian
spam filtering per se seem to have been two
given at the same conference in 1998,
one by Pantel and Lin [2],
and another by a group from
Microsoft Research [3].When I heard about this work I was a bit surprised.  If
people had been onto Bayesian filtering four years ago,
why wasn't everyone using it?
When I read the papers I found out why.  Pantel and Lin's filter was the
more effective of the two, but it
only caught 92% of spam, with 1.16% false positives.When I tried writing a Bayesian spam filter,
it caught 99.5% of spam with less than .03% false
positives [4].
It's always alarming when two people
trying the same experiment get widely divergent results.
It's especially alarming here because those two sets of numbers
might yield opposite conclusions.
Different users have different requirements, but I think for
many people a filtering rate of 92% with 1.16% false positives means
that filtering is not an acceptable solution, whereas
99.5% with less than .03% false positives means that it is.So why did we get such different numbers?
I haven't tried to reproduce Pantel and Lin's results, but
from reading the paper I see five things that probably account
for the difference.One is simply that they trained their filter on very little
data: 160 spam and 466 nonspam mails.
Filter performance should still be climbing with data
sets that small.  So their numbers may not even be an accurate
measure of the performance of their algorithm, let alone of
Bayesian spam filtering in general.But I think the most important difference is probably
that they ignored message headers.  To anyone who has worked
on spam filters, this will seem a perverse decision.
And yet in the very first filters I tried writing, I ignored the
headers too.  Why?  Because I wanted to keep the problem neat.
I didn't know much about mail headers then, and they seemed to me
full of random stuff.  There is a lesson here for filter
writers: don't ignore data.  You'd think this lesson would
be too obvious to mention, but I've had to learn it several times.Third, Pantel and Lin stemmed the tokens, meaning they reduced e.g. both
``mailing'' and ``mailed'' to the root ``mail''.   They may
have felt they were forced to do this by the small size
of their corpus, but if so this is a kind of premature 
optimization.Fourth, they calculated probabilities differently.
They used all the tokens, whereas I only
use the 15 most significant.  If you use all the tokens
you'll tend to miss longer spams, the type where someone tells you their life
story up to the point where they got rich from some multilevel
marketing scheme.  And such an algorithm
would be easy for spammers to spoof: just add a big
chunk of random text to counterbalance the spam terms.Finally, they didn't bias against false positives.
I think
any spam filtering algorithm ought to have a convenient
knob you can twist to decrease the
false positive rate at the expense of the filtering rate.
I do this by counting the occurrences
of tokens in the nonspam corpus double.  
I don't think it's a good idea to treat spam filtering as
a straight text classification problem.  You can use
text classification techniques, but solutions can and should
reflect the fact that the text is email, and spam
in particular.  Email is not just text; it has structure.
Spam filtering is not just classification, because
false positives are so much worse than false negatives
that you should treat them as a different kind of error.
And the source of error is not just random variation, but
a live human spammer working actively to defeat your filter.TokensAnother project I heard about
after the Slashdot article was Bill Yerazunis' 
CRM114 [5].
This is the counterexample to the design principle I
just mentioned.  It's a straight text classifier,
but such a stunningly effective one that it manages to filter
spam almost perfectly without even knowing that's
what it's doing.Once I understood how CRM114 worked, it seemed
inevitable that I would eventually have to move from filtering based
on single words to an approach like this.  But first, I thought,
I'll see how far I can get with single words.  And the answer is,
surprisingly far.Mostly I've been working on smarter tokenization.  On
current spam, I've been able to achieve filtering rates that
approach CRM114's.  These techniques are mostly orthogonal to Bill's;
an optimal solution might incorporate both.``A Plan for Spam'' uses a very simple
definition of a token.  Letters, digits, dashes, apostrophes,
and dollar signs are constituent characters, and everything
else is a token separator.  I also ignored case.Now I have a more complicated definition of a token:

 Case is preserved. Exclamation points are constituent characters. Periods and commas are constituents if they occur
 between two digits.  This lets me get ip addresses
 and prices intact. A price range like $20-25 yields two tokens,
 $20 and $25. Tokens that occur within the
 To, From, Subject, and Return-Path lines, or within urls,
 get marked accordingly.  E.g. ``foo'' in the Subject line
 becomes ``Subject*foo''.  (The asterisk could
 be any character you don't allow as a constituent.)

Such measures increase the filter's vocabulary, which
makes it more discriminating.  For example, in the current
filter, ``free'' in the Subject line
has a spam probability of 98%, whereas the same token
in the body has a spam probability of only 65%.Here are some of the current probabilities [6]:
Subject*FREE      0.9999
free!!            0.9999
To*free           0.9998
Subject*free      0.9782
free!             0.9199
Free              0.9198
Url*free          0.9091
FREE              0.8747
From*free         0.7636
free              0.6546

In the Plan for Spam filter, all these tokens would have had the
same probability, .7602.  That filter recognized about 23,000
tokens.  The current one recognizes about 187,000.The disadvantage of having a larger universe of tokens
is that there is more
chance of misses.
Spreading your corpus out over more tokens
has the same effect as making it smaller.
If you consider exclamation points as
constituents, for example, then you could end up
not having a spam probability for free with seven exclamation
points, even though you know that free with just two   
exclamation points has a probability of 99.99%.One solution to this is what I call degeneration.  If you
can't find an exact match for a token,
treat it as if it were a less specific
version.  I consider terminal exclamation
points, uppercase letters, and occurring in one of the
five marked contexts as making a token more specific.
For example, if I don't find a probability for
``Subject*free!'', I look for probabilities for
``Subject*free'', ``free!'', and ``free'', and take whichever one
is farthest from .5.Here are the alternatives [7]
considered if the filter sees ``FREE!!!'' in the
Subject line and doesn't have a probability for it.
Subject*Free!!!
Subject*free!!!
Subject*FREE!
Subject*Free!
Subject*free!
Subject*FREE
Subject*Free
Subject*free
FREE!!!
Free!!!
free!!!
FREE!
Free!
free!
FREE
Free
free              

If you do this, be sure to consider versions with initial
caps as well as all uppercase and all lowercase.  Spams
tend to have more sentences in imperative mood, and in
those the first word is a verb.  So verbs with initial caps
have higher spam probabilities than they would in all 
lowercase.  In my filter, the spam probability of ``Act''
is 98% and for ``act'' only 62%.If you increase your filter's vocabulary, you can end up
counting the same word multiple times, according to your old
definition of ``same''.
Logically, they're not the
same token anymore.  But if this still bothers you, let
me add from experience that the words you seem to be
counting multiple times tend to be exactly the ones you'd
want to.Another effect of a larger vocabulary is that when you
look at an incoming mail you find more interesting tokens,
meaning those with probabilities far from .5.  I use the
15 most interesting to decide if mail is spam.
But you can run into a problem when you use a fixed number
like this.  If you find a lot of maximally interesting tokens,
the result can end up being decided by whatever random factor
determines the ordering of equally interesting tokens.
One way to deal with this is to treat some
as more interesting than others.For example, the
token ``dalco'' occurs 3 times in my spam corpus and never
in my legitimate corpus.  The token ``Url*optmails''
(meaning ``optmails'' within a url) occurs 1223 times.
And yet, as I used to calculate probabilities for tokens,
both would have the same spam probability, the threshold of .99.That doesn't feel right.  There are theoretical
arguments for giving these two tokens substantially different
probabilities (Pantel and Lin do), but I haven't tried that yet.
It does seem at least that if we find more than 15 tokens
that only occur in one corpus or the other, we ought to
give priority to the ones that occur a lot.  So now
there are two threshold values.  For tokens that occur only
in the spam corpus, the probability is .9999 if they
occur more than 10 times and .9998 otherwise.  Ditto
at the other end of the scale for tokens found
only in the legitimate corpus.I may later scale token probabilities substantially,
but this tiny amount of scaling at least ensures that 
tokens get sorted the right way.Another possibility would be to consider not
just 15 tokens, but all the tokens over a certain
threshold of interestingness.  Steven Hauser does this
in his statistical spam filter [8].
If you use a threshold, make it very high, or
spammers could spoof you by packing messages with
more innocent words.Finally, what should one do
about html?  I've tried the whole spectrum of options, from
ignoring it to parsing it all.  Ignoring html is a bad idea,
because it's full of useful spam signs.  But if you parse 
it all, your filter might degenerate into a mere html   
recognizer.  The most effective approach
seems to be the middle course, to notice some tokens but not
others.  I look at a, img, and font tags, and ignore the
rest.  Links and images you should certainly look at, because
they contain urls.I could probably be smarter about dealing with html, but I
don't think it's worth putting a lot of time into this.
Spams full of html are easy to filter.  The smarter
spammers already avoid it.  So
performance in the future should not depend much on how
you deal with html.PerformanceBetween December 10 2002 and January 10 2003 I got about
1750 spams.  
Of these, 4 got through.  That's a filtering
rate of about 99.75%.Two of the four spams I missed got through because they
happened to use words that occur often in my legitimate
email.The third was one of those that exploit
an insecure cgi script to send mail to third parties.
They're hard to filter based just
on the content because the headers are innocent and   
they're careful about the words they use.  Even so I can
usually catch them.  This one squeaked by with a
probability of .88, just under the threshold of .9.Of course, looking at multiple token sequences
would catch it easily.  ``Below is the result of
your feedback form'' is an instant giveaway.The fourth spam was what I call
a spam-of-the-future, because this is what I expect spam to
evolve into: some completely neutral
text followed by a url.  In this case it was was from
someone saying they had finally finished their homepage
and would I go look at it.  (The page was of course an    
ad for a porn site.)If the spammers are careful about the headers and use a
fresh url, there is nothing in spam-of-the-future for filters
to notice.  We can of course counter by sending a
crawler to look at the page.  But that might not be necessary.
The response rate for spam-of-the-future must
be low, or everyone would be doing it.
If it's low enough,
it won't pay for spammers to send it, and we won't 
have to work too hard on filtering it.Now for the really shocking news: during that same one-month
period I got three false positives.In a way it's
a relief to get some false positives.  When I wrote ``A Plan
for Spam'' I hadn't had any, and I didn't know what they'd
be like.  Now that I've had a few, I'm relieved to find
they're not as bad as I feared.
False positives yielded by statistical
filters turn out to be mails that sound a lot like spam, and
these tend to be the ones you would least mind missing [9].Two of the false positives were newsletters
from companies I've bought things from.  I never
asked to receive them, so arguably they
were spams, but I count them as false positives because
I hadn't been deleting them as spams before.  The reason
the filters caught them was that both companies in   
January switched to commercial email senders
instead of sending the mails from their own servers,  
and both the headers and the bodies became much spammier.The third false positive was a bad one, though.  It was 
from someone in Egypt and written in all uppercase.  This was
a direct result of making tokens case sensitive; the Plan
for Spam filter wouldn't have caught it.It's hard to say what the overall false positive rate is,
because we're up in the noise, statistically.
Anyone who has worked on filters (at least, effective filters) will
be aware of this problem.
With some emails it's
hard to say whether they're spam or not, and these are
the ones you end up looking at when you get filters       
really tight.  For example, so far the filter has
caught two emails that were sent to my address because
of a typo, and one sent to me in the belief that I was 
someone else.  Arguably, these are neither my spam
nor my nonspam mail.Another false positive was from a vice president at Virtumundo.
I wrote to them pretending to be a customer,
and since the reply came back through Virtumundo's 
mail servers it had the most incriminating
headers imaginable.  Arguably this isn't a real false
positive either, but a sort of Heisenberg uncertainty
effect: I only got it because I was writing about spam  
filtering.Not counting these, I've had a total of five false positives
so far, out of about 7740 legitimate emails, a rate of .06%.
The other two were a notice that something I bought
was back-ordered, and a party reminder from Evite.I don't think this number can be trusted, partly
because the sample is so small, and partly because
I think I can fix the filter not to catch
some of these.False positives seem to me a different kind of error from
false negatives.
Filtering rate is a measure of performance.  False
positives I consider more like bugs.  I approach improving the
filtering rate as optimization, and decreasing false
positives as debugging.So these five false positives are my bug list.  For example, 
the mail from Egypt got nailed because the uppercase text
made it look to the filter like a Nigerian spam.
This really is kind of a bug.  As with
html, the email being all uppercase is really conceptually one
feature, not one for each word.  I need to handle case in a
more sophisticated way.So what to make of this .06%?  Not much, I think.  You could
treat it as an upper bound, bearing in mind the small sample size.
But at this stage it is more a measure of the bugs
in my implementation than some intrinsic false positive rate
of Bayesian filtering.FutureWhat next?  Filtering is an optimization problem,
and the key to optimization is profiling.  Don't
try to guess where your code is slow, because you'll
guess wrong.  Look at where your code is slow,
and fix that.  In filtering, this translates to:   
look at the spams you miss, and figure out what you
could have done to catch them.For example, spammers are now working aggressively to   
evade filters, and one of the things they're doing is
breaking up and misspelling words to prevent filters from
recognizing them.  But working on this is not my first
priority, because I still have no trouble catching these
spams [10].There are two kinds of spams I currently do
have trouble with.
One is the type that pretends to be an email from 
a woman inviting you to go chat with her or see her profile on a dating
site.  These get through because they're the one type of
sales pitch you can make without using sales talk.  They use
the same vocabulary as ordinary email.The other kind of spams I have trouble filtering are those
from companies in e.g. Bulgaria offering contract programming 
services.   These get through because I'm a programmer too, and
the spams are full of the same words as my real mail.I'll probably focus on the personal ad type first.  I think if
I look closer I'll be able to find statistical differences
between these and my real mail.  The style of writing is
certainly different, though it may take multiword filtering
to catch that.
Also, I notice they tend to repeat the url,
and someone including a url in a legitimate mail wouldn't do that [11].The outsourcing type are going to be hard to catch.  Even if 
you sent a crawler to the site, you wouldn't find a smoking
statistical gun.
Maybe the only answer is a central list of
domains advertised in spams [12].  But there can't be that
many of this type of mail.  If the only
spams left were unsolicited offers of contract programming
services from Bulgaria, we could all probably move on to
working on something else.Will statistical filtering actually get us to that point?
I don't know.  Right now, for me personally, spam is
not a problem.  But spammers haven't yet made a serious
effort to spoof statistical filters.  What will happen when they do?I'm not optimistic about filters that work at the
network level [13].
When there is a static obstacle worth getting past, spammers
are pretty efficient at getting past it.  There
is already a company called Assurance Systems that will
run your mail through Spamassassin and tell you whether 
it will get filtered out.Network-level filters won't be completely useless.
They may be enough to kill all the ""opt-in""
spam, meaning spam from companies like Virtumundo and
Equalamail who claim that they're really running opt-in lists.
You can filter those based just on the headers, no
matter what they say in the body.  But anyone willing to
falsify headers or use open relays, presumably including
most porn spammers, should be able to get some message past
network-level filters if they want to.  (By no means the
message they'd like to send though, which is something.)The kind of filters I'm optimistic about are ones that
calculate probabilities based on each individual user's mail.
These can be much more effective, not only in
avoiding false positives, but in filtering too: for example,
finding the recipient's email address base-64 encoded anywhere in
a message is a very good spam indicator.But the real advantage of individual filters is that they'll all be
different.  If everyone's filters have different probabilities,
it will make the spammers' optimization loop, what programmers
would call their edit-compile-test cycle, appallingly slow.  
Instead of just tweaking a spam till it gets through a copy of
some filter they have on their desktop, they'll have to do a
test mailing for each tweak.  It would be like programming in
a language without an interactive toplevel, 
and I wouldn't wish that
on anyone.Notes[1]
Paul Graham.  ``A Plan for Spam.'' August 2002.
http://paulgraham.com/spam.html.Probabilities in this algorithm are
calculated using a degenerate case of Bayes' Rule.  There are
two simplifying assumptions: that the probabilities
of features (i.e. words) are independent, and that we know
nothing about the prior probability of an email being
spam.The first assumption is widespread in text classification.
Algorithms that use it are called ``naive Bayesian.''The second assumption I made because the proportion of spam in
my incoming mail fluctuated so much from day to day (indeed,
from hour to hour) that the overall prior ratio seemed
worthless as a predictor.  If you assume that P(spam) and
P(nonspam) are both .5, they cancel out and you can
remove them from the formula.If you were doing Bayesian filtering in a situation where  
the ratio of spam to nonspam was consistently very high or
(especially) very low, you could probably improve filter
performance by incorporating prior probabilities.  To do
this right you'd have to track ratios by time of day, because
spam and legitimate mail volume both have distinct daily
patterns.[2]
Patrick Pantel and Dekang Lin. ``SpamCop-- A Spam
Classification & Organization Program.''  Proceedings of AAAI-98
Workshop on Learning for Text Categorization.[3]
Mehran Sahami, Susan Dumais, David Heckerman and Eric Horvitz.
``A Bayesian Approach to Filtering Junk E-Mail.'' Proceedings of AAAI-98
Workshop on Learning for Text Categorization.[4] At the time I had zero false positives out of about 4,000 
legitimate emails.  If the next legitimate email was
a false positive, this would give us .03%.  These false positive
rates are untrustworthy, as I explain later. I quote
a number here only to emphasize that whatever the false positive rate
is, it is less than 1.16%.
[5] Bill Yerazunis. ``Sparse Binary Polynomial Hash Message
Filtering and The CRM114 Discriminator.''  Proceedings of 2003
Spam Conference.[6] In ``A Plan for Spam'' I used thresholds of .99 and .01.
It seems justifiable to use thresholds proportionate to the
size of the corpora.  Since I now have on the order of 10,000 of each
type of mail, I use .9999 and .0001.[7] There is a flaw here I should probably fix.  Currently,
when ``Subject*foo'' degenerates to just ``foo'', what that means is
you're getting the stats for occurrences of ``foo'' in
the body or header lines other than those I mark.
What I should do is keep track of statistics for ``foo''
overall as well as specific versions, and degenerate from
``Subject*foo'' not to ``foo'' but to ``Anywhere*foo''.  Ditto for
case: I should degenerate from uppercase to any-case, not
lowercase.It would probably be a win to do this with prices
too, e.g. to degenerate from ``$129.99'' to ``$--9.99'', ``$--.99'',
and ``$--''.You could also degenerate from words to their stems,
but this would probably only improve filtering rates early on 
when you had small corpora.[8] Steven Hauser.  ``Statistical Spam Filter Works for Me.''
http://www.sofbot.com.[9] False positives are not all equal, and we should remember
this when comparing techniques for stopping spam.
Whereas many of the false positives caused by filters
will be near-spams that you wouldn't mind missing,
false positives caused by blacklists, for example, will be just
mail from people who chose the wrong ISP.  In both
cases you catch mail that's near spam, but for blacklists nearness
is physical, and for filters it's textual.
[10] If spammers get good enough at obscuring tokens   
for this to be a problem, we can respond by simply removing
whitespace, periods, commas, etc.  and using a dictionary to
pick the words out of the resulting sequence.
And of course finding words this way that weren't visible in
the original text would in itself be evidence of spam.Picking out the words won't be trivial.  It will require 
more than just reconstructing word boundaries; spammers
both add (``xHot nPorn cSite'') and omit (``P#rn'') letters.
Vision research may be useful here, since human vision is
the limit that such tricks will approach.[11] 
In general, spams are more repetitive than regular email.   
They want to pound that message home.  I currently don't
allow duplicates in the top 15 tokens, because
you could get a false positive if the sender happens to use
some bad word multiple times. (In my current filter, ``dick'' has
a spam probabilty of .9999, but it's also a name.)
It seems we should at least notice duplication though,
so I may try allowing up to two of each token, as Brian Burton does in
SpamProbe.[12]  This is what approaches like Brightmail's will
degenerate into once spammers are pushed into using mad-lib
techniques to generate everything else in the message.[13]
It's sometimes argued that we should be working on filtering
at the network level, because it is more efficient.  What people
usually mean when they say this is: we currently filter at the
network level, and we don't want to start over from scratch.
But you can't dictate the problem to fit your solution.Historically, scarce-resource arguments have been the losing
side in debates about software design.
People only tend to use them to justify choices
(inaction in particular) made for other reasons.Thanks to Sarah Harlin, Trevor Blackwell, and
Dan Giffin for reading drafts of this paper, and to Dan again
for most of the infrastructure that this filter runs on.Related:A Plan for SpamPlan for Spam FAQ2003 Spam Conference ProceedingsJapanese TranslationChinese TranslationTest of These Suggestions","programming advice
",human,"human
","human
"
10,10,"August 2021When people say that in their experience all programming languages
are basically equivalent, they're making a statement not about
languages but about the kind of programming they've done.99.5% of programming consists of gluing together calls to library
functions. All popular languages are equally good at this. So one
can easily spend one's whole career operating in the intersection
of popular programming languages.But the other .5% of programming is disproportionately interesting.
If you want to learn what it consists of, the weirdness of weird
languages is a good clue to follow.Weird languages aren't weird by accident. Not the good ones, at
least. The weirdness of the good ones usually implies the existence
of some form of programming that's not just the usual gluing together
of library calls.A concrete example: Lisp macros. Lisp macros seem weird even to
many Lisp programmers. They're not only not in the intersection of
popular languages, but by their nature would be hard to implement
properly in a language without turning it into a dialect of
Lisp. And macros are definitely evidence of techniques that go
beyond glue programming. For example, solving problems by first
writing a language for problems of that type, and then writing
your specific application in it. Nor is this all you can do with
macros; it's just one region in a space of program-manipulating
techniques that even now is far from fully explored.So if you want to expand your concept of what programming can be,
one way to do it is by learning weird languages. Pick a language
that most programmers consider weird but whose median user is smart,
and then focus on the differences between this language and the
intersection of popular languages. What can you say in this language
that would be impossibly inconvenient to say in others? In the
process of learning how to say things you couldn't previously say,
you'll probably be learning how to think things you couldn't
previously think.
Thanks to Trevor Blackwell, Patrick Collison, Daniel Gackle, Amjad
Masad, and Robert Morris for reading drafts of this.
Japanese Translation","programming advice
",human,"human
","human
"
11,11,"May 2003(This essay is derived from a guest lecture at Harvard, which incorporated
an earlier talk at Northeastern.)When I finished grad school in computer science I went
to art school to study painting.  A lot of people seemed surprised
that someone interested in computers would also be interested in painting.
They seemed to think that
hacking and painting were very different kinds of work-- that
hacking was cold, precise, and methodical, and that
painting was the frenzied expression of some primal urge.Both of these images are wrong.  Hacking and painting have a
lot in common.  In fact, of all the different types of people I've
known, hackers and painters are among the most alike.What hackers and painters have in common is that they're
both makers.  Along with composers, architects, and writers,
what hackers and painters are trying to do is make good things.
They're not doing research per se, though if in the course of
trying to make good things they discover some new technique,
so much the better.I've never liked the term ""computer science.""  The main
reason I don't like it is that there's no such thing.
Computer science is a
grab bag of tenuously related areas thrown together
by an accident of history, like Yugoslavia.
At one end you have people who are really mathematicians,
but call what they're doing computer science so they can get DARPA grants.
In the middle you have people working on
something like the natural history of computers-- studying the
behavior of algorithms for routing data through
networks, for example.  And then at the other extreme you
have the hackers, who are trying to
write interesting software, and for whom computers are just a
medium of expression, as concrete is for architects or
paint for painters.  It's as if
mathematicians, physicists, and architects all had to be in
the same department.Sometimes what the hackers do is called ""software engineering,""
but this term is just as misleading.
Good software designers are no more engineers than architects are.
The border between architecture and engineering is not sharply
defined, but it's there.
It falls between what and how: architects decide what to do,
and engineers figure out how to do it.What and how should not be kept too separate.  You're
asking for trouble if you try to decide what to do without
understanding how to do it.
But hacking can certainly be more than just deciding how to
implement some spec.  At its best, it's creating the spec-- though
it turns out the best way to do that is to implement it.Perhaps one day
""computer science"" will, like Yugoslavia, get broken up into its
component parts.  That might be a good thing.  Especially if it
meant independence for my native land, hacking.Bundling all these different types of work together in one
department may be convenient administratively, but it's confusing
intellectually. That's the other reason I don't like the name
""computer science.""  Arguably the people in the middle are doing
something like an experimental science.  But the people at either
end, the hackers and the mathematicians, are not actually doing science.The mathematicians don't seem bothered by this.  They happily
set to work proving theorems like the other mathematicians
over in the math department, and probably soon stop noticing
that the building they work in says ``computer science'' on the
outside.  But for the hackers this label is a problem.
If what they're doing is called science, it makes them feel they
ought to be acting scientific.
So instead of doing what they really want to do, which is   
to design beautiful software, hackers in universities and
research labs feel they ought to be writing research papers.In the best case, the papers are just a formality.  Hackers write
cool software, and then write a paper about it, and the paper
becomes a proxy for the achievement represented by the software.
But often this mismatch causes problems.  It's easy to
drift away from building beautiful things toward building ugly
things that make more suitable subjects for research papers.Unfortunately, beautiful things don't always make the
best subjects for papers.
Number one, research must be original-- and
as anyone who has written a PhD dissertation knows, the way to
be sure that you're exploring virgin territory is to stake
out a piece of ground that no one wants.  Number two, research must be
substantial-- and awkward systems yield meatier papers,
because you can write about the obstacles you have to overcome
in order to get things done.  Nothing yields meaty problems like
starting with the wrong assumptions.  Most of AI is an example
of this rule; if you assume that knowledge can be represented
as a list of predicate logic expressions whose arguments represent
abstract concepts, you'll have a lot of
papers to write about how to make this work.  As Ricky Ricardo
used to say, ""Lucy, you got a lot of explaining to do.""The way to create something beautiful is often to make subtle
tweaks to something that already exists, or to combine existing
ideas in a slightly new way.  This kind of work is hard to
convey in a research paper.So why do universities and research labs continue to judge
hackers by publications?
For the same reason that ""scholastic aptitude""
gets measured by simple-minded standardized tests, or
the productivity of programmers gets measured in lines of code.
These tests
are easy to apply, and there is nothing so tempting as an easy test
that kind of works.Measuring what hackers are actually trying to do, designing
beautiful software, would be much more difficult.  You need
a good sense of design to judge 
good design.  And
there is no correlation, except possibly
a negative 
one, between people's ability to recognize good
design and their confidence that they can.The only external test is time.  Over time, beautiful
things tend to thrive, and ugly
things tend to get discarded.  Unfortunately, the amounts of time
involved can be longer than human lifetimes.  Samuel Johnson
said it took a hundred years for a writer's reputation to
converge.  You have to wait for the writer's
influential friends to die, and then for all their followers
to die.I think hackers just have to resign themselves to having a large random
component in their reputations.  In this they are no different
from other makers.  In fact, they're lucky by comparison.   
The influence of fashion is not nearly so great in hacking as it
is in painting.There are worse things than having people misunderstand your
work.  A worse danger is that you
will yourself misunderstand your work.  Related fields are
where you go looking for ideas.  If you find yourself in the computer science
department, there is a natural temptation to believe, for example,
that hacking is the applied version of what theoretical computer
science is the theory of.   All
the time I was in graduate school I had an uncomfortable feeling
in the back of my mind that I ought to know more theory,
and that it was very remiss of me to have forgotten all that
stuff within three weeks of the final exam.Now I realize I was
mistaken.  Hackers need to understand the theory of computation
about as much as painters need to understand paint chemistry.
You need to know how to calculate time and
space complexity and about
Turing completeness.  You might also want to remember at
least the concept of a state machine, in case you have to write
a parser or a regular expression library.  Painters in fact   
have to remember a good deal more about paint chemistry than 
that.I've found that the best sources of ideas
are not the other fields that have the word ""computer"" in
their names, but the other fields inhabited by makers.
Painting has been a much richer source of ideas than the
theory of computation.For example, I was taught in college
that one ought to figure out a program
completely on paper
before even going near a computer.  I found that I did not
program this way.  I found that I liked to program
sitting in front of a computer, not a piece of paper.  Worse
still, instead of patiently writing out a complete program
and assuring myself it was correct, I tended to just spew
out code that was hopelessly broken, and gradually beat it into
shape.  Debugging, I was taught, was a kind of final pass where
you caught typos and oversights.  The way I worked, it
seemed like programming consisted of debugging.For a long time I felt bad about this, just as I once
felt bad that I didn't hold my pencil the way they taught me
to in elementary school.
If I had only looked over at
the other makers, the painters or the architects, I would
have realized that there was a name for what I was doing:
sketching.  As far as I can tell, the
way they taught me to program in college was all wrong.
You should figure out programs as you're writing them,
just as writers and painters and architects do.Realizing this has real implications for software design.
It means that a programming language should, above all, be
malleable.  A programming language is for 
thinking of
programs, not for expressing programs you've already thought
of.  It should be a pencil, not a pen.  Static typing would
be a fine idea if people actually did write programs the way
they taught me to in college.  But that's not how any of the  
hackers I know write programs.  We need a language that lets us
scribble and smudge and smear, not a language where you have
to sit with a teacup of types balanced on your knee and make
polite conversation with a strict old aunt of a compiler.While we're on the subject of static typing, identifying with
the makers will save us from another problem that afflicts
the sciences: math envy.  Everyone in the sciences
secretly believes that mathematicians are smarter than they are.  
I think mathematicians also believe this.  At any rate,
the result is that scientists tend to make their
work look as mathematical as possible.  In a field like
physics this probably doesn't do much harm, but the further you
get from the natural sciences, the more of a problem it
becomes.A page of formulas just looks so impressive.
(Tip: for extra impressiveness, use Greek variables.)  And
so there is a great temptation to work on problems you
can treat formally, rather than problems that are, say,
important.If hackers identified with other makers, like writers and
painters, they wouldn't feel tempted to do      
this.  Writers and painters don't suffer from math envy.
They feel as if they're doing something completely unrelated.
So are hackers, I think.If universities and research labs keep hackers from doing
the kind of work they want to do,
perhaps the place for them is in companies.
Unfortunately, most companies won't let hackers do what they
want either.  Universities and research labs force hackers
to be scientists, and companies force them to be engineers.I only discovered this myself quite recently.  When Yahoo bought
Viaweb, they asked me what I wanted to do.  I had never
liked the business side very much, and said that I just wanted to
hack.  When I got to Yahoo, I found that what hacking meant
to them was implementing software, not designing it.  Programmers
were seen as technicians who translated the visions (if
that is the word) of product managers into code.This seems to be the
default plan in big companies.  They do it because
it decreases the standard deviation of the outcome.
Only a small percentage of hackers can actually design software,
and it's hard for the
people running a company to pick these out.  So instead of
entrusting the future of the software to
one brilliant hacker, most companies set things up so that it is
designed by committee, and the hackers merely
implement the design.If you want to make money at some point, remember this,
because this is one of the reasons startups win.  Big companies want
to decrease the standard deviation of design outcomes because they
want to avoid disasters.  But when you damp oscillations, you
lose the high points as well as the low.  This is not a problem for
big companies, because they don't win by making great
products.  Big companies win by sucking less than other big companies.So if you can figure out a way to get in a
design war with a company big enough that its software is   
designed by product managers, they'll never be able to keep up
with you.  These opportunities are not easy to find, though.
It's hard to engage a big company in a design war,
just as it's hard to engage an opponent inside a castle in hand
to hand combat.  It would be pretty easy to write a better
word processor than Microsoft Word, for example, but Microsoft,
within the castle of their operating system monopoly,
probably wouldn't even notice if you did.The place to fight design wars is in new markets, where no one
has yet managed to establish any fortifications.  That's where
you can win big by taking the bold approach to design, and
having the same people both design and implement the product.  
Microsoft themselves did this at the start.  So did Apple.
And Hewlett-Packard.  I suspect almost every successful startup
has.So one way to build great software is to start your own
startup.  There are two problems with this, though.  One is
that in a startup you have to do so much besides write software.
At Viaweb I considered myself lucky if I
got to hack a quarter of the time.  And the things I had to   
do the other three quarters of the time ranged from tedious
to terrifying.  I have a benchmark for this, because I
once had to leave a board meeting to have
some cavities filled.  I remember sitting back in the
dentist's chair, waiting for the drill, and feeling like
I was on vacation.The other problem with startups is that there is not much
overlap between the kind of software that makes money and the
kind that's interesting to write.  Programming languages
are interesting to write, and Microsoft's first product was
one, in fact, but no one will pay for programming languages
now.  If you want to make money, you tend to be forced to work
on problems that are too nasty for anyone to solve for free.All makers face this problem.  Prices are
determined by supply and demand, and there is just not as much
demand for things that are fun to work on as there is for
things that solve the mundane problems of individual customers.
Acting in off-Broadway plays just doesn't pay as well as
wearing a gorilla suit in someone's booth at a
trade show.  Writing novels doesn't pay as well as writing
ad copy for garbage disposals.
And hacking programming languages doesn't pay as well
as figuring out how to connect some company's
legacy database to their Web server.I think the answer to this problem, in the case of software,
is a concept known to nearly all makers: the day job.
This phrase began with musicians, who
perform at night.  More generally, it means that you have one
kind of work you do for money, and another for love.Nearly all makers have day jobs early in their careers.
Painters and writers notoriously do.  If you're lucky
you can get a day job that's closely
related to your real work.  Musicians often
seem to work in record stores.  A hacker working on some
programming language or operating system might likewise be able to
get a day job using it.  [1]When I say that the answer is for hackers to have day jobs, 
and work on beautiful software on the side, I'm not proposing
this as a new idea.  This is what open-source hacking is all   
about.  What I'm saying is that open-source is probably the right
model, because it has been independently confirmed by all the  
other makers.It seems surprising to me that any employer would be reluctant
to let hackers work on open-source projects.
At Viaweb, we would have been reluctant to hire anyone
who didn't.  When we interviewed
programmers, the main
thing we cared about was what kind of software they
wrote in their spare time.
You can't do anything really well unless
you love it, and if you love to hack you'll inevitably
be working on projects of your own. [2]Because hackers are makers rather than scientists,
the right place to look for metaphors is not in the
sciences, but among other kinds of makers.  What else can painting
teach us about hacking?One thing we can learn, or at least confirm, from the
example of painting is how to learn to hack.  You learn to
paint mostly by doing it.
Ditto for hacking.  Most hackers don't learn to hack by
taking college courses in programming.  They learn to hack
by writing programs of their own at age thirteen.  Even in   
college classes, you learn to hack mostly by hacking. [3]Because painters leave a trail of work behind them, you
can watch them learn by doing.  If you look at the work
of a painter in chronological order, you'll find that each  
painting builds on things that have been learned in previous
ones.  When there's something in
a painting that works very well, you can usually find version 
1 of it in a smaller form in some earlier painting.I think most makers work this way.  Writers and architects seem
to as well.  Maybe it would be good for hackers
to act more like painters, and regularly start over from scratch,
instead of continuing to work for years on one project, and
trying to incorporate all their later ideas as revisions.The fact that hackers learn to hack by doing it is another
sign of how different hacking is from the sciences.  Scientists
don't learn science by doing it, but by doing labs and problem sets.
Scientists start out doing work that's perfect, in the sense
that they're just trying to reproduce work someone else has 
already done for them.
Eventually, they get
to the point where they can do original work.
Whereas hackers, from the start, are doing original work; it's
just very bad.  So hackers start original, and get good, and
scientists start good, and get original.
The other way makers learn is from examples.
For a painter, a museum is a reference library of techniques.
For hundreds of years it has been part of the traditional
education of painters to copy the works of the great masters,
because copying forces you to look closely
at the way a painting is made.Writers do this too.
Benjamin Franklin learned to write by summarizing the points   
in the essays of Addison and Steele and then trying to
reproduce them.  Raymond Chandler did the same thing
with detective stories.Hackers, likewise, can learn to program by looking at 
good programs-- not just at what they do, but the source
code too.  One of the less publicized benefits
of the open-source movement is that it has made it easier
to learn to program.  When I learned to program, we had to rely
mostly on examples in books.  The one big chunk of
code available then was Unix, but even this was not   
open source.  Most of the people who read the source
read it in illicit photocopies of John Lions' book, which
though written in 1977 was not allowed to be published
until 1996.Another example we can take from painting is the way that
paintings are created by gradual refinement.  Paintings usually
begin with a sketch.
Gradually the details get filled in.
But it is not merely a process of filling in. Sometimes   
the original plans turn out to be mistaken.
Countless paintings,
when you look at them in xrays, turn out to have limbs that
have been moved or facial features that have been readjusted.Here's a case where we can learn from painting.  I think hacking
should work this way too.  It's unrealistic
to expect that the specifications for a program will be
perfect. You're
better off if you admit this up front, and write programs in
a way that allows specifications to change on the fly.(The structure of large companies makes this hard for them
to do, so here is another place where startups have an advantage.)Everyone by now presumably knows about the danger of premature
optimization.  I think we should be just as worried about
premature design-- deciding too early what
a program should do.The right tools can help us avoid
this danger.
A good programming language should, like oil paint, make it
easy to change your mind.  Dynamic typing is a win here because
you don't have to
commit to specific data representations up front.
But the key to flexibility, I think, is to make the language
very abstract.
The easiest program to change is one that's very short.
This sounds like a paradox, but a great painting
has to be better than it has to be.
For example, when Leonardo
painted the portrait of Ginevra de Benci
in the National Gallery, he put a juniper bush behind her head.
In it he carefully
painted each individual leaf.  Many painters might have thought,
this is just something to put in the background to frame
her head.  No one will look that closely at it.Not Leonardo.  How hard he worked on part of a painting didn't
depend at all on how closely he expected anyone to look at it.
He was like Michael Jordan.  Relentless.Relentlessness wins because, in the aggregate, unseen details
become visible.
When people walk by the portrait of Ginevra de Benci,
their attention is often immediately arrested by it,
even before they look at the label and notice that it says
Leonardo da Vinci.  All those unseen details combine to produce
something that's just stunning, like a thousand barely audible
voices all singing in tune.Great software, likewise, requires a fanatical devotion to
beauty.  If you look inside good software, you find that
parts no one is ever supposed to see are beautiful too.
I'm not claiming I write great software, but I
know that when it comes to code I behave in a way that would
make me eligible for prescription drugs if I approached everyday
life the same way.
It drives me crazy to see code that's badly indented,
or that uses ugly variable names.If a hacker were a mere implementor, turning a spec into code, then
he could just work his way through it from one end to the other like
someone digging a ditch.  But if the hacker is a creator, we have
to take inspiration into account.In hacking, like painting,
work comes in cycles.  Sometimes you get excited about some
new project and you want to work sixteen hours a day on it. 
Other times nothing seems interesting.To do good work you have to take these cycles into
account, because they're affected by how you react to them.
When you're driving a
car with a manual transmission on a hill, you have to back off
the clutch sometimes to avoid stalling.  Backing
off can likewise prevent ambition from stalling.
In both painting and hacking there are some
tasks that are terrifyingly ambitious, and others that are
comfortingly routine.  It's a good idea to save some easy
tasks for moments when you would otherwise stall.In hacking, this can literally mean saving up bugs.
I like debugging: it's the
one time that hacking is as straightforward as   
people think it is.  You have a
totally constrained problem, and all you have to do is solve
it.  Your program is supposed to do x.  Instead it does y.
Where does it go wrong? You know you're going to win
in the end.  It's as relaxing as painting a wall.The example of painting can teach us not only how to manage our
own work, but how to work together.  A lot of the
great art of the past is the work of multiple hands, though
there may only be one name on the wall next to it in the
museum.  Leonardo was an apprentice in the workshop of
Verrocchio and painted one of the angels in his Baptism of
Christ.  This sort of thing was the rule, not the exception.
Michelangelo was considered especially dedicated for insisting
on painting all the figures on the ceiling of the Sistine
Chapel himself.As far as I know, when painters worked together on a painting,
they never worked on the same parts.  It was common
for the master to paint the principal figures and for assistants
to paint the others and the background.  But you never had
one guy painting over the work of another.I think this is the right model for collaboration in software
too.  Don't push it too far.  When a piece of code is
being hacked by three or four different people, no one of whom
really owns it, it will end up being like a common-room.  It will
tend to feel bleak and abandoned, and accumulate cruft.
The right
way to collaborate, I think, is to divide projects into sharply
defined modules, each with a definite owner, and with interfaces
between them that are as carefully designed and, if possible,
as articulated as programming languages.Like painting, most software is intended for
a human audience.  And so hackers, like painters, must have
empathy to do really great work.  You have to be able to see
things from the user's point of view.When I was a kid I was always being told to look at things from
someone else's point of view.  What this always meant in
practice was to do what someone else wanted, instead of what
I wanted.  This of course gave empathy a bad name, and I made a
point of not cultivating it.Boy, was I wrong.  It turns out that looking at things from 
other people's point of view is practically the secret of
success.  It doesn't necessarily mean being self-sacrificing.
Far from it.  Understanding how someone else sees things
doesn't imply that you'll act in his interest; in some
situations-- in war, for example-- you want to do exactly
the opposite. [4]Most makers make things for a human audience.
And to engage an audience you have to understand what they need.
Nearly all the greatest paintings are paintings of people,
for example, because people are what people are interested in.Empathy is probably the single most important difference
between a good hacker and a great one.  Some hackers
are quite smart, but when it comes to empathy are
practically solipsists.  It's hard for such      
people to design great software [5], because they can't
see things from the user's point of view.One way to tell how good people are at empathy is to watch
them explain a technical question to someone without a technical
background.  We probably all know people who, though otherwise smart,
are just comically bad at this.  If someone asks them at
a dinner party what a programming language is, they'll
say something like ``Oh, a high-level language is what
the compiler uses as input to generate object code.''
High-level language?  Compiler?  Object code?  Someone who 
doesn't know what a programming language is obviously doesn't
know what these things are, either.Part of what software has to do is explain itself.  So to   
write good software you have to understand how little users   
understand.
They're going to walk up to the software with no preparation, and
it had better do what they guess it will, because they're
not going to read the manual.  The best system I've ever seen 
in this respect was the original Macintosh, in 1985.
It did what software almost never does: it just worked. [6]Source code, too, should explain itself.  If I could get people to
remember just one quote about programming, it would be the
one at the beginning of Structure and Interpretation of Computer
Programs.

Programs should be written for people to read, and
only incidentally for machines to execute.

You need to have 
empathy not just for your users, but for your readers.  It's in  
your interest, because you'll be one of them.
Many a hacker has written a program only to
find on returning to it six months later that he has no idea   
how it works.  I know several people who've sworn off Perl after
such experiences. [7]Lack of empathy is associated with intelligence, to the point
that there is even something of a fashion for it in some places.
But I don't think there's any correlation.
You can do well in math and
the natural sciences without having to learn empathy, and people in these
fields tend to be smart, so the two qualities have come to be
associated.  But there are plenty of dumb people who are bad at
empathy too.  Just listen to the people who call in with questions on
talk shows.  They ask whatever it is they're asking in
such a roundabout way
that the hosts often have to rephrase the question for them.So, if hacking works like painting and writing, is it as cool?
After all, you only get one life.
You might as well spend it working on something great.Unfortunately, the question is hard to answer.  There is always
a big time lag in prestige.  It's like light from a distant star.
Painting has prestige now because of great work people did five hundred
years ago.  At the time, no one thought
these paintings were as important as we do today.  It would have
seemed very odd to people at the time that Federico da Montefeltro,
the Duke of Urbino, would one day be known mostly as the guy
with the strange nose in a painting 
by Piero della Francesca.So while I admit that hacking doesn't seem as cool as painting now,
we should remember that painting itself didn't seem as cool in
its glory days as it does now.What we can say with some confidence is that these are the glory
days of hacking.  In most fields the great work is done early on.
The paintings made between 1430 and 1500 are still unsurpassed.
Shakespeare appeared just as professional theater was being born,

and pushed the medium
so far that every playwright since has had to live in his shadow.
Albrecht Durer did the same thing with engraving, and Jane Austen
with the novel.Over and over we see the same pattern.  A new medium appears, and
people are so excited about it that they explore most of its
possibilities in the first couple generations.   Hacking seems
to be in this phase now.Painting was not, in Leonardo's time, as cool as his work
helped make it.
How cool hacking turns out to be will depend on what we can
do with this new medium.  

Notes[1] The greatest damage that photography has done
to painting may be the fact that it killed the best day job.
Most of the great painters in history supported
themselves by painting portraits.  [2] I've been told that Microsoft discourages
employees from contributing to open-source projects, even in
their spare time.
But so many of the best hackers work on open-source
projects now that the main effect of this policy may be
to ensure that they won't be able to hire any first-rate
programmers.[3] What you learn about programming in college is much like
what you learn about books or clothes or dating: what bad taste you
had in high school.[4] Here's an example of applied empathy.
At Viaweb, if we couldn't decide between two alternatives, we'd
ask, what would our competitors hate most?  At one point a
competitor added a feature to their software that was basically
useless, but since it was one of few they had that we didn't, they
made much of it in the trade press.
We could have tried to explain that the feature was useless,
but we decided it would annoy our competitor more if we
just implemented it ourselves, so we hacked together our own
version that afternoon.[5] Except text editors and compilers.  Hackers don't need empathy to
design these, because they are themselves typical users.[6] Well, almost.  They overshot the available RAM somewhat,
causing much inconvenient disk swapping, but this could be fixed
within a few months by buying an additional disk drive.[7] The way to make programs easy to read is not to
stuff them with comments. I would take Abelson and Sussman's
quote a step further.  Programming languages should be designed
to express algorithms, and only incidentally to tell computers
how to execute them.  A good programming language
ought to be better for explaining software than English.
You should only
need comments when there is some kind of kludge you need to warn
readers about, just as on a road there are only
arrows on parts with unexpectedly sharp curves.
Thanks to Trevor Blackwell, Robert Morris, Dan Giffin, and Lisa
Randall for reading drafts of this, and to Henry Leitner
and Larry Finkelstein for inviting me to speak.Japanese TranslationSpanish TranslationGerman TranslationPortuguese TranslationCzech TranslationWhy Good Design Comes from Bad DesignKnuth: Computer Programming as an Art


You'll find this essay and 14 others in
Hackers & Painters.

","programming advice
",human,"human
","human
"
12,12,"January 2017Because biographies of famous scientists tend to 
edit out their mistakes, we underestimate the 
degree of risk they were willing to take.
And because anything a famous scientist did that
wasn't a mistake has probably now become the
conventional wisdom, those choices don't
seem risky either.Biographies of Newton, for example, understandably focus
more on physics than alchemy or theology.
The impression we get is that his unerring judgment
led him straight to truths no one else had noticed.
How to explain all the time he spent on alchemy
and theology?  Well, smart people are often kind of
crazy.But maybe there is a simpler explanation. Maybe
the smartness and the craziness were not as separate
as we think. Physics seems to us a promising thing
to work on, and alchemy and theology obvious wastes
of time. But that's because we know how things
turned out. In Newton's day the three problems 
seemed roughly equally promising. No one knew yet
what the payoff would be for inventing what we
now call physics; if they had, more people would 
have been working on it. And alchemy and theology
were still then in the category Marc Andreessen would 
describe as ""huge, if true.""Newton made three bets. One of them worked. But 
they were all risky.Japanese Translation","historical analysis
",human,"human
","human
"
13,13,"January 2012A few hours before the Yahoo acquisition was announced in June 1998
I took a snapshot of Viaweb's
site.  I thought it might be interesting to look at one day.The first thing one notices is is how tiny the pages are.  Screens
were a lot smaller in 1998.  If I remember correctly, our frontpage
used to just fit in the size window people typically used then.Browsers then (IE 6 was still 3 years in the future) had few fonts
and they weren't antialiased.  If you wanted to make pages that
looked good, you had to render display text as images.You may notice a certain similarity between the Viaweb and Y Combinator logos.  We did that
as an inside joke when we started YC.  Considering how basic a red
circle is, it seemed surprising to me when we started Viaweb how
few other companies used one as their logo.  A bit later I realized
why.On the Company
page you'll notice a mysterious individual called John McArtyem.
Robert Morris (aka Rtm) was so publicity averse after the 
Worm that he
didn't want his name on the site.  I managed to get him to agree
to a compromise: we could use his bio but not his name.  He has
since relaxed a bit
on that point.Trevor graduated at about the same time the acquisition closed, so in the
course of 4 days he went from impecunious grad student to millionaire
PhD.  The culmination of my career as a writer of press releases
was one celebrating
his graduation, illustrated with a drawing I did of him during
a meeting.(Trevor also appears as Trevino
Bagwell in our directory of web designers merchants could hire
to build stores for them.  We inserted him as a ringer in case some
competitor tried to spam our web designers.   We assumed his logo
would deter any actual customers, but it did not.)Back in the 90s, to get users you had to get mentioned in magazines
and newspapers.  There were not the same ways to get found online
that there are today.  So we used to pay a PR
firm $16,000 a month to get us mentioned in the press.  Fortunately
reporters liked
us.In our advice about
getting traffic from search engines (I don't think the term SEO
had been coined yet), we say there are only 7 that matter: Yahoo,
AltaVista, Excite, WebCrawler, InfoSeek, Lycos, and HotBot.  Notice
anything missing?  Google was incorporated that September.We supported online transactions via a company called 
Cybercash,
since if we lacked that feature we'd have gotten beaten up in product
comparisons.  But Cybercash was so bad and most stores' order volumes
were so low that it was better if merchants processed orders like phone orders.  We had a page in our site trying to talk merchants
out of doing real time authorizations.The whole site was organized like a funnel, directing people to the
test drive.
It was a novel thing to be able to try out software online.  We put
cgi-bin in our dynamic urls to fool competitors about how our
software worked.We had some well
known users.  Needless to say, Frederick's of Hollywood got the
most traffic.  We charged a flat fee of $300/month for big stores,
so it was a little alarming to have users who got lots of traffic.
I once calculated how much Frederick's was costing us in bandwidth,
and it was about $300/month.Since we hosted all the stores, which together were getting just
over 10 million page views per month in June 1998, we consumed what
at the time seemed a lot of bandwidth.  We had 2 T1s (3 Mb/sec)
coming into our offices.  In those days there was no AWS.  Even
colocating servers seemed too risky, considering how often things
went wrong with them.  So we had our servers in our offices.  Or
more precisely, in Trevor's office.  In return for the unique
privilege of sharing his office with no other humans, he had to
share it with 6 shrieking tower servers.  His office was nicknamed
the Hot Tub on account of the heat they generated.  Most days his
stack of window air conditioners could keep up.For describing pages, we had a template language called RTML, which
supposedly stood for something, but which in fact I named after
Rtm.  RTML was Common Lisp augmented by some macros and libraries,
and concealed under a structure editor that made it look like it
had syntax.Since we did continuous releases, our software didn't actually have
versions.  But in those days the trade press expected versions, so
we made them up.  If we wanted to get lots of attention, we made
the version number an
integer.  That ""version 4.0"" icon was generated by our own
button generator, incidentally.  The whole Viaweb site was made
with our software, even though it wasn't an online store, because
we wanted to experience what our users did.At the end of 1997, we released a general purpose shopping search
engine called Shopfind.  It
was pretty advanced for the time.  It had a programmable crawler
that could crawl most of the different stores online and pick out
the products.","historical analysis
",human,"human
","human
"
14,14,"February 2021Before college the two main things I worked on, outside of school,
were writing and programming. I didn't write essays. I wrote what
beginning writers were supposed to write then, and probably still
are: short stories. My stories were awful. They had hardly any plot,
just characters with strong feelings, which I imagined made them
deep.The first programs I tried writing were on the IBM 1401 that our
school district used for what was then called ""data processing.""
This was in 9th grade, so I was 13 or 14. The school district's
1401 happened to be in the basement of our junior high school, and
my friend Rich Draves and I got permission to use it. It was like
a mini Bond villain's lair down there, with all these alien-looking
machines — CPU, disk drives, printer, card reader — sitting up
on a raised floor under bright fluorescent lights.The language we used was an early version of Fortran. You had to
type programs on punch cards, then stack them in the card reader
and press a button to load the program into memory and run it. The
result would ordinarily be to print something on the spectacularly
loud printer.I was puzzled by the 1401. I couldn't figure out what to do with
it. And in retrospect there's not much I could have done with it.
The only form of input to programs was data stored on punched cards,
and I didn't have any data stored on punched cards. The only other
option was to do things that didn't rely on any input, like calculate
approximations of pi, but I didn't know enough math to do anything
interesting of that type. So I'm not surprised I can't remember any
programs I wrote, because they can't have done much. My clearest
memory is of the moment I learned it was possible for programs not
to terminate, when one of mine didn't. On a machine without
time-sharing, this was a social as well as a technical error, as
the data center manager's expression made clear.With microcomputers, everything changed. Now you could have a
computer sitting right in front of you, on a desk, that could respond
to your keystrokes as it was running instead of just churning through
a stack of punch cards and then stopping. 
[1]The first of my friends to get a microcomputer built it himself.
It was sold as a kit by Heathkit. I remember vividly how impressed
and envious I felt watching him sitting in front of it, typing
programs right into the computer.Computers were expensive in those days and it took me years of
nagging before I convinced my father to buy one, a TRS-80, in about
1980. The gold standard then was the Apple II, but a TRS-80 was
good enough. This was when I really started programming. I wrote
simple games, a program to predict how high my model rockets would
fly, and a word processor that my father used to write at least one
book. There was only room in memory for about 2 pages of text, so
he'd write 2 pages at a time and then print them out, but it was a
lot better than a typewriter.Though I liked programming, I didn't plan to study it in college.
In college I was going to study philosophy, which sounded much more
powerful. It seemed, to my naive high school self, to be the study
of the ultimate truths, compared to which the things studied in
other fields would be mere domain knowledge. What I discovered when
I got to college was that the other fields took up so much of the
space of ideas that there wasn't much left for these supposed
ultimate truths. All that seemed left for philosophy were edge cases
that people in other fields felt could safely be ignored.I couldn't have put this into words when I was 18. All I knew at
the time was that I kept taking philosophy courses and they kept
being boring. So I decided to switch to AI.AI was in the air in the mid 1980s, but there were two things
especially that made me want to work on it: a novel by Heinlein
called The Moon is a Harsh Mistress, which featured an intelligent
computer called Mike, and a PBS documentary that showed Terry
Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh
Mistress, so I don't know how well it has aged, but when I read it
I was drawn entirely into its world. It seemed only a matter of
time before we'd have Mike, and when I saw Winograd using SHRDLU,
it seemed like that time would be a few years at most. All you had
to do was teach SHRDLU more words.There weren't any classes in AI at Cornell then, not even graduate
classes, so I started trying to teach myself. Which meant learning
Lisp, since in those days Lisp was regarded as the language of AI.
The commonly used programming languages then were pretty primitive,
and programmers' ideas correspondingly so. The default language at
Cornell was a Pascal-like language called PL/I, and the situation
was similar elsewhere. Learning Lisp expanded my concept of a program
so fast that it was years before I started to have a sense of where
the new limits were. This was more like it; this was what I had
expected college to do. It wasn't happening in a class, like it was
supposed to, but that was ok. For the next couple years I was on a
roll. I knew what I was going to do.For my undergraduate thesis, I reverse-engineered SHRDLU. My God
did I love working on that program. It was a pleasing bit of code,
but what made it even more exciting was my belief — hard to imagine
now, but not unique in 1985 — that it was already climbing the
lower slopes of intelligence.I had gotten into a program at Cornell that didn't make you choose
a major. You could take whatever classes you liked, and choose
whatever you liked to put on your degree. I of course chose ""Artificial
Intelligence."" When I got the actual physical diploma, I was dismayed
to find that the quotes had been included, which made them read as
scare-quotes. At the time this bothered me, but now it seems amusingly
accurate, for reasons I was about to discover.I applied to 3 grad schools: MIT and Yale, which were renowned for
AI at the time, and Harvard, which I'd visited because Rich Draves
went there, and was also home to Bill Woods, who'd invented the
type of parser I used in my SHRDLU clone. Only Harvard accepted me,
so that was where I went.I don't remember the moment it happened, or if there even was a
specific moment, but during the first year of grad school I realized
that AI, as practiced at the time, was a hoax. By which I mean the
sort of AI in which a program that's told ""the dog is sitting on
the chair"" translates this into some formal representation and adds
it to the list of things it knows.What these programs really showed was that there's a subset of
natural language that's a formal language. But a very proper subset.
It was clear that there was an unbridgeable gap between what they
could do and actually understanding natural language. It was not,
in fact, simply a matter of teaching SHRDLU more words. That whole
way of doing AI, with explicit data structures representing concepts,
was not going to work. Its brokenness did, as so often happens,
generate a lot of opportunities to write papers about various
band-aids that could be applied to it, but it was never going to
get us Mike.So I looked around to see what I could salvage from the wreckage
of my plans, and there was Lisp. I knew from experience that Lisp
was interesting for its own sake and not just for its association
with AI, even though that was the main reason people cared about
it at the time. So I decided to focus on Lisp. In fact, I decided
to write a book about Lisp hacking. It's scary to think how little
I knew about Lisp hacking when I started writing that book. But
there's nothing like writing a book about something to help you
learn it. The book, On Lisp, wasn't published till 1993, but I wrote
much of it in grad school.Computer Science is an uneasy alliance between two halves, theory
and systems. The theory people prove things, and the systems people
build things. I wanted to build things. I had plenty of respect for
theory — indeed, a sneaking suspicion that it was the more admirable
of the two halves — but building things seemed so much more exciting.The problem with systems work, though, was that it didn't last.
Any program you wrote today, no matter how good, would be obsolete
in a couple decades at best. People might mention your software in
footnotes, but no one would actually use it. And indeed, it would
seem very feeble work. Only people with a sense of the history of
the field would even realize that, in its time, it had been good.There were some surplus Xerox Dandelions floating around the computer
lab at one point. Anyone who wanted one to play around with could
have one. I was briefly tempted, but they were so slow by present
standards; what was the point? No one else wanted one either, so
off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would
last.In this dissatisfied state I went in 1988 to visit Rich Draves at
CMU, where he was in grad school. One day I went to visit the
Carnegie Institute, where I'd spent a lot of time as a kid. While
looking at a painting there I realized something that might seem
obvious, but was a big surprise to me. There, right on the wall,
was something you could make that would last. Paintings didn't
become obsolete. Some of the best ones were hundreds of years old.And moreover this was something you could make a living doing. Not
as easily as you could by writing software, of course, but I thought
if you were really industrious and lived really cheaply, it had to
be possible to make enough to survive. And as an artist you could
be truly independent. You wouldn't have a boss, or even need to get
research funding.I had always liked looking at paintings. Could I make them? I had
no idea. I'd never imagined it was even possible. I knew intellectually
that people made art — that it didn't just appear spontaneously
— but it was as if the people who made it were a different species.
They either lived long ago or were mysterious geniuses doing strange
things in profiles in Life magazine. The idea of actually being
able to make art, to put that verb before that noun, seemed almost
miraculous.That fall I started taking art classes at Harvard. Grad students
could take classes in any department, and my advisor, Tom Cheatham,
was very easy going. If he even knew about the strange classes I
was taking, he never said anything.So now I was in a PhD program in computer science, yet planning to
be an artist, yet also genuinely in love with Lisp hacking and
working away at On Lisp. In other words, like many a grad student,
I was working energetically on multiple projects that were not my
thesis.I didn't see a way out of this situation. I didn't want to drop out
of grad school, but how else was I going to get out? I remember
when my friend Robert Morris got kicked out of Cornell for writing
the internet worm of 1988, I was envious that he'd found such a
spectacular way to get out of grad school.Then one day in April 1990 a crack appeared in the wall. I ran into
professor Cheatham and he asked if I was far enough along to graduate
that June. I didn't have a word of my dissertation written, but in
what must have been the quickest bit of thinking in my life, I
decided to take a shot at writing one in the 5 weeks or so that
remained before the deadline, reusing parts of On Lisp where I
could, and I was able to respond, with no perceptible delay ""Yes,
I think so. I'll give you something to read in a few days.""I picked applications of continuations as the topic. In retrospect
I should have written about macros and embedded languages. There's
a whole world there that's barely been explored. But all I wanted
was to get out of grad school, and my rapidly written dissertation
sufficed, just barely.Meanwhile I was applying to art schools. I applied to two: RISD in
the US, and the Accademia di Belli Arti in Florence, which, because
it was the oldest art school, I imagined would be good. RISD accepted
me, and I never heard back from the Accademia, so off to Providence
I went.I'd applied for the BFA program at RISD, which meant in effect that
I had to go to college again. This was not as strange as it sounds,
because I was only 25, and art schools are full of people of different
ages. RISD counted me as a transfer sophomore and said I had to do
the foundation that summer. The foundation means the classes that
everyone has to take in fundamental subjects like drawing, color,
and design.Toward the end of the summer I got a big surprise: a letter from
the Accademia, which had been delayed because they'd sent it to
Cambridge England instead of Cambridge Massachusetts, inviting me
to take the entrance exam in Florence that fall. This was now only
weeks away. My nice landlady let me leave my stuff in her attic. I
had some money saved from consulting work I'd done in grad school;
there was probably enough to last a year if I lived cheaply. Now
all I had to do was learn Italian.Only stranieri (foreigners) had to take this entrance exam. In
retrospect it may well have been a way of excluding them, because
there were so many stranieri attracted by the idea of studying
art in Florence that the Italian students would otherwise have been
outnumbered. I was in decent shape at painting and drawing from the
RISD foundation that summer, but I still don't know how I managed
to pass the written exam. I remember that I answered the essay
question by writing about Cezanne, and that I cranked up the
intellectual level as high as I could to make the most of my limited
vocabulary. 
[2]I'm only up to age 25 and already there are such conspicuous patterns.
Here I was, yet again about to attend some august institution in
the hopes of learning about some prestigious subject, and yet again
about to be disappointed. The students and faculty in the painting
department at the Accademia were the nicest people you could imagine,
but they had long since arrived at an arrangement whereby the
students wouldn't require the faculty to teach anything, and in
return the faculty wouldn't require the students to learn anything.
And at the same time all involved would adhere outwardly to the
conventions of a 19th century atelier. We actually had one of those
little stoves, fed with kindling, that you see in 19th century
studio paintings, and a nude model sitting as close to it as possible
without getting burned. Except hardly anyone else painted her besides
me. The rest of the students spent their time chatting or occasionally
trying to imitate things they'd seen in American art magazines.Our model turned out to live just down the street from me. She made
a living from a combination of modelling and making fakes for a
local antique dealer. She'd copy an obscure old painting out of a
book, and then he'd take the copy and maltreat it to make it look
old. 
[3]While I was a student at the Accademia I started painting still
lives in my bedroom at night. These paintings were tiny, because
the room was, and because I painted them on leftover scraps of
canvas, which was all I could afford at the time. Painting still
lives is different from painting people, because the subject, as
its name suggests, can't move. People can't sit for more than about
15 minutes at a time, and when they do they don't sit very still.
So the traditional m.o. for painting people is to know how to paint
a generic person, which you then modify to match the specific person
you're painting. Whereas a still life you can, if you want, copy
pixel by pixel from what you're seeing. You don't want to stop
there, of course, or you get merely photographic accuracy, and what
makes a still life interesting is that it's been through a head.
You want to emphasize the visual cues that tell you, for example,
that the reason the color changes suddenly at a certain point is
that it's the edge of an object. By subtly emphasizing such things
you can make paintings that are more realistic than photographs not
just in some metaphorical sense, but in the strict information-theoretic
sense. 
[4]I liked painting still lives because I was curious about what I was
seeing. In everyday life, we aren't consciously aware of much we're
seeing. Most visual perception is handled by low-level processes
that merely tell your brain ""that's a water droplet"" without telling
you details like where the lightest and darkest points are, or
""that's a bush"" without telling you the shape and position of every
leaf. This is a feature of brains, not a bug. In everyday life it
would be distracting to notice every leaf on every bush. But when
you have to paint something, you have to look more closely, and
when you do there's a lot to see. You can still be noticing new
things after days of trying to paint something people usually take
for granted, just as you can  after
days of trying to write an essay about something people usually
take for granted.This is not the only way to paint. I'm not 100% sure it's even a
good way to paint. But it seemed a good enough bet to be worth
trying.Our teacher, professor Ulivi, was a nice guy. He could see I worked
hard, and gave me a good grade, which he wrote down in a sort of
passport each student had. But the Accademia wasn't teaching me
anything except Italian, and my money was running out, so at the
end of the first year I went back to the US.I wanted to go back to RISD, but I was now broke and RISD was very
expensive, so I decided to get a job for a year and then return to
RISD the next fall. I got one at a company called Interleaf, which
made software for creating documents. You mean like Microsoft Word?
Exactly. That was how I learned that low end software tends to eat
high end software. But Interleaf still had a few years to live yet.
[5]Interleaf had done something pretty bold. Inspired by Emacs, they'd
added a scripting language, and even made the scripting language a
dialect of Lisp. Now they wanted a Lisp hacker to write things in
it. This was the closest thing I've had to a normal job, and I
hereby apologize to my boss and coworkers, because I was a bad
employee. Their Lisp was the thinnest icing on a giant C cake, and
since I didn't know C and didn't want to learn it, I never understood
most of the software. Plus I was terribly irresponsible. This was
back when a programming job meant showing up every day during certain
working hours. That seemed unnatural to me, and on this point the
rest of the world is coming around to my way of thinking, but at
the time it caused a lot of friction. Toward the end of the year I
spent much of my time surreptitiously working on On Lisp, which I
had by this time gotten a contract to publish.The good part was that I got paid huge amounts of money, especially
by art student standards. In Florence, after paying my part of the
rent, my budget for everything else had been $7 a day. Now I was
getting paid more than 4 times that every hour, even when I was
just sitting in a meeting. By living cheaply I not only managed to
save enough to go back to RISD, but also paid off my college loans.I learned some useful things at Interleaf, though they were mostly
about what not to do. I learned that it's better for technology
companies to be run by product people than sales people (though
sales is a real skill and people who are good at it are really good
at it), that it leads to bugs when code is edited by too many people,
that cheap office space is no bargain if it's depressing, that
planned meetings are inferior to corridor conversations, that big,
bureaucratic customers are a dangerous source of money, and that
there's not much overlap between conventional office hours and the
optimal time for hacking, or conventional offices and the optimal
place for it.But the most important thing I learned, and which I used in both
Viaweb and Y Combinator, is that the low end eats the high end:
that it's good to be the ""entry level"" option, even though that
will be less prestigious, because if you're not, someone else will
be, and will squash you against the ceiling. Which in turn means
that prestige is a danger sign.When I left to go back to RISD the next fall, I arranged to do
freelance work for the group that did projects for customers, and
this was how I survived for the next several years. When I came
back to visit for a project later on, someone told me about a new
thing called HTML, which was, as he described it, a derivative of
SGML. Markup language enthusiasts were an occupational hazard at
Interleaf and I ignored him, but this HTML thing later became a big
part of my life.In the fall of 1992 I moved back to Providence to continue at RISD.
The foundation had merely been intro stuff, and the Accademia had
been a (very civilized) joke. Now I was going to see what real art
school was like. But alas it was more like the Accademia than not.
Better organized, certainly, and a lot more expensive, but it was
now becoming clear that art school did not bear the same relationship
to art that medical school bore to medicine. At least not the
painting department. The textile department, which my next door
neighbor belonged to, seemed to be pretty rigorous. No doubt
illustration and architecture were too. But painting was post-rigorous.
Painting students were supposed to express themselves, which to the
more worldly ones meant to try to cook up some sort of distinctive
signature style.A signature style is the visual equivalent of what in show business
is known as a ""schtick"": something that immediately identifies the
work as yours and no one else's. For example, when you see a painting
that looks like a certain kind of cartoon, you know it's by Roy
Lichtenstein. So if you see a big painting of this type hanging in
the apartment of a hedge fund manager, you know he paid millions
of dollars for it. That's not always why artists have a signature
style, but it's usually why buyers pay a lot for such work.
[6]There were plenty of earnest students too: kids who ""could draw""
in high school, and now had come to what was supposed to be the
best art school in the country, to learn to draw even better. They
tended to be confused and demoralized by what they found at RISD,
but they kept going, because painting was what they did. I was not
one of the kids who could draw in high school, but at RISD I was
definitely closer to their tribe than the tribe of signature style
seekers.I learned a lot in the color class I took at RISD, but otherwise I
was basically teaching myself to paint, and I could do that for
free. So in 1993 I dropped out. I hung around Providence for a bit,
and then my college friend Nancy Parmet did me a big favor. A
rent-controlled apartment in a building her mother owned in New
York was becoming vacant. Did I want it? It wasn't much more than
my current place, and New York was supposed to be where the artists
were. So yes, I wanted it!
[7]Asterix comics begin by zooming in on a tiny corner of Roman Gaul
that turns out not to be controlled by the Romans. You can do
something similar on a map of New York City: if you zoom in on the
Upper East Side, there's a tiny corner that's not rich, or at least
wasn't in 1993. It's called Yorkville, and that was my new home.
Now I was a New York artist — in the strictly technical sense of
making paintings and living in New York.I was nervous about money, because I could sense that Interleaf was
on the way down. Freelance Lisp hacking work was very rare, and I
didn't want to have to program in another language, which in those
days would have meant C++ if I was lucky. So with my unerring nose
for financial opportunity, I decided to write another book on Lisp.
This would be a popular book, the sort of book that could be used
as a textbook. I imagined myself living frugally off the royalties
and spending all my time painting. (The painting on the cover of
this book, ANSI Common Lisp, is one that I painted around this
time.)The best thing about New York for me was the presence of Idelle and
Julian Weber. Idelle Weber was a painter, one of the early
photorealists, and I'd taken her painting class at Harvard. I've
never known a teacher more beloved by her students. Large numbers
of former students kept in touch with her, including me. After I
moved to New York I became her de facto studio assistant.She liked to paint on big, square canvases, 4 to 5 feet on a side.
One day in late 1994 as I was stretching one of these monsters there
was something on the radio about a famous fund manager. He wasn't
that much older than me, and was super rich. The thought suddenly
occurred to me: why don't I become rich? Then I'll be able to work
on whatever I want.Meanwhile I'd been hearing more and more about this new thing called
the World Wide Web. Robert Morris showed it to me when I visited
him in Cambridge, where he was now in grad school at Harvard. It
seemed to me that the web would be a big deal. I'd seen what graphical
user interfaces had done for the popularity of microcomputers. It
seemed like the web would do the same for the internet.If I wanted to get rich, here was the next train leaving the station.
I was right about that part. What I got wrong was the idea. I decided
we should start a company to put art galleries online. I can't
honestly say, after reading so many Y Combinator applications, that
this was the worst startup idea ever, but it was up there. Art
galleries didn't want to be online, and still don't, not the fancy
ones. That's not how they sell. I wrote some software to generate
web sites for galleries, and Robert wrote some to resize images and
set up an http server to serve the pages. Then we tried to sign up
galleries. To call this a difficult sale would be an understatement.
It was difficult to give away. A few galleries let us make sites
for them for free, but none paid us.Then some online stores started to appear, and I realized that
except for the order buttons they were identical to the sites we'd
been generating for galleries. This impressive-sounding thing called
an ""internet storefront"" was something we already knew how to build.So in the summer of 1995, after I submitted the camera-ready copy
of ANSI Common Lisp to the publishers, we started trying to write
software to build online stores. At first this was going to be
normal desktop software, which in those days meant Windows software.
That was an alarming prospect, because neither of us knew how to
write Windows software or wanted to learn. We lived in the Unix
world. But we decided we'd at least try writing a prototype store
builder on Unix. Robert wrote a shopping cart, and I wrote a new
site generator for stores — in Lisp, of course.We were working out of Robert's apartment in Cambridge. His roommate
was away for big chunks of time, during which I got to sleep in his
room. For some reason there was no bed frame or sheets, just a
mattress on the floor. One morning as I was lying on this mattress
I had an idea that made me sit up like a capital L. What if we ran
the software on the server, and let users control it by clicking
on links? Then we'd never have to write anything to run on users'
computers. We could generate the sites on the same server we'd serve
them from. Users wouldn't need anything more than a browser.This kind of software, known as a web app, is common now, but at
the time it wasn't clear that it was even possible. To find out,
we decided to try making a version of our store builder that you
could control through the browser. A couple days later, on August
12, we had one that worked. The UI was horrible, but it proved you
could build a whole store through the browser, without any client
software or typing anything into the command line on the server.Now we felt like we were really onto something. I had visions of a
whole new generation of software working this way. You wouldn't
need versions, or ports, or any of that crap. At Interleaf there
had been a whole group called Release Engineering that seemed to
be at least as big as the group that actually wrote the software.
Now you could just update the software right on the server.We started a new company we called Viaweb, after the fact that our
software worked via the web, and we got $10,000 in seed funding
from Idelle's husband Julian. In return for that and doing the
initial legal work and giving us business advice, we gave him 10%
of the company. Ten years later this deal became the model for Y
Combinator's. We knew founders needed something like this, because
we'd needed it ourselves.At this stage I had a negative net worth, because the thousand
dollars or so I had in the bank was more than counterbalanced by
what I owed the government in taxes. (Had I diligently set aside
the proper proportion of the money I'd made consulting for Interleaf?
No, I had not.) So although Robert had his graduate student stipend,
I needed that seed funding to live on.We originally hoped to launch in September, but we got more ambitious
about the software as we worked on it. Eventually we managed to
build a WYSIWYG site builder, in the sense that as you were creating
pages, they looked exactly like the static ones that would be
generated later, except that instead of leading to static pages,
the links all referred to closures stored in a hash table on the
server.It helped to have studied art, because the main goal of an online
store builder is to make users look legit, and the key to looking
legit is high production values. If you get page layouts and fonts
and colors right, you can make a guy running a store out of his
bedroom look more legit than a big company.(If you're curious why my site looks so old-fashioned, it's because
it's still made with this software. It may look clunky today, but
in 1996 it was the last word in slick.)In September, Robert rebelled. ""We've been working on this for a
month,"" he said, ""and it's still not done."" This is funny in
retrospect, because he would still be working on it almost 3 years
later. But I decided it might be prudent to recruit more programmers,
and I asked Robert who else in grad school with him was really good.
He recommended Trevor Blackwell, which surprised me at first, because
at that point I knew Trevor mainly for his plan to reduce everything
in his life to a stack of notecards, which he carried around with
him. But Rtm was right, as usual. Trevor turned out to be a
frighteningly effective hacker.It was a lot of fun working with Robert and Trevor. They're the two
most independent-minded people 
I know, and in completely different
ways. If you could see inside Rtm's brain it would look like a
colonial New England church, and if you could see inside Trevor's
it would look like the worst excesses of Austrian Rococo.We opened for business, with 6 stores, in January 1996. It was just
as well we waited a few months, because although we worried we were
late, we were actually almost fatally early. There was a lot of
talk in the press then about ecommerce, but not many people actually
wanted online stores.
[8]There were three main parts to the software: the editor, which
people used to build sites and which I wrote, the shopping cart,
which Robert wrote, and the manager, which kept track of orders and
statistics, and which Trevor wrote. In its time, the editor was one
of the best general-purpose site builders. I kept the code tight
and didn't have to integrate with any other software except Robert's
and Trevor's, so it was quite fun to work on. If all I'd had to do
was work on this software, the next 3 years would have been the
easiest of my life. Unfortunately I had to do a lot more, all of
it stuff I was worse at than programming, and the next 3 years were
instead the most stressful.There were a lot of startups making ecommerce software in the second
half of the 90s. We were determined to be the Microsoft Word, not
the Interleaf. Which meant being easy to use and inexpensive. It
was lucky for us that we were poor, because that caused us to make
Viaweb even more inexpensive than we realized. We charged $100 a
month for a small store and $300 a month for a big one. This low
price was a big attraction, and a constant thorn in the sides of
competitors, but it wasn't because of some clever insight that we
set the price low. We had no idea what businesses paid for things.
$300 a month seemed like a lot of money to us.We did a lot of things right by accident like that. For example,
we did what's now called ""doing things that 
don't scale,"" although
at the time we would have described it as ""being so lame that we're
driven to the most desperate measures to get users."" The most common
of which was building stores for them. This seemed particularly
humiliating, since the whole raison d'etre of our software was that
people could use it to make their own stores. But anything to get
users.We learned a lot more about retail than we wanted to know. For
example, that if you could only have a small image of a man's shirt
(and all images were small then by present standards), it was better
to have a closeup of the collar than a picture of the whole shirt.
The reason I remember learning this was that it meant I had to
rescan about 30 images of men's shirts. My first set of scans were
so beautiful too.Though this felt wrong, it was exactly the right thing to be doing.
Building stores for users taught us about retail, and about how it
felt to use our software. I was initially both mystified and repelled
by ""business"" and thought we needed a ""business person"" to be in
charge of it, but once we started to get users, I was converted,
in much the same way I was converted to 
fatherhood once I had kids.
Whatever users wanted, I was all theirs. Maybe one day we'd have
so many users that I couldn't scan their images for them, but in
the meantime there was nothing more important to do.Another thing I didn't get at the time is that 
growth rate is the
ultimate test of a startup. Our growth rate was fine. We had about
70 stores at the end of 1996 and about 500 at the end of 1997. I
mistakenly thought the thing that mattered was the absolute number
of users. And that is the thing that matters in the sense that
that's how much money you're making, and if you're not making enough,
you might go out of business. But in the long term the growth rate
takes care of the absolute number. If we'd been a startup I was
advising at Y Combinator, I would have said: Stop being so stressed
out, because you're doing fine. You're growing 7x a year. Just don't
hire too many more people and you'll soon be profitable, and then
you'll control your own destiny.Alas I hired lots more people, partly because our investors wanted
me to, and partly because that's what startups did during the
Internet Bubble. A company with just a handful of employees would
have seemed amateurish. So we didn't reach breakeven until about
when Yahoo bought us in the summer of 1998. Which in turn meant we
were at the mercy of investors for the entire life of the company.
And since both we and our investors were noobs at startups, the
result was a mess even by startup standards.It was a huge relief when Yahoo bought us. In principle our Viaweb
stock was valuable. It was a share in a business that was profitable
and growing rapidly. But it didn't feel very valuable to me; I had
no idea how to value a business, but I was all too keenly aware of
the near-death experiences we seemed to have every few months. Nor
had I changed my grad student lifestyle significantly since we
started. So when Yahoo bought us it felt like going from rags to
riches. Since we were going to California, I bought a car, a yellow
1998 VW GTI. I remember thinking that its leather seats alone were
by far the most luxurious thing I owned.The next year, from the summer of 1998 to the summer of 1999, must
have been the least productive of my life. I didn't realize it at
the time, but I was worn out from the effort and stress of running
Viaweb. For a while after I got to California I tried to continue
my usual m.o. of programming till 3 in the morning, but fatigue
combined with Yahoo's prematurely aged
culture and grim cube farm
in Santa Clara gradually dragged me down. After a few months it
felt disconcertingly like working at Interleaf.Yahoo had given us a lot of options when they bought us. At the
time I thought Yahoo was so overvalued that they'd never be worth
anything, but to my astonishment the stock went up 5x in the next
year. I hung on till the first chunk of options vested, then in the
summer of 1999 I left. It had been so long since I'd painted anything
that I'd half forgotten why I was doing this. My brain had been
entirely full of software and men's shirts for 4 years. But I had
done this to get rich so I could paint, I reminded myself, and now
I was rich, so I should go paint.When I said I was leaving, my boss at Yahoo had a long conversation
with me about my plans. I told him all about the kinds of pictures
I wanted to paint. At the time I was touched that he took such an
interest in me. Now I realize it was because he thought I was lying.
My options at that point were worth about $2 million a month. If I
was leaving that kind of money on the table, it could only be to
go and start some new startup, and if I did, I might take people
with me. This was the height of the Internet Bubble, and Yahoo was
ground zero of it. My boss was at that moment a billionaire. Leaving
then to start a new startup must have seemed to him an insanely,
and yet also plausibly, ambitious plan.But I really was quitting to paint, and I started immediately.
There was no time to lose. I'd already burned 4 years getting rich.
Now when I talk to founders who are leaving after selling their
companies, my advice is always the same: take a vacation. That's
what I should have done, just gone off somewhere and done nothing
for a month or two, but the idea never occurred to me.So I tried to paint, but I just didn't seem to have any energy or
ambition. Part of the problem was that I didn't know many people
in California. I'd compounded this problem by buying a house up in
the Santa Cruz Mountains, with a beautiful view but miles from
anywhere. I stuck it out for a few more months, then in desperation
I went back to New York, where unless you understand about rent
control you'll be surprised to hear I still had my apartment, sealed
up like a tomb of my old life. Idelle was in New York at least, and
there were other people trying to paint there, even though I didn't
know any of them.When I got back to New York I resumed my old life, except now I was
rich. It was as weird as it sounds. I resumed all my old patterns,
except now there were doors where there hadn't been. Now when I was
tired of walking, all I had to do was raise my hand, and (unless
it was raining) a taxi would stop to pick me up. Now when I walked
past charming little restaurants I could go in and order lunch. It
was exciting for a while. Painting started to go better. I experimented
with a new kind of still life where I'd paint one painting in the
old way, then photograph it and print it, blown up, on canvas, and
then use that as the underpainting for a second still life, painted
from the same objects (which hopefully hadn't rotted yet).Meanwhile I looked for an apartment to buy. Now I could actually
choose what neighborhood to live in. Where, I asked myself and
various real estate agents, is the Cambridge of New York? Aided by
occasional visits to actual Cambridge, I gradually realized there
wasn't one. Huh.Around this time, in the spring of 2000, I had an idea. It was clear
from our experience with Viaweb that web apps were the future. Why
not build a web app for making web apps? Why not let people edit
code on our server through the browser, and then host the resulting
applications for them?
[9]
You could run all sorts of services
on the servers that these applications could use just by making an
API call: making and receiving phone calls, manipulating images,
taking credit card payments, etc.I got so excited about this idea that I couldn't think about anything
else. It seemed obvious that this was the future. I didn't particularly
want to start another company, but it was clear that this idea would
have to be embodied as one, so I decided to move to Cambridge and
start it. I hoped to lure Robert into working on it with me, but
there I ran into a hitch. Robert was now a postdoc at MIT, and
though he'd made a lot of money the last time I'd lured him into
working on one of my schemes, it had also been a huge time sink.
So while he agreed that it sounded like a plausible idea, he firmly
refused to work on it.Hmph. Well, I'd do it myself then. I recruited Dan Giffin, who had
worked for Viaweb, and two undergrads who wanted summer jobs, and
we got to work trying to build what it's now clear is about twenty
companies and several open source projects worth of software. The
language for defining applications would of course be a dialect of
Lisp. But I wasn't so naive as to assume I could spring an overt
Lisp on a general audience; we'd hide the parentheses, like Dylan
did.By then there was a name for the kind of company Viaweb was, an
""application service provider,"" or ASP. This name didn't last long
before it was replaced by ""software as a service,"" but it was current
for long enough that I named this new company after it: it was going
to be called Aspra.I started working on the application builder, Dan worked on network
infrastructure, and the two undergrads worked on the first two
services (images and phone calls). But about halfway through the
summer I realized I really didn't want to run a company — especially
not a big one, which it was looking like this would have to be. I'd
only started Viaweb because I needed the money. Now that I didn't
need money anymore, why was I doing this? If this vision had to be
realized as a company, then screw the vision. I'd build a subset
that could be done as an open source project.Much to my surprise, the time I spent working on this stuff was not
wasted after all. After we started Y Combinator, I would often
encounter startups working on parts of this new architecture, and
it was very useful to have spent so much time thinking about it and
even trying to write some of it.The subset I would build as an open source project was the new Lisp,
whose parentheses I now wouldn't even have to hide. A lot of Lisp
hackers dream of building a new Lisp, partly because one of the
distinctive features of the language is that it has dialects, and
partly, I think, because we have in our minds a Platonic form of
Lisp that all existing dialects fall short of. I certainly did. So
at the end of the summer Dan and I switched to working on this new
dialect of Lisp, which I called Arc, in a house I bought in Cambridge.The following spring, lightning struck. I was invited to give a
talk at a Lisp conference, so I gave one about how we'd used Lisp
at Viaweb. Afterward I put a postscript file of this talk online,
on paulgraham.com, which I'd created years before using Viaweb but
had never used for anything. In one day it got 30,000 page views.
What on earth had happened? The referring urls showed that someone
had posted it on Slashdot.
[10]Wow, I thought, there's an audience. If I write something and put
it on the web, anyone can read it. That may seem obvious now, but
it was surprising then. In the print era there was a narrow channel
to readers, guarded by fierce monsters known as editors. The only
way to get an audience for anything you wrote was to get it published
as a book, or in a newspaper or magazine. Now anyone could publish
anything.This had been possible in principle since 1993, but not many people
had realized it yet. I had been intimately involved with building
the infrastructure of the web for most of that time, and a writer
as well, and it had taken me 8 years to realize it. Even then it
took me several years to understand the implications. It meant there
would be a whole new generation of 
essays.
[11]In the print era, the channel for publishing essays had been
vanishingly small. Except for a few officially anointed thinkers
who went to the right parties in New York, the only people allowed
to publish essays were specialists writing about their specialties.
There were so many essays that had never been written, because there
had been no way to publish them. Now they could be, and I was going
to write them.
[12]I've worked on several different things, but to the extent there
was a turning point where I figured out what to work on, it was
when I started publishing essays online. From then on I knew that
whatever else I did, I'd always write essays too.I knew that online essays would be a 
marginal medium at first.
Socially they'd seem more like rants posted by nutjobs on their
GeoCities sites than the genteel and beautifully typeset compositions
published in The New Yorker. But by this point I knew enough to
find that encouraging instead of discouraging.One of the most conspicuous patterns I've noticed in my life is how
well it has worked, for me at least, to work on things that weren't
prestigious. Still life has always been the least prestigious form
of painting. Viaweb and Y Combinator both seemed lame when we started
them. I still get the glassy eye from strangers when they ask what
I'm writing, and I explain that it's an essay I'm going to publish
on my web site. Even Lisp, though prestigious intellectually in
something like the way Latin is, also seems about as hip.It's not that unprestigious types of work are good per se. But when
you find yourself drawn to some kind of work despite its current
lack of prestige, it's a sign both that there's something real to
be discovered there, and that you have the right kind of motives.
Impure motives are a big danger for the ambitious. If anything is
going to lead you astray, it will be the desire to impress people.
So while working on things that aren't prestigious doesn't guarantee
you're on the right track, it at least guarantees you're not on the
most common type of wrong one.Over the next several years I wrote lots of essays about all kinds
of different topics. O'Reilly reprinted a collection of them as a
book, called Hackers & Painters after one of the essays in it. I
also worked on spam filters, and did some more painting. I used to
have dinners for a group of friends every thursday night, which
taught me how to cook for groups. And I bought another building in
Cambridge, a former candy factory (and later, twas said, porn
studio), to use as an office.One night in October 2003 there was a big party at my house. It was
a clever idea of my friend Maria Daniels, who was one of the thursday
diners. Three separate hosts would all invite their friends to one
party. So for every guest, two thirds of the other guests would be
people they didn't know but would probably like. One of the guests
was someone I didn't know but would turn out to like a lot: a woman
called Jessica Livingston. A couple days later I asked her out.Jessica was in charge of marketing at a Boston investment bank.
This bank thought it understood startups, but over the next year,
as she met friends of mine from the startup world, she was surprised
how different reality was. And how colorful their stories were. So
she decided to compile a book of 
interviews with startup founders.When the bank had financial problems and she had to fire half her
staff, she started looking for a new job. In early 2005 she interviewed
for a marketing job at a Boston VC firm. It took them weeks to make
up their minds, and during this time I started telling her about
all the things that needed to be fixed about venture capital. They
should make a larger number of smaller investments instead of a
handful of giant ones, they should be funding younger, more technical
founders instead of MBAs, they should let the founders remain as
CEO, and so on.One of my tricks for writing essays had always been to give talks.
The prospect of having to stand up in front of a group of people
and tell them something that won't waste their time is a great
spur to the imagination. When the Harvard Computer Society, the
undergrad computer club, asked me to give a talk, I decided I would
tell them how to start a startup. Maybe they'd be able to avoid the
worst of the mistakes we'd made.So I gave this talk, in the course of which I told them that the
best sources of seed funding were successful startup founders,
because then they'd be sources of advice too. Whereupon it seemed
they were all looking expectantly at me. Horrified at the prospect
of having my inbox flooded by business plans (if I'd only known),
I blurted out ""But not me!"" and went on with the talk. But afterward
it occurred to me that I should really stop procrastinating about
angel investing. I'd been meaning to since Yahoo bought us, and now
it was 7 years later and I still hadn't done one angel investment.Meanwhile I had been scheming with Robert and Trevor about projects
we could work on together. I missed working with them, and it seemed
like there had to be something we could collaborate on.As Jessica and I were walking home from dinner on March 11, at the
corner of Garden and Walker streets, these three threads converged.
Screw the VCs who were taking so long to make up their minds. We'd
start our own investment firm and actually implement the ideas we'd
been talking about. I'd fund it, and Jessica could quit her job and
work for it, and we'd get Robert and Trevor as partners too.
[13]Once again, ignorance worked in our favor. We had no idea how to
be angel investors, and in Boston in 2005 there were no Ron Conways
to learn from. So we just made what seemed like the obvious choices,
and some of the things we did turned out to be novel.There are multiple components to Y Combinator, and we didn't figure
them all out at once. The part we got first was to be an angel firm.
In those days, those two words didn't go together. There were VC
firms, which were organized companies with people whose job it was
to make investments, but they only did big, million dollar investments.
And there were angels, who did smaller investments, but these were
individuals who were usually focused on other things and made
investments on the side. And neither of them helped founders enough
in the beginning. We knew how helpless founders were in some respects,
because we remembered how helpless we'd been. For example, one thing
Julian had done for us that seemed to us like magic was to get us
set up as a company. We were fine writing fairly difficult software,
but actually getting incorporated, with bylaws and stock and all
that stuff, how on earth did you do that? Our plan was not only to
make seed investments, but to do for startups everything Julian had
done for us.YC was not organized as a fund. It was cheap enough to run that we
funded it with our own money. That went right by 99% of readers,
but professional investors are thinking ""Wow, that means they got
all the returns."" But once again, this was not due to any particular
insight on our part. We didn't know how VC firms were organized.
It never occurred to us to try to raise a fund, and if it had, we
wouldn't have known where to start.
[14]The most distinctive thing about YC is the batch model: to fund a
bunch of startups all at once, twice a year, and then to spend three
months focusing intensively on trying to help them. That part we
discovered by accident, not merely implicitly but explicitly due
to our ignorance about investing. We needed to get experience as
investors. What better way, we thought, than to fund a whole bunch
of startups at once? We knew undergrads got temporary jobs at tech
companies during the summer. Why not organize a summer program where
they'd start startups instead? We wouldn't feel guilty for being
in a sense fake investors, because they would in a similar sense
be fake founders. So while we probably wouldn't make much money out
of it, we'd at least get to practice being investors on them, and
they for their part would probably have a more interesting summer
than they would working at Microsoft.We'd use the building I owned in Cambridge as our headquarters.
We'd all have dinner there once a week — on tuesdays, since I was
already cooking for the thursday diners on thursdays — and after
dinner we'd bring in experts on startups to give talks.We knew undergrads were deciding then about summer jobs, so in a
matter of days we cooked up something we called the Summer Founders
Program, and I posted an 
announcement 
on my site, inviting undergrads
to apply. I had never imagined that writing essays would be a way
to get ""deal flow,"" as investors call it, but it turned out to be
the perfect source.
[15]
We got 225 applications for the Summer
Founders Program, and we were surprised to find that a lot of them
were from people who'd already graduated, or were about to that
spring. Already this SFP thing was starting to feel more serious
than we'd intended.We invited about 20 of the 225 groups to interview in person, and
from those we picked 8 to fund. They were an impressive group. That
first batch included reddit, Justin Kan and Emmett Shear, who went
on to found Twitch, Aaron Swartz, who had already helped write the
RSS spec and would a few years later become a martyr for open access,
and Sam Altman, who would later become the second president of YC.
I don't think it was entirely luck that the first batch was so good.
You had to be pretty bold to sign up for a weird thing like the
Summer Founders Program instead of a summer job at a legit place
like Microsoft or Goldman Sachs.The deal for startups was based on a combination of the deal we did
with Julian ($10k for 10%) and what Robert said MIT grad students
got for the summer ($6k). We invested $6k per founder, which in the
typical two-founder case was $12k, in return for 6%. That had to
be fair, because it was twice as good as the deal we ourselves had
taken. Plus that first summer, which was really hot, Jessica brought
the founders free air conditioners.
[16]Fairly quickly I realized that we had stumbled upon the way to scale
startup funding. Funding startups in batches was more convenient
for us, because it meant we could do things for a lot of startups
at once, but being part of a batch was better for the startups too.
It solved one of the biggest problems faced by founders: the
isolation. Now you not only had colleagues, but colleagues who
understood the problems you were facing and could tell you how they
were solving them.As YC grew, we started to notice other advantages of scale. The
alumni became a tight community, dedicated to helping one another,
and especially the current batch, whose shoes they remembered being
in. We also noticed that the startups were becoming one another's
customers. We used to refer jokingly to the ""YC GDP,"" but as YC
grows this becomes less and less of a joke. Now lots of startups
get their initial set of customers almost entirely from among their
batchmates.I had not originally intended YC to be a full-time job. I was going
to do three things: hack, write essays, and work on YC. As YC grew,
and I grew more excited about it, it started to take up a lot more
than a third of my attention. But for the first few years I was
still able to work on other things.In the summer of 2006, Robert and I started working on a new version
of Arc. This one was reasonably fast, because it was compiled into
Scheme. To test this new Arc, I wrote Hacker News in it. It was
originally meant to be a news aggregator for startup founders and
was called Startup News, but after a few months I got tired of
reading about nothing but startups. Plus it wasn't startup founders
we wanted to reach. It was future startup founders. So I changed
the name to Hacker News and the topic to whatever engaged one's
intellectual curiosity.HN was no doubt good for YC, but it was also by far the biggest
source of stress for me. If all I'd had to do was select and help
founders, life would have been so easy. And that implies that HN
was a mistake. Surely the biggest source of stress in one's work
should at least be something close to the core of the work. Whereas
I was like someone who was in pain while running a marathon not
from the exertion of running, but because I had a blister from an
ill-fitting shoe. When I was dealing with some urgent problem during
YC, there was about a 60% chance it had to do with HN, and a 40%
chance it had do with everything else combined.
[17]As well as HN, I wrote all of YC's internal software in Arc. But
while I continued to work a good deal in Arc, I gradually stopped
working on Arc, partly because I didn't have time to, and partly
because it was a lot less attractive to mess around with the language
now that we had all this infrastructure depending on it. So now my
three projects were reduced to two: writing essays and working on
YC.YC was different from other kinds of work I've done. Instead of
deciding for myself what to work on, the problems came to me. Every
6 months there was a new batch of startups, and their problems,
whatever they were, became our problems. It was very engaging work,
because their problems were quite varied, and the good founders
were very effective. If you were trying to learn the most you could
about startups in the shortest possible time, you couldn't have
picked a better way to do it.There were parts of the job I didn't like. Disputes between cofounders,
figuring out when people were lying to us, fighting with people who
maltreated the startups, and so on. But I worked hard even at the
parts I didn't like. I was haunted by something Kevin Hale once
said about companies: ""No one works harder than the boss."" He meant
it both descriptively and prescriptively, and it was the second
part that scared me. I wanted YC to be good, so if how hard I worked
set the upper bound on how hard everyone else worked, I'd better
work very hard.One day in 2010, when he was visiting California for interviews,
Robert Morris did something astonishing: he offered me unsolicited
advice. I can only remember him doing that once before. One day at
Viaweb, when I was bent over double from a kidney stone, he suggested
that it would be a good idea for him to take me to the hospital.
That was what it took for Rtm to offer unsolicited advice. So I
remember his exact words very clearly. ""You know,"" he said, ""you
should make sure Y Combinator isn't the last cool thing you do.""At the time I didn't understand what he meant, but gradually it
dawned on me that he was saying I should quit. This seemed strange
advice, because YC was doing great. But if there was one thing rarer
than Rtm offering advice, it was Rtm being wrong. So this set me
thinking. It was true that on my current trajectory, YC would be
the last thing I did, because it was only taking up more of my
attention. It had already eaten Arc, and was in the process of
eating essays too. Either YC was my life's work or I'd have to leave
eventually. And it wasn't, so I would.In the summer of 2012 my mother had a stroke, and the cause turned
out to be a blood clot caused by colon cancer. The stroke destroyed
her balance, and she was put in a nursing home, but she really
wanted to get out of it and back to her house, and my sister and I
were determined to help her do it. I used to fly up to Oregon to
visit her regularly, and I had a lot of time to think on those
flights. On one of them I realized I was ready to hand YC over to
someone else.I asked Jessica if she wanted to be president, but she didn't, so
we decided we'd try to recruit Sam Altman. We talked to Robert and
Trevor and we agreed to make it a complete changing of the guard.
Up till that point YC had been controlled by the original LLC we
four had started. But we wanted YC to last for a long time, and to
do that it couldn't be controlled by the founders. So if Sam said
yes, we'd let him reorganize YC. Robert and I would retire, and
Jessica and Trevor would become ordinary partners.When we asked Sam if he wanted to be president of YC, initially he
said no. He wanted to start a startup to make nuclear reactors.
But I kept at it, and in October 2013 he finally agreed. We decided
he'd take over starting with the winter 2014 batch. For the rest
of 2013 I left running YC more and more to Sam, partly so he could
learn the job, and partly because I was focused on my mother, whose
cancer had returned.She died on January 15, 2014. We knew this was coming, but it was
still hard when it did.I kept working on YC till March, to help get that batch of startups
through Demo Day, then I checked out pretty completely. (I still
talk to alumni and to new startups working on things I'm interested
in, but that only takes a few hours a week.)What should I do next? Rtm's advice hadn't included anything about
that. I wanted to do something completely different, so I decided
I'd paint. I wanted to see how good I could get if I really focused
on it. So the day after I stopped working on YC, I started painting.
I was rusty and it took a while to get back into shape, but it was
at least completely engaging.
[18]I spent most of the rest of 2014 painting. I'd never been able to
work so uninterruptedly before, and I got to be better than I had
been. Not good enough, but better. Then in November, right in the
middle of a painting, I ran out of steam. Up till that point I'd
always been curious to see how the painting I was working on would
turn out, but suddenly finishing this one seemed like a chore. So
I stopped working on it and cleaned my brushes and haven't painted
since. So far anyway.I realize that sounds rather wimpy. But attention is a zero sum
game. If you can choose what to work on, and you choose a project
that's not the best one (or at least a good one) for you, then it's
getting in the way of another project that is. And at 50 there was
some opportunity cost to screwing around.I started writing essays again, and wrote a bunch of new ones over
the next few months. I even wrote a couple that 
weren't about
startups. Then in March 2015 I started working on Lisp again.The distinctive thing about Lisp is that its core is a language
defined by writing an interpreter in itself. It wasn't originally
intended as a programming language in the ordinary sense. It was
meant to be a formal model of computation, an alternative to the
Turing machine. If you want to write an interpreter for a language
in itself, what's the minimum set of predefined operators you need?
The Lisp that John McCarthy invented, or more accurately discovered,
is an answer to that question.
[19]McCarthy didn't realize this Lisp could even be used to program
computers till his grad student Steve Russell suggested it. Russell
translated McCarthy's interpreter into IBM 704 machine language,
and from that point Lisp started also to be a programming language
in the ordinary sense. But its origins as a model of computation
gave it a power and elegance that other languages couldn't match.
It was this that attracted me in college, though I didn't understand
why at the time.McCarthy's 1960 Lisp did nothing more than interpret Lisp expressions.
It was missing a lot of things you'd want in a programming language.
So these had to be added, and when they were, they weren't defined
using McCarthy's original axiomatic approach. That wouldn't have
been feasible at the time. McCarthy tested his interpreter by
hand-simulating the execution of programs. But it was already getting
close to the limit of interpreters you could test that way — indeed,
there was a bug in it that McCarthy had overlooked. To test a more
complicated interpreter, you'd have had to run it, and computers
then weren't powerful enough.Now they are, though. Now you could continue using McCarthy's
axiomatic approach till you'd defined a complete programming language.
And as long as every change you made to McCarthy's Lisp was a
discoveredness-preserving transformation, you could, in principle,
end up with a complete language that had this quality. Harder to
do than to talk about, of course, but if it was possible in principle,
why not try? So I decided to take a shot at it. It took 4 years,
from March 26, 2015 to October 12, 2019. It was fortunate that I
had a precisely defined goal, or it would have been hard to keep
at it for so long.I wrote this new Lisp, called Bel, 
in itself in Arc. That may sound
like a contradiction, but it's an indication of the sort of trickery
I had to engage in to make this work. By means of an egregious
collection of hacks I managed to make something close enough to an
interpreter written in itself that could actually run. Not fast,
but fast enough to test.I had to ban myself from writing essays during most of this time,
or I'd never have finished. In late 2015 I spent 3 months writing
essays, and when I went back to working on Bel I could barely
understand the code. Not so much because it was badly written as
because the problem is so convoluted. When you're working on an
interpreter written in itself, it's hard to keep track of what's
happening at what level, and errors can be practically encrypted
by the time you get them.So I said no more essays till Bel was done. But I told few people
about Bel while I was working on it. So for years it must have
seemed that I was doing nothing, when in fact I was working harder
than I'd ever worked on anything. Occasionally after wrestling for
hours with some gruesome bug I'd check Twitter or HN and see someone
asking ""Does Paul Graham still code?""Working on Bel was hard but satisfying. I worked on it so intensively
that at any given time I had a decent chunk of the code in my head
and could write more there. I remember taking the boys to the
coast on a sunny day in 2015 and figuring out how to deal with some
problem involving continuations while I watched them play in the
tide pools. It felt like I was doing life right. I remember that
because I was slightly dismayed at how novel it felt. The good news
is that I had more moments like this over the next few years.In the summer of 2016 we moved to England. We wanted our kids to
see what it was like living in another country, and since I was a
British citizen by birth, that seemed the obvious choice. We only
meant to stay for a year, but we liked it so much that we still
live there. So most of Bel was written in England.In the fall of 2019, Bel was finally finished. Like McCarthy's
original Lisp, it's a spec rather than an implementation, although
like McCarthy's Lisp it's a spec expressed as code.Now that I could write essays again, I wrote a bunch about topics
I'd had stacked up. I kept writing essays through 2020, but I also
started to think about other things I could work on. How should I
choose what to do? Well, how had I chosen what to work on in the
past? I wrote an essay for myself to answer that question, and I
was surprised how long and messy the answer turned out to be. If
this surprised me, who'd lived it, then I thought perhaps it would
be interesting to other people, and encouraging to those with
similarly messy lives. So I wrote a more detailed version for others
to read, and this is the last sentence of it.
Notes[1]
My experience skipped a step in the evolution of computers:
time-sharing machines with interactive OSes. I went straight from
batch processing to microcomputers, which made microcomputers seem
all the more exciting.[2]
Italian words for abstract concepts can nearly always be
predicted from their English cognates (except for occasional traps
like polluzione). It's the everyday words that differ. So if you
string together a lot of abstract concepts with a few simple verbs,
you can make a little Italian go a long way.[3]
I lived at Piazza San Felice 4, so my walk to the Accademia
went straight down the spine of old Florence: past the Pitti, across
the bridge, past Orsanmichele, between the Duomo and the Baptistery,
and then up Via Ricasoli to Piazza San Marco. I saw Florence at
street level in every possible condition, from empty dark winter
evenings to sweltering summer days when the streets were packed with
tourists.[4]
You can of course paint people like still lives if you want
to, and they're willing. That sort of portrait is arguably the apex
of still life painting, though the long sitting does tend to produce
pained expressions in the sitters.[5]
Interleaf was one of many companies that had smart people and
built impressive technology, and yet got crushed by Moore's Law.
In the 1990s the exponential growth in the power of commodity (i.e.
Intel) processors rolled up high-end, special-purpose hardware and
software companies like a bulldozer.[6]
The signature style seekers at RISD weren't specifically
mercenary. In the art world, money and coolness are tightly coupled.
Anything expensive comes to be seen as cool, and anything seen as
cool will soon become equally expensive.[7]
Technically the apartment wasn't rent-controlled but
rent-stabilized, but this is a refinement only New Yorkers would
know or care about. The point is that it was really cheap, less
than half market price.[8]
Most software you can launch as soon as it's done. But when
the software is an online store builder and you're hosting the
stores, if you don't have any users yet, that fact will be painfully
obvious. So before we could launch publicly we had to launch
privately, in the sense of recruiting an initial set of users and
making sure they had decent-looking stores.[9]
We'd had a code editor in Viaweb for users to define their
own page styles. They didn't know it, but they were editing Lisp
expressions underneath. But this wasn't an app editor, because the
code ran when the merchants' sites were generated, not when shoppers
visited them.[10]
This was the first instance of what is now a familiar experience,
and so was what happened next, when I read the comments and found
they were full of angry people. How could I claim that Lisp was
better than other languages? Weren't they all Turing complete?
People who see the responses to essays I write sometimes tell me
how sorry they feel for me, but I'm not exaggerating when I reply
that it has always been like this, since the very beginning. It
comes with the territory. An essay must tell readers things they
don't already know, and some 
people dislike being told such things.[11]
People put plenty of stuff on the internet in the 90s of
course, but putting something online is not the same as publishing
it online. Publishing online means you treat the online version as
the (or at least a) primary version.[12]
There is a general lesson here that our experience with Y
Combinator also teaches: Customs continue to constrain you long
after the restrictions that caused them have disappeared. Customary
VC practice had once, like the customs about publishing essays,
been based on real constraints. Startups had once been much more
expensive to start, and proportionally rare. Now they could be cheap
and common, but the VCs' customs still reflected the old world,
just as customs about writing essays still reflected the constraints
of the print era.Which in turn implies that people who are independent-minded (i.e.
less influenced by custom) will have an advantage in fields affected
by rapid change (where customs are more likely to be obsolete).Here's an interesting point, though: you can't always predict which
fields will be affected by rapid change. Obviously software and
venture capital will be, but who would have predicted that essay
writing would be?[13]
Y Combinator was not the original name. At first we were
called Cambridge Seed. But we didn't want a regional name, in case
someone copied us in Silicon Valley, so we renamed ourselves after
one of the coolest tricks in the lambda calculus, the Y combinator.I picked orange as our color partly because it's the warmest, and
partly because no VC used it. In 2005 all the VCs used staid colors
like maroon, navy blue, and forest green, because they were trying
to appeal to LPs, not founders. The YC logo itself is an inside
joke: the Viaweb logo had been a white V on a red circle, so I made
the YC logo a white Y on an orange square.[14]
YC did become a fund for a couple years starting in 2009,
because it was getting so big I could no longer afford to fund it
personally. But after Heroku got bought we had enough money to go
back to being self-funded.[15]
I've never liked the term ""deal flow,"" because it implies
that the number of new startups at any given time is fixed. This
is not only false, but it's the purpose of YC to falsify it, by
causing startups to be founded that would not otherwise have existed.[16]
She reports that they were all different shapes and sizes,
because there was a run on air conditioners and she had to get
whatever she could, but that they were all heavier than she could
carry now.[17]
Another problem with HN was a bizarre edge case that occurs
when you both write essays and run a forum. When you run a forum,
you're assumed to see if not every conversation, at least every
conversation involving you. And when you write essays, people post
highly imaginative misinterpretations of them on forums. Individually
these two phenomena are tedious but bearable, but the combination
is disastrous. You actually have to respond to the misinterpretations,
because the assumption that you're present in the conversation means
that not responding to any sufficiently upvoted misinterpretation
reads as a tacit admission that it's correct. But that in turn
encourages more; anyone who wants to pick a fight with you senses
that now is their chance.[18]
The worst thing about leaving YC was not working with Jessica
anymore. We'd been working on YC almost the whole time we'd known
each other, and we'd neither tried nor wanted to separate it from
our personal lives, so leaving was like pulling up a deeply rooted
tree.[19]
One way to get more precise about the concept of invented vs
discovered is to talk about space aliens. Any sufficiently advanced
alien civilization would certainly know about the Pythagorean
theorem, for example. I believe, though with less certainty, that
they would also know about the Lisp in McCarthy's 1960 paper.But if so there's no reason to suppose that this is the limit of
the language that might be known to them. Presumably aliens need
numbers and errors and I/O too. So it seems likely there exists at
least one path out of McCarthy's Lisp along which discoveredness
is preserved.Thanks to Trevor Blackwell, John Collison, Patrick Collison, Daniel
Gackle, Ralph Hazell, Jessica Livingston, Robert Morris, and Harj
Taggar for reading drafts of this.","personal experience report
",human,"human
","human
"
15,15,"April 2008There are some topics I save up because they'll be so much fun to
write about.  This is one of them: a list of my heroes.I'm not claiming this is a list of the n most admirable people.
Who could make such a list, even if they wanted to?Einstein isn't on the list, for example, even though he probably
deserves to be on any shortlist of admirable people.  I once asked
a physicist friend if Einstein was really as smart as his fame
implies, and she said that yes, he was.  So why isn't he on the
list?  Because I had to ask.  This is a list of people who've
influenced me, not people who would have if I understood their work.My test was to think of someone and ask ""is this person my
hero?""  It often returned surprising answers.  For example,
it returned false for Montaigne, who was arguably the inventor of
the essay.  Why?  When I thought
about what it meant to call someone a hero, it meant I'd decide what
to do by asking what they'd do in the same situation.  That's a 
stricter standard than admiration.After I made the list, I looked to see if there was a pattern, and
there was, a very clear one.  Everyone on the list had two qualities:
they cared almost excessively about their work, and they were
absolutely honest.  By honest I don't mean trustworthy so much as
that they never pander: they never say or do something because
that's what the audience wants.  They are all fundamentally subversive
for this reason, though they conceal it to varying degrees.
Jack LambertI grew up in Pittsburgh in the 1970s.  Unless you were there it's
hard to imagine how that town felt about the Steelers.   Locally,
all the news was bad.  The steel industry was dying.  But the
Steelers were the best team in football — and moreover, in a
way that seemed to reflect the personality of the city.  They didn't
do anything fancy.  They just got the job done.Other players were more famous: Terry Bradshaw, Franco Harris, Lynn
Swann.  But they played offense, and you always get more attention
for that.  It seemed to me as a twelve year old football expert
that the best of them all was 
Jack Lambert.  And what made him so
good was that he was utterly relentless.  He didn't just care about
playing well; he cared almost too much.  He seemed to regard it as
a personal insult when someone from the other team had possession
of the ball on his side of the line of scrimmage.The suburbs of Pittsburgh in the 1970s were a pretty dull place.
School was boring.  All the adults around were bored with their
jobs working for big companies. Everything that came to us through
the mass media was (a) blandly uniform and (b) produced elsewhere.
Jack Lambert was the exception.  He was like nothing else I'd seen.
Kenneth ClarkKenneth Clark is the best nonfiction writer I know of, on any
subject.  Most people who write about art history don't really like
art; you can tell from a thousand little signs.  But Clark did, and
not just intellectually, but the way one anticipates a delicious
dinner.What really makes him stand out, though, is the quality of his
ideas.  His style is deceptively casual, but there is more in 
his books than in a library
of art monographs.  Reading 
The Nude is like a ride in a
Ferrari.  Just as you're getting settled, you're slammed back in
your seat by the acceleration.  Before you can adjust, you're thrown
sideways as the car screeches into the first turn.  His brain throws
off ideas almost too fast to grasp them.  Finally at the end of the
chapter you come to a halt, with your eyes wide and a big smile on
your face.Kenneth Clark was a star in his day, thanks to the documentary
series 
Civilisation.  And if you read only one book about
art history, 
Civilisation is the one I'd recommend.  It's
much better than the drab Sears Catalogs of art that undergraduates
are forced to buy for Art History 101.
Larry MihalkoA lot of people have a great teacher at some point in their childhood.
Larry Mihalko was mine.  When I look back it's like there's a line
drawn between third and fourth grade.  After Mr. Mihalko, everything
was different.Why?  First of all, he was intellectually curious.  I had a few
other teachers who were smart, but I wouldn't describe them as
intellectually curious.  In retrospect, he was out of place as an
elementary school teacher, and I think he knew it.  That must have
been hard for him, but it was wonderful for us, his students.  His
class was a constant adventure.  I used to like going to school
every day.The other thing that made him different was that he liked us.  Kids
are good at telling that.  The other teachers were at best benevolently
indifferent.  But Mr.  Mihalko seemed like he actually wanted to
be our friend.  On the last day of fourth grade, he got out one of
the heavy school record players and played James Taylor's ""You've
Got a Friend"" to us.  Just call out my name, and you know wherever
I am, I'll come running.  He died at 59 of lung cancer.  I've never
cried like I cried at his funeral.
LeonardoOne of the things I've learned about making things that I didn't
realize when I was a kid is that much of the best stuff isn't made
for audiences, but for oneself.  You see paintings and drawings in
museums and imagine they were made for you to look at.  Actually a
lot of the best ones were made as a way of exploring the world, not
as a way to please other people.  The best of these explorations
are sometimes more pleasing than stuff made explicitly to please.Leonardo did a lot of things.  One of his most admirable qualities
was that he did so many different things that were admirable.  What
people know of him now is his paintings and his more flamboyant
inventions, like flying machines.  That makes him seem like some
kind of dreamer who sketched artists' conceptions of rocket ships
on the side.  In fact he made a large number of far more practical
technical discoveries.  He was as good an engineer as a painter.His most impressive work, to me, is his 
drawings.  They're clearly
made more as a way of studying the world than producing something
beautiful. And yet they can hold their own with any work of art
ever made.  No one else, before or since, was that good when no one
was looking.
Robert MorrisRobert Morris has a very unusual quality: he's never wrong.  It
might seem this would require you to be omniscient, but actually
it's surprisingly easy. Don't say anything unless you're fairly
sure of it.  If you're not omniscient, you just don't end up saying
much.More precisely, the trick is to pay careful attention to how you
qualify what you say.  By using this trick, Robert has, as far as
I know, managed to be mistaken only once, and that was when he was
an undergrad.  When the Mac came out, he said that little desktop
computers would never be suitable for real hacking.It's wrong to call it a trick in his case, though.  If it were a
conscious trick, he would have slipped in a moment of excitement.
With Robert this quality is wired-in.  He has an almost superhuman
integrity.  He's not just generally correct, but also correct about
how correct he is.You'd think it would be such a great thing never to be wrong that
everyone would do this.  It doesn't seem like that much extra work
to pay as much attention to the error on an idea as to the idea
itself.  And yet practically no one does.  I know how hard it is,
because since meeting Robert I've tried to do in software what he
seems to do in hardware.
P. G. WodehousePeople are finally starting to admit that Wodehouse was a great
writer.  If you want to be thought a great novelist in your own
time, you have to sound intellectual.  If what you write is popular,
or entertaining, or funny, you're ipso facto suspect.  That makes
Wodehouse doubly impressive, because it meant that to write as he
wanted to, he had to commit to being despised in his own lifetime.Evelyn Waugh called him a great writer, but to most people at the
time that would have read as a chivalrous or deliberately perverse
gesture. At the time any random autobiographical novel by a recent
college grad could count on more respectful treatment from the
literary establishment.Wodehouse may have begun with simple atoms, but the way he composed
them into molecules was near faultless.  His rhythm in particular.
It makes me self-conscious to write about it.  I can think of only
two other writers who came near him for style: Evelyn Waugh and
Nancy Mitford.  Those three used the English language like they
owned it.But Wodehouse has something neither of them did.  He's at ease.
Evelyn Waugh and Nancy Mitford cared what other people thought of
them: he wanted to seem aristocratic; she was afraid she wasn't
smart enough.  But Wodehouse didn't give a damn what anyone thought
of him.  He wrote exactly what he wanted.
Alexander CalderCalder's on this list because he makes me happy.  Can his work stand
up to Leonardo's?  Probably not.  There might not be anything from
the 20th Century that can.  But what was good about Modernism,
Calder had, and had in a way that he made seem effortless.What was good about Modernism was its freshness.  Art became stuffy
in the nineteenth century.  The paintings that were popular at the
time were mostly the art equivalent of McMansions—big,
pretentious, and fake.  Modernism meant starting over, making things
with the same earnest motives that children might.  The artists who
benefited most from this were the ones who had preserved a child's
confidence, like Klee and Calder.Klee was impressive because he could work in so many different
styles.  But between the two I like Calder better, because his work
seemed happier.  Ultimately the point of art is to engage the viewer.
It's hard to predict what will; often something that seems interesting
at first will bore you after a month.  Calder's 
sculptures never
get boring.  They just sit there quietly radiating optimism, like
a battery that never runs out.  As far as I can tell from books and
photographs, the happiness of Calder's work is his own happiness
showing through.
Jane AustenEveryone admires Jane Austen.  Add my name to the list.  To me she
seems the best novelist of all time.I'm interested in how things work.  When I read most novels, I pay
as much attention to the author's choices as to the story.  But in
her novels I can't see the gears at work.  Though I'd really like
to know how she does what she does, I can't figure it out, because
she's so good that her stories don't seem made up.  I feel like I'm
reading a description of something that actually happened.I used to read a lot of novels when I was younger.  I can't read
most anymore, because they don't have enough information in them.
Novels seem so impoverished compared to history and biography.  But 
reading Austen is like reading
nonfiction.  She writes so well you don't even notice her.
John McCarthyJohn McCarthy invented Lisp, the field of (or at least the term)
artificial intelligence, and was an early member of both of the top
two computer science departments, MIT and Stanford.  No one would
dispute that he's one of the greats, but he's an especial hero to
me because of 
Lisp.It's hard for us now to understand what a conceptual leap that was
at the time.  Paradoxically, one of the reasons his achievement is
hard to appreciate is that it was so successful.  Practically every
programming language invented in the last 20 years includes ideas
from Lisp, and each year the median language gets more Lisplike.In 1958 these ideas were anything but obvious.  In 1958 there seem
to have been two ways of thinking about programming.  Some people
thought of it as math, and proved things about Turing Machines.
Others thought of it as a way to get things done, and designed
languages all too influenced by the technology of the day.  McCarthy
alone bridged the gap.  He designed a language that was math.  But
designed is not really the word; discovered is more like it.
The SpitfireAs I was making this list I found myself thinking of people like
Douglas Bader 
and 
R.J. Mitchell
 and 
Jeffrey Quill and I realized
that though all of them had done many things in their lives, there
was one factor above all that connected them: the Spitfire.This is supposed to be a list of heroes.  How can a machine be on
it?  Because that machine was not just a machine.  It was a lens
of heroes.  Extraordinary devotion went into it, and extraordinary
courage came out.It's a cliche to call World War II a contest between good and evil,
but between fighter designs, it really was.  The Spitfire's original
nemesis, the ME 109, was a brutally practical plane.  It was a
killing machine.  The Spitfire was optimism embodied.  And not just
in its beautiful lines: it was at the edge of what could be
manufactured.  But taking the high road worked.  In the air, beauty
had the edge, just.
Steve JobsPeople alive when Kennedy was killed  usually remember exactly where
they were when they heard about it.  I remember exactly where I was
when a friend asked if I'd heard Steve Jobs had cancer.  It was
like the floor dropped out.   A few seconds later she told me that
it was a rare operable type, and that he'd be ok.  But those seconds
seemed long.I wasn't sure whether to include Jobs on this list.  A lot of people
at Apple seem to be afraid of him, which is a bad sign.  But he
compels admiration.There's no name for what Steve Jobs is, because there hasn't been
anyone quite like him before.  He doesn't design Apple's products
himself.  Historically the closest analogy to what he does are the
great Renaissance patrons of the arts.  As the CEO of a company,
that makes him unique.Most CEOs delegate 
taste to a subordinate.
The 
design paradox
 means they're choosing more or less at random.  But Steve
Jobs actually has taste himself — such good taste that he's shown
the world how much more important taste is than they realized.
Isaac NewtonNewton has a strange role in my pantheon of heroes: he's the one I
reproach myself with.  He worked on big things, at least for part
of his life.  It's so easy to get distracted working on small stuff.
The questions you're answering are pleasantly familiar.  You get
immediate rewards — in fact, you get bigger rewards in your
time if you work on matters of passing importance.  But I'm
uncomfortably aware that this is the route to well-deserved obscurity.To do really great things, you have to seek out questions people
didn't even realize were questions.  There have probably been other
people who did this as well as Newton, for their time, but Newton
is my model of this kind of thought.  I can just begin to understand
what it must have felt like for him.You only get one life.  Why not do something huge?  The phrase ""paradigm
shift"" is overused now, but Kuhn was onto something.  And you know
more are out there, separated from us by what will later seem a
surprisingly thin wall of laziness and stupidity.  If we work like
Newton.Thanks to Trevor Blackwell, Jessica Livingston, and Jackie McDonough for reading drafts of this.Japanese Translation","personal experience report
",human,"human
","human
"
16,16,"March 2006, rev August 2009A couple days ago I found to my surprise that I'd been granted a
patent.
It issued in 2003, but no one told me.  I wouldn't know about it
now except that a few months ago, while visiting Yahoo, I happened
to run into a Big Cheese I knew from working there in the late
nineties.  He brought up something called Revenue Loop, which Viaweb
had been working on when they bought us.The idea is basically that you sort search results not in order of
textual ""relevance"" (as search engines did then) nor in order of
how much advertisers bid (as Overture did) but in order of the bid
times the number of transactions.  Ordinarily you'd do this for
shopping searches, though in fact one of the features of our scheme
is that it automatically detects which searches are shopping searches.If you just order the results in order of bids, you can make the
search results useless, because the first results could be dominated
by lame sites that had bid the most.  But if you order results by
bid multiplied by transactions, far from selling out, you're getting
a better measure of relevance.  What could be a better sign that
someone was satisfied with a search result than going to the site
and buying something?And, of course, this algorithm automatically maximizes the revenue
of the search engine.Everyone is focused on this type of approach now, but few were in
1998.  In 1998 it was all about selling banner ads.  We didn't know
that, so we were pretty excited when we figured out what seemed to
us the optimal way of doing shopping searches.When Yahoo was thinking of buying us, we had a meeting with Jerry
Yang in New York.  For him, I now realize, this was supposed to be
one of those meetings when you check out a company you've pretty
much decided to buy, just to make sure they're ok guys.  We weren't
expected to do more than chat and seem smart and reasonable.  He
must have been dismayed when I jumped up to the whiteboard and
launched into a presentation of our exciting new technology.I was just as dismayed when he didn't seem to care at all about it.
At the time I thought, ""boy, is this guy poker-faced.  We present
to him what has to be the optimal way of sorting product search
results, and he's not even curious.""  I didn't realize till much later
why he didn't care.  In 1998, advertisers were overpaying enormously
for ads on web sites.  
In 1998, if advertisers paid the maximum that traffic was worth to
them, Yahoo's revenues would have decreased.Things are different now, of course.  Now this sort of thing is all
the rage.  So when I ran into the Yahoo exec I knew from the old
days in the Yahoo cafeteria a few months ago, the first thing he
remembered was not (fortunately) all the fights I had with him, but
Revenue Loop.""Well,"" I said, ""I think we actually applied for a patent on it.
I'm not sure what happened to the application after I left.""""Really?  That would be an important patent.""So someone investigated, and sure enough, that patent application
had continued in the pipeline for several years after, and finally
issued in 2003.The main thing that struck me on reading it, actually, is that
lawyers at some point messed up my nice clear writing.  Some clever
person with a spell checker reduced one section to Zen-like incomprehensibility:

  Also, common spelling errors will tend to get fixed. For example,
  if users searching for ""compact disc player"" end up spending
  considerable money at sites offering compact disc players, then
  those pages will have a higher relevance for that search phrase,
  even though the phrase ""compact disc player"" is not present on
  those pages.

(That ""compat disc player"" wasn't a typo, guys.)For the fine prose of the original, see the provisional application
of February 1998, back when we were still Viaweb and couldn't afford
to pay lawyers to turn every ""a lot of"" into ""considerable.""","personal experience report
",human,"human
","human
"
17,17,"July 2008At this year's startup school, David Heinemeier Hansson gave a
 talk
in which he suggested that startup founders
should do things the old fashioned way.  Instead of hoping to get
rich by building a valuable company and then selling stock in a
""liquidity event,"" founders should start companies that make money
and live off the revenues.Sounds like a good plan.  Let's think about the optimal way to do
this.One disadvantage of living off the revenues of your company is that
you have to keep running it.  And as anyone who runs their own
business can tell you, that requires your complete attention.  You
can't just start a business and check out once things are going
well, or they stop going well surprisingly fast.The main economic motives of startup founders seem to be freedom
and security.  They want enough money that (a) they don't have to
worry about running out of money and (b) they can spend their time
how they want.  Running your own business offers neither.  You
certainly don't have freedom: no boss is so demanding.  Nor do you
have security, because if you stop paying attention to the company,
its revenues go away, and with them your income.The best case, for most people, would be if you could hire someone
to manage the company for you once you'd grown it to a certain size.
Suppose you could find a really good manager.  Then you would have
both freedom and security.  You could pay as little attention to
the business as you wanted, knowing that your manager would keep
things running smoothly.  And that being so, revenues would continue
to flow in, so you'd have security as well.There will of course be some founders who wouldn't like that idea:
the ones who like running their company so much that there's nothing
else they'd rather do.  But this group must be small.  The way you
succeed in most businesses is to be fanatically attentive
to customers' needs.  What are the odds that your own desires would
coincide exactly with the demands of this powerful, external force?Sure, running your own company can be fairly interesting.  Viaweb
was more interesting than any job I'd had before.  And since I made
much more money from it, it offered the highest ratio of income to
boringness of anything I'd done, by orders of magnitude.  But was
it the most interesting work I could imagine doing?  No.Whether the number of founders in the same position is asymptotic
or merely large, there are certainly a lot of them.  For them the
right approach would be to hand the company over to a professional
manager eventually, if they could find one who was good enough._____So far so good.  But what if your manager was hit by a bus?  What
you really want is a management company to run your company for
you.  Then you don't depend on any one person.If you own rental property, there are companies you can hire to
manage it for you.  Some will do everything, from finding tenants
to fixing leaks.  Of course, running companies is a lot more
complicated than managing rental property, but let's suppose there
were management companies that could do it for you. They'd charge
a lot, but wouldn't it be worth it?  I'd sacrifice a large percentage
of the income for the extra peace of mind.I realize what I'm describing already sounds too good to be true, but I
can think of a way to make it even more attractive.  If
company management companies existed, there would be an additional
service they could offer clients: they could let them insure their
returns by pooling their risk.  After all, even a perfect manager can't save a company
when, as sometimes happens, its whole market dies, just as property
managers can't save you from the building burning down.  But a
company that managed a large enough number of companies could say
to all its clients: we'll combine the revenues from all your
companies, and pay you your proportionate share.If such management companies existed, they'd offer the maximum of
freedom and security.  Someone would run your company for you, and
you'd be protected even if it happened to die.Let's think about how such a management company might be organized.
The simplest way would be to have a new kind of stock representing
the total pool of companies they were managing.  When you signed
up, you'd trade your company's stock for shares of this pool, in
proportion to an estimate of your company's value that you'd both
agreed upon.  Then you'd automatically get your share of the returns
of the whole pool.The catch is that because this kind of trade would be hard to undo,
you couldn't switch management companies.  But there's a way they
could fix that: suppose all the company management companies got
together and agreed to allow their clients to exchange shares in
all their pools.  Then you could, in effect, simultaneously choose
all the management companies to run yours for you, in whatever
proportion you wanted, and change your mind later as often as you
wanted.If such pooled-risk company management companies existed, signing
up with one would seem the ideal plan for most people following the
route David advocated.Good news: they do exist.  What I've just
described is an acquisition by a public company._____Unfortunately, though public acquirers are structurally identical
to pooled-risk company management companies, they don't think of
themselves that way.  With a property management company, you can
just walk in whenever you want and say ""manage my rental property
for me"" and they'll do it.  Whereas acquirers are, as of this
writing, extremely fickle.  Sometimes they're in a buying mood and
they'll overpay enormously; other times they're not interested.
They're like property management companies run by madmen.  Or more
precisely, by Benjamin Graham's Mr. Market.So while on average public acquirers behave like pooled-risk company
managers, you need a window of several years to get average case
performance.  If you wait long enough (five years, say) you're
likely to hit an up cycle where some acquirer is hot to buy you.
But you can't choose when it happens.You can't assume investors will carry you for as long as you might
have to wait. Your company has to make money.  Opinions are divided
about how early to focus on that.  
Joe Kraus says you should try
charging customers right away.  And yet some of the most successful
startups, including Google, ignored revenue at first and concentrated
exclusively on development.  The answer probably depends on the
type of company you're starting.  I can imagine some where trying
to make sales would be a good heuristic for product design, and
others where it would just be a distraction.  The test is probably
whether it helps you to understand your users.You can choose whichever revenue strategy you think is best for the
type of company you're starting, so long as you're profitable.
Being profitable ensures you'll get at least the average of the
acquisition market—in which public companies do behave as pooled-risk
company management companies.David isn't mistaken in saying you should start a company to live
off its revenues.  The mistake is thinking this is somehow opposed
to starting a company and selling it.  In fact, for most people the
latter is merely the optimal case of the former.Thanks to Trevor Blackwell, Jessica Livingston, Michael
Mandel, Robert Morris, and Fred Wilson for reading drafts of this.
Russian Translation","startup advice
",human,"human
","human
"
18,18,"

Want to start a startup?  Get funded by
Y Combinator.




August 2013When people hurt themselves lifting heavy things, it's usually
because they try to lift with their back.  The right way to lift
heavy things is to let your legs do the work.  Inexperienced founders
make the same mistake when trying to convince investors.  They try
to convince with their pitch.  Most would be better off if they let
their startup do the work — if they started by understanding why
their startup is worth investing in, then simply explained this
well to investors.Investors are looking for startups that will be very successful.
But that test is not as simple as it sounds.  In startups, as in a
lot of other domains, the distribution of outcomes follows a power
law, but in startups the curve is startlingly steep.  The big
successes are so big they 
dwarf the rest.  And since there are only
a handful each year (the conventional wisdom is 15), investors treat
""big success"" as if it were binary.  Most are interested in you if
you seem like you have a chance, however small, of being one of the
15 big successes, and otherwise not.
[1](There are a handful of angels who'd be interested in a company
with a high probability of being moderately successful.  But angel
investors like big successes too.)How do you seem like you'll be one of the big successes?  You need
three things: formidable founders, a promising market, and (usually)
some evidence of success so far.FormidableThe most important ingredient is formidable founders.  Most investors
decide in the first few minutes whether you seem like a winner or
a loser, and once their opinion is set it's hard to change. [2]
Every startup has reasons both to invest and not to invest.  If
investors think you're a winner they focus on the former, and if
not they focus on the latter.  For example, it might be a rich
market, but with a slow sales cycle.  If investors are impressed
with you as founders, they say they want to invest because it's a
rich market, and if not, they say they can't invest because of the
slow sales cycle.They're not necessarily trying to mislead you.  Most investors are
genuinely unclear in their own minds why they like or dislike
startups.  If you seem like a winner, they'll like your idea more.
But don't be too smug about this weakness of theirs, because you
have it too; almost everyone does.There is a role for ideas of course.  They're fuel for the fire
that starts with liking the founders.  Once investors like you,
you'll see them reaching for ideas: they'll be saying ""yes, and you
could also do x.""  (Whereas when they don't like you, they'll be
saying ""but what about y?"")But the foundation of convincing investors is to seem formidable,
and since this isn't a word most people use in conversation much,
I should explain what it means.  A formidable person is one who
seems like they'll get what they want, regardless of whatever
obstacles are in the way.  Formidable is close to confident, except
that someone could be confident and mistaken.  Formidable is roughly
justifiably confident.There are a handful of people who are really good at seeming
formidable — some because they actually are very formidable and
just let it show, and others because they are more or less con
artists.
[3]
But most founders, including many who will go on
to start very successful companies, are not that good at seeming
formidable the first time they try fundraising.  What should they
do?
[4]What they should not do is try to imitate the swagger of more
experienced founders.  Investors are not always that good at judging
technology, but they're good at judging confidence.  If you try to
act like something you're not, you'll just end up in an uncanny
valley.  You'll depart from sincere, but never arrive at convincing.TruthThe way to seem most formidable as an inexperienced founder is to
stick to the truth.   How formidable you seem isn't a constant.  It
varies depending on what you're saying.  Most people can seem
confident when they're saying ""one plus one is two,"" because they
know it's true.  The most diffident person would be puzzled and
even slightly contemptuous if they told a VC ""one plus one is two""
and the VC reacted with skepticism.  The magic ability of people
who are good at seeming formidable is that they can do this with
the sentence ""we're going to make a billion dollars a year.""  But
you can do the same, if not with that sentence with some fairly
impressive ones, so long as you convince yourself first.That's the secret.  Convince yourself that your startup is worth
investing in, and then when you explain this to investors they'll
believe you.  And by convince yourself, I don't mean play mind games
with yourself to boost your confidence.  I mean truly evaluate
whether your startup is worth investing in.  If it isn't, don't try
to raise money.
[5]
But if it is, you'll be telling the truth
when you tell investors it's worth investing in, and they'll sense
that.  You don't have to be a smooth presenter if you understand
something well and tell the truth about it.To evaluate whether your startup is worth investing in, you have
to be a domain expert.  If you're not a domain expert, you can be
as convinced as you like about your idea, and it will seem to
investors no more than an instance of the Dunning-Kruger effect.
Which in fact it will usually be.  And investors can tell fairly
quickly whether you're a domain expert by how well you answer their
questions.  Know everything about your market.
[6]Why do founders persist in trying to convince investors of things
they're not convinced of themselves?  Partly because we've all been
trained to.When my friends Robert Morris and Trevor Blackwell were in grad
school, one of their fellow students was on the receiving end of a
question from their faculty advisor that we still quote today.  When
the unfortunate fellow got to his last slide, the professor burst
out:

  Which one of these conclusions do you actually believe?

One of the artifacts of the way schools are organized is that we
all get trained to talk even when we have nothing to say.  If you
have a ten page paper due, then ten pages you must write, even if
you only have one page of ideas.  Even if you have no ideas.  You
have to produce something.  And all too many startups go into
fundraising in the same spirit.  When they think it's time to raise
money, they try gamely to make the best case they can for their
startup.  Most never think of pausing beforehand to ask whether
what they're saying is actually convincing, because they've all
been trained to treat the need to present as a given — as an area
of fixed size, over which however much truth they have must needs
be spread, however thinly.The time to raise money is not when you need it, or when you reach
some artificial deadline like a Demo Day.  It's when you can convince
investors, and not before. 
[7]And unless you're a good con artist, you'll never convince investors
if you're not convinced yourself.  They're far better at detecting
bullshit than you are at producing it, even if you're producing it
unknowingly.  If you try to convince investors before you've convinced
yourself, you'll be wasting both your time.But pausing first to convince yourself will do more than save you
from wasting your time.  It will force you to organize your thoughts.
To convince yourself that your startup is worth investing in, you'll
have to figure out why it's worth investing in.   And if you can
do that you'll end up with more than added confidence.  You'll also
have a provisional roadmap of how to succeed.MarketNotice I've been careful to talk about whether a startup is worth
investing in, rather than whether it's going to succeed.  No one
knows whether a startup is going to succeed.  And it's a good thing
for investors that this is so, because if you could know in advance
whether a startup would succeed, the stock price would already be
the future price, and there would be no room for investors to make
money.  Startup investors know that every investment is a bet, and
against pretty long odds.So to prove you're worth investing in, you don't have to prove
you're going to succeed, just that you're a sufficiently good bet.
What makes a startup a sufficiently good bet?  In addition to
formidable founders, you need a plausible path to owning a big piece
of a big market.  Founders think of startups as ideas, but investors
think of them as markets.  If there are x number of customers who'd
pay an average of $y per year for what you're making, then the total
addressable market, or TAM, of your company is $xy.  Investors don't
expect you to collect all that money, but it's an upper bound on
how big you can get.Your target market has to be big, and it also has to be capturable
by you.  But the market doesn't have to be big yet, nor do you
necessarily have to be in it yet.  Indeed, it's often better to
start in a small market that will either turn into a big one or
from which you can move into a big one.  There just has to be some
plausible sequence of hops that leads to dominating a big market a
few years down the line.The standard of plausibility varies dramatically depending on the
age of the startup.  A three month old company at Demo Day only
needs to be a promising experiment that's worth funding to see how
it turns out.  Whereas a two year old company raising a series A
round needs to be able to show the experiment worked. 
[8]But every company that gets really big is ""lucky"" in the sense that
their growth is due mostly to some external wave they're riding,
so to make a convincing case for becoming huge, you have to identify
some specific trend you'll benefit from.  Usually you can find this
by asking ""why now?""  If this is such a great idea, why hasn't
someone else already done it?  Ideally the answer is that it only
recently became a good idea, because something changed, and no one
else has noticed yet.Microsoft for example was not going to grow huge selling Basic
interpreters.  But by starting there they were perfectly poised to
expand up the stack of microcomputer software as microcomputers
grew powerful enough to support one. And microcomputers turned out
to be a really huge wave, bigger than even the most optimistic
observers would have predicted in 1975.But while Microsoft did really well and there is thus a temptation
to think they would have seemed a great bet a few months in, they
probably didn't.  Good, but not great.  No company, however successful,
ever looks more than a pretty good bet a few months in. Microcomputers
turned out to be a big deal, and Microsoft both executed well and
got lucky.  But it was by no means obvious that this was how things
would play out.  Plenty of companies seem as good a bet a few months
in.  I don't know about startups in general, but at least half the
startups we fund could make as good a case as Microsoft could have
for being on a path to dominating a large market.  And who can
reasonably expect more of a startup than that?RejectionIf you can make as good a case as Microsoft could have, will you
convince investors?  Not always.  A lot of VCs would have rejected
Microsoft.
[9]
Certainly some rejected Google.  And getting
rejected will put you in a slightly awkward position, because as
you'll see when you start fundraising, the most common question
you'll get from investors will be ""who else is investing?"" What do
you say if you've been fundraising for a while and no one has
committed yet? 
[10]The people who are really good at acting formidable often solve
this problem by giving investors the impression that while no
investors have committed yet, several are about to.  This is arguably
a permissible tactic.  It's slightly dickish of investors to care
more about who else is investing than any other aspect of your
startup, and misleading them about how far along you are with other
investors seems the complementary countermove.  It's arguably an
instance of scamming a scammer. But I don't recommend this approach
to most founders, because most founders wouldn't be able to carry
it off.  This is the single most common lie told to investors, and
you have to be really good at lying to tell members of some profession
the most common lie they're told.If you're not a master of negotiation (and perhaps even if you are)
the best solution is to tackle the problem head-on, and to explain
why investors have turned you down and why they're mistaken.  If
you know you're on the right track, then you also know why investors
were wrong to reject you. Experienced investors are well aware that
the best ideas are also the scariest.  They all know about the VCs
who rejected Google.  If instead of seeming evasive and ashamed
about having been turned down (and thereby implicitly agreeing with
the verdict) you talk candidly about what scared investors about
you, you'll seem more confident, which they like, and you'll probably
also do a better job of presenting that aspect of your startup.  At
the very least, that worry will now be out in the open instead of
being a gotcha left to be discovered by the investors you're currently
talking to, who will be proud of and thus attached to their discovery.
[11]This strategy will work best with the best investors, who are both
hard to bluff and who already believe most other investors are
conventional-minded drones doomed always to miss the big outliers.
Raising money is not like applying to college, where you can assume
that if you can get into MIT, you can also get into Foobar State.
Because the best investors are much smarter than the rest, and the
best startup ideas look initially like 
bad ideas, it's not uncommon
for a startup to be rejected by all the VCs except the best ones.
That's what happened to Dropbox.  Y Combinator started in Boston,
and for the first 3 years we ran alternating batches in Boston and
Silicon Valley.  Because Boston investors were so few and so timid,
we used to ship Boston batches out for a second Demo Day in Silicon
Valley.  Dropbox was part of a Boston batch, which means all those
Boston investors got the first look at Dropbox, and none of them
closed the deal.  Yet another backup and syncing thing, they all
thought.  A couple weeks later, Dropbox raised a series A round
from Sequoia.
[12]DifferentNot understanding that investors view investments as bets combines
with the ten page paper mentality to prevent founders from even
considering the possibility of being certain of what they're saying.
They think they're trying to convince investors of something very
uncertain — that their startup will be huge — and convincing anyone
of something like that must obviously entail some wild feat of
salesmanship.  But in fact when you raise money you're trying to
convince investors of something so much less speculative — whether
the company has all the elements of a good bet — that you can
approach the problem in a qualitatively different way.  You can
convince yourself, then convince them.And when you convince them, use the same matter-of-fact language
you used to convince yourself.  You wouldn't use vague, grandiose
marketing-speak among yourselves.  Don't use it with investors
either.  It not only doesn't work on them, but seems a mark of
incompetence.  Just be concise.  Many investors explicitly use that
as a test, reasoning (correctly) that if you can't explain your
plans concisely, you don't really understand them.  But even investors
who don't have a rule about this will be bored and frustrated by
unclear explanations.
[13]So here's the recipe for impressing investors when you're not already
good at seeming formidable:

 Make something worth investing in. Understand why it's worth investing in. Explain that clearly to investors.

If you're saying something you know is true, you'll seem confident
when you're saying it.  Conversely, never let pitching draw you
into bullshitting.  As long as you stay on the territory of truth,
you're strong.  Make the truth good, then just tell it.Notes[1]
There's no reason to believe this number is a constant.  In
fact it's our explicit goal at Y Combinator to increase it, by
encouraging people to start startups who otherwise wouldn't have.[2]
Or more precisely, investors decide whether you're a loser
or possibly a winner.  If you seem like a winner, they may then,
depending on how much you're raising, have several more meetings
with you to test whether that initial impression holds up.But if you seem like a loser they're done, at least for the next
year or so.  And when they decide you're a loser they usually decide
in way less than the 50 minutes they may have allotted for the first
meeting.  Which explains the astonished stories one always hears
about VC inattentiveness.  How could these people make investment
decisions well when they're checking their messages during startups'
presentations?  The solution to that mystery is that they've already
made the decision.[3]
The two are not mutually exclusive.  There are people who are
both genuinely formidable, and also really good at acting that way.[4]
How can people who will go on to create giant companies not
seem formidable early on?  I think the main reason is that their
experience so far has trained them to keep their wings folded, as
it were.  Family, school, and jobs encourage cooperation, not
conquest.  And it's just as well they do, because even being Genghis
Khan is probably 99% cooperation.  But the result is that most
people emerge from the tube of their upbringing in their early
twenties compressed into the shape of the tube.  Some find they
have wings and start to spread them.  But this takes a few years.
In the beginning even they don't know yet what they're capable of.[5]
In fact, change what you're doing.  You're investing your own
time in your startup.  If you're not convinced that what you're
working on is a sufficiently good bet, why are you even working on
that?[6]
When investors ask you a question you don't know the answer
to, the best response is neither to bluff nor give up, but instead
to explain how you'd figure out the answer.  If you can work out a
preliminary answer on the spot, so much the better, but explain
that's what you're doing.[7]
At YC we try to ensure startups are ready to raise money on
Demo Day by encouraging them to ignore investors and instead focus
on their companies till about a week before.  That way most reach
the stage where they're sufficiently convincing well before Demo
Day.  But not all do, so we also give any startup that wants to the
option of deferring to a later Demo Day.[8]
Founders are often surprised by how much harder it is to raise
the next round.  There is a qualitative difference in investors'
attitudes.  It's like the difference between being judged as a kid
and as an adult.  The next time you raise money, it's not enough
to be promising.  You have to be delivering results.So although it works well to show growth graphs at either stage,
investors treat them differently.  At three months, a growth graph
is mostly evidence that the founders are effective.  At two years,
it has to be evidence of a promising market and a company tuned to
exploit it.[9]
By this I mean that if the present day equivalent of the 3
month old Microsoft presented at a Demo Day, there would be investors
who turned them down.  Microsoft itself didn't raise outside money,
and indeed the venture business barely existed when they got started
in 1975.[10]
The best investors rarely care who else is investing, but
mediocre investors almost all do.  So you can use this question as
a test of investor quality.[11]
To use this technique, you'll have to find out why investors
who rejected you did so, or at least what they claim was the reason.
That may require asking, because investors don't always volunteer
a lot of detail.  Make it clear when you ask that you're not trying
to dispute their decision — just that if there is some weakness in
your plans, you need to know about it.  You won't always get a real
reason out of them, but you should at least try.[12]
Dropbox wasn't rejected by all the East Coast VCs.  There was
one firm that wanted to invest but tried to lowball them.[13]
Alfred Lin points out that it's doubly important for the
explanation of a startup to be clear and concise, because it has
to convince at one remove: it has to work not just on the partner
you talk to, but when that partner re-tells it to colleagues.We consciously optimize for this at YC.  When we work with founders
create a Demo Day pitch, the last step is to imagine how an investor
would sell it to colleagues.
Thanks to Marc Andreessen, Sam Altman, Patrick Collison, Ron Conway,
Chris Dixon, Alfred Lin, Ben Horowitz, Steve Huffman, Jessica
Livingston, Greg Mcadoo, Andrew Mason, Geoff Ralston, Yuri Sagalov,
Emmett Shear, Rajat Suri, Garry Tan, Albert Wenger, Fred Wilson,
and Qasar Younis for reading drafts of this.","startup advice
",human,"human
","human
"
19,19,"

Want to start a startup?  Get funded by
Y Combinator.




August 2010Two years ago I
wrote about what I called ""a huge, unexploited
opportunity in startup funding:"" the growing disconnect between
VCs, whose current business model requires them to invest large
amounts, and a large class of startups that need less than they
used to.  Increasingly, startups want a couple hundred thousand
dollars, not a couple million. 
[1]The opportunity is a lot less unexploited now.  Investors have
poured into this territory from both directions.  VCs are much more
likely to make angel-sized investments than they were a year ago.
And meanwhile the past year has seen a dramatic increase in a new
type of investor: the super-angel, who operates like an angel, but
using other people's money, like a VC.Though a lot of investors are entering this territory, there is
still room for more.  The distribution of investors should mirror
the distribution of startups, which has the usual power law dropoff.
So there should be a lot more people investing tens or hundreds of
thousands than millions. 
[2]In fact, it may be good for angels that there are more people doing
angel-sized deals, because if angel rounds become more legitimate,
then startups may start to opt for angel rounds even when they
could, if they wanted, raise series A rounds from VCs.  One reason
startups prefer series A rounds is that they're more prestigious.
But if angel investors become more active and better known, they'll
increasingly be able to compete with VCs in brand.Of course, prestige isn't the main reason to prefer a series A
round.  A startup will probably get more attention from investors
in a series A round than an angel round.  So if a startup is choosing
between an angel round and an A round from a good VC fund, I usually
advise them to take the A round. 
[3]But while series A rounds aren't going away, I think VCs should be
more worried about super-angels than vice versa.  Despite their
name, the super-angels are really mini VC funds, and they clearly
have existing VCs in their sights.They would seem to have history on their side.  
The pattern here seems the same
one we see when startups and established companies enter a new
market.  Online video becomes possible, and YouTube plunges right
in, while existing media companies embrace it only half-willingly,
driven more by fear than hope, and aiming more to protect their
turf than to do great things for users.  Ditto for PayPal.  This
pattern is repeated over and over, and it's usually the invaders
who win. In this case the super-angels are the invaders.  Angel
rounds are their whole business, as online video was for YouTube.
Whereas VCs who make angel investments mostly do it as a way to
generate deal flow for series A rounds.
[4]On the other hand, startup investing is a very strange business.
Nearly all the returns are concentrated in a few big winners.  If
the super-angels merely fail to invest in (and to some extent
produce) the big winners, they'll be out of business, even if they
invest in all the others.VCsWhy don't VCs start doing smaller series A rounds?  The sticking
point is board seats.  In a traditional series A round, the partner
whose deal it is takes a seat on the startup's board.  If we assume
the average startup runs for 6 years and a partner can bear to be
on 12 boards at once, then a VC fund can do 2 series A deals per
partner per year.It has always seemed to me the solution is to take fewer board
seats.  You don't have to be on the board to help a startup.  Maybe
VCs feel they need the power that comes with board membership to
ensure their money isn't wasted.  But have they tested that theory?
Unless they've tried not taking board seats and found their returns
are lower, they're not bracketing the problem.I'm not saying VCs don't help startups.  The good ones help them a
lot.  What I'm saying is that the kind of help that matters, you
may not have to be a board member to give.
[5]How will this all play out?  Some VCs will probably adapt, by doing
more, smaller deals.  I wouldn't be surprised if by streamlining
their selection process and taking fewer board seats, VC funds could
do 2 to 3 times as many series A rounds with no loss of quality.But other VCs will make no more than superficial changes.  VCs are
conservative, and the threat to them isn't mortal.  The VC funds
that don't adapt won't be violently displaced.  They'll edge gradually
into a different business without realizing it.  They'll still do
what they will call series A rounds, but these will increasingly
be de facto series B rounds.
[6]In such rounds they won't get the 25 to 40% of the company they do
now.  You don't give up as much of the company in later rounds
unless something is seriously wrong.  Since the VCs who don't adapt
will be investing later, their returns from winners may be smaller.
But investing later should also mean they have fewer losers.  So
their ratio of risk to return may be the same or even better.
They'll just have become a different, more conservative, type of
investment.AngelsIn the big angel rounds that increasingly compete with series A
rounds, the investors won't take as much equity as VCs do now.  And
VCs who try to compete with angels by doing more, smaller deals
will probably find they have to take less equity to do it. Which
is good news for founders: they'll get to keep more of the company.The deal terms of angel rounds will become less restrictive
too—not just less restrictive than series A terms, but less
restrictive than angel terms have traditionally been.In the future, angel rounds will less often be for specific amounts
or have a lead investor.  In the old days, the standard m.o. for
startups was to find one angel to act as the lead investor.  You'd
negotiate a round size and valuation with the lead, who'd supply
some but not all of the money.  Then the startup and the lead would
cooperate to find the rest.The future of angel rounds looks more like this: instead of a fixed
round size, startups will do a rolling close, where they take money
from investors one at a time till they feel they have enough.
[7]
And though there's going to be one investor who gives them the first
check, and his or her help in recruiting other investors will
certainly be welcome, this initial investor will no longer be the
lead in the old sense of managing the round.  The startup will now
do that themselves.There will continue to be lead investors in the sense of investors
who take the lead in advising a startup.  They may also make
the biggest investment.  But they won't always have to be the one
terms are negotiated with, or be the first money in, as they have
in the past.  Standardized paperwork will do away with the need to
negotiate anything except the valuation, and that will get easier
too.If multiple investors have to share a valuation, it will be whatever
the startup can get from the first one to write a check, limited
by their guess at whether this will make later investors balk.  But
there may not have to be just one valuation.  Startups are increasingly
raising money on convertible notes, and convertible notes have not
valuations but at most valuation caps: caps on what the
effective valuation will be when the debt converts to equity (in a
later round, or upon acquisition if that happens first).  That's
an important difference because it means a startup could do multiple
notes at once with different caps.  This is now starting to happen,
and I predict it will become more common.SheepThe reason things are moving this way is that the old way sucked
for startups.  Leads could (and did) use a fixed size round as a
legitimate-seeming way of saying what all founders hate to hear:
I'll invest if other people will.  Most investors, unable to judge
startups for themselves, rely instead on the opinions of other
investors.  If everyone wants in, they want in too; if not, not.
Founders hate this because it's a recipe for deadlock, and delay
is the thing a startup can least afford.  Most investors know this
m.o. is lame, and few say openly that they're doing it.  But the
craftier ones achieve the same result by offering to lead rounds
of fixed size and supplying only part of the money.  If the startup
can't raise the rest, the lead is out too.  How could they go ahead
with the deal?  The startup would be underfunded!In the future, investors will increasingly be unable to offer
investment subject to contingencies like other people investing.
Or rather, investors who do that will get last place in line.
Startups will go to them only to fill up rounds that are mostly
subscribed.  And since hot startups tend to have rounds that are
oversubscribed, being last in line means they'll probably miss the
hot deals.  Hot deals and successful startups are not identical,
but there is a significant correlation. 
[8]
So investors who won't invest unilaterally will have lower returns.Investors will probably find they do better when deprived of this
crutch anyway.   Chasing hot deals doesn't make investors choose
better; it just makes them feel better about their choices.  I've
seen feeding frenzies both form and fall apart many times, and as
far as I can tell they're mostly random. 
[9]
If investors can
no longer rely on their herd instincts, they'll have to think more
about each startup before investing.  They may be surprised how
well this works.Deadlock wasn't the only disadvantage of letting a lead investor
manage an angel round.  The investors would not infrequently collude
to push down the valuation.  And rounds took too long to close,
because however motivated the lead was to get the round closed, he
was not a tenth as motivated as the startup.Increasingly, startups are taking charge of their own angel rounds.
Only a few do so far, but I think we can already declare the old
way dead, because those few are the best startups.  They're the
ones in a position to tell investors how the round is going to work.
And if the startups you want to invest in do things a certain way,
what difference does it make what the others do?TractionIn fact, it may be slightly misleading to say that angel rounds
will increasingly take the place of series A rounds.  What's really
happening is that startup-controlled rounds are taking the place
of investor-controlled rounds.This is an instance of a very important meta-trend, one that Y
Combinator itself has been based on from the beginning: founders
are becoming increasingly powerful relative to investors.  So if
you want to predict what the future of venture funding will be like,
just ask: how would founders like it to be?  One by one, all the
things founders dislike about raising money are going to get
eliminated. 
[10]Using that heuristic, I'll predict a couple more things.  One is
that investors will increasingly be unable to wait for startups to
have ""traction"" before they put in significant money.  It's hard
to predict in advance which startups will succeed.  So most investors
prefer, if they can, to wait till the startup is already succeeding,
then jump in quickly with an offer.  Startups hate this as well,
partly because it tends to create deadlock, and partly because it
seems kind of slimy.  If you're a promising startup but don't yet
have significant growth, all the investors are your friends in
words, but few are in actions.  They all say they love you, but
they all wait to invest. Then when you start to see growth, they
claim they were your friend all along, and are aghast at the thought
you'd be so disloyal as to leave them out of your round.  If founders
become more powerful, they'll be able to make investors give them
more money upfront.(The worst variant of this behavior is the tranched deal, where the
investor makes a small initial investment, with more to follow if
the startup does well.  In effect, this structure gives the investor
a free option on the next round, which they'll only take if it's
worse for the startup than they could get in the open market.
Tranched deals are an abuse.  They're increasingly rare, and they're
going to get rarer.) 
[11]Investors don't like trying to predict which startups will succeed,
but increasingly they'll have to.  Though the way that happens won't
necessarily be that the behavior of existing investors will change;
it may instead be that they'll be replaced by other investors with
different behavior—that investors who understand startups
well enough to take on the hard problem of predicting their trajectory
will tend to displace suits whose skills lie more in raising money
from LPs.SpeedThe other thing founders hate most about fundraising is how long
it takes.  So as founders become more powerful, rounds should start
to close faster.Fundraising is still terribly distracting for startups. If you're
a founder in the middle of raising a round, the round is the top idea in your mind, which means working on the
company isn't.  If a round takes 2 months to close, which is
reasonably fast by present standards, that means 2 months during
which the company is basically treading water.  That's the worst
thing a startup could do.So if investors want to get the best deals, the way to do it will
be to close faster.   Investors don't need weeks to make up their
minds anyway.  We decide based on about 10 minutes of reading an
application plus 10 minutes of in person interview, and we only
regret about 10% of our decisions.  If we can decide in 20 minutes,
surely the next round of investors can decide in a couple days.
[12]There are a lot of institutionalized delays in startup funding: the
multi-week mating dance with investors; the distinction between
termsheets and deals; the fact that each series A has enormously
elaborate, custom paperwork.  Both founders and investors tend to
take these for granted.  It's the way things have always been.  But
ultimately the reason these delays exist is that they're to the
advantage of investors.  More time gives investors more information
about a startup's trajectory, and it also tends to make startups
more pliable in negotiations, since they're usually short of money.These conventions weren't designed to drag out the funding process,
but that's why they're allowed to persist.  Slowness is to the
advantage of investors, who have in the past been the ones with the
most power.  But there is no need for rounds to take months or even
weeks to close, and once founders realize that, it's going to stop.
Not just in angel rounds, but in series A rounds too.  The future
is simple deals with standard terms, done quickly.One minor abuse that will get corrected in the process is option
pools.  In a traditional series A round, before the VCs invest they
make the company set aside a block of stock for future hires—usually
between 10 and 30% of the company.  The point is to ensure this
dilution is borne by the existing shareholders.  The practice isn't
dishonest; founders know what's going on.  But it makes deals
unnecessarily complicated.  In effect the valuation is 2 numbers.
There's no need to keep doing this.
[13]The final thing founders want is to be able to sell some of
their own stock in later rounds.  This won't be a change, 
because the practice is now quite common.  A lot of investors
hated the idea, but the world hasn't exploded as a result,
so it will happen more, and more openly.SurpriseI've talked here about a bunch of changes that will be forced on
investors as founders become more powerful.  Now the good news:
investors may actually make more money as a result.A couple days ago an interviewer 
asked 
me if founders having more
power would be better or worse for the world.  I was surprised,
because I'd never considered that question.  Better or worse, it's
happening.  But after a second's reflection, the answer seemed
obvious.  Founders understand their companies better than investors,
and it has to be better if the people with more knowledge have more
power.One of the mistakes novice pilots make is overcontrolling the
aircraft: applying corrections too vigorously, so the aircraft
oscillates about the desired configuration instead of approaching
it asymptotically.  It seems probable that investors have till now
on average been overcontrolling their portfolio companies.  In a
lot of startups, the biggest source of stress for the founders is
not competitors but investors.  Certainly it was for us at Viaweb.
And this is not a new phenomenon: investors were James Watt's biggest
problem too.   If having less power prevents investors from
overcontrolling startups, it should be better not just for founders
but for investors too.Investors may end up with less stock per startup, but startups will
probably do better with founders more in control, and there will
almost certainly be more of them.  Investors all compete with one
another for deals, but they aren't one another's main competitor.
Our main competitor is employers.  And so far that competitor is
crushing us.  Only a tiny fraction of people who could start a
startup do.  Nearly all customers choose the competing product, a
job.  Why?  Well, let's look at the product we're offering.  An
unbiased review would go something like this:

  Starting a startup gives you more freedom and the opportunity to
  make a lot more money than a job, but it's also hard work and at
  times very stressful.

Much of the stress comes from dealing with investors.  If reforming
the investment process removed that stress, we'd make our product
much more attractive.  The kind of people who make good startup
founders don't mind dealing with technical problems—they enjoy
technical problems—but they hate the type of problems investors
cause.Investors have no
idea that when they maltreat one startup, they're preventing 10
others from happening, but they are.  Indirectly, but they are.  So
when investors stop trying to squeeze a little more out of their
existing deals, they'll find they're net ahead, because so many
more new deals appear.One of our axioms at Y Combinator is not to think of deal flow as
a zero-sum game.  Our main focus is to encourage more startups to happen,
not to win a larger share of the existing stream.  We've found this
principle very useful, and we think as it spreads outward it will
help later stage investors as well.""Make something people want""
applies to us too.Notes[1]
In this essay I'm talking mainly about software startups.
These points don't apply to types of startups that are still expensive
to start, e.g. in energy or biotech.Even the cheap kinds of startups will generally raise large amounts
at some point, when they want to hire a lot of people.  What has
changed is how much they can get done before that.[2]
It's not the distribution of good startups that has a power
law dropoff, but the distribution of potentially good startups,
which is to say, good deals.  There are lots of potential winners,
from which a few actual winners emerge with superlinear certainty.[3]
As I was writing this, I asked some founders who'd taken
series A rounds from top VC funds whether it was worth it, and they
unanimously said yes.The quality of investor is more important than the type of round,
though.  I'd take an angel round from good angels over a series A
from a mediocre VC.[4]
Founders also worry that taking an angel investment from a
VC means they'll look bad if the VC declines to participate in the
next round.  The trend of VC angel investing is so new that it's
hard to say how justified this worry is.Another danger, pointed out by Mitch Kapor, is that if VCs are only
doing angel deals to generate series A deal flow, then their
incentives aren't aligned with the founders'.  The founders want
the valuation of the next round to be high, and the VCs want it to
be low.  Again, hard to say yet how much of a problem this will be.[5]
Josh Kopelman pointed out that another way to be on fewer
boards at once is to take board seats for shorter periods.[6]
Google was in this respect as so many others the pattern for
the future.  It would be great for VCs if the similarity extended
to returns.  That's probably too much to hope for, but the returns
may be somewhat higher, as I explain later.[7]
Doing a rolling close doesn't mean the company is always
raising money.  That would be a distraction.  The point of a rolling
close is to make fundraising take less time, not more.  With a
classic fixed sized round, you don't get any money till all the
investors agree, and that often creates a situation where they all
sit waiting for the others to act.  A rolling close usually prevents
this.
[8]
There are two (non-exclusive) causes of hot deals: the quality
of the company, and domino effects among investors.  The former is
obviously a better predictor of success.[9]
Some of the randomness is concealed by the fact that investment
is a self fulfilling prophecy.[10]
The shift in power to founders is exaggerated now because
it's a seller's market.  On the next downtick it will seem like I
overstated the case.  But on the next uptick after that, founders
will seem more powerful than ever.[11]
More generally, it will become less common for the same
investor to invest in successive rounds, except when exercising an
option to maintain their percentage.  When the same investor invests
in successive rounds, it often means the startup isn't getting
market price.  They may not care; they may prefer to work with an
investor they already know; but as the investment market becomes
more efficient, it will become increasingly easy to get market price
if they want it.  Which in turn means the investment community will
tend to become more stratified.[12]
The two 10 minuteses have 3 weeks between them so founders
can get cheap plane tickets, but except for that they could be
adjacent.[13]
I'm not saying option pools themselves will go away.  They're
an administrative convenience.  What will go away is investors
requiring them.
Thanks to Sam Altman, John Bautista, Trevor Blackwell,
Paul Buchheit, Jeff Clavier,
Patrick Collison, Ron Conway, Matt Cohler, Chris Dixon, Mitch Kapor,
Josh Kopelman, Pete Koomen, Carolynn Levy, Jessica Livingston, Ariel
Poler, Geoff Ralston, Naval Ravikant, Dan Siroker, Harj Taggar, and 
Fred Wilson
for reading drafts of this.","startup advice
",human,"human
","human
"
20,20,"April 2009Inc recently asked me who I thought were the 5 most
interesting startup founders of the last 30 years.  How do
you decide who's the most interesting?  The best test seemed
to be influence: who are the 5
who've influenced me most?  Who do I use as examples when I'm
talking to companies we fund?  Who do I find myself quoting?1. Steve JobsI'd guess Steve is the most influential founder not just for me but
for most people you could ask.  A lot of startup culture is Apple
culture.  He was the original young founder.  And while the concept
of ""insanely great"" already existed in the arts, it was a novel
idea to introduce into a company in the 1980s.More remarkable still, he's stayed interesting for 30 years.  People
await new Apple products the way they'd await new books by a popular
novelist.  Steve may not literally design them, but they wouldn't
happen if he weren't CEO.Steve is clever and driven, but so are a lot of people in the Valley.
What makes him unique is his 
sense of 
design.  Before him, most
companies treated design as a frivolous extra.  Apple's competitors
now know better.2. TJ RodgersTJ Rodgers isn't as famous as Steve Jobs, but he may be the best
writer among Silicon Valley CEOs.  I've probably learned more from
him about the startup way of thinking than from anyone else.  Not
so much from specific things he's written as by reconstructing the
mind that produced them: brutally candid; aggressively garbage-collecting
outdated ideas; and yet driven by pragmatism rather than ideology.The first essay of his that I read was so electrifying that I
remember exactly where I was at the time.  It was 
High
Technology Innovation: Free Markets or Government Subsidies? and
I was downstairs in the Harvard Square T Station.  It felt as if
someone had flipped on a light switch inside my head.3. Larry & SergeyI'm sorry to treat Larry and Sergey as one person.  I've always
thought that was unfair to them.  But it does seem as if Google was a
collaboration.Before Google, companies in Silicon Valley already knew it was
important to have the best hackers.  So they claimed, at least.
But Google pushed this idea further than anyone had before.  Their
hypothesis seems to have been that, in the initial stages at least,
all you need is good hackers: if you hire all the smartest people
and put them to work on a problem where their success can be measured,
you win.  All the other stuff—which includes all the stuff that
business schools think business consists of—you can figure out
along the way.  The results won't be perfect, but they'll be optimal.
If this was their hypothesis, it's now been verified experimentally.4. Paul BuchheitFew know this, but one person, Paul Buchheit, is responsible for
three of the best things Google has done.  He was the original
author of GMail, which is the most impressive thing Google has after
search.  He also wrote the first prototype of AdSense, and was the
author of Google's mantra ""Don't be evil.""PB made a point in a talk once that I now mention to every startup
we fund: that it's better, initially, to make a small number of
users really love you than a large number kind of like you.   If I
could tell startups only 
ten sentences, 
this would be one of them.Now he's cofounder of a startup called Friendfeed.  It's only a
year old, but already everyone in the Valley is watching them.
Someone responsible for three of the biggest ideas at Google is
going to come up with more.5. Sam AltmanI was told I shouldn't mention founders of YC-funded companies in
this list.   But Sam Altman can't be stopped by such flimsy rules.
If he wants to be on this list, he's going to be.Honestly, Sam is, along with Steve Jobs, the founder I refer to
most when I'm advising startups.  On questions of design, I ask
""What would Steve do?"" but on questions of strategy or ambition I
ask ""What would Sama do?""What I learned from meeting Sama is that the doctrine of the elect
applies to startups.  It applies way less than most people think:
startup investing does not consist of trying to pick winners the
way you might in a horse race.  But there are a few people with
such force of will that they're going to get whatever they want.","startup advice
",human,"human
","human
"
21,21,"

Want to start a startup?  Get funded by
Y Combinator.




October 2006(This essay is derived from a talk at MIT.)Till recently graduating seniors had two choices: get a job or go
to grad school.  I think there will increasingly be a third option:
to start your own startup.  But how common will that be?I'm sure the default will always be to get a job, but starting a
startup could well become as popular as grad school.  In the late
90s my professor friends used to complain that they couldn't get
grad students, because all the undergrads were going to work for
startups.  I wouldn't be surprised if that situation returns, but
with one difference: this time they'll be starting their own
instead of going to work for other people's.The most ambitious students will at this point be asking: Why wait
till you graduate?  Why not start a startup while you're in college?
In fact, why go to college at all?  Why not start a startup instead?A year and a half ago I gave a talk 
where I said that the average age of the founders of
Yahoo, Google, and Microsoft was 24, and that if grad students could
start startups, why not undergrads?  I'm glad I phrased that as a
question, because now I can pretend it wasn't merely a rhetorical
one.  At the time I couldn't imagine why there should be any lower
limit for the age of startup founders.  Graduation is a bureaucratic
change, not a biological one.  And certainly there are undergrads
as competent technically as most grad students.  So why shouldn't
undergrads be able to start startups as well as grad students?I now realize that something does change at graduation: you lose a
huge excuse for failing.  Regardless of how complex your life is,
you'll find that everyone else, including your family and friends,
will discard all the low bits and regard you as having a single
occupation at any given time.  If you're in college and have a
summer job writing software, you still read as a student.  Whereas
if you graduate and get a job programming, you'll be instantly
regarded by everyone as a programmer.The problem with starting a startup while you're still in school
is that there's a built-in escape hatch.  If you start a startup
in the summer between your junior and senior year, it reads to
everyone as a summer job.
So if it goes nowhere, big deal; you return to school in the
fall with all the other seniors; no one regards you as a failure,
because your occupation is student, and you didn't fail at that.
Whereas if you start a startup just one year later, after you
graduate, as long as you're not accepted to grad school in the fall
the startup reads to everyone as your occupation.  You're
now a startup founder, so you have to do well at that.For nearly everyone, the opinion of one's peers is the most powerful
motivator of all—more powerful even than the nominal goal of most
startup founders, getting rich.  
[1]
About a month into each funding
cycle we have an event called Prototype Day where each startup
presents to the others what they've got so far.  You might think
they wouldn't need any more motivation.  They're working on their
cool new idea; they have funding for the immediate future; and
they're playing a game with only two outcomes: wealth or failure.
You'd think that would be motivation enough.  And yet the prospect
of a demo pushes most of them into a
rush of activity.Even if you start a startup explicitly to get rich, the money you
might get seems pretty theoretical most of the time.  What drives
you day to day is not wanting to look bad.You probably can't change that.  Even if you could, I don't think
you'd want to; someone who really, truly doesn't care what his peers
think of him is probably a psychopath.  So the best you can do is
consider this force like a wind, and set up your boat accordingly.
If you know your peers are going to push you in some direction,
choose good peers, and position yourself so they push you in a
direction you like.Graduation changes the prevailing winds, and those make a difference.
Starting a startup is so hard
that it's a close call even for the ones that succeed.  However
high a startup may be flying now, it probably has a few leaves stuck
in the landing gear from those trees it barely cleared at the end
of the runway.  In such a close game, the smallest increase in the
forces against you can be enough to flick you over the edge into
failure.When we first started Y Combinator 
we encouraged people to start
startups while they were still in college.  That's partly because
Y Combinator began as a kind of summer program.  We've kept the
program shape—all of us having dinner together once a week turns
out to be a good idea—but we've decided now
that the party line should be to tell people to wait till they
graduate.Does that mean you can't start a startup in college?  Not at all.
Sam Altman, the co-founder of Loopt,
had just finished his sophomore year when we funded them, and Loopt
is probably the most promising of all the startups we've funded so
far.  But Sam Altman is a very unusual guy.  Within about three
minutes of meeting him, I remember thinking ""Ah, so this is what
Bill Gates must have been like when he was 19.""If it can work to start a startup during college, why do
we tell people not to?  For the same reason that the probably
apocryphal violinist, whenever he was asked to judge someone's
playing, would always say they didn't have enough talent to make
it as a pro.  Succeeding as a musician takes determination as well
as talent, so this answer works out to be the right advice for
everyone.  The ones who are uncertain believe it and give up, and
the ones who are sufficiently determined think ""screw that, I'll
succeed anyway.""So our official policy now is only to fund undergrads we can't talk
out of it.  And frankly, if you're not certain, you should wait.
It's not as if all the opportunities to start companies are going
to be gone if you don't do it now.  Maybe the window will close on
some idea you're working on, but that won't be the last idea you'll
have.  For every idea that times out, new ones become feasible.
Historically the opportunities to start startups have only increased
with time.In that case, you might ask, why not wait longer?  Why not go work
for a while, or go to grad school, and then start a startup?  And
indeed, that might be a good idea.  If I had to pick the sweet spot
for startup founders, based on who we're most excited to see
applications from, I'd say it's probably the mid-twenties.  Why?
What advantages does someone in their mid-twenties have over someone
who's 21?  And why isn't it older?  What can 25 year olds do that
32 year olds can't?  Those turn out to be questions worth examining.PlusIf you start a startup soon after college, you'll be a young founder
by present standards, so you should know what the relative advantages
of young founders are.  They're not what you might think.  As a
young founder your strengths are: stamina, poverty, rootlessness,
colleagues, and ignorance.The importance of stamina shouldn't be surprising.  If you've heard
anything about startups you've probably heard about the long hours.
As far as I can tell these are universal.  I can't think of any
successful startups whose founders worked 9 to 5.  And it's
particularly necessary for younger founders to work long hours
because they're probably not as efficient as they'll be later.Your second advantage, poverty, might not sound like an advantage,
but it is a huge one.  Poverty implies you can live cheaply,
and this is critically important for startups.  Nearly every startup
that fails, fails by running out of money.  It's a little misleading
to put it this way, because there's usually some other underlying
cause.  But regardless of the source of your problems, a low burn
rate gives you more opportunity to recover from them.  And since
most startups make all kinds of mistakes at first, room to recover
from mistakes is a valuable thing to have.Most startups end up doing something different than they planned.
The way the successful ones find something that works is by trying
things that don't.  So the worst thing you can do in a startup is
to have a rigid, pre-ordained plan and then start spending a lot
of money to implement it.  Better to operate cheaply and give your
ideas time to evolve.Recent grads can live on practically nothing, and this gives you
an edge over older founders, because the main cost in software
startups is people.  The guys with kids and mortgages are at a
real disadvantage.  This is one reason I'd bet on the 25 year old
over the 32 year old.  The 32 year old probably is a better programmer,
but probably also has a much more expensive life.  Whereas a 25
year old has some work experience (more on that later) but can live
as cheaply as an undergrad.Robert Morris and I were 29 and 30 respectively when we started
Viaweb, but fortunately we still lived like 23 year olds.  We both had
roughly zero assets.  I would have loved to have a mortgage,
since that would have meant I had a house.  But in retrospect
having nothing turned out to be convenient. I wasn't tied down and
I was used to living cheaply.Even more important than living cheaply, though, is thinking cheaply.
One reason the Apple II was so popular was that it was cheap.  The
computer itself was cheap, and it used cheap, off-the-shelf peripherals
like a cassette tape recorder for data storage and a TV as a monitor.
And you know why?  Because Woz designed this computer for himself,
and he couldn't afford anything more.We benefitted from the same phenomenon.  Our prices were
daringly low for the time.  The top level of service was
$300 a month, which was an order of magnitude below the norm.  In
retrospect this was a smart move, but we didn't do it because we
were smart.  $300 a month seemed like a lot of money to us.  Like
Apple, we created something inexpensive, and therefore popular,
simply because we were poor.A lot of startups have that form: someone comes along and makes
something for a tenth or a hundredth of what it used to cost, and
the existing players can't follow because they don't even want to
think about a world in which that's possible.  Traditional long
distance carriers, for example, didn't even want to think about
VoIP.  (It was coming, all the same.)  Being poor helps in this
game, because your own personal bias points in the same direction
technology evolves in.The advantages of rootlessness are similar to those of poverty.
When you're young you're more mobile—not just because you don't
have a house or much stuff, but also because you're less likely to
have serious relationships.  This turns out to be important, because
a lot of startups involve someone moving.The founders of Kiko, for example, are now en route to the Bay Area
to start their next startup.  It's a better place for what they
want to do.  And it was easy for them to decide to go, because
neither as far as I know has a serious girlfriend, and everything
they own will fit in one car—or more precisely, will either fit
in one car or is crappy enough that they don't mind leaving it
behind.They at least were in Boston.  What if they'd been in Nebraska,
like Evan Williams was at their age?   Someone wrote recently that
the drawback of Y Combinator was that you had to move to participate.
It couldn't be any other way. The kind of conversations we have
with founders, we have to have in person.  We fund a dozen startups
at a time, and we can't be in a dozen places at once.  But even if
we could somehow magically save people from moving, we wouldn't.
We wouldn't be doing founders a favor by letting them stay in
Nebraska.  Places that aren't 
startup hubs are toxic to startups.
You can tell that from indirect evidence.  You can tell how hard
it must be to start a startup in Houston or Chicago or Miami from
the microscopically small number, per capita, that succeed 
there.  I don't know exactly what's suppressing all the startups in these
towns—probably a hundred subtle little things—but something
must be.
[2]Maybe this will change.  Maybe the increasing cheapness of startups
will mean they'll be able to survive anywhere, instead of only in
the most hospitable environments.  Maybe 37signals is the pattern
for the future.  But maybe not.  Historically there have always
been certain towns that were centers for certain industries, and
if you weren't in one of them you were at a disadvantage.  So my
guess is that 37signals is an anomaly.  We're looking at a pattern
much older than ""Web 2.0"" here.Perhaps the reason more startups per capita happen in the Bay Area
than Miami is simply that there are more founder-type people there.
Successful startups are almost never started by one person.  Usually
they begin with a conversation in which someone mentions that
something would be a good idea for a company, and his friend says,
""Yeah, that is a good idea, let's try it.""  If you're missing that
second person who says ""let's try it,"" the startup never happens.
And that is another area where undergrads have an edge.  They're
surrounded by people willing to say that.  At a good college you're
concentrated together with a lot of other ambitious and technically
minded people—probably more concentrated than you'll ever be
again.  If your nucleus spits out a neutron, there's a good chance
it will hit another nucleus.The number one question people ask us at Y Combinator is: Where can
I find a co-founder?  That's the biggest problem for someone starting
a startup at 30.  When they were in school they knew a lot of good
co-founders, but by 30 they've either lost touch with them or these
people are tied down by jobs they don't want to leave.Viaweb was an anomaly in this respect too.  Though we were comparatively
old, we weren't tied down by impressive jobs.  I was trying to be
an artist, which is not very constraining, and Robert, though 29,
was still in grad school due to a little interruption in his academic
career back in 1988.  So arguably the Worm made Viaweb possible.
Otherwise Robert would have been a junior professor at that age,
and he wouldn't have had time to work on crazy speculative projects
with me.Most of the questions people ask Y Combinator we have some kind of
answer for, but not the co-founder question.  There is no good
answer.  Co-founders really should be people you already know.  And
by far the best place to meet them is school. You have a large
sample of smart people; you get to compare how they all perform on
identical tasks; and everyone's life is pretty fluid.  A lot of
startups grow out of schools for this reason.  Google, Yahoo, and
Microsoft, among others, were all founded by people who met in
school.  (In Microsoft's case, it was high school.)Many students feel they should wait and get a little more experience
before they start a company.  All other things being equal, they
should.  But all other things are not quite as equal as they look.
Most students don't realize how rich they are in the scarcest
ingredient in startups, co-founders.  If you wait too long, you may
find that your friends are now involved in some project they don't
want to abandon.  The better they are, the more likely this is to
happen.One way to mitigate this problem might be to actively plan your
startup while you're getting those n years of experience.  Sure,
go off and get jobs or go to grad school or whatever, but get
together regularly to scheme, so the idea of starting a startup
stays alive in everyone's brain.  I don't know if this works, but
it can't hurt to try.It would be helpful just to realize what an advantage you have as
students.  Some of your classmates are probably going to be successful
startup founders; at a great technical university, that is a near
certainty.  So which ones?  If I were you I'd look for the people
who are not just smart, but incurable 
builders.  
Look
for the people who keep starting projects, and finish at least some
of them.  That's what we look for.  Above all else, above academic
credentials and even the idea you apply with, we look for people
who build things.The other place co-founders meet is at work.  Fewer do than at
school, but there are things you can do to improve the odds.  The
most important, obviously, is to work somewhere that has a lot of
smart, young people.  Another is to work for a company located in
a startup hub.  It will be easier to talk a co-worker into quitting
with you in a place where startups are happening all around you.You might also want to look at the employment agreement you sign
when you get hired.  Most will say that any ideas you think of while
you're employed by the company belong to them.  In practice it's
hard for anyone to prove what ideas you had when, so the line gets
drawn at code.  If you're going to start a startup, don't write any
of the code while you're still employed.  Or at least discard any
code you wrote while still employed and start over.  It's not so
much that your employer will find out and sue you.  It won't come
to that; investors or acquirers or (if you're so lucky) underwriters
will nail you first.  Between t = 0 and when you buy that yacht,
someone is going to ask if any of your code legally belongs
to anyone else, and you need to be able to say no.
[3]The most overreaching employee agreement I've seen so far is Amazon's.
In addition to the usual clauses about owning your ideas, you also
can't be a founder of a startup that has another founder who worked
at Amazon—even if you didn't know them or even work there at the
same time. I suspect they'd have a hard time enforcing this, but
it's a bad sign they even try.  There are plenty of other places
to work; you may as well choose one that keeps more of your options
open.Speaking of cool places to work, there is of course Google.  But I
notice something slightly frightening about Google: zero startups
come out of there.  In that respect it's a black hole.  People seem
to like working at Google too much to leave.  So if you hope to start
a startup one day, the evidence so far suggests you shouldn't work
there.I realize this seems odd advice.  If they make your life so good
that you don't want to leave, why not work there?  Because, in
effect, you're probably getting a local maximum.  You need a certain
activation energy to start a startup.  So an employer who's fairly
pleasant to work for can lull you into staying indefinitely, even
if it would be a net win for you to leave.
[4]The best place to work, if you want to start a startup, is probably
a startup.  In addition to being the right sort of experience, one
way or another it will be over quickly.  You'll either end up rich,
in which case problem solved, or the startup will get bought, in
which case it it will start to suck to work there and it will be
easy to leave, or most likely, the thing will blow up and you'll
be free again.Your final advantage, ignorance, may not sound very useful.  I
deliberately used a controversial word for it; you might equally
call it innocence.  But it seems to be a powerful force.  My Y
Combinator co-founder Jessica Livingston is just about to publish
a book of interviews
with startup founders, and I noticed a remarkable pattern in them.
One after another said that if they'd known how hard it would be,
they would have been too intimidated to start.Ignorance can be useful when it's a counterweight to other forms
of stupidity.  It's useful in starting startups because you're
capable of more than you realize.  Starting startups is harder than
you expect, but you're also capable of more than you expect, so
they balance out.Most people look at a company like Apple and think, how could I
ever make such a thing?  Apple is an institution, and I'm just a
person.  But every institution was at one point just a handful of
people in a room deciding to start something.  Institutions are
made up, and made up by people no different from you.I'm not saying everyone could start a startup.  I'm sure most people
couldn't; I don't know much about the population at large.  When
you get to groups I know well, like hackers, I can say more precisely.
At the top schools, I'd guess as many as a quarter of the CS majors
could make it as startup founders if they wanted.That ""if they wanted"" is an important qualification—so important
that it's almost cheating to append it like that—because once you
get over a certain threshold of intelligence, which most CS majors
at top schools are past, the deciding factor in whether you succeed
as a founder is how much you want to.  You don't have to be that
smart.  If you're not a genius, just start a startup in some unsexy
field where you'll have less competition, like software for human
resources departments.  I picked that example at random, but I feel
safe in predicting that whatever they have now, it wouldn't take
genius to do better.   There are a lot of people out there working
on boring stuff who are desperately in need of better software, so
however short you think you fall of Larry and Sergey, you can ratchet
down the coolness of the idea far enough to compensate.As well as preventing you from being intimidated, ignorance can
sometimes help you discover new ideas.  Steve Wozniak
put this very strongly:

  All the best things that I did at Apple came from (a) not having
  money and (b) not having done it before, ever. Every single thing
  that we came out with that was really great, I'd never once done
  that thing in my life.

When you know nothing, you have to reinvent stuff for yourself, and
if you're smart your reinventions may be better than what preceded
them.  This is especially true in fields where the rules change.
All our ideas about software were developed in a time when processors
were slow, and memories and disks were tiny.  Who knows what obsolete
assumptions are embedded in the conventional wisdom?  And the way
these assumptions are going to get fixed is not by explicitly
deallocating them, but by something more akin to garbage collection.
Someone ignorant but smart will come along and reinvent everything,
and in the process simply fail to reproduce certain existing ideas.MinusSo much for the advantages of young founders.  What about the
disadvantages? I'm going to start with what goes wrong and try to
trace it back to the root causes.What goes wrong with young founders is that they build stuff that
looks like class projects.  It was only recently that we figured
this out ourselves.  We noticed a lot of similarities between the
startups that seemed to be falling behind, but we couldn't figure
out how to put it into words.  Then finally we realized what it
was: they were building class projects.But what does that really mean?  What's wrong with class projects?
What's the difference between a class project and a real startup?
If we could answer that question it would be useful not just to
would-be startup founders but to students in general, because we'd
be a long way toward explaining the mystery of the so-called real
world.There seem to be two big things missing in class projects: (1) an
iterative definition of a real problem and (2) intensity.The first is probably unavoidable.  Class projects will inevitably
solve fake problems.  For one thing, real problems are rare and
valuable.  If a professor wanted to have students solve real problems,
he'd face the same paradox as someone trying to give an example of
whatever ""paradigm"" might succeed the Standard Model of physics.
There may well be something that does, but if you could think of
an example you'd be entitled to the Nobel Prize.  Similarly, good
new problems are not to be had for the asking.In technology the difficulty is compounded by the fact that real
startups tend to discover the problem they're solving by a process
of evolution.  Someone has an idea for something; they build it;
and in doing so (and probably only by doing so) they realize
the problem they should be solving is another one.  Even if the
professor let you change your project description on the fly, there
isn't time enough to do that in a college class, or a market to
supply evolutionary pressures.  So class
projects are mostly about implementation, which is the least
of your problems in a startup.It's not just that in a startup you work on the idea as well as
implementation.  The very implementation is different.  Its main
purpose is to refine the idea.  Often the only value of most of the
stuff you build in the first six months is that it proves your
initial idea was mistaken.  And that's extremely valuable.  If
you're free of a misconception that everyone else still shares,
you're in a powerful position.  But you're not thinking that way
about a class project.  Proving your initial plan was mistaken would
just get you a bad grade.  Instead of building stuff to throw away,
you tend to want every line of code to go toward that final goal
of showing you did a lot of work.That leads to our second difference: the way class projects are
measured.  Professors will tend to judge you by the distance between
the starting point and where you are now.  If someone has achieved
a lot, they should get a good grade.  But customers will judge you
from the other direction: the distance remaining between where you
are now and the features they need.  The market doesn't give a shit
how hard you worked.  Users just want your software to do what they
need, and you get a zero otherwise.  That is one of the most
distinctive differences between school and the real world: there
is no reward for putting in a good effort.  In fact, the whole
concept of a ""good effort"" is a fake idea adults invented to encourage
kids.  It is not found in nature.Such lies seem to be helpful to kids.  But unfortunately when you
graduate they don't give you a list of all the lies they told you
during your education.  You have to get them beaten out of you by
contact with the real world.  And this is why so many jobs want
work experience.  I couldn't understand that when I was in college.
I knew how to program.  In fact, I could tell I knew how to program
better than most people doing it for a living.  So what was this
mysterious ""work experience"" and why did I need it?Now I know what it is, and part of the confusion is grammatical.
Describing it as ""work experience"" implies it's like experience
operating a certain kind of machine, or using a certain programming
language.  But really what work experience refers to is not some
specific expertise, but the elimination of certain habits left over
from childhood.One of the defining qualities of kids is that they flake.  When
you're a kid and you face some hard test, you can cry and say ""I
can't"" and they won't make you do it.  Of course, no one can make
you do anything in the grownup world either.  What they do instead
is fire you.  And when motivated by that
you find you can do a lot more than you realized.  So one of the
things employers expect from someone with ""work experience"" is the
elimination of the flake reflex—the ability to get things done,
with no excuses.The other thing you get from work experience is an understanding
of what work is, and in particular, how intrinsically horrible it
is.  Fundamentally the equation is a brutal one: you have to spend
most of your waking hours doing stuff someone else wants, or starve.
There are a few places where the work is so interesting that this
is concealed, because what other people want done happens to coincide
with what you want to work on.  But you only have to imagine what
would happen if they diverged to see the underlying reality.It's not so much that adults lie to kids about this as never explain
it.  They never explain what the deal is with money.  You know from
an early age that you'll have some sort of job, because everyone
asks what you're going to ""be"" when you grow up. What they
don't tell you is that as a kid you're sitting on the shoulders of
someone else who's treading water, and that starting working means
you get thrown into the water on your own, and have to start treading
water yourself or sink.  ""Being"" something is incidental; the
immediate problem is not to drown.The relationship between work and money tends to dawn on you only
gradually.  At least it did for me.  One's first thought tends to
be simply ""This sucks.  I'm in debt. Plus I have to get up on monday
and go to work.""  Gradually you realize that these two things are
as tightly connected as only a market can make them.So the most important advantage 24 year old founders have over 20
year old founders is that they know what they're trying to avoid.
To the average undergrad the idea of getting rich translates into
buying Ferraris, or being admired.  To someone who has learned from
experience about the relationship between money and work, it
translates to something way more important: it means you get to opt
out of the brutal equation that governs the lives of 99.9% of people.
Getting rich means you can stop treading water.Someone who gets this will work much harder at making a startup
succeed—with the proverbial energy of a drowning man, in fact.
But understanding the relationship between money and work also
changes the way you work.  You don't get money just for working,
but for doing things other people want.  Someone who's figured that
out will automatically focus more on the user.  And that cures the
other half of the class-project syndrome.  After you've been working
for a while, you yourself tend to measure what you've done the same
way the market does.Of course, you don't have to spend years working to learn this
stuff.  If you're sufficiently perceptive you can grasp these things
while you're still in school.  Sam Altman did.  He must have, because
Loopt is no class project.  And as his example suggests, this can
be valuable knowledge.  At a minimum, if you get this stuff, you
already have most of what you gain from the ""work experience""
employers consider so desirable.  But of course if you really get
it, you can use this information in a way that's more valuable to
you than that.NowSo suppose you think you might start a startup at some point, either
when you graduate or a few years after.  What should you do now?
For both jobs and grad school, there are ways to prepare while
you're in college.  If you want to get a job when you graduate, you
should get summer jobs at places you'd like to work.  If you want
to go to grad school, it will help to work on research projects as
an undergrad.  What's the equivalent for startups?  How do you keep
your options maximally open?One thing you can do while you're still in school is to learn how
startups work.  Unfortunately that's not easy.  Few if any colleges
have classes about startups.  There may be business school classes
on entrepreneurship, as they call it over there, but these are
likely to be a waste of time.  Business schools like to talk about
startups, but philosophically they're at the opposite end of the
spectrum.  Most books on startups also seem to be useless.  I've
looked at a few and none get it right.  Books in most fields are
written by people who know the subject from experience, but for
startups there's a unique problem:  by definition the founders of
successful startups don't need to write books to make money. As a
result most books on the subject end up being written by people who
don't understand it.So I'd be skeptical of classes and books.  The way to learn about
startups is by watching them in action, preferably by working at
one.  How do you do that as an undergrad?  Probably by sneaking in
through the back door. Just hang around a lot and gradually start
doing things for them.  Most startups are (or should be) very
cautious about hiring.  Every hire increases the burn rate, and bad
hires early on are hard to recover from.  However, startups usually
have a fairly informal atmosphere, and there's always a lot that
needs to be done.  If you just start doing stuff for them, many
will be too busy to shoo you away.  You can thus gradually work
your way into their confidence, and maybe turn it into an official
job later, or not, whichever you prefer.  This won't work for all
startups, but it would work for most I've known.Number two, make the most of the great advantage of school: the
wealth of co-founders.  Look at the people around you and ask
yourself which you'd like to work with.  When you apply that test,
you may find you get surprising results.  You may find you'd prefer
the quiet guy you've mostly ignored to someone who seems impressive
but has an attitude to match.  I'm not suggesting you suck up to
people you don't really like because you think one day they'll be
successful.  Exactly the opposite, in fact: you should only start
a startup with someone you like, because a startup will put your
friendship through a stress test.  I'm just saying you should think
about who you really admire and hang out with them, instead of
whoever circumstances throw you together with.Another thing you can do is learn skills that will be useful to you
in a startup.  These may be different from the skills you'd learn
to get a job.  For example, thinking about getting a job will make
you want to learn programming languages you think employers want,
like Java and C++.  Whereas if you start a startup, you get to pick
the language, so you have to think about which will actually let
you get the most done.  If you use that test you might end up
learning Ruby or Python instead.
But the most important skill for a startup founder isn't a programming
technique.  It's a knack for understanding users and figuring out
how to give them what they want.  I know I repeat this, but that's
because it's so important.  And it's a skill you can learn, though
perhaps habit might be a better word.  Get into the habit of thinking
of software as having users.  What do those users want?  What would
make them say wow?This is particularly valuable for undergrads, because the concept
of users is missing from most college programming classes.  The way
you get taught programming in college would be like teaching writing
as grammar, without mentioning that its purpose is to communicate
something to an audience.  Fortunately an audience for software is
now only an http request away.  So in addition to the programming
you do for your classes, why not build some kind of website people
will find useful?  At the very least it will teach you how to write
software with users.  In the best case, it might not just be
preparation for a startup, but the startup itself, like it was for
Yahoo and Google.Notes[1]
Even the desire to protect one's children seems weaker, judging
from things people have historically done to their kids
rather than risk their community's disapproval.  (I assume we still
do things that will be regarded in the future as barbaric, but
historical abuses are easier for us to see.)[2]
Worrying that Y Combinator makes founders move for 3 months
also suggests one underestimates how hard it is to start a startup.
You're going to have to put up with much greater inconveniences than
that.[3]
Most employee agreements
say that any idea relating to the company's present or potential
future business belongs to them.  Often as not the second clause could
include any possible startup, and anyone doing due diligence for an 
investor or acquirer will assume the worst.To be safe either (a) don't use code written while you
were still employed in your previous job, or (b) get your employer to
renounce, in writing, any claim to the code you write for your side   
project.  Many will consent to (b) rather than
lose a prized employee.  The downside is that you'll have to tell them
exactly what your project does.[4]
Geshke and Warnock only founded Adobe because Xerox ignored
them.  If Xerox had used what they built, they would probably 
never have left PARC.Thanks to Jessica Livingston and Robert Morris for reading
drafts of this, and to Jeff Arnold and the SIPB for inviting me to
speak.

Comment on this essay.Chinese TranslationArabic Translation","startup advice
",human,"human
","human
"
22,22,"September 2022I recently told applicants to Y Combinator that the best advice I
could give for getting in, per word, was 

  Explain what you've learned from users.

That tests a lot of things: whether you're paying attention to
users, how well you understand them, and even how much they need
what you're making.Afterward I asked myself the same question. What have I learned
from YC's users, the startups we've funded?The first thing that came to mind was that most startups have the
same problems. No two have exactly the same problems, but it's
surprising how much the problems remain the same, regardless of
what they're making. Once you've advised 100 startups all doing
different things, you rarely encounter problems you haven't seen
before.This fact is one of the things that makes YC work. But I didn't
know it when we started YC. I only had a few data points: our own
startup, and those started by friends. It was a surprise to me how
often the same problems recur in different forms. Many later stage
investors might never realize this, because later stage investors
might not advise 100 startups in their whole career, but a YC partner
will get this much experience in the first year or two.That's one advantage of funding large numbers of early stage companies
rather than smaller numbers of later-stage ones. You get a lot of
data. Not just because you're looking at more companies, but also
because more goes wrong.But knowing (nearly) all the problems startups can encounter doesn't
mean that advising them can be automated, or reduced to a formula.
There's no substitute for individual office hours with a YC partner.
Each startup is unique, which means they have to be advised
by specific partners who know them well.
[1]We learned that the hard way, in the notorious ""batch that broke
YC"" in the summer of 2012. Up till that point we treated the partners
as a pool. When a startup requested office hours, they got the next
available slot posted by any partner. That meant every partner had
to know every startup. This worked fine up to 60 startups, but when
the batch grew to 80, everything broke. The founders probably didn't
realize anything was wrong, but the partners were confused and
unhappy because halfway through the batch they still didn't know
all the companies yet.
[2]At first I was puzzled. How could things be fine at 60 startups and
broken at 80? It was only a third more. Then I realized what had
happened. We were using an O(n2) algorithm. So of course it blew
up.The solution we adopted was the classic one in these situations.
We sharded the batch into smaller groups of startups, each overseen
by a dedicated group of partners. That fixed the problem, and has
worked fine ever since. But the batch that broke YC was a powerful
demonstration of how individualized the process of advising startups
has to be.Another related surprise is how bad founders can be at realizing
what their problems are. Founders will sometimes come in to talk
about some problem, and we'll discover another much bigger one in
the course of the conversation. For example (and this case is all
too common), founders will come in to talk about the difficulties
they're having raising money, and after digging into their situation,
it turns out the reason is that the company is doing badly, and
investors can tell. Or founders will come in worried that they still
haven't cracked the problem of user acquisition, and the reason turns out
to be that their product isn't good enough. There have been times
when I've asked ""Would you use this yourself, if you hadn't built
it?"" and the founders, on thinking about it, said ""No."" Well, there's
the reason you're having trouble getting users.Often founders know what their problems are, but not their relative
importance.
[3]
They'll come in to talk about three problems
they're worrying about. One is of moderate importance, one doesn't
matter at all, and one will kill the company if it isn't addressed
immediately. It's like watching one of those horror movies where
the heroine is deeply upset that her boyfriend cheated on her, and
only mildly curious about the door that's mysteriously ajar. You
want to say: never mind about your boyfriend, think about that door!
Fortunately in office hours you can. So while startups still die
with some regularity, it's rarely because they wandered into a room
containing a murderer. The YC partners can warn them where the
murderers are.Not that founders listen. That was another big surprise: how often
founders don't listen to us. A couple weeks ago I talked to a partner
who had been working for YC for a couple batches and was starting
to see the pattern. ""They come back a year later,"" she said, ""and
say 'We wish we'd listened to you.'""It took me a long time to figure out why founders don't listen. At
first I thought it was mere stubbornness. That's part of the reason,
but another and probably more important reason is that so much about
startups is counterintuitive. 
And when you tell someone something
counterintuitive, what it sounds to them is wrong. So the reason
founders don't listen to us is that they don't believe us. At
least not till experience teaches them otherwise.
[4]The reason startups are so counterintuitive is that they're so
different from most people's other experiences. No one knows what
it's like except those who've done it. Which is why YC partners
should usually have been founders themselves. But strangely enough,
the counterintuitiveness of startups turns out to be another of the
things that make YC work. If it weren't counterintuitive, founders
wouldn't need our advice about how to do it.Focus is doubly important for early stage startups, because not
only do they have a hundred different problems, they don't have
anyone to work on them except the founders. If the founders focus
on things that don't matter, there's no one focusing on the things
that do. So the essence of what happens at YC is to figure out which
problems matter most, then cook up ideas for solving them — ideally
at a resolution of a week or less — and then try those ideas and
measure how well they worked. The focus is on action, with measurable,
near-term results.This doesn't imply that founders should rush forward regardless of
the consequences. If you correct course at a high enough frequency,
you can be simultaneously decisive at a micro scale and tentative
at a macro scale. The result is a somewhat winding path, but executed
very rapidly, like the path a running back takes downfield. And in
practice there's less backtracking than you might expect. Founders
usually guess right about which direction to run in, especially if
they have someone experienced like a YC partner to bounce their
hypotheses off. And when they guess wrong, they notice fast, because
they'll talk about the results at office hours the next week.
[5]A small improvement in navigational ability can make you a lot
faster, because it has a double effect: the path is shorter, and
you can travel faster along it when you're more certain it's the
right one. That's where a lot of YC's value lies, in helping founders
get an extra increment of focus that lets them move faster. And
since moving fast is the essence of a startup, YC in effect makes
startups more startup-like.Speed defines startups. Focus enables speed. YC improves focus.Why are founders uncertain about what to do? Partly because startups
almost by definition are doing something new, which means no one
knows how to do it yet, or in most cases even what ""it"" is. Partly
because startups are so counterintuitive generally. And partly
because many founders, especially young and ambitious ones, have
been trained to win the wrong way. That took me years to figure
out. The educational system in most countries trains you to win by
hacking the test 
instead of actually doing whatever it's supposed
to measure. But that stops working when you start a startup. So
part of what YC does is to retrain founders to stop trying to hack
the test. (It takes a surprisingly long time. A year in, you still
see them reverting to their old habits.)YC is not simply more experienced founders passing on their knowledge.
It's more like specialization than apprenticeship. The knowledge
of the YC partners and the founders have different shapes: It
wouldn't be worthwhile for a founder to acquire the encyclopedic
knowledge of startup problems that a YC partner has, just as it
wouldn't be worthwhile for a YC partner to acquire the depth of
domain knowledge that a founder has. That's why it can still be
valuable for an experienced founder to do YC, just as it can still
be valuable for an experienced athlete to have a coach.The other big thing YC gives founders is colleagues, and this may
be even more important than the advice of partners. If you look at
history, great work clusters around certain places and institutions:
Florence in the late 15th century, the University of Göttingen in
the late 19th, The New Yorker under Ross, Bell Labs, Xerox PARC.
However good you are, good colleagues make you better. Indeed, very
ambitious people probably need colleagues more than anyone else,
because they're so starved for them in everyday life.Whether or not YC manages one day to be listed alongside those
famous clusters, it won't be for lack of trying. We were very aware
of this historical phenomenon and deliberately designed YC to be
one. By this point it's not bragging to say that it's the biggest
cluster of great startup founders. Even people trying to attack YC
concede that.Colleagues and startup founders are two of the most powerful forces
in the world, so you'd expect it to have a big effect to combine
them. Before YC, to the extent people thought about the question
at all, most assumed they couldn't be combined — that loneliness
was the price of independence. That was how it felt to us when we
started our own startup in Boston in the 1990s. We had a handful
of older people we could go to for advice (of varying quality), but
no peers. There was no one we could commiserate with about the
misbehavior of investors, or speculate with about the future of
technology. I often tell founders to make something they themselves
want, and YC is certainly that: it was designed to be exactly what
we wanted when we were starting a startup.One thing we wanted was to be able to get seed funding without
having to make the rounds of random rich people. That has become a
commodity now, at least in the US. But great colleagues can never
become a commodity, because the fact that they cluster in some
places means they're proportionally absent from the rest.Something magical happens where they do cluster though. The energy
in the room at a YC dinner is like nothing else I've experienced.
We would have been happy just to have one or two other startups to
talk to. When you have a whole roomful it's another thing entirely.YC founders aren't just inspired by one another. They also help one
another. That's the happiest thing I've learned about startup
founders: how generous they can be in helping one another. We noticed
this in the first batch and consciously designed YC to magnify it.
The result is something far more intense than, say, a university.
Between the partners, the alumni, and their batchmates, founders
are surrounded by people who want to help them, and can.Notes[1] 
This is why I've never liked it when people refer to YC as a
""bootcamp."" It's intense like a bootcamp, but the opposite in
structure. Instead of everyone doing the same thing, they're each
talking to YC partners to figure out what their specific startup
needs.[2] 
When I say the summer 2012 batch was broken, I mean it felt
to the partners that something was wrong. Things weren't yet so
broken that the startups had a worse experience. In fact that batch
did unusually well.[3] 
This situation reminds me of the research showing that people
are much better at answering questions than they are at judging how
accurate their answers are. The two phenomena feel very similar.[4] 
The Airbnbs were 
particularly good at listening — partly
because they were flexible and disciplined, but also because they'd
had such a rough time during the preceding year. They were ready
to listen.[5] 
The optimal unit of decisiveness depends on how long it takes
to get results, and that depends on the type of problem you're
solving. When you're negotiating with investors, it could be a
couple days, whereas if you're building hardware it could be months.
Thanks to Trevor Blackwell, Jessica Livingston, 
Harj Taggar, and Garry Tan for reading drafts of this.","startup advice
",human,"human
","human
"
23,23,"March 2012Y Combinator's 7th birthday was March 11.   As usual we were so
busy we didn't notice till a few days after.  I don't think we've
ever managed to remember our birthday on our birthday.

On March 11 2005, Jessica and I were walking home from dinner in
Harvard Square.  Jessica was working at an investment bank at the
time, but she didn't like it much, so she had interviewed for a job
as director of marketing at a Boston VC fund.  The VC fund was doing
what now seems a comically familiar thing for a VC fund to do:
taking a long time to make up their mind.  Meanwhile I had been
telling Jessica all the things they should change about the VC
business — essentially the ideas now underlying Y Combinator:
investors
should be making more, smaller investments, they should be funding
hackers instead of suits, they should be willing to fund younger
founders, etc.

At the time I had been thinking about doing some angel investing.  I
had just given a talk to the undergraduate computer club at Harvard
about
how to start a
startup, and it
hit me afterward that although I had always
meant to do angel investing, 7 years had now passed since I got
enough money to do it, and I still hadn't started.  I had also
been thinking about ways to work with Robert Morris and Trevor
Blackwell again.  A few hours before I had
sent them an email trying to figure out what we could do together.

Between Harvard Square and my house the idea gelled.  We'd start
our own investment firm and Jessica could work for that instead.
As we turned onto Walker Street we decided to do it.  I agreed to
put $100k into the new fund and Jessica agreed to quit her job to
work for it.  Over the next couple days I recruited Robert
and Trevor, who put in another $50k each.  So YC
started with $200k.

Jessica was so happy to be able to quit her job and start her own
company that I took her picture
 when we got home.

The company wasn't called Y Combinator yet. At first we called it
Cambridge Seed.  But that name never saw the light of day, because
by the time we announced it a few days later, we'd changed the name
to Y Combinator.  We realized early on that what we were doing could
be national in scope and we didn't want a name that tied us to one
place.

Initially we only had part of the idea. We were going to do
seed funding with standardized terms.  Before YC, seed funding was
very haphazard. You'd get that first $10k from your friend's rich
uncle. The deal terms were often a disaster; often neither the
investor nor the founders nor the lawyer knew what the documents
should look like.  Facebook's early history as a Florida LLC shows
how random things could be in those days.  We were going to be
something there had not been before: a standard source of seed
funding.

We modelled YC on the seed funding we ourselves had taken
when we started Viaweb.  We started Viaweb with $10k we got from
our friend Julian Weber,
the husband of Idelle Weber, whose
painting class I took as a grad student at Harvard.  Julian knew
about business, but you would not describe him as a suit.  Among
other things he'd been president of the National Lampoon.  He was
also a lawyer, and got all our paperwork set up properly.  In return
for $10k, getting us set up as a company, teaching us what
business was about, and remaining calm in times of crisis, Julian
got 10% of Viaweb.  I remember thinking once what a good deal
Julian got.  And then a second later I realized that without
Julian, Viaweb would never have made it.  So even though it was a
good deal for him, it was a good deal for us too.  That's why I
knew there was room for something like Y Combinator.

Initially we didn't have what turned out to be the most important
idea: funding startups synchronously, instead of asynchronously as
it had always been done before.  Or rather we had the idea, but we
didn't realize its significance.  We decided very early 
that the first thing we'd do would
be to fund a bunch of startups over the coming summer.  But we
didn't realize initially that this would be the way we'd do all our
investing.  The reason we began by funding a bunch of startups at
once was not that we thought it would be a better way to fund
startups, but simply because we wanted to learn how to be angel
investors, and a summer program for undergrads seemed the fastest
way to do it.  No one takes summer jobs that seriously.  The
opportunity cost for a bunch of undergrads to spend a summer working
on startups was low enough that we wouldn't feel guilty encouraging
them to do it.

We knew students would already be making plans for the summer, so
we did what we're always telling startups to do: we launched fast.
Here are the
initial announcement
and description of what
was at the time called the Summer Founders Program.

We got lucky in that the length and structure of a summer program
turns out to be perfect for what we do.
The structure of the YC cycle is still almost identical to what
it was that first summer.

We also got lucky in who the first batch of founders were.  We never
expected to make any money from that first batch.  We thought of
the money we were investing as a combination of an educational expense
and a charitable donation.  But the
founders in the first batch turned out to be surprisingly good.
And great people too.  We're still friends with a lot of them today.

It's hard for people to realize now how inconsequential YC seemed at the
time.  I can't blame people who didn't take us seriously, because
we ourselves didn't take that first summer program seriously in the
very beginning.  But as the summer progressed we were increasingly
impressed by how well the startups were doing.  Other people started
to be impressed too.  Jessica and I invented a term, ""the Y Combinator
effect,"" to describe the moment when the realization hit someone
that YC was not totally lame.  When people came to YC to speak
at the dinners that first summer, they came in the spirit of someone
coming to address a Boy Scout troop.  By the time they left the
building they were all saying some variant of ""Wow, these
companies might actually succeed.""

Now YC is well enough known that people are no longer surprised
when the companies we fund are legit, but it took a
while for reputation to catch up with reality.  That's one of the
reasons we especially like funding ideas that might be dismissed
as ""toys"" — because YC itself was dismissed as one initially.

When we saw how well it worked to fund companies synchronously,
we decided we'd keep doing that.  We'd fund two batches of
startups a year.

We funded the second batch in Silicon Valley.  That was
a last minute decision.  In retrospect I think what pushed me over
the edge was going to Foo Camp that fall.  The density of startup
people in the Bay Area was so much greater than in Boston, and the
weather was so nice.  I remembered that from living there in the
90s.  Plus I didn't want someone else to copy us and describe it
as the Y Combinator of Silicon Valley.  I wanted YC to be the Y Combinator 
of Silicon Valley.  So doing the winter batch in California
seemed like one of those rare cases where the self-indulgent choice
and the ambitious one were the same.

If we'd had enough time to do what we wanted, Y Combinator would
have been in Berkeley.  That was our favorite part of the Bay Area.
But we didn't have time to get a building in Berkeley.  We didn't
have time to get our own building anywhere. The only way to get
enough space in time was to convince Trevor to let us take over
part of his (as it then seemed) giant building in Mountain View.
Yet again we lucked out, because Mountain View turned out to be the
ideal place to put something like YC.  But even then we barely made
it.  The first dinner in California, we had to warn all the founders
not to touch the walls, because the paint was still wet.","startup advice
",human,"human
","human
"
24,24,"

Want to start a startup?  Get funded by
Y Combinator.




August 2007(This is a talk I gave at the last 
Y Combinator dinner of the summer. 
Usually we don't have a speaker at the last dinner; it's more of
a party.  But it seemed worth spoiling the atmosphere if I could
save some of the startups from
preventable deaths.  So at the last minute I cooked up this rather
grim talk.  I didn't mean this as an essay; I wrote it down
because I only had two hours before dinner and think fastest while
writing.)
A couple days ago I told a reporter that we expected about a third
of the companies we funded to succeed.  Actually I was being
conservative.  I'm hoping it might be as much as a half.  Wouldn't
it be amazing if we could achieve a 50% success rate?Another way of saying that is that half of you are going to die.  Phrased
that way, it doesn't sound good at all.  In fact, it's kind of weird
when you think about it, because our definition of success is that
the founders get rich.  If half the startups we fund succeed, then
half of you are going to get rich and the other half are going to
get nothing.If you can just avoid dying, you get rich.  That sounds like a joke,
but it's actually a pretty good description of what happens in a
typical startup.  It certainly describes what happened in Viaweb.
We avoided dying till we got rich.It was really close, too.  When we were visiting Yahoo to talk about
being acquired, we had to interrupt everything and borrow one of
their conference rooms to talk down an investor who was about to
back out of a new funding round we needed to stay alive.   So even
in the middle of getting rich we were fighting off the grim reaper.You may have heard that quote about luck consisting of opportunity
meeting preparation.  You've now done the preparation.  The work
you've done so far has, in effect, put you in a position to get
lucky: you can now get rich by not letting your company die.  That's
more than most people have.  So let's talk about how not to die.We've done this five times now, and we've seen a bunch of startups
die.  About 10 of them so far.  We don't know exactly what happens
when they die, because they generally don't die loudly and heroically.
Mostly they crawl off somewhere and die.For us the main indication of impending doom is when we don't hear
from you.  When we haven't heard from, or about, a startup for a
couple months, that's a bad sign.  If we send them an email asking
what's up, and they don't reply, that's a really bad sign.  So far
that is a 100% accurate predictor of death.Whereas if a startup regularly does new deals and releases and
either sends us mail or shows up at YC events, they're probably
going to live.I realize this will sound naive, but maybe the linkage works in
both directions.  Maybe if you can arrange that we keep hearing
from you, you won't die.That may not be so naive as it sounds.  You've probably noticed
that having dinners every Tuesday with us and the other founders
causes you to get more done than you would otherwise, because every
dinner is a mini Demo Day.  Every dinner is a kind of a deadline.
So the mere constraint of staying in regular contact with us will
push you to make things happen, because otherwise you'll be embarrassed
to tell us that you haven't done anything new since the last time
we talked.If this works, it would be an amazing hack.  It would be pretty
cool if merely by staying in regular contact with us you could get
rich.  It sounds crazy, but there's a good chance that would work.A variant is to stay in touch with other YC-funded startups.  There
is now a whole neighborhood of them in San Francisco.  If you move
there, the peer pressure that made you work harder all summer will
continue to operate.When startups die, the official cause of death is always either
running out of money or a critical founder bailing.  Often the two
occur simultaneously.  But I think the underlying cause is usually
that they've become demoralized.  You rarely hear of a startup
that's working around the clock doing deals and pumping out new
features, and dies because they can't pay their bills and their ISP
unplugs their server.Startups rarely die in mid keystroke.  So keep typing!If so many startups get demoralized and fail when merely by hanging
on they could get rich, you have to assume that running a startup
can be demoralizing.  That is certainly true.  I've been there, and
that's why I've never done another startup.  The low points in a
startup are just unbelievably low.  I bet even Google had moments
where things seemed hopeless.Knowing that should help.  If you know it's going to feel terrible
sometimes, then when it feels terrible you won't think ""ouch, this
feels terrible, I give up.""  It feels that way for everyone.  And
if you just hang on, things will probably get better.  The metaphor
people use to describe the way a startup feels is at least a roller
coaster and not drowning.  You don't just sink and sink; there are
ups after the downs.Another feeling that seems alarming but is in fact normal in a
startup is the feeling that what you're doing isn't working.  The
reason you can expect to feel this is that what you do probably
won't work.  Startups almost never get it right the first time.
Much more commonly you launch something, and no one cares.  Don't
assume when this happens that you've failed.  That's normal for
startups.  But don't sit around doing nothing.  Iterate.I like Paul Buchheit's suggestion of trying to make something that
at least someone really loves.  As long as you've made something
that a few users are ecstatic about, you're on the right track.  It
will be good for your morale to have even a handful of users who
really love you, and startups run on morale.  But also it
will tell you what to focus on.  What is it about you that they
love?  Can you do more of that?  Where can you find more people who
love that sort of thing?  As long as you have some core of users
who love you, all you have to do is expand it.  It may take a while,
but as long as you keep plugging away, you'll win in the end.  Both
Blogger and Delicious did that.  Both took years to succeed.  But
both began with a core of fanatically devoted users, and all Evan
and Joshua had to do was grow that core incrementally.  
Wufoo is
on the same trajectory now.So when you release something and it seems like no one cares, look
more closely.  Are there zero users who really love you, or is there
at least some little group that does?  It's quite possible there
will be zero.  In that case, tweak your product and try again.
Every one of you is working on a space that contains at least one
winning permutation somewhere in it.  If you just keep trying,
you'll find it.Let me mention some things not to do.  The number one thing not to
do is other things.  If you find yourself saying a sentence that
ends with ""but we're going to keep working on the startup,"" you are
in big trouble.  Bob's going to grad school, but we're going to
keep working on the startup.  We're moving back to Minnesota, but
we're going to keep working on the startup.  We're taking on some
consulting projects, but we're going to keep working on the startup.
You may as well just translate these to ""we're giving up on the
startup, but we're not willing to admit that to ourselves,"" because
that's what it means most of the time.  A startup is so hard that
working on it can't be preceded by ""but.""In particular, don't go to graduate school, and don't start other
projects.  Distraction is fatal to startups.  Going to (or back to)
school is a huge predictor of death because in addition to the
distraction it gives you something to say you're doing.  If you're
only doing a startup, then if the startup fails, you fail.  If
you're in grad school and your startup fails, you can say later ""Oh
yeah, we had this startup on the side when I was in grad school,
but it didn't go anywhere.""You can't use euphemisms like ""didn't go anywhere"" for something
that's your only occupation.  People won't let you.One of the most interesting things we've discovered from working
on Y Combinator is that founders are more motivated by the fear of
looking bad than by the hope of getting millions of dollars.   So
if you want to get millions of dollars, put yourself in a position
where failure will be public and humiliating.When we first met the founders of 
Octopart, they seemed very smart,
but not a great bet to succeed, because they didn't seem especially
committed.  One of the two founders was still in grad school.  It
was the usual story: he'd drop out if it looked like the startup
was taking off.  Since then he has not only dropped out of grad
school, but appeared full length in 
Newsweek 
with the word ""Billionaire""
printed across his chest.  He just cannot fail now.  Everyone he
knows has seen that picture.  Girls who dissed him in high school
have seen it.  His mom probably has it on the fridge.  It would be
unthinkably humiliating to fail now.  At this point he is committed
to fight to the death.I wish every startup we funded could appear in a Newsweek article
describing them as the next generation of billionaires, because
then none of them would be able to give up.  The success rate would
be 90%.  I'm not kidding.When we first knew the Octoparts they were lighthearted, cheery
guys.  Now when we talk to them they seem grimly determined.  The
electronic parts distributors are trying to squash them to keep
their monopoly pricing.  (If it strikes you as odd that people still
order electronic parts out of thick paper catalogs in 2007, there's
a reason for that.  The distributors want to prevent the transparency
that comes from having prices online.)  I feel kind of bad that
we've transformed these guys from lighthearted to grimly determined.
But that comes with the territory.  If a startup succeeds, you get
millions of dollars, and you don't get that kind of money just by
asking for it.  You have to assume it takes some amount of pain.And however tough things get for the Octoparts, I predict they'll
succeed.  They may have to morph themselves into something totally
different, but they won't just crawl off and die.  They're smart;
they're working in a promising field; and they just cannot give up.All of you guys already have the first two.  You're all smart and
working on promising ideas.  Whether you end up among the living
or the dead comes down to the third ingredient, not giving up.So I'll tell you now: bad shit is coming.  It always is in a startup.
The odds of getting from launch to liquidity without some kind of
disaster happening are one in a thousand.  So don't get demoralized.
When the disaster strikes, just say to yourself, ok, this was what
Paul was talking about.  What did he say to do?  Oh, yeah.  Don't
give up.Japanese TranslationArabic Translation","startup advice
",human,"human
","human
"
25,25,"October 2015When I talk to a startup that's been operating for more than 8 or
9 months, the first thing I want to know is almost always the same.
Assuming their expenses remain constant and their revenue growth
is what it has been over the last several months, do they make it to
profitability on the money they have left?  Or to put it more
dramatically, by default do they live or die?The startling thing is how often the founders themselves don't know.
Half the founders I talk to don't know whether they're default alive
or default dead.If you're among that number, Trevor Blackwell has made a handy
calculator you can use to find out.The reason I want to know first whether a startup is default alive
or default dead is that the rest of the conversation depends on the
answer.  If the company is default alive, we can talk about ambitious
new things they could do.  If it's default dead, we probably need
to talk about how to save it.  We know the current trajectory ends
badly.  How can they get off that trajectory?Why do so few founders know whether they're default alive or default
dead?  Mainly, I think, because they're not used to asking that.
It's not a question that makes sense to ask early on, any more than
it makes sense to ask a 3 year old how he plans to support
himself.  But as the company grows older, the question switches from
meaningless to critical.  That kind of switch often takes people
by surprise.I propose the following solution: instead of starting to ask too
late whether you're default alive or default dead, start asking too
early.  It's hard to say precisely when the question switches
polarity.  But it's probably not that dangerous to start worrying
too early that you're default dead, whereas it's very dangerous to
start worrying too late.The reason is a phenomenon I wrote about earlier: the
fatal pinch.
The fatal pinch is default dead + slow growth + not enough
time to fix it.  And the way founders end up in it is by not realizing
that's where they're headed.There is another reason founders don't ask themselves whether they're
default alive or default dead: they assume it will be easy to raise
more money.  But that assumption is often false, and worse still, the
more you depend on it, the falser it becomes.Maybe it will help to separate facts from hopes. Instead of thinking
of the future with vague optimism, explicitly separate the components.
Say ""We're default dead, but we're counting on investors to save
us."" Maybe as you say that, it will set off the same alarms in your
head that it does in mine.  And if you set off the alarms sufficiently
early, you may be able to avoid the fatal pinch.It would be safe to be default dead if you could count on investors
saving you.  As a rule their interest is a function of
growth.  If you have steep revenue growth, say over 5x a year, you
can start to count on investors being interested even if you're not
profitable.
[1]
But investors are so fickle that you can never
do more than start to count on them.  Sometimes something about your
business will spook investors even if your growth is great.  So no
matter how good your growth is, you can never safely treat fundraising
as more than a plan A. You should always have a plan B as well: you
should know (as in write down) precisely what you'll need to do to
survive if you can't raise more money, and precisely when you'll 
have to switch to plan B if plan A isn't working.In any case, growing fast versus operating cheaply is far from the
sharp dichotomy many founders assume it to be.  In practice there
is surprisingly little connection between how much a startup spends
and how fast it grows.  When a startup grows fast, it's usually
because the product hits a nerve, in the sense of hitting some big
need straight on.  When a startup spends a lot, it's usually because
the product is expensive to develop or sell, or simply because
they're wasteful.If you're paying attention, you'll be asking at this point not just
how to avoid the fatal pinch, but how to avoid being default dead.
That one is easy: don't hire too fast.  Hiring too fast is by far
the biggest killer of startups that raise money.
[2]Founders tell themselves they need to hire in order to grow.  But
most err on the side of overestimating this need rather than
underestimating it.  Why?  Partly because there's so much work to
do.  Naive founders think that if they can just hire enough
people, it will all get done.  Partly because successful startups have
lots of employees, so it seems like that's what one does in order
to be successful.  In fact the large staffs of successful startups
are probably more the effect of growth than the cause.  And
partly because when founders have slow growth they don't want to
face what is usually the real reason: the product is not appealing
enough.Plus founders who've just raised money are often encouraged to
overhire by the VCs who funded them.  Kill-or-cure strategies are
optimal for VCs because they're protected by the portfolio effect.
VCs want to blow you up, in one sense of the phrase or the other.
But as a founder your incentives are different.  You want above all
to survive.
[3]Here's a common way startups die.  They make something moderately
appealing and have decent initial growth. They raise their first
round fairly easily, because the founders seem smart and the idea
sounds plausible. But because the product is only moderately
appealing, growth is ok but not great.  The founders convince
themselves that hiring a bunch of people is the way to boost growth.
Their investors agree.  But (because the product is only moderately
appealing) the growth never comes.  Now they're rapidly running out
of runway.  They hope further investment will save them. But because
they have high expenses and slow growth, they're now unappealing
to investors. They're unable to raise more, and the company dies.What the company should have done is address the fundamental problem:
that the product is only moderately appealing.  Hiring people is
rarely the way to fix that.  More often than not it makes it harder.
At this early stage, the product needs to evolve more than to be
""built out,"" and that's usually easier with fewer people.
[4]Asking whether you're default alive or default dead may save you
from this.  Maybe the alarm bells it sets off will counteract the
forces that push you to overhire.  Instead you'll be compelled to
seek growth in other ways. For example, by doing
things that don't scale, or by redesigning the product in the
way only founders can.
And for many if not most startups, these paths to growth will be
the ones that actually work.Airbnb waited 4 months after raising money at the end of Y Combinator
before they hired their first employee.  In the meantime the founders
were terribly overworked.  But they were overworked evolving Airbnb
into the astonishingly successful organism it is now.Notes[1]
Steep usage growth will also interest investors.  Revenue
will ultimately be a constant multiple of usage, so x% usage growth
predicts x% revenue growth.  But in practice investors discount
merely predicted revenue, so if you're measuring usage you need a
higher growth rate to impress investors.[2]
Startups that don't raise money are saved from hiring too
fast because they can't afford to. But that doesn't mean you should
avoid raising money in order to avoid this problem, any more than
that total abstinence is the only way to avoid becoming an alcoholic.[3]
I would not be surprised if VCs' tendency to push founders
to overhire is not even in their own interest.  They don't know how
many of the companies that get killed by overspending might have
done well if they'd survived.  My guess is a significant number.[4]
After reading a draft, Sam Altman wrote:""I think you should make the hiring point more strongly.  I think
it's roughly correct to say that YC's most successful companies
have never been the fastest to hire, and one of the marks of a great
founder is being able to resist this urge.""Paul Buchheit adds:""A related problem that I see a lot is premature scaling—founders
take a small business that isn't really working (bad unit economics,
typically) and then scale it up because they want impressive growth
numbers. This is similar to over-hiring in that it makes the business
much harder to fix once it's big, plus they are bleeding cash really
fast.""
Thanks to Sam Altman, Paul Buchheit, Joe Gebbia, Jessica Livingston,
and Geoff Ralston for reading drafts of this.","startup advice
",human,"human
","human
"
26,26,"January 2015No one, VC or angel, has invested in more of the top startups than
Ron Conway.  He knows what happened in every deal in the Valley,
half the time because he arranged it.And yet he's a super nice guy.  In fact, nice is not the word.
Ronco is good. I know of zero instances in which he has behaved
badly.  It's hard even to imagine.When I first came to Silicon Valley I thought ""How lucky that someone
so powerful is so benevolent.""  But gradually I realized it wasn't
luck.  It was by being benevolent that Ronco became so powerful.
All the deals he gets to invest in come to him through referrals.
Google did. Facebook did. Twitter was a referral from Evan Williams
himself.  And the reason so many people refer deals to him is that
he's proven himself to be a good guy.Good does not mean being a pushover.  I would not want to face an
angry Ronco.  But if Ron's angry at you, it's because you did
something wrong.  Ron is so old school he's Old Testament.  He will
smite you in his just wrath, but there's no malice in it.In almost every domain there are advantages to seeming good.  It
makes people trust you.  But actually being good is an expensive
way to seem good.  To an amoral person it might seem to be overkill.In some fields it might be, but apparently not in the startup world.
Though plenty of investors are jerks, there is a clear trend among
them: the most successful investors are also the most upstanding. 
[1]It was not always this way.  I would not feel confident saying that
about investors twenty years ago.What changed?  The startup world became more transparent and more
unpredictable.  Both make it harder to seem good without actually
being good.It's obvious why transparency has that effect.  When an investor
maltreats a founder now, it gets out.  Maybe not all the way to the
press, but other founders hear about it, and that investor
starts to lose deals. 
[2]The effect of unpredictability is more subtle.  It increases the
work of being inconsistent.  If you're going to be two-faced, you
have to know who you should be nice to and who you can get away
with being nasty to.  In the startup world, things change so rapidly
that you can't tell.  The random college kid you talk to today might
in a couple years be the CEO of the hottest startup in the Valley.
If you can't tell who to be nice to, you have to be nice to everyone.
And probably the only people who can manage that are the people who
are genuinely good.In a sufficiently connected and unpredictable world, you can't seem
good without being good.As often happens, Ron discovered how to be the investor of the
future by accident.  He didn't foresee the future of startup
investing, realize it would pay to be upstanding, and force himself
to behave that way. It would feel unnatural to him to behave any
other way.  He was already 
living in the future.Fortunately that future is not limited to the startup world.  The
startup world is more transparent and unpredictable than most, but
almost everywhere the trend is in that direction.Notes[1]
I'm not saying that if you sort investors by benevolence
you've also sorted them by returns, but rather that if you do a
scatterplot with benevolence on the x axis and returns on the y,
you'd see a clear upward trend.[2]
Y Combinator in particular, because it aggregates data
from so many startups, has a pretty comprehensive view of
investor behavior.
Thanks to Sam Altman and Jessica Livingston for reading drafts of
this.Japanese Translation","startup advice
",human,"human
","human
"
27,27,"

Want to start a startup?  Get funded by
Y Combinator.




July 2010I realized recently that what one thinks about in the shower in the
morning is more important than I'd thought.  I knew it was a good
time to have ideas.  Now I'd go further: now I'd say it's hard to
do a really good job on anything you don't think about in the shower.Everyone who's worked on difficult problems is probably familiar
with the phenomenon of working hard to figure something out, failing,
and then suddenly seeing the answer a bit later while doing something
else. There's a kind of thinking you do without trying to.  I'm
increasingly convinced this type of thinking is not merely helpful
in solving hard problems, but necessary.  The tricky part is, you
can only control it indirectly.
[1]I think most people have one top idea in their mind at any given
time.  That's the idea their thoughts will drift toward when they're
allowed to drift freely.  And this idea will thus tend to get all
the benefit of that type of thinking, while others are starved of
it.  Which means it's a disaster to let the wrong idea become the
top one in your mind.What made this clear to me was having an idea I didn't want as the
top one in my mind for two long stretches.I'd noticed startups got way less done when they started raising
money, but it was not till we ourselves raised money that I understood
why.  The problem is not the actual time it takes to meet with
investors.  The problem is that once you start raising money, raising
money becomes the top idea in your mind.  That becomes what you
think about when you take a shower in the morning.  And that means
other questions aren't.I'd hated raising money when I was running Viaweb, but I'd forgotten
why I hated it so much.  When we raised money for Y Combinator, I
remembered.  Money matters are particularly likely to become the
top idea in your mind.  The reason is that they have to be.  It's
hard to get money.  It's not the sort of thing that happens by
default.  It's not going to happen unless you let it become the
thing you think about in the shower.  And then you'll make little
progress on anything else you'd rather be working on.
[2](I hear similar complaints from friends who are professors.  Professors
nowadays seem to have become professional fundraisers who do a
little research on the side.  It may be time to fix that.)The reason this struck me so forcibly is that for most of the
preceding 10 years I'd been able to think about what I wanted.  So
the contrast when I couldn't was sharp.  But I don't think this
problem is unique to me, because just about every startup I've seen
grinds to a halt when they start raising money — or talking
to acquirers.You can't directly control where your thoughts drift.  If you're
controlling them, they're not drifting.  But you can control them
indirectly, by controlling what situations you let yourself get
into.  That has been the lesson for me: be careful what you let
become critical to you.  Try to get yourself into situations where
the most urgent problems are ones you want to think about.You don't have complete control, of course.  An emergency could
push other thoughts out of your head.  But barring emergencies you
have a good deal of indirect control over what becomes the top idea
in your mind.I've found there are two types of thoughts especially worth
avoiding — thoughts like the Nile Perch in the way they push
out more interesting ideas.  One I've already mentioned: thoughts
about money. Getting money is almost by definition an attention
sink.
The other is disputes.  These too are engaging in the
wrong way: they have the same velcro-like shape as genuinely
interesting ideas, but without the substance.  So avoid disputes
if you want to get real work done.
[3]Even Newton fell into this trap.  After publishing his theory of
colors in 1672 he found himself distracted by disputes for years,
finally concluding that the only solution was to stop publishing:

  I see I have made myself a slave to Philosophy, but if I get free
  of Mr Linus's business I will resolutely bid adew to it eternally,
  excepting what I do for my privat satisfaction or leave to come
  out after me.  For I see a man must either resolve to put out
  nothing new or become a slave to defend it.
[4]

Linus and his students at Liege were among the more tenacious
critics.  Newton's biographer Westfall seems to feel he was
overreacting:

  Recall that at the time he wrote, Newton's ""slavery"" consisted
  of five replies to Liege, totalling fourteen printed pages, over
  the course of a year.

I'm more sympathetic to Newton. The problem was not the 14 pages,
but the pain of having this stupid controversy constantly reintroduced
as the top idea in a mind that wanted so eagerly to think about
other things.Turning the other cheek turns out to have selfish advantages.
Someone who does you an injury hurts you twice: first by the injury
itself, and second by taking up your time afterward thinking about
it.  If you learn to ignore injuries you can at least avoid the
second half.  I've found I can to some extent avoid thinking about
nasty things people have done to me by telling myself: this doesn't
deserve space in my head.  I'm always delighted to find I've forgotten
the details of disputes, because that means I hadn't been thinking
about them.  My wife thinks I'm more forgiving than she is, but my
motives are purely selfish.I suspect a lot of people aren't sure what's the top idea in their
mind at any given time.  I'm often mistaken about it.  I tend to
think it's the idea I'd want to be the top one, rather than the one
that is.  But it's easy to figure this out: just take a shower.
What topic do your thoughts keep returning to?  If it's not what
you want to be thinking about, you may want to change something.Notes[1]
No doubt there are already names for this type of thinking, but
I call it ""ambient thought.""[2]
This was made particularly clear in our case, because neither
of the funds we raised was difficult, and yet in both cases the
process dragged on for months.  Moving large amounts of money around
is never something people treat casually.  The attention required
increases with the amount—maybe not linearly, but definitely
monotonically.[3]
Corollary: Avoid becoming an administrator, or your job will
consist of dealing with money and disputes.[4]
Letter to Oldenburg, quoted in Westfall, Richard, Life of
Isaac Newton, p. 107.Thanks to Sam Altman, Patrick Collison, Jessica Livingston,
and Robert Morris for reading drafts of this.","startup advice
",human,"human
","human
"
28,28,"

Want to start a startup?  Get funded by
Y Combinator.




October 2005(This essay is derived from a talk at the 2005 
Startup School.)How do you get good ideas for 
startups?  That's probably the number
one question people ask me.I'd like to reply with another question: why do people think it's
hard to come up with ideas for startups?That might seem a stupid thing to ask.  Why do they think
it's hard?  If people can't do it, then it is hard, at least
for them.  Right?Well, maybe not.  What people usually say is not that they can't
think of ideas, but that they don't have any.  That's not quite the
same thing.  It could be the reason they don't have any is that
they haven't tried to generate them.I think this is often the case.  I think people believe that coming
up with ideas for startups is very hard-- that it must be
very hard-- and so they don't try do to it.  They assume ideas are
like miracles: they either pop into your head or they don't.I also have a theory about why people think this.  They overvalue
ideas.  They think creating a startup is just a matter of implementing
some fabulous initial idea.  And since a successful startup is worth
millions of dollars, a good idea is therefore a million dollar idea.If coming up with an idea for a startup equals coming up with a
million dollar idea, then of course it's going to seem hard.  Too
hard to bother trying.  Our instincts tell us something so valuable
would not be just lying around for anyone to discover.Actually, startup ideas are not million dollar ideas, and here's
an experiment you can try to prove it: just try to sell one.  Nothing
evolves faster than markets.  The fact that there's no market for
startup ideas suggests there's no demand.  Which means, in the
narrow sense of the word, that startup ideas are worthless.QuestionsThe fact is, most startups end up nothing like the initial idea.
It would be closer to the truth to say the main value of your initial
idea is that, in the process of discovering it's broken, you'll
come up with your real idea.The initial idea is just a starting point-- not a blueprint, but a
question.  It might help if they were expressed that way.  Instead
of saying that your idea is to make a collaborative, web-based
spreadsheet, say: could one make a collaborative, web-based
spreadsheet?  A few grammatical tweaks, and a woefully incomplete
idea becomes a promising question to explore.There's a real difference, because an assertion provokes objections
in a way a question doesn't.  If you say: I'm going to build a
web-based spreadsheet, then critics-- the most dangerous of which
are in your own head-- will immediately reply that you'd be competing
with Microsoft, that you couldn't give people the kind of UI they
expect, that users wouldn't want to have their data on your servers,
and so on.A question doesn't seem so challenging.  It becomes: let's try
making a web-based spreadsheet and see how far we get.  And everyone
knows that if you tried this you'd be able to make something
useful.  Maybe what you'd end up with wouldn't even be a spreadsheet.
Maybe it would be some kind of new spreasheet-like collaboration
tool that doesn't even have a name yet.  You wouldn't have thought
of something like that except by implementing your way toward it.Treating a startup idea as a question changes what you're looking
for.  If an idea is a blueprint, it has to be right.  But if it's
a question, it can be wrong, so long as it's wrong in a way that
leads to more ideas.One valuable way for an idea to be wrong is to be only a partial
solution.  When someone's working on a problem that seems too
big, I always ask: is there some way to bite off some subset of the
problem, then gradually expand from there?  That will generally
work unless you get trapped on a local maximum, like 1980s-style
AI, or C.UpwindSo far, we've reduced the problem from thinking of a million dollar
idea to thinking of a mistaken question.  That doesn't seem so hard,
does it?To generate such questions you need two things: to be familiar with 
promising new technologies, and to have the right kind of friends.
New technologies are the ingredients startup ideas are made of, and
conversations with friends are the kitchen they're cooked in.Universities have both, and that's why so many startups grow out
of them.  They're filled with new technologies, because they're
trying to produce research, and only things that are new count as
research.  And they're full of exactly the right kind of people to   
have ideas with: the other students, who will be not only smart but
elastic-minded to a fault.The opposite extreme would be a well-paying but boring job at a big
company.  Big companies are biased against new technologies, and
the people you'd meet there would be wrong too.In an essay I wrote for high school students,  
I said a good rule of thumb was to stay upwind-- to
work on things that maximize your future options. The principle
applies for adults too, though perhaps it has to be modified to:
stay upwind for as long as you can, then cash in the potential
energy you've accumulated when you need to pay for kids.I don't think people consciously realize this, but one reason
downwind jobs like churning out Java for a bank pay so well is   
precisely that they are downwind.  The market price for that kind
of work is higher because it gives you fewer options for the future.
A job that lets you work on exciting new stuff will tend to pay
less, because part of the compensation is in the form of the new
skills you'll learn.Grad school is the other end of the spectrum from a coding job at
a big company: the pay's low but you spend most of your time working
on new stuff.  And of course, it's called ""school,"" which makes
that clear to everyone, though in fact all jobs are some percentage
school.The right environment for having startup ideas need not be a
university per se.  It just has to be a situation with a large
percentage of school.It's obvious why you want exposure to new technology, but why do 
you need other people?  Can't you just think of new ideas yourself?
The empirical answer is: no.  Even Einstein needed people to bounce
ideas off.  Ideas get developed in the process of explaining them
to the right kind of person.  You need that resistance, just
as a carver needs the resistance of the wood.This is one reason Y Combinator has a rule against investing in 
startups with only one founder.  Practically every successful company
has at least two.  And because startup founders work under great   
pressure, it's critical they be friends.I didn't realize it till I was writing this, but that may help
explain why there are so few female startup founders.  I read on
the Internet (so it must be true) that only 1.7% of VC-backed
startups are founded by women.  The percentage of female hackers
is small, but not that small.  So why the discrepancy?When you realize that successful startups tend to have multiple
founders who were already friends, a
possible explanation emerges.  People's best friends are likely to  
be of the same sex, and if one group is a minority in some population,
pairs of them will be a minority squared.
[1]DoodlingWhat these groups of co-founders do together is more complicated 
than just sitting down and trying to think of ideas.  I suspect the  
most productive setup is a kind of together-alone-together sandwich.
Together you talk about some hard problem, probably getting nowhere.
Then, the next morning, one of you has an idea in the shower about
how to solve it.  He runs eagerly to to tell the others, and together
they work out the kinks.What happens in that shower?  It seems to me that ideas just pop
into my head.  But can we say more than that?Taking a shower is like a form of meditation.  You're alert, but
there's nothing to distract you.  It's in a situation like this,
where your mind is free to roam, that it bumps into new ideas.What happens when your mind wanders?  It may be like doodling.  Most
people have characteristic ways of doodling.  This habit is  
unconscious, but not random: I found my doodles changed after I 
started studying painting.  I started to make the kind of gestures
I'd make if I were drawing from life.  They were atoms of drawing,  
but arranged randomly.
[2]Perhaps letting your mind wander is like doodling with ideas.  You
have certain mental gestures you've learned in your work, and when
you're not paying attention, you keep making these same gestures,   
but somewhat randomly.  In effect, you call the same functions on
random arguments.  That's what a metaphor is: a function applied   
to an argument of the wrong type.Conveniently, as I was writing this, my mind wandered: would it be
useful to have metaphors in a programming language?  I don't know;
I don't have time to think about this.  But it's convenient because
this is an example of what I mean by habits of mind.  I spend a lot
of time thinking about language design, and my habit of always   
asking ""would x be useful in a programming language"" just got
invoked.If new ideas arise like doodles, this would explain why you have
to work at something for a while before you have any.  It's not
just that you can't judge ideas till you're an expert in a field.
You won't even generate ideas, because you won't have any habits
of mind to invoke.Of course the habits of mind you invoke on some field don't have
to be derived from working in that field.  In fact, it's often
better if they're not.  You're not just looking for good ideas, but
for good new ideas, and you have a better chance of generating
those if you combine stuff from distant fields.  As hackers, one
of our habits of mind is to ask, could one open-source x?  For  
example, what if you made an open-source operating system?  A fine
idea, but not very novel.  Whereas if you ask, could you make an
open-source play?  you might be onto something.Are some kinds of work better sources of habits of mind than others?
I suspect harder fields may be better sources, because to attack
hard problems you need powerful solvents.  I find math is a good
source of metaphors-- good enough that it's worth studying just for
that.  Related fields are also good sources, especially when they're
related in unexpected ways.  Everyone knows computer science and
electrical engineering are related, but precisely because everyone
knows it, importing ideas from one to the other doesn't yield great
profits.  It's like importing something from Wisconsin to Michigan.  
Whereas (I claim) hacking and painting are
also related, in the sense that hackers and painters are both 
makers,
and this source of new ideas is practically virgin territory.ProblemsIn theory you could stick together ideas at random and see what you
came up with.  What if you built a peer-to-peer dating site?  Would
it be useful to have an automatic book?  Could you turn theorems
into a commodity?  When you assemble ideas at random like this, 
they may not be just stupid, but semantically ill-formed.  What 
would it even mean to make theorems a commodity?  You got me.  I
didn't think of that idea, just its name.You might come up with something useful this way, but I never have.
It's like knowing a fabulous sculpture is hidden inside a block of
marble, and all you have to do is remove the marble that isn't part
of it.  It's an encouraging thought, because it reminds you there   
is an answer, but it's not much use in practice because the search
space is too big.I find that to have good ideas I need to be working on some problem.
You can't start with randomness.  You have to start with a problem,
then let your mind wander just far enough for new ideas to form.In a way, it's harder to see problems than their solutions.  Most  
people prefer to remain in denial about problems.  It's obvious
why: problems are irritating.  They're problems!  Imagine if people
in 1700 saw their lives the way we'd see them.  It would have been
unbearable.  This denial is such a powerful force that, even when 
presented with possible solutions, people often prefer to believe
they wouldn't work.I saw this phenomenon when I worked on spam filters.  In 2002, most
people preferred to ignore spam, and most of those who didn't
preferred to believe the heuristic filters then available were the
best you could do.I found spam intolerable, and I felt it had to be possible to
recognize it statistically.  And it turns out that was all you  
needed to solve the problem.  The algorithm I used was ridiculously
simple.  Anyone who'd really tried to solve the problem would have
found it.  It was just that no one had really tried to solve the
problem.
[3]Let me repeat that recipe: finding the problem intolerable and  
feeling it must be possible to solve it.  Simple as it seems, that's
the recipe for a lot of startup ideas.WealthSo far most of what I've said applies to ideas in general.  What's  
special about startup ideas?  Startup ideas are ideas for companies,
and companies have to make money.  And the way to make money is to
make something people want.Wealth is what people want.  I don't mean that as some kind of  
philosophical statement; I mean it as a tautology.So an idea for a startup is an idea for something people want.
Wouldn't any good idea be something people want?  Unfortunately 
not.  I think new theorems are a fine thing to create, but there
is no great demand for them.  Whereas there appears to be great
demand for celebrity gossip magazines.  Wealth is defined democratically.
Good ideas and valuable ideas are not quite the same thing; the
difference is individual tastes.But valuable ideas are very close to good ideas, especially in
technology.  I think they're so close that you can get away with
working as if the goal were to discover good ideas, so long as, in
the final stage, you stop and ask: will people actually pay for 
this?  Only a few ideas are likely to make it that far and then get
shot down; RPN calculators might be one example.One way to make something people want is to look at stuff people   
use now that's broken.  Dating sites are a prime example.  They   
have millions of users, so they must be promising something people 
want.  And yet they work horribly.  Just ask anyone who uses them.  
It's as if they used the worse-is-better approach but stopped after
the first stage and handed the thing over to marketers.Of course, the most obvious breakage in the average computer user's 
life is Windows itself.  But this is a special case: you can't
defeat a monopoly by a frontal attack.  Windows can and will be     
overthrown, but not by giving people a better desktop OS.  The way
to kill it is to redefine the problem as a superset of the current 
one.  The problem is not, what operating system should people use
on desktop computers?  but how should people use applications?
There are answers to that question that don't even involve desktop
computers.Everyone thinks Google is going to solve this problem, but it is a
very subtle one, so subtle that a company as big as Google might
well get it wrong.  I think the odds are better than 50-50 that the
Windows killer-- or more accurately, Windows transcender-- will
come from some little startup.Another classic way to make something people want is to take a
luxury and make it into a commmodity.  People must want something
if they pay a lot for it.  And it is a very rare product that can't
be made dramatically cheaper if you try.This was Henry Ford's plan.  He made cars, which had been a luxury
item, into a commodity.  But the idea is much older than Henry Ford.
Water mills transformed mechanical power from a luxury into a
commodity, and they were used in the Roman empire.  Arguably
pastoralism transformed a luxury into a commodity.When you make something cheaper you can sell more of them.  But if
you make something dramatically cheaper you often get qualitative
changes, because people start to use it in different ways.  For
example, once computers get so cheap that most people can have one
of their own, you can use them as communication devices.Often to make something dramatically cheaper you have to redefine 
the problem.  The Model T didn't have all the features previous
cars did.  It only came in black, for example.  But it solved the
problem people cared most about, which was getting from place to
place.One of the most useful mental habits I know I learned from Michael
Rabin: that the best way to solve a problem is often to redefine
it.  A lot of people use this technique without being consciously
aware of it, but Rabin was spectacularly explicit.  You need a big
prime number?  Those are pretty expensive.  How about if I give you
a big number that only has a 10 to the minus 100 chance of not being
prime?  Would that do?  Well, probably; I mean, that's probably
smaller than the chance that I'm imagining all this anyway.Redefining the problem is a particularly juicy heuristic when you
have competitors, because it's so hard for rigid-minded people to 
follow.  You can work in plain sight and they don't realize the 
danger.  Don't worry about us. We're just working on search.  Do   
one thing and do it well, that's our motto.Making things cheaper is actually a subset of a more general
technique: making things easier.  For a long time it was most of  
making things easier, but now that the things we build are so
complicated, there's another rapidly growing subset: making things  
easier to use.This is an area where there's great room for improvement.  What you
want to be able to say about technology is: it just works.  How
often do you say that now?Simplicity takes effort-- genius, even.  The average programmer   
seems to produce UI designs that are almost willfully bad. I was   
trying to use the stove at my mother's house a couple weeks ago. 
It was a new one, and instead of physical knobs it had buttons and
an LED display.  I tried pressing some buttons I thought would cause
it to get hot, and you know what it said?  ""Err.""  Not even ""Error.""
""Err.""  You can't just say ""Err"" to the user of a stove.
You should design the UI so that errors are impossible.  And the  
boneheads who designed this stove even had an example of such a UI
to work from: the old one.  You turn one knob to set the temperature
and another to set the timer.  What was wrong with that?  It just
worked.It seems that, for the average engineer, more options just means
more rope to hang yourself.  So if you want to start a startup, you
can take almost any existing technology produced by a big company, 
and assume you could build something way easier to use.Design for ExitSuccess for a startup approximately equals getting bought.  You
need some kind of exit strategy, because you can't get the smartest
people to work for you without giving them options likely to be
worth something.  Which means you either have to get bought or go
public, and the number of startups that go public is very small.If success probably means getting bought, should you make that a
conscious goal?  The old answer was no: you were supposed to pretend
that you wanted to create a giant, public company, and act surprised
when someone made you an offer.  Really, you want to buy us? Well,
I suppose we'd consider it, for the right price.I think things are changing.  If 98% of the time success means   
getting bought, why not be open about it?  If 98% of the time you're
doing product development on spec for some big company, why not
think of that as your task?  One advantage of this approach is that
it gives you another source of ideas: look at big companies, think
what they should 
be doing, and do it yourself.  Even if
they already know it, you'll probably be done faster.Just be sure to make something multiple acquirers will want.  Don't
fix Windows, because the only potential acquirer is Microsoft, and  
when there's only one acquirer, they don't have to hurry.  They can
take their time and copy you instead of buying you.  If you want
to get market price, work on something where there's competition.If an increasing number of startups are created to do product
development on spec, it will be a natural counterweight to monopolies.
Once some type of technology is captured by a monopoly, it will    
only evolve at big company rates instead of startup rates, whereas
alternatives will evolve with especial speed.  A free market
interprets monopoly as damage and routes around it.The Woz RouteThe most productive way to generate startup ideas is also the
most unlikely-sounding: by accident.  If you look at how famous
startups got started, a lot of them weren't initially supposed to  
be startups.  Lotus began with a program Mitch Kapor wrote for a
friend. Apple got started because Steve Wozniak wanted to build
microcomputers, and his employer, Hewlett-Packard, wouldn't let him
do it at work.  Yahoo began as David Filo's personal collection of
links.This is not the only way to start startups.  You can sit down and
consciously come up with an idea for a company; we did.  But measured
in total market cap, the build-stuff-for-yourself model might be  
more fruitful.  It certainly has to be the most fun way to come up
with startup ideas.  And since a startup ought to have multiple
founders who were already friends before they decided to start a 
company, the rather surprising conclusion is that the best way to   
generate startup ideas is to do what hackers do for fun: cook up
amusing hacks with your friends.It seems like it violates some kind of conservation law, but there
it is: the best way to get a ""million dollar idea"" is just to do
what hackers enjoy doing anyway.
Notes[1]
This phenomenon may account for a number of discrepancies
currently blamed on various forbidden isms.  Never attribute to    
malice what can be explained by math.[2]   
A lot of classic abstract expressionism is doodling of this type:
artists trained to paint from life using the same gestures but
without using them to represent anything.  This explains why such
paintings are (slightly) more interesting than random marks would be.[3]
Bill Yerazunis had solved the problem, but he got there by
another path.  He made a general-purpose file classifier so good
that it also worked for spam.One Specific IdeaRomanian TranslationJapanese TranslationTraditional Chinese TranslationRussian TranslationArabic Translation","startup advice
",human,"human
","human
"
29,29,"February 2008The fiery reaction to the release of Arc had
an unexpected consequence: it made me realize I had a design
philosophy.  The main complaint of the more articulate critics was
that Arc seemed so flimsy. After years of working on it, all I had
to show for myself were a few thousand lines of macros?  Why hadn't
I worked on more substantial problems?As I was mulling over these remarks it struck me how familiar they
seemed.  This was exactly the kind of thing people said at first
about Viaweb, and Y Combinator, and most of my essays.When we launched Viaweb, it seemed laughable to VCs and e-commerce
""experts.""  We were just a couple guys in an apartment,
which did not seem cool in 1995 the way it does now.  And the thing
we'd built, as far as they could tell, wasn't even software.
Software, to them, equalled big, honking Windows apps.  Since Viaweb
was the first web-based app 
they'd seen, it seemed to be nothing
more than a website.  They were even more contemptuous when they
discovered that Viaweb didn't process credit card transactions (we
didn't for the whole first year).  Transaction processing seemed
to them what e-commerce was all about.  It sounded serious and
difficult.And yet, mysteriously, Viaweb ended up crushing all its competitors.The initial reaction to 
Y Combinator was almost identical.  It
seemed laughably lightweight.  Startup funding meant series A rounds:
millions of dollars given to a small number of startups founded by
people with established credentials after months of serious,
businesslike meetings, on terms described in a document a foot
thick.  Y Combinator seemed inconsequential.  It's too early to say
yet whether Y Combinator will turn out like Viaweb, but judging
from the number of imitations, a lot of people seem to think we're
on to something.I can't measure whether my essays are successful, except in page
views, but the reaction to them is at least different from when I
started.  At first the default reaction of the Slashdot trolls was
(translated into articulate terms): ""Who is this guy and what
authority does he have to write about these topics?  I haven't read
the essay, but there's no way anything so short and written in such
an informal style could have anything useful to say about such and
such topic, when people with degrees in the subject have already
written many thick books about it."" Now there's a new generation
of trolls on a new generation of sites, but they have at least
started to omit the initial ""Who is this guy?""Now people are saying the same things about Arc that they said at
first about Viaweb and Y Combinator and most of my essays.  Why the
pattern?  The answer, I realized, is that my m.o. for all four has
been the same.Here it is: I like to find (a) simple solutions (b) to overlooked
problems (c) that actually need to be solved, and (d) deliver them
as informally as possible, (e) starting with a very crude version
1, then (f) iterating rapidly.When I first laid out these principles explicitly, I noticed something
striking: this is practically a recipe for generating a contemptuous
initial reaction.  Though simple solutions are better, they don't
seem as impressive as complex ones.  Overlooked problems are by
definition problems that most people think don't matter.  Delivering
solutions in an informal way means that instead of judging something
by the way it's presented, people have to actually understand it,
which is more work.  And starting with a crude version 1 means your
initial effort is always small and incomplete.I'd noticed, of course, that people never seemed to grasp new ideas
at first.  I thought it was just because most people were stupid.
Now I see there's more to it than that.   Like a
contrarian investment fund, someone following this strategy will
almost always be doing things that seem wrong to the average person.As with contrarian investment strategies, that's exactly the point.
This technique is successful (in the long term) because it gives you
all the advantages other people forgo by trying to seem legit.  If
you work on overlooked problems, you're more likely to discover new
things, because you have less competition.  If you deliver solutions
informally, you (a) save all the effort you would have had to expend
to make them look impressive, and (b) avoid the danger of fooling
yourself as well as your audience.  And if you release a crude
version 1 then iterate, your solution can benefit from the imagination
of nature, which, as Feynman pointed out, is more powerful than
your own.In the case of Viaweb, the simple solution was to make the software
run on the server.  The overlooked problem was to generate web sites
automatically; in 1995, online stores were all made by hand by human
designers, but we knew this wouldn't scale.  The part that actually
mattered was graphic design, not transaction processing.
The informal delivery mechanism was me, showing up in jeans and a
t-shirt at some retailer's office.  And the crude version 1 was,
if I remember correctly, less than 10,000 lines of code when we
launched.The power of this technique extends beyond startups and programming
languages and essays.  It probably extends to any kind of creative
work.  Certainly it can be used in painting: this is exactly 
what Cezanne and Klee did.At Y Combinator we bet money on it, in the sense that we encourage
the startups we fund to work this way. There are always new ideas
right under your nose.  So look for simple things that other people
have overlooked—things people will later claim were 
""obvious""—especially when they've been led astray by obsolete 
conventions,
or by trying to do things that are superficially impressive.  Figure
out what the real problem is, and make sure you solve that.  Don't
worry about trying to look corporate; the product is what wins in
the long term.  And launch as soon as you can, so you start learning
from users what you should have been making.Reddit is a classic example of 
this approach.  When Reddit first
launched, it seemed like there was nothing to it.  To the graphically
unsophisticated its deliberately minimal design seemed like no
design at all.  But Reddit solved the real problem, which was to
tell people what was new and otherwise stay out of the way.  As a
result it became massively successful.  Now that conventional ideas
have caught up with it, it seems obvious.  People look at Reddit
and think the founders were lucky.  Like all such things, it was
harder than it looked.  The Reddits pushed so hard against the
current that they reversed it; now it looks like they're merely
floating downstream.So when you look at something like Reddit and think ""I wish I could
think of an idea like that,"" remember: ideas like that are all
around you.  But you ignore them because they look wrong.","startup advice
",human,"human
","human
"
30,30,"March 2024I met the Reddits before we even started Y Combinator. In fact they
were one of the reasons we started it.YC grew out of a talk I gave to the Harvard Computer Society (the
undergrad computer club) about how to start a startup. Everyone
else in the audience was probably local, but Steve and Alexis came
up on the train from the University of Virginia, where they were
seniors. Since they'd come so far I agreed to meet them for coffee.
They told me about the startup idea we'd later fund them to drop:
a way to order fast food on your cellphone.This was before smartphones. They'd have had to make deals with
cell carriers and fast food chains just to get it launched. So it
was not going to happen. It still doesn't exist, 19 years later.
But I was impressed with their brains and their energy. In fact I
was so impressed with them and some of the other people I met at
that talk that I decided to start something to fund them. A few
days later I told Steve and Alexis that we were starting Y Combinator,
and encouraged them to apply.That first batch we didn't have any way to identify applicants, so
we made up nicknames for them. The Reddits were the ""Cell food
muffins."" ""Muffin"" is a term of endearment Jessica uses for things
like small dogs and two year olds. So that gives you some idea what
kind of impression Steve and Alexis made in those days. They had
the look of slightly ruffled surprise that baby birds have.Their idea was bad though. And since we thought then that we were
funding ideas rather than founders, we rejected them. But we felt
bad about it. Jessica was sad that we'd rejected the muffins. And
it seemed wrong to me to turn down the people we'd been inspired
to start YC to fund.I don't think the startup sense of the word ""pivot"" had been invented
yet, but we wanted to fund Steve and Alexis, so if their idea was
bad, they'd have to work on something else. And I knew what else.
In those days there was a site called Delicious where you could
save links. It had a page called del.icio.us/popular that listed
the most-saved links, and people were using this page as a de facto
Reddit. I knew because a lot of the traffic to my site was coming
from it. There needed to be something like del.icio.us/popular, but
designed for sharing links instead of being a byproduct of saving
them.So I called Steve and Alexis and said that we liked them, just not
their idea, so we'd fund them if they'd work on something else.
They were on the train home to Virginia at that point. They got off
at the next station and got on the next train north, and by the end
of the day were committed to working on what's now called Reddit.They would have liked to call it Snoo, as in ""What snoo?"" But
snoo.com was too expensive, so they settled for calling the mascot
Snoo and picked a name for the site that wasn't registered. Early
on Reddit was just a provisional name, or so they told me at least,
but it's probably too late to change it now.As with all the really great startups, there's an uncannily close
match between the company and the founders. Steve in particular.
Reddit has a certain personality — curious, skeptical, ready to
be amused — and that personality is Steve's.Steve will roll his eyes at this, but he's an intellectual; he's
interested in ideas for their own sake. That was how he came to be
in that audience in Cambridge in the first place. He knew me because
he was interested in a programming language I've written about
called Lisp, and Lisp is one of those languages few people learn
except out of intellectual curiosity. Steve's kind of vacuum-cleaner
curiosity is exactly what you want when you're starting a site
that's a list of links to literally anything interesting.Steve was not a big fan of authority, so he also liked the idea of
a site without editors. In those days the top forum for programmers
was a site called Slashdot. It was a lot like Reddit, except the
stories on the frontpage were chosen by human moderators. And though
they did a good job, that one small difference turned out to be a
big difference. Being driven by user submissions meant Reddit was
fresher than Slashdot. News there was newer, and users will always
go where the newest news is.I pushed the Reddits to launch fast. A version one didn't need to
be more than a couple hundred lines of code. How could that take
more than a week or two to build? And they did launch comparatively
fast, about three weeks into the first YC batch. The first users
were Steve, Alexis, me, and some of their YC batchmates and college
friends. It turns out you don't need that many users to collect a
decent list of interesting links, especially if you have multiple
accounts per user.Reddit got two more people from their YC batch: Chris Slowe and
Aaron Swartz, and they too were unusually smart. Chris was just
finishing his PhD in physics at Harvard. Aaron was younger, a college
freshman, and even more anti-authority than Steve. It's not
exaggerating to describe him as a martyr for what authority later
did to him.Slowly but inexorably Reddit's traffic grew. At first the numbers
were so small they were hard to distinguish from background noise.
But within a few weeks it was clear that there was a core of real
users returning regularly to the site. And although all kinds of
things have happened to Reddit the company in the years since,
Reddit the site never looked back.Reddit the site (and now app) is such a fundamentally useful thing
that it's almost unkillable. Which is why, despite a long stretch
after Steve left when the management strategy ranged from benign
neglect to spectacular blunders, traffic just kept growing. You
can't do that with most companies. Most companies you take your eye
off the ball for six months and you're in deep trouble. But Reddit
was special, and when Steve came back in 2015, I knew the world was
in for a surprise.People thought they had Reddit's number: one of the players in
Silicon Valley, but not one of the big ones. But those who knew
what had been going on behind the scenes knew there was more to the
story than this. If Reddit could grow to the size it had with
management that was harmless at best, what could it do if Steve
came back? We now know the answer to that question. Or at least a
lower bound on the answer. Steve is not out of ideas yet.","startup advice
",human,"human
","human
"
31,31,"

Want to start a startup?  Get funded by
Y Combinator.




January 2012There are great startup ideas lying around unexploited right under
our noses.  One reason we don't see them is a phenomenon I call
schlep blindness.  Schlep was originally a Yiddish word but has
passed into general use in the US.  It means a tedious, unpleasant
task.No one likes schleps, but hackers especially dislike them.  
Most hackers who start startups wish they could do it by just writing
some clever software, putting it on a server somewhere, and watching
the money roll in—without ever having to talk to users, or negotiate
with other companies, or deal with other people's broken code.
Maybe that's possible, but I haven't seen it.One of the many things we do at Y Combinator is teach hackers about
the inevitability of schleps.  No, you can't start a startup by
just writing code.  I remember going through this realization myself.
There was a point in 1995 when I was still trying to convince myself
I could start a company by just writing code.  But I soon learned
from experience that schleps are not merely inevitable, but pretty
much what business consists of.  A company is defined by the schleps
it will undertake.  And schleps should be dealt with the same way
you'd deal with a cold swimming pool: just jump in.  Which is not
to say you should seek out unpleasant work per se, but that you
should never shrink from it if it's on the path to something great.The most dangerous thing about our dislike of schleps is that much
of it is unconscious.  Your unconscious won't even let you see ideas
that involve painful schleps.  That's schlep blindness.The phenomenon isn't limited to startups.  Most people don't
consciously decide not to be in as good physical shape as Olympic
athletes, for example.  Their unconscious mind decides for them,
shrinking from the work involved.The most striking example I know of schlep blindness is 
Stripe, or
rather Stripe's idea.  For over a decade, every hacker who'd ever
had to process payments online knew how painful the experience was.
Thousands of people must have known about this problem.  And yet
when they started startups, they decided to build recipe sites, or
aggregators for local events.  Why?  Why work on problems few care
much about and no one will pay for, when you could fix one of the
most important components of the world's infrastructure?  Because
schlep blindness prevented people from even considering the idea
of fixing payments.Probably no one who applied to Y Combinator to work on a recipe
site began by asking ""should we fix payments, or build a recipe
site?"" and chose the recipe site.  Though the idea of fixing payments
was right there in plain sight, they never saw it, because their
unconscious mind shrank from the complications involved.  You'd
have to make deals with banks.  How do you do that?  Plus you're
moving money, so you're going to have to deal with fraud, and people
trying to break into your servers.  Plus there are probably all
sorts of regulations to comply with.  It's a lot more intimidating
to start a startup like this than a recipe site.That scariness makes ambitious ideas doubly valuable.  In addition
to their intrinsic value, they're like undervalued stocks in the
sense that there's less demand for them among founders.  If you
pick an ambitious idea, you'll have less competition, because
everyone else will have been frightened off by the challenges
involved.  (This is also true of starting a startup generally.)How do you overcome schlep blindness?  Frankly, the most valuable
antidote to schlep blindness is probably ignorance.  Most successful
founders would probably say that if they'd known when they were
starting their company about the obstacles they'd have to overcome,
they might never have started it.  Maybe that's one reason the most
successful startups of all so often have young founders.In practice the founders grow with the problems.  But no one seems
able to foresee that, not even older, more experienced founders.
So the reason younger founders have an advantage is that they make
two mistakes that cancel each other out.  They don't know how much
they can grow, but they also don't know how much they'll need to.
Older founders only make the first mistake.Ignorance can't solve everything though.  Some ideas so obviously
entail alarming schleps that anyone can see them.  How do you see
ideas like that?  The trick I recommend is to take yourself out of
the picture.  Instead of asking ""what problem should I solve?"" ask
""what problem do I wish someone else would solve for me?""  If someone
who had to process payments before Stripe had tried asking that,
Stripe would have been one of the first things they wished for.It's too late now to be Stripe, but there's plenty still broken in
the world, if you know how to see it.Thanks to Sam Altman, Paul Buchheit, Patrick Collison,
Aaron Iba, Jessica Livingston, Emmett Shear, and Harj Taggar
for reading drafts of this.","startup advice
",human,"human
","human
"
32,32,"

Want to start a startup?  Get funded by
Y Combinator.




September 2013Most startups that raise money do it more than once.  A typical
trajectory might be (1) to get started with a few tens of thousands
from something like Y Combinator or individual angels, then 
(2) raise a few hundred thousand to a few million to build the company,
and then (3) once the company is clearly succeeding, raise one or
more later rounds to accelerate growth.Reality can be messier.  Some companies raise money twice in phase
2.  Others skip phase 1 and go straight to phase 2.  And at Y Combinator 
we get an increasing number of companies that have already
raised amounts in the hundreds of thousands.  But the three phase
path is at least the one about which individual startups' paths
oscillate.This essay focuses on phase 2 fundraising.  That's the type the
startups we fund are doing on Demo Day, and this essay is the advice
we give them.
ForcesFundraising is hard in both senses: hard like lifting a heavy weight,
and hard like solving a puzzle.  It's hard like lifting a weight
because it's intrinsically hard to convince people to part with
large sums of money.  That problem is irreducible; it should be
hard.  But much of the other kind of difficulty can be eliminated.
Fundraising only seems a puzzle because it's an alien world to most
founders, and I hope to fix that by supplying a map through it.To founders, the behavior of investors is often opaque — partly
because their motivations are obscure, but partly because they
deliberately mislead you.  And the misleading ways of investors
combine horribly with the wishful thinking of inexperienced founders.
At YC we're always warning founders about this danger, and investors
are probably more circumspect with YC startups than with other
companies they talk to, and even so we witness a constant series
of explosions as these two volatile components combine.
[1]If you're an inexperienced founder, the only way to survive is by
imposing external constraints on yourself.  You can't trust your
intuitions.  I'm going to give you a set of rules here that will
get you through this process if anything will.  At certain moments
you'll be tempted to ignore them.  So rule number zero is: these
rules exist for a reason.  You wouldn't need a rule to keep you
going in one direction if there weren't powerful forces pushing you
in another.The ultimate source of the forces acting on you are the forces
acting on investors.  Investors are pinched between two kinds of
fear: fear of investing in startups that fizzle, and fear of missing
out on startups that take off.  The cause of all this fear is the
very thing that makes startups such attractive investments: the
successful ones grow very fast.  But that fast growth means investors
can't wait around.  If you wait till a startup is obviously a
success, it's too late.  To get the really high returns, you have
to invest in startups when it's still unclear how they'll do.  But
that in turn makes investors nervous they're about to invest in a
flop.  As indeed they often are.What investors would like to do, if they could, is wait.  When a
startup is only a few months old, every week that passes gives you
significantly more information about them.  But if you wait too
long, other investors might take the deal away from you.  And of
course the other investors are all subject to the same forces.  So
what tends to happen is that they all wait as long as they can,
then when some act the rest have to.
Don't raise money unless you want it and it wants you.Such a high proportion of successful startups raise money that it
might seem fundraising is one of the defining qualities of a startup.
Actually it isn't.  Rapid growth is what
makes a company a startup.  Most companies in a position to grow
rapidly find that (a) taking outside money helps them grow faster,
and (b) their growth potential makes it easy to attract such money.
It's so common for both (a) and (b) to be true of a successful
startup that practically all do raise outside money.  But there may
be cases where a startup either wouldn't want to grow faster, or
outside money wouldn't help them to, and if you're one of them,
don't raise money.The other time not to raise money is when you won't be able to.  If
you try to raise money before you can convince
investors, you'll not only waste your time, but also burn your
reputation with those investors.
Be in fundraising mode or not.One of the things that surprises founders most about fundraising
is how distracting it is.  When you start fundraising, everything
else grinds to a halt.  The problem is not the time fundraising
consumes but that it becomes the top idea in
your mind.  A startup can't endure that level of distraction
for long.  An early stage startup grows mostly because the founders
make it grow, and if the founders look away,
growth usually drops sharply.Because fundraising is so distracting, a startup should either be
in fundraising mode or not.  And when you do decide to raise money,
you should focus your whole attention on it so you can get it done
quickly and get back to work.
[2]You can take money from investors when you're not in fundraising
mode.  You just can't expend any attention on it.  There are two
things that take attention: convincing investors, and negotiating
with them.  So when you're not in fundraising mode, you should take
money from investors only if they require no convincing, and are
willing to invest on terms you'll take without negotiation.  For
example, if a reputable investor is willing to invest on a convertible
note, using standard paperwork, that is either uncapped or capped
at a good valuation, you can take that without having to think.
[3]
The terms will be whatever they turn out to be in your next
equity round.  And ""no convincing"" means just that: zero time spent
meeting with investors or preparing materials for them.  If an
investor says they're ready to invest, but they need you to come
in for one meeting to meet some of the partners, tell them no, if
you're not in fundraising mode, because that's fundraising. 
[4]
Tell them politely; tell them you're focusing on the company right
now, and that you'll get back to them when you're fundraising; but
do not get sucked down the slippery slope.Investors will try to lure you into fundraising when you're not.
It's great for them if they can, because they can thereby get a
shot at you before everyone else.  They'll send you emails saying
they want to meet to learn more about you.  If you get cold-emailed
by an associate at a VC firm, you shouldn't meet even if you are
in fundraising mode.  Deals don't happen that way.
[5]
But even
if you get an email from a partner you should try to delay meeting
till you're in fundraising mode.  They may say they just want to
meet and chat, but investors never just want to meet and chat.  What
if they like you?  What if they start to talk about giving you
money?  Will you be able to resist having that conversation?  Unless
you're experienced enough at fundraising to have a casual conversation
with investors that stays casual, it's safer to tell them that you'd
be happy to later, when you're fundraising, but that right now you
need to focus on the company.
[6]Companies that are successful at raising money in phase 2 sometimes
tack on a few investors after leaving fundraising mode.  This is
fine; if fundraising went well, you'll be able to do it without
spending time convincing them or negotiating about terms.
Get introductions to investors.Before you can talk to investors, you have to be introduced to them.
If you're presenting at a Demo Day, you'll be introduced to a whole
bunch simultaneously.  But even if you are, you should supplement
these with intros you collect yourself.Do you have to be introduced?  In phase 2, yes.  Some investors
will let you email them a business plan, but you can tell from the
way their sites are organized that they don't really want startups
to approach them directly.Intros vary greatly in effectiveness.  The best type of intro is
from a well-known investor who has just invested in you.  So when
you get an investor to commit, ask them to introduce you to other
investors they respect.
[7]
The next best type of intro is from a
founder of a company they've funded.  You can also get intros from
other people in the startup community, like lawyers and reporters.There are now sites like AngelList, FundersClub, and WeFunder that
can introduce you to investors.  We recommend startups treat them
as auxiliary sources of money.  Raise money first from leads you
get yourself.  Those will on average be better investors.  Plus
you'll have an easier time raising money on these sites once you
can say you've already raised some from well-known investors.
Hear no till you hear yes.Treat investors as saying no till they unequivocally say yes, in
the form of a definite offer with no contingencies.I mentioned earlier that investors prefer to wait if they can.
What's particularly dangerous for founders is the way they wait.
Essentially, they lead you on.  They seem like they're about to
invest right up till the moment they say no.  If they even say no.
Some of the worse ones never actually do say no; they just stop
replying to your emails.  They hope that way to get a free option
on investing.  If they decide later that they want to invest — usually
because they've heard you're a hot deal — they can pretend they
just got distracted and then restart the conversation as if they'd
been about to.
[8]That's not the worst thing investors will do.  Some will use language
that makes it sound as if they're committing, but which doesn't
actually commit them.  And wishful thinking founders are happy to
meet them half way.
[9]Fortunately, the next rule is a tactic for neutralizing this behavior.
But to work it depends on you not being tricked by the no that
sounds like yes.  It's so common for founders to be misled/mistaken
about this that we designed a protocol to fix the
problem.  If you believe an investor has committed, get them to
confirm it.  If you and they have different views of reality, whether
the source of the discrepancy is their sketchiness or your wishful
thinking, the prospect of confirming a commitment in writing will
flush it out.  And till they confirm, regard them as saying no.
Do breadth-first search weighted by expected value.When you talk to investors your m.o. should be breadth-first search,
weighted by expected value.  You should always talk to investors
in parallel rather than serially.  You can't afford the time it
takes to talk to investors serially, plus if you only talk to one
investor at a time, they don't have the pressure of other investors
to make them act.  But you shouldn't pay the same attention to every
investor, because some are more promising prospects than others.
The optimal solution is to talk to all potential investors in
parallel, but give higher priority to the more promising ones. 
[10]Expected value = how likely an investor is to say yes, multiplied
by how good it would be if they did.  So for example, an eminent
investor who would invest a lot, but will be hard to convince, might
have the same expected value as an obscure angel who won't invest
much, but will be easy to convince.  Whereas an obscure angel who
will only invest a small amount, and yet needs to meet multiple
times before making up his mind, has very low expected value.  Meet
such investors last, if at all. 
[11]Doing breadth-first search weighted by expected value will save you
from investors who never explicitly say no but merely drift away,
because you'll drift away from them at the same rate.  It protects
you from investors who flake in much the same way that a distributed
algorithm protects you from processors that fail.  If some investor
isn't returning your emails, or wants to have lots of meetings but
isn't progressing toward making you an offer, you automatically
focus less on them.  But you have to be disciplined about assigning
probabilities.  You can't let how much you want an investor influence
your estimate of how much they want you.
Know where you stand.How do you judge how well you're doing with an investor, when
investors habitually seem more positive than they are?  By looking
at their actions rather than their words.  Every investor has some
track they need to move along from the first conversation to wiring
the money, and you should always know what that track consists of,
where you are on it, and how fast you're moving forward.Never leave a meeting with an investor without asking what happens
next.  What more do they need in order to decide?  Do they need
another meeting with you?  To talk about what?  And how soon?  Do
they need to do something internally, like talk to their partners,
or investigate some issue?  How long do they expect it to take?
Don't be too pushy, but know where you stand.  If investors are
vague or resist answering such questions, assume the worst; investors
who are seriously interested in you will usually be happy to talk
about what has to happen between now and wiring the money, because
they're already running through that in their heads. 
[12]If you're experienced at negotiations, you already know how to ask
such questions.
[13]
If you're not, there's a trick you can use
in this situation.  Investors know you're inexperienced at raising
money.  Inexperience there doesn't make you unattractive.  Being a
noob at technology would, if you're starting a technology startup,
but not being a noob at fundraising.  Larry and Sergey were noobs
at fundraising.  So you can just confess that you're inexperienced
at this and ask how their process works and where you are in it.
[14]
Get the first commitment.The biggest factor in most investors' opinions of you is the opinion
of other investors.  Once you start getting
investors to commit, it becomes increasingly easy to get more to.
But the other side of this coin is that it's often hard to get the
first commitment.Getting the first substantial offer can be half the total difficulty
of fundraising.  What counts as a substantial offer depends on who
it's from and how much it is.  Money from friends and family doesn't
usually count, no matter how much.  But if you get $50k from a well
known VC firm or angel investor, that will usually be enough to set
things rolling.
[15]
Close committed money.It's not a deal till the money's in the bank.  I often hear
inexperienced founders say things like ""We've raised $800,000,""
only to discover that zero of it is in the bank so far.  Remember
the twin fears that torment investors?  The fear of missing out
that makes them jump early, and the fear of jumping onto a turd
that results?  This is a market where people are exceptionally prone
to buyer's remorse.  And it's also one that furnishes them plenty
of excuses to gratify it.  The public markets snap startup investing
around like a whip.  If the Chinese economy blows up tomorrow, all
bets are off.  But there are lots of surprises for individual
startups too, and they tend to be concentrated around fundraising.
Tomorrow a big competitor could appear, or you could get C&Ded, or
your cofounder could quit.
[16]Even a day's delay can bring news that causes an investor to change
their mind.  So when someone commits, get the money.  Knowing where
you stand doesn't end when they say they'll invest.  After they say
yes, know what the timetable is for getting the money, and then
babysit that process till it happens.  Institutional investors have
people in charge of wiring money, but you may have to hunt angels
down in person to collect a check.Inexperienced investors are the ones most likely to get buyer's
remorse.  Established ones have learned to treat saying yes as like
diving off a diving board, and they also have more brand to preserve.
But I've heard of cases of even top-tier VC firms welching on deals.
Avoid investors who don't ""lead.""Since getting the first offer is most of the difficulty of fundraising,
that should be part of your calculation of expected value when you
start.  You have to estimate not just the probability that an
investor will say yes, but the probability that they'd be the first
to say yes, and the latter is not simply a constant fraction of the
former.  Some investors are known for deciding quickly, and those
are extra valuable early on.Conversely, an investor who will only invest once other investors
have is worthless initially.  And while most investors are influenced
by how interested other investors are in you, there are some who
have an explicit policy of only investing after other investors
have.  You can recognize this contemptible subspecies of investor
because they often talk about ""leads.""  They say that they don't
lead, or that they'll invest once you have a lead.  Sometimes they
even claim to be willing to lead themselves, by which they mean
they won't invest till you get $x from other investors.  (It's great
if by ""lead"" they mean they'll invest unilaterally, and in addition
will help you raise more.  What's lame is when they use the term
to mean they won't invest unless you can raise more elsewhere.)
[17]Where does this term ""lead"" come from?  Up till a few years ago,
startups raising money in phase 2 would usually raise equity rounds
in which several investors invested at the same time using the same
paperwork.  You'd negotiate the terms with one ""lead"" investor, and
then all the others would sign the same documents and all the money
change hands at the closing.Series A rounds still work that way, but things now work differently
for most fundraising prior to the series A.  Now there are rarely
actual rounds before the A round, or leads for them.  Now startups
simply raise money from investors one at a time till they feel they
have enough.Since there are no longer leads, why do investors use that term?
Because it's a more legitimate-sounding way of saying what they
really mean.  All they really mean is that their interest in you
is a function of other investors' interest in you.  I.e. the spectral
signature of all mediocre investors.  But when phrased in terms of
leads, it sounds like there is something structural and therefore
legitimate about their behavior.When an investor tells you ""I want to invest in you, but I don't
lead,"" translate that in your mind to ""No, except yes if you turn
out to be a hot deal.""  And since that's the default opinion of any
investor about any startup, they've essentially just told you
nothing.When you first start fundraising, the expected value of an investor
who won't ""lead"" is zero, so talk to such investors last if at all.
Have multiple plans.Many investors will ask how much you're planning to raise. This
question makes founders feel they should be planning to raise a
specific amount.  But in fact you shouldn't.  It's a mistake to
have fixed plans in an undertaking as unpredictable as fundraising.So why do investors ask how much you plan to raise?  For much the
same reasons a salesperson in a store will ask ""How much were you
planning to spend?"" if you walk in looking for a gift for a friend.
You probably didn't have a precise amount in mind; you just want
to find something good, and if it's inexpensive, so much the better.
The salesperson asks you this not because you're supposed to have
a plan to spend a specific amount, but so they can show you only
things that cost the most you'll pay.Similarly, when investors ask how much you plan to raise, it's not
because you're supposed to have a plan.  It's to see whether you'd
be a suitable recipient for the size of investment they like to
make, and also to judge your ambition, reasonableness, and how far
you are along with fundraising.If you're a wizard at fundraising, you can say ""We plan to raise
a $7 million series A round, and we'll be accepting termsheets next
tuesday.""  I've known a handful of founders who could pull that off
without having VCs laugh in their faces.  But if you're in the
inexperienced but earnest majority, the solution is analogous to
the solution I recommend for pitching
your startup: do the right thing and then just tell investors what
you're doing.And the right strategy, in fundraising, is to have multiple plans
depending on how much you can raise.  Ideally you should be able
to tell investors something like: we can make it to profitability
without raising any more money, but if we raise a few hundred
thousand we can hire one or two smart friends, and if we raise a
couple million, we can hire a whole engineering team, etc.Different plans match different investors.  If you're talking to a
VC firm that only does series A rounds (though there are few of
those left), it would be a waste of time talking about any but your
most expensive plan.  Whereas if you're talking to an angel who
invests $20k at a time and you haven't raised any money yet, you
probably want to focus on your least expensive plan.If you're so fortunate as to have to think about the upper limit
on what you should raise, a good rule of thumb is to multiply the
number of people you want to hire times $15k times 18 months.  In
most startups, nearly all the costs are a function of the number
of people, and $15k per month is the conventional total cost
(including benefits and even office space) per person.  $15k per
month is high, so don't actually spend that much.  But it's ok to
use a high estimate when fundraising to add a margin for error.  If
you have additional expenses, like manufacturing, add in those at
the end.  Assuming you have none and you think you might hire 20
people, the most you'd want to raise is 20 x $15k x 18 = $5.4
million.
[18]
Underestimate how much you want.Though you can focus on different plans when talking to different
types of investors, you should on the whole err on the side of
underestimating the amount you hope to raise.For example, if you'd like to raise $500k, it's better to say
initially that you're trying to raise $250k.  Then when you reach
$150k you're more than half done.  That sends two useful signals
to investors: that you're doing well, and that they have to decide
quickly because you're running out of room.  Whereas if you'd said
you were raising $500k, you'd be less than a third done at $150k.
If fundraising stalled there for an appreciable time, you'd start
to read as a failure.Saying initially that you're raising $250k doesn't limit you to
raising that much.  When you reach your initial target and you still
have investor interest, you can just decide to raise more.  Startups
do that all the time.  In fact, most startups that are very successful
at fundraising end up raising more than they originally intended.I'm not saying you should lie, but that you should lower your
expectations initially.  There is almost no downside in starting
with a low number.  It not only won't cap the amount you raise, but
will on the whole tend to increase it.A good metaphor here is angle of attack.  If you try to fly at too
steep an angle of attack, you just stall.  If you say right out of
the gate that you want to raise a $5 million series A round, unless
you're in a very strong position, you not only won't get that but
won't get anything. Better to start at a low angle of attack, build
up speed, and then gradually increase the angle if you want.
Be profitable if you can.You will be in a much stronger position if your collection of plans
includes one for raising zero dollars — i.e. if you can make
it to profitability without raising any additional money.  Ideally
you want to be able to say to investors ""We'll succeed no matter
what, but raising money will help us do it faster.""There are many analogies between fundraising and dating, and this
is one of the strongest.  No one wants you if you seem desperate.
And the best way not to seem desperate is not to be desperate.
That's one reason we urge startups during YC to keep expenses low
and to try to make it to ramen
profitability before Demo Day.  Though it sounds slightly
paradoxical, if you want to raise money, the best thing you can do
is get yourself to the point where you don't need to.There are almost two distinct modes of fundraising: one in which
founders who need money knock on doors seeking it, knowing that
otherwise the company will die or at the very least people will
have to be fired, and one in which founders who don't need money
take some to grow faster than they could merely on their own revenues.
To emphasize the distinction I'm going to name them: type A fundraising
is when you don't need money, and type B fundraising is when you
do.Inexperienced founders read about famous startups doing what was
type A fundraising, and decide they should raise money too, since
that seems to be how startups work. Except when they raise money
they don't have a clear path to profitability and are thus doing
type B fundraising.  And they are then surprised how difficult and
unpleasant it is.Of course not all startups can make it to ramen profitability in a
few months.  And some that don't still manage to have the upper
hand over investors, if they have some other advantage like
extraordinary growth numbers or exceptionally formidable founders.
But as time passes it gets increasingly difficult to fundraise from
a position of strength without being profitable.
[19]
Don't optimize for valuation.When you raise money, what should your valuation be?  The most
important thing to understand about valuation is that it's not that
important.Founders who raise money at high valuations tend to be unduly proud
of it.  Founders are often competitive people, and since valuation
is usually the only visible number attached to a startup, they end
up competing to raise money at the highest valuation.  This is
stupid, because fundraising is not the test that matters.  The real
test is revenue.  Fundraising is just a means to that end.  Being
proud of how well you did at fundraising is like being proud of
your college grades.Not only is fundraising not the test that matters, valuation is not
even the thing to optimize about fundraising.  The number one thing
you want from phase 2 fundraising is to get the money you need, so
you can get back to focusing on the real test, the success of your
company.  Number two is good investors. Valuation is at best third.The empirical evidence shows just how unimportant it is.  Dropbox
and Airbnb are the most successful companies we've funded so far,
and they raised money after Y Combinator at premoney valuations of
$4 million and $2.6 million respectively. Prices are so much higher
now that if you can raise money at all you'll probably raise it at
higher valuations than Dropbox and Airbnb.  So let that satisfy
your competitiveness.  You're doing better than Dropbox and Airbnb!
At a test that doesn't matter.When you start fundraising, your initial valuation (or valuation
cap) will be set by the deal you make with the first investor who
commits.  You can increase the price for later investors, if you
get a lot of interest, but by default the valuation you got from
the first investor becomes your asking price.So if you're raising money from multiple investors, as most companies
do in phase 2, you have to be careful to avoid raising the first
from an over-eager investor at a price you won't be able to
sustain.  You can of course lower your price if you need to (in
which case you should give the same terms to investors who invested
earlier at a higher price), but you may lose a bunch of leads in
the process of realizing you need to do this.What you can do if you have eager first investors is raise money
from them on an uncapped convertible note with an MFN clause.  This
is essentially a way of saying that the valuation cap of the note
will be determined by the next investors you raise money from.It will be easier to raise money at a lower valuation.  It shouldn't
be, but it is.  Since phase 2 prices vary at most 10x and the big
successes generate returns of at least 100x, investors should pick
startups entirely based on their estimate of the probability that
the company will be a big success and hardly at all on price.  But
although it's a mistake for investors to care about price, a
significant number do.  A startup that investors seem to like but
won't invest in at a cap of $x will have an easier time at $x/2.
[20]
Yes/no before valuation.Some investors want to know what your valuation is before they even
talk to you about investing.  If your valuation has already been
set by a prior investment at a specific valuation or cap, you can
tell them that number.  But if it isn't set because you haven't
closed anyone yet, and they try to push you to name a price, resist
doing so.  If this would be the first investor you've closed, then
this could be the tipping point of fundraising. That means closing
this investor is the first priority, and you need to get the
conversation onto that instead of being dragged sideways into a
discussion of price.Fortunately there is a way to avoid naming a price in this situation.
And it is not just a negotiating trick; it's how you (both) should
be operating.  Tell them that valuation is not the most important
thing to you and that you haven't thought much about it, that you
are looking for investors you want to partner with and who want to
partner with you, and that you should talk first about whether they
want to invest at all.  Then if they decide they do want to invest,
you can figure out a price. But first things first.Since valuation isn't that important and getting fundraising rolling
is, we usually tell founders to give the first investor who commits
as low a price as they need to.  This is a safe technique so long
as you combine it with the next one. 
[21]
Beware ""valuation sensitive"" investors.Occasionally you'll encounter investors who describe themselves as
""valuation sensitive.""  What this means in practice is that they
are compulsive negotiators who will suck up a lot of your time
trying to push your price down.  You should therefore never approach
such investors first. While you shouldn't chase high valuations,
you also don't want your valuation to be set artificially low because
the first investor who committed happened to be a compulsive
negotiator.  Some such investors have value, but the time to approach
them is near the end of fundraising, when you're in a position to
say ""this is the price everyone else has paid; take it or leave it""
and not mind if they leave it.  This way, you'll not only get market
price, but it will also take less time.Ideally you know which investors have a reputation for being
""valuation sensitive"" and can postpone dealing with them till last,
but occasionally one you didn't know about will pop up early on.
The rule of doing breadth first search weighted by expected value
already tells you what to do in this case: slow down your interactions
with them.There are a handful of investors who will try to invest at a lower
valuation even when your price has already been set.  Lowering your
price is a backup plan you resort to when you discover you've let
the price get set too high to close all the money you need.  So
you'd only want to talk to this sort of investor if you were about
to do that anyway.  But since investor meetings have to be arranged
at least a few days in advance and you can't predict when you'll
need to resort to lowering your price, this means in practice that
you should approach this type of investor last if at all.If you're surprised by a lowball offer, treat it as a backup offer
and delay responding to it.  When someone makes an offer in good
faith, you have a moral obligation to respond in a reasonable time.
But lowballing you is a dick move that should be met with the
corresponding countermove.
Accept offers greedily.I'm a little leery of using the term ""greedily"" when writing about
fundraising lest non-programmers misunderstand me, but a greedy
algorithm is simply one that doesn't try to look into the future.
A greedy algorithm takes the best of the options in front of it
right now.  And that is how startups should approach fundraising
in phases 2 and later.  Don't try to look into the future because
(a) the future is unpredictable, and indeed in this business you're
often being deliberately misled about it and (b) your first priority
in fundraising should be to get it finished and get back to work
anyway.If someone makes you an acceptable offer, take it.  If you have
multiple incompatible offers, take the best.  Don't reject an
acceptable offer in the hope of getting a better one in the future.These simple rules cover a wide variety of cases.  If you're raising
money from many investors, roll them up as they say yes.  As you
start to feel you've raised enough, the threshold for acceptable
will start to get higher.In practice offers exist for stretches of time, not points.  So
when you get an acceptable offer that would be incompatible with
others (e.g. an offer to invest most of the money you need), you
can tell the other investors you're talking to that you have an
offer good enough to accept, and give them a few days to make their
own.  This could lose you some that might have made an offer if
they had more time.  But by definition you don't care; the initial
offer was acceptable.Some investors will try to prevent others from having time to decide
by giving you an ""exploding"" offer, meaning one that's only valid
for a few days.  Offers from the very best investors explode less
frequently and less rapidly — Fred Wilson never gives exploding
offers, for example — because they're confident you'll pick
them.  But lower-tier investors sometimes give offers with very
short fuses, because they believe no one who had other options would
choose them.  A deadline of three working days is acceptable.  You
shouldn't need more than that if you've been talking to investors
in parallel.  But a deadline any shorter is a sign you're dealing
with a sketchy investor.  You can usually call their bluff, and you
may need to.
[22]It might seem that instead of accepting offers greedily, your goal
should be to get the best investors as partners.  That is certainly
a good goal, but in phase 2 ""get the best investors"" only rarely
conflicts with ""accept offers greedily,"" because the best investors
don't usually take any longer to decide than the others.  The only
case where the two strategies give conflicting advice is when you
have to forgo an offer from an acceptable investor to see if you'll
get an offer from a better one.  If you talk to investors in parallel
and push back on exploding offers with excessively short deadlines,
that will almost never happen.  But if it does, ""get the best
investors"" is in the average case bad advice.  The best investors
are also the most selective, because they get their pick of all the
startups.  They reject nearly everyone they talk to, which means
in the average case it's a bad trade to exchange a definite offer
from an acceptable investor for a potential offer from a better
one.(The situation is different in phase 1.  You can't apply to all the
incubators in parallel, because some offset their schedules to
prevent this.  In phase 1, ""accept offers greedily"" and ""get the
best investors"" do conflict, so if you want to apply to multiple
incubators, you should do it in such a way that the ones you want
most decide first.)Sometimes when you're raising money from multiple investors, a
series A will emerge out of those conversations, and these rules
even cover what to do in that case.  When an investor starts to
talk to you about a series A, keep taking smaller investments till
they actually give you a termsheet.  There's no practical difficulty.
If the smaller investments are on convertible notes, they'll just
convert into the series A round.  The series A investor won't like
having all these other random investors as bedfellows, but if it
bothers them so much they should get on with giving you a termsheet.
Till they do, you don't know for sure they will, and the greedy
algorithm tells you what to do.
[23]
Don't sell more than 25% in phase 2.If you do well, you will probably raise a series A round eventually.
I say probably because things are changing with series A rounds.
Startups may start to skip them.  But only one company we've funded
has so far, so tentatively assume the path to huge passes through
an A round.
[24]Which means you should avoid doing things in earlier rounds that
will mess up raising an A round.  For example, if you've sold more
than about 40% of your company total, it starts to get harder to
raise an A round, because VCs worry there will not be enough stock
left to keep the founders motivated.Our rule of thumb is not to sell more than 25% in phase 2, on top
of whatever you sold in phase 1, which should be less than 15%.  If
you're raising money on uncapped notes, you'll have to guess what
the eventual equity round valuation might be.  Guess conservatively.(Since the goal of this rule is to avoid messing up the series A,
there's obviously an exception if you end up raising a series A in
phase 2, as a handful of startups do.)
Have one person handle fundraising.If you have multiple founders, pick one to handle fundraising so
the other(s) can keep working on the company.  And since the danger
of fundraising is not the time taken up by the actual meetings but
that it becomes the top idea in your mind, the founder who handles
fundraising should make a conscious effort to insulate the other
founder(s) from the details of the process.
[25](If the founders mistrust one another, this could cause some friction.
But if the founders mistrust one another, you have worse problems
to worry about than how to organize fundraising.)The founder who handles fundraising should be the CEO, who should
in turn be the most formidable of the founders.  Even if the CEO
is a programmer and another founder is a salesperson?  Yes.  If you
happen to be that type of founding team, you're effectively a single
founder when it comes to fundraising.It's ok to bring all the founders to meet an investor who will
invest a lot, and who needs this meeting as the final step before
deciding.  But wait till that point.  Introducing an investor to
your cofounder(s) should be like introducing a girl/boyfriend to
your parents — something you do only when things reach a certain
stage of seriousness.Even if there are still one or more founders focusing on the company
during fundraising, growth will slow.  But try to get as much growth
as you can, because fundraising is a segment of time, not a point,
and what happens to the company during that time affects the outcome.
If your numbers grow significantly between two investor meetings,
investors will be hot to close, and if your numbers are flat or
down they'll start to get cold feet.
You'll need an executive summary and (maybe) a deck.Traditionally phase 2 fundraising consists of presenting a slide
deck in person to investors.  Sequoia describes what such a deck
should contain, and
since they're the customer you can take their word for it.I say ""traditionally"" because I'm ambivalent about decks, and (though
perhaps this is wishful thinking) they seem to be on the way out.
A lot of the most successful startups we fund never make decks in
phase 2.  They just talk to investors and explain what they plan
to do.  Fundraising usually takes off fast for the startups that
are most successful at it, and they're thus able to excuse themselves
by saying that they haven't had time to make a deck.You'll also want an executive summary, which should be no more than
a page long and describe in the most matter of fact language what
you plan to do, why it's a good idea, and what progress you've made
so far.  The point of the summary is to remind the investor (who
may have met many startups that day) what you talked about.Assume that if you give someone a copy of your deck or executive
summary, it will be passed on to whoever you'd least like to have
it.  But don't refuse on that account to give copies to investors
you meet.  You just have to treat such leaks as a cost of doing
business.  In practice it's not that high a cost.  Though founders
are rightly indignant when their plans get leaked to competitors,
I can't think of a startup whose outcome has been affected by it.Sometimes an investor will ask you to send them your deck and/or
executive summary before they decide whether to meet with you.  I
wouldn't do that.  It's a sign they're not really interested.
Stop fundraising when it stops working.When do you stop fundraising?  Ideally when you've raised enough.
But what if you haven't raised as much as you'd like?  When do you
give up?It's hard to give general advice about this, because there have
been cases of startups that kept trying to raise money even when
it seemed hopeless, and miraculously succeeded. But what I usually
tell founders is to stop fundraising when you start to get a lot
of air in the straw.  When you're drinking through a straw, you can
tell when you get to the end of the liquid because you start to get
a lot of air in the straw.  When your fundraising options run out,
they usually run out in the same way.  Don't keep sucking on the
straw if you're just getting air.  It's not going to get better.
Don't get addicted to fundraising.Fundraising is a chore for most founders, but some find it more
interesting than working on their startup.  The work at an early
stage startup often consists of unglamorous schleps.  Whereas fundraising, when it's
going well, can be quite the opposite.  Instead of sitting in your
grubby apartment listening to users complain about bugs in your
software, you're being offered millions of dollars by famous investors
over lunch at a nice restaurant.
[26]The danger of fundraising is particularly acute for people who are
good at it.  It's always fun to work on something you're good at.
If you're one of these people, beware.  Fundraising is not what
will make your company successful.  Listening to users complain
about bugs in your software is what will make you successful.  And
the big danger of getting addicted to fundraising is not merely
that you'll spend too long on it or raise too much money.  It's
that you'll start to think of yourself as being already successful,
and lose your taste for the schleps you need to undertake to actually
be successful.  Startups can be destroyed by this.When I see a startup with young founders that is fabulously successful
at fundraising, I mentally decrease my estimate of the probability
that they'll succeed.  The press may be writing about them as if
they'd been anointed as the next Google, but I'm thinking ""this is
going to end badly.""
Don't raise too much.Though only a handful of startups have to worry about this, it is
possible to raise too much.  The dangers of raising too much are
subtle but insidious.  One is that it will set impossibly high
expectations.  If you raise an excessive amount of money, it will
be at a high valuation, and the danger of raising money at too high
a valuation is that you won't be able to increase it sufficiently
the next time you raise money.A company's valuation is expected to rise each time it raises money.
If not it's a sign of a company in trouble, which makes you
unattractive to investors.  So if you raise money in phase 2 at a
post-money valuation of $30 million, the pre-money valuation of
your next round, if you want to raise one, is going to have to be
at least $50 million.  And you have to be doing really, really well
to raise money at $50 million.It's very dangerous to let the competitiveness of your current round
set the performance threshold you have to meet to raise your next
one, because the two are only loosely coupled.But the money itself may be more dangerous than the valuation.  The
more you raise, the more you spend, and spending a lot of money can
be disastrous for an early stage startup.  Spending a lot makes it
harder to become profitable, and perhaps even worse, it makes you
more rigid, because the main way to spend money is people, and the
more people you have, the harder it is to change directions.  So
if you do raise a huge amount of money, don't spend it.  (You will
find that advice almost impossible to follow, so hot will be the
money burning a hole in your pocket, but I feel obliged at least
to try.)
Be nice.Startups raising money occasionally alienate investors by seeming
arrogant.  Sometimes because they are arrogant, and sometimes because
they're noobs clumsily attempting to mimic the toughness they've
observed in experienced founders.It's a mistake to behave arrogantly to investors.  While there are
certain situations in which certain investors like certain kinds
of arrogance, investors vary greatly in this respect, and a flick
of the whip that will bring one to heel will make another roar with
indignation.  The only safe strategy is never to seem arrogant at
all.That will require some diplomacy if you follow the advice I've given
here, because the advice I've given is essentially how to play
hardball back.  When you refuse to meet an investor because you're
not in fundraising mode, or slow down your interactions with an
investor who moves too slow, or treat a contingent offer as the no
it actually is and then, by accepting offers greedily, end up leaving
that investor out, you're going to be doing things investors don't
like.  So you must cushion the blow with soft words.  At YC we tell
startups they can blame us.  And now that I've written this, everyone
else can blame me if they want.  That plus the inexperience card
should work in most situations: sorry, we think you're great, but
PG said startups shouldn't ___, and since we're new to fundraising,
we feel like we have to play it safe.The danger of behaving arrogantly is greatest when you're doing
well.  When everyone wants you, it's hard not to let it go to your
head.  Especially if till recently no one wanted you.  But restrain
yourself.  The startup world is a small place, and startups have
lots of ups and downs.  This is a domain where it's more true than
usual that pride goeth before a fall.
[27]Be nice when investors reject you as well.  The best investors are
not wedded to their initial opinion of you.  If they reject you in
phase 2 and you end up doing well, they'll often invest in phase
3.  In fact investors who reject you are some of your warmest leads
for future fundraising.  Any investor who spent significant time
deciding probably came close to saying yes.  Often you have some
internal champion who only needs a little more evidence to convince
the skeptics.  So it's wise not merely to be nice to investors who
reject you, but (unless they behaved badly) to treat it as the
beginning of a relationship.
The bar will be higher next time.Assume the money you raise in phase 2 will be the last you ever
raise.  You must make it to profitability on this money if you can.Over the past several years, the investment community has evolved
from a strategy of anointing a small number of winners early and
then supporting them for years to a strategy of spraying money at
early stage startups and then ruthlessly culling them at the next
stage.  This is probably the optimal strategy for investors.  It's
too hard to pick winners early on.  Better to let the market do it
for you.  But it often comes as a surprise to startups how much
harder it is to raise money in phase 3.When your company is only a couple months old, all it has to be is
a promising experiment that's worth funding to see how it turns
out.  The next time you raise money, the experiment has to have
worked.  You have to be on a trajectory that leads to going public.
And while there are some ideas where the proof that the experiment
worked might consist of e.g. query response times, usually the proof
is profitability.  Usually phase 3 fundraising has to be type A
fundraising.In practice there are two ways startups hose themselves between
phases 2 and 3.  Some are just too slow to become profitable.  They
raise enough money to last for two years.  There doesn't seem any
particular urgency to be profitable.  So they don't make any effort
to make money for a year.  But by that time, not making money has
become habitual.  When they finally decide to try, they find they
can't.The other way companies hose themselves is by letting their expenses
grow too fast.  Which almost always means hiring too many people.
You usually shouldn't go out and hire 8 people as soon as you raise
money at phase 2.  Usually you want to wait till you have growth
(and thus usually revenues) to justify them.  A lot of VCs will
encourage you to hire aggressively.  VCs generally tell you to spend
too much, partly because as money people they err on the side of
solving problems by spending money, and partly because they want
you to sell them more of your company in subsequent rounds.  Don't
listen to them.
Don't make things complicated.I realize it may seem odd to sum up this huge treatise by saying
that my overall advice is not to make fundraising too complicated,
but if you go back and look at this list you'll see it's basically
a simple recipe with a lot of implications and edge cases.  Avoid
investors till you decide to raise money, and then when you do,
talk to them all in parallel, prioritized by expected value, and
accept offers greedily.  That's fundraising in one sentence.  Don't
introduce complicated optimizations, and don't let investors introduce
complications either.Fundraising is not what will make you successful.  It's just a means
to an end.  Your primary goal should be to get it over with and get
back to what will make you successful — making things and talking
to users — and the path I've described will for most startups
be the surest way to that destination.Be good, take care of yourselves, and don't leave the path.
Notes[1]
The worst explosions happen when unpromising-seeming startups
encounter mediocre investors.  Good investors don't lead startups
on; their reputations are too valuable.  And startups that seem
promising can usually get enough money from good investors that
they don't have to talk to mediocre ones.  It is the unpromising-seeming
startups that have to resort to raising money from mediocre investors.
And it's particularly damaging when these investors flake, because
unpromising-seeming startups are usually more desperate for money.(Not all unpromising-seeming startups do badly.  Some are merely
ugly ducklings in the sense that they violate current startup
fashions.)[2]
One YC founder told me:

  I think in general we've done ok at fundraising, but I managed
  to screw up twice at the exact same thing — trying to focus
  on building the company and fundraising at the same time.

[3]
There is one subtle danger you have to watch out for here, which
I warn about later: beware of getting too high a valuation from an
eager investor, lest that set an impossibly high target when raising
additional money.[4]
If they really need a meeting, then they're not ready to invest,
regardless of what they say. They're still deciding, which means
you're being asked to come in and convince them. Which is fundraising.[5]
Associates at VC firms regularly cold email startups.  Naive
founders think ""Wow, a VC is interested in us!""  But an associate
is not a VC.  They have no decision-making power.  And while they
may introduce startups they like to partners at their firm, the
partners discriminate against deals that come to them this way.   I
don't know of a single VC investment that began with an associate
cold-emailing a startup.  If you want to approach a specific firm,
get an intro to a partner from someone they respect.It's ok to talk to an associate if you get an intro to a VC firm
or they see you at a Demo Day and they begin by having an associate
vet you.  That's not a promising lead and should therefore get low
priority, but it's not as completely worthless as a cold email.Because the title ""associate"" has gotten a bad reputation, a few
VC firms have started to give their associates the title ""partner,""
which can make things very confusing.  If you're a YC startup you
can ask us who's who; otherwise you may have to do some research
online.  There may be a special title for actual partners.  If
someone speaks for the firm in the press or a blog on the firm's
site, they're probably a real partner.  If they're on boards of
directors they're probably a real partner.There are titles between ""associate"" and ""partner,"" including
""principal"" and ""venture partner.""  The meanings of these titles
vary too much to generalize.[6]
For similar reasons, avoid casual conversations with potential
acquirers.  They can lead to distractions even more dangerous than
fundraising.  Don't even take a meeting with a potential acquirer
unless you want to sell your company right now.[7]
Joshua Reeves specifically suggests asking each investor to
intro you to two more investors.Don't ask investors who say no for introductions to other investors.
That will in many cases be an anti-recommendation.[8]
This is not always as deliberate as its sounds.  A lot of the
delays and disconnects between founders and investors are induced
by the customs of the venture business, which have evolved the way
they have because they suit investors' interests.[9]
One YC founder who read a draft of this essay wrote:

  This is the most important section. I think it might bear stating
  even more clearly. ""Investors will deliberately affect more
  interest than they have to preserve optionality. If an investor
  seems very interested in you, they still probably won't invest.
  The solution for this is to assume the worst — that an investor
  is just feigning interest — until you get a definite commitment.""

[10]
Though you should probably pack investor meetings as closely
as you can, Jeff Byun mentions one reason not to: if you pack
investor meetings too closely, you'll have less time for your pitch
to evolve.Some founders deliberately schedule a handful of lame investors
first, to get the bugs out of their pitch.[11]
There is not an efficient market in this respect.  Some of the
most useless investors are also the highest maintenance.[12]
Incidentally, this paragraph is sales 101.  If you want to see
it in action, go talk to a car dealer.[13]
I know one very smooth founder who used to end investor meetings
with ""So, can I count you in?"" delivered as if it were ""Can you
pass the salt?""   Unless you're very smooth (if you're not sure...),
do not do this yourself.  There is nothing more unconvincing, for
an investor, than a nerdy founder trying to deliver the lines meant
for a smooth one.Investors are fine with funding nerds.  So if you're a nerd, just
try to be a good nerd, rather than doing a bad imitation of a smooth
salesman.[14]
Ian Hogarth suggests a good way to tell how serious potential
investors are: the resources they expend on you after the first
meeting.  An investor who's seriously interested will already be
working to help you even before they've committed.[15]
In principle you might have to think about so-called ""signalling
risk.""  If a prestigious VC makes a small seed investment in you,
what if they don't want to invest the next time you raise money?
Other investors might assume that the VC knows you well, since
they're an existing investor, and if they don't want to invest in
your next round, that must mean you suck.  The reason I say ""in
principle"" is that in practice signalling hasn't been much of a
problem so far.  It rarely arises, and in the few cases where it
does, the startup in question usually is doing badly and is doomed
anyway.If you have the luxury of choosing among seed investors, you can
play it safe by excluding VC firms.  But it isn't critical to.[16]
Sometimes a competitor will deliberately threaten you with a
lawsuit just as you start fundraising, because they know you'll
have to disclose the threat to potential investors and they hope
this will make it harder for you to raise money.  If this happens
it will probably frighten you more than investors.  Experienced
investors know about this trick, and know the actual lawsuits rarely
happen. So if you're attacked in this way, be forthright with
investors.  They'll be more alarmed if you seem evasive than if you
tell them everything.[17]
A related trick is to claim that they'll only invest contingently
on other investors doing so because otherwise you'd be ""undercapitalized.""
This is almost always bullshit.  They can't estimate your minimum
capital needs that precisely.[18]
You won't hire all those 20 people at once, and you'll probably
have some revenues before 18 months are out.  But those too are
acceptable or at least accepted additions to the margin for error.[19]
Type A fundraising is so much better that it might even be
worth doing something different if it gets you there sooner.  One
YC founder told me that if he were a first-time founder again he'd
""leave ideas that are up-front capital intensive to founders with
established reputations.""[20]
I don't know whether this happens because they're innumerate,
or because they believe they have zero ability to predict startup
outcomes (in which case this behavior at least wouldn't be irrational).
In either case the implications are similar.[21]
If you're a YC startup and you have an investor who for some
reason insists that you decide the price, any YC partner can estimate
a market price for you.[22]
You should respond in kind when investors behave upstandingly
too.  When an investor makes you a clean offer with no deadline,
you have a moral obligation to respond promptly.[23]
Tell the investors talking to you about an A round about the
smaller investments you raise as you raise them.  You owe them such
updates on your cap table, and this is also a good way to pressure
them to act.  They won't like you raising other money and may
pressure you to stop, but they can't legitimately ask you to commit
to them till they also commit to you.  If they want you to stop
raising money, the way to do it is to give you a series A termsheet
with a no-shop clause.You can relent a little if the potential series A investor has a
great reputation and they're clearly working fast to get you a
termsheet, particularly if a third party like YC is involved to
ensure there are no misunderstandings.  But be careful.[24]
The company is Weebly, which made it to profitability on a
seed investment of $650k.  They did try to raise a series A in the
fall of 2008 but (no doubt partly because it was the fall of 2008)
the terms they were offered were so bad that they decided to skip
raising an A round.[25]
Another advantage of having one founder take fundraising
meetings is that you never have to negotiate in real time, which
is something inexperienced founders should avoid.  One YC founder
told me:

  Investors are professional negotiators and can negotiate on the
  spot very easily.  If only one founder is in the room, you can
  say ""I need to circle back with my co-founder"" before making any
  commitments. I used to do this all the time.

[26]
You'll be lucky if fundraising feels pleasant enough to become
addictive.  More often you have to worry about the other
extreme — becoming demoralized when investors reject you.  As
one (very successful) YC founder wrote after reading a draft of
this:

  It's hard to mentally deal with the sheer scale of rejection in
  fundraising and if you are not in the right mindset you will fail.
  Users may love you but these supposedly smart investors may not
  understand you at all. At this point for me, rejection still
  rankles but I've come to accept that investors are just not super
  thoughtful for the most part and you need to play the game according
  to certain somewhat depressing rules (many of which you are
  listing) in order to win.

[27]
The actual sentence in the King James Bible is ""Pride goeth
before destruction, and an haughty spirit before a fall.""Thanks to Slava Akhmechet, Sam Altman, Nate Blecharczyk,
Adora Cheung, Bill Clerico, John Collison, Patrick Collison, Parker
Conrad, Ron Conway, Travis Deyle, Jason Freedman, Joe Gebbia, Mattan
Griffel, Kevin Hale, Jacob Heller, Ian Hogarth, Justin Kan, Professor
Moriarty, Nikhil Nirmel, David Petersen, Geoff Ralston, Joshua
Reeves, Yuri Sagalov, Emmett Shear, Rajat Suri, Garry Tan, and Nick
Tomarello for reading drafts of this.Russian Translation","startup advice
",human,"human
","human
"
33,33,"

April 2009I usually avoid politics, but since we now seem to have an administration that's open to suggestions, I'm going to risk making one.  The single biggest thing the government could do to increase the number of startups in this country is a policy that would cost nothing: establish a new class of visa for startup founders.The biggest constraint on the number of new startups that get created in the US is not tax policy or employment law or even Sarbanes-Oxley.  It's that we won't let the people who want to start them into the country.Letting just 10,000 startup founders into the country each year could have a visible effect on the economy.  If we assume 4 people per startup, which is probably an overestimate, that's 2500 new companies.  Each year.  They wouldn't all grow as big as Google, but out of 2500 some would come close.By definition these 10,000 founders wouldn't be taking jobs from Americans: it could be part of the terms of the visa that they couldn't work for existing companies, only new ones they'd founded.  In fact they'd cause there to be 
more jobs for Americans, because the companies they started would hire more employees as they grew.The tricky part might seem to be how one defined a startup. But that could be solved quite easily: let the market decide.  Startup investors work hard to find the best startups.  The government could not do better than to piggyback on their expertise, and use investment by recognized startup investors as the test of whether a company was a real startup.How would the government decide who's a startup investor?  The same way they decide what counts as a university for student visas. We'll establish our own accreditation procedure. We know who one another are.10,000 people is a drop in the bucket by immigration standards, but would represent a huge increase in the pool of startup founders.  I think this would have such a visible effect on the economy that it would make the legislator who introduced the bill famous.  The only way to know for sure would be to try it, and that would cost practically nothing.
Thanks to Trevor Blackwell, Paul Buchheit, Jeff Clavier, David Hornik, Jessica Livingston, Greg Mcadoo, Aydin Senkut, and Fred Wilson for reading drafts of this.Related:The United States of Entrepreneurs About Half of VC-Backed Company Founders are Immigrants","startup advice
",human,"human
","human
"
34,34,"

Want to start a startup?  Get funded by
Y Combinator.




October 2010

(I wrote this for Forbes, who asked me to write something
about the qualities we look for in founders.  In print they had to cut
the last item because they didn't have room.)1. DeterminationThis has turned out to be the most important quality in startup
founders.  We thought when we started Y Combinator that the most
important quality would be intelligence.  That's the myth in the
Valley. And certainly you don't want founders to be stupid.  But
as long as you're over a certain threshold of intelligence, what
matters most is determination.  You're going to hit a lot of
obstacles.  You can't be the sort of person who gets demoralized
easily.Bill Clerico and Rich Aberman of WePay 
are a good example.  They're
doing a finance startup, which means endless negotiations with big,
bureaucratic companies.  When you're starting a startup that depends
on deals with big companies to exist, it often feels like they're
trying to ignore you out of existence.  But when Bill Clerico starts
calling you, you may as well do what he asks, because he is not
going away.
2. FlexibilityYou do not however want the sort of determination implied by phrases
like ""don't give up on your dreams.""  The world of startups is so
unpredictable that you need to be able to modify your dreams on the
fly.  The best metaphor I've found for the combination of determination
and flexibility you need is a running back.  
He's determined to get
downfield, but at any given moment he may need to go sideways or
even backwards to get there.The current record holder for flexibility may be Daniel Gross of
Greplin.  He applied to YC with 
some bad ecommerce idea.  We told
him we'd fund him if he did something else.  He thought for a second,
and said ok.  He then went through two more ideas before settling
on Greplin.  He'd only been working on it for a couple days when
he presented to investors at Demo Day, but he got a lot of interest.
He always seems to land on his feet.
3. ImaginationIntelligence does matter a lot of course.  It seems like the type
that matters most is imagination.  It's not so important to be able
to solve predefined problems quickly as to be able to come up with
surprising new ideas.  In the startup world, most good ideas 
seem
bad initially.  If they were obviously good, someone would already
be doing them.  So you need the kind of intelligence that produces
ideas with just the right level of craziness.Airbnb is that kind of idea.  
In fact, when we funded Airbnb, we
thought it was too crazy.  We couldn't believe large numbers of
people would want to stay in other people's places.  We funded them
because we liked the founders so much.  As soon as we heard they'd
been supporting themselves by selling Obama and McCain branded
breakfast cereal, they were in.  And it turned out the idea was on
the right side of crazy after all.
4. NaughtinessThough the most successful founders are usually good people, they
tend to have a piratical gleam in their eye.  They're not Goody
Two-Shoes type good.  Morally, they care about getting the big
questions right, but not about observing proprieties.  That's why
I'd use the word naughty rather than evil.  They delight in 
breaking
rules, but not rules that matter.  This quality may be redundant
though; it may be implied by imagination.Sam Altman of Loopt 
is one of the most successful alumni, so we
asked him what question we could put on the Y Combinator application
that would help us discover more people like him.  He said to ask
about a time when they'd hacked something to their advantage—hacked in the sense of beating the system, not breaking into
computers.  It has become one of the questions we pay most attention
to when judging applications.
5. FriendshipEmpirically it seems to be hard to start a startup with just 
one
founder.  Most of the big successes have two or three.  And the
relationship between the founders has to be strong.  They must
genuinely like one another, and work well together.  Startups do
to the relationship between the founders what a dog does to a sock:
if it can be pulled apart, it will be.Emmett Shear and Justin Kan of Justin.tv 
are a good example of close
friends who work well together.  They've known each other since
second grade.  They can practically read one another's minds.  I'm
sure they argue, like all founders, but I have never once sensed
any unresolved tension between them.Thanks to Jessica Livingston and Chris Steiner for reading drafts of this.","startup advice
",human,"human
","human
"
35,35,"

Want to start a startup?  Get funded by
Y Combinator.




May 2005(This essay is derived from a talk at the Berkeley CSUA.)The three big powers on the Internet now are Yahoo, Google, and
Microsoft.  Average age of their founders: 24.  So it is pretty
well established now that grad students can start successful
companies.  And if grad students can do it, why not undergrads?Like everything else in technology, the cost of starting a startup
has decreased dramatically.  Now it's so low that it has disappeared
into the noise. The main cost of starting a Web-based
startup is food and rent.  Which means it doesn't cost much more
to start a company than to be a total slacker.  You can probably
start a startup on ten thousand dollars of seed funding, if you're
prepared to live on ramen.The less it costs to start a company, the less you need the permission
of investors to do it.  So a lot of people will be able to start
companies now who never could have before.The most interesting subset may be those in their early twenties.
I'm not so excited about founders who have everything investors
want except intelligence, or everything except energy.  The most
promising group to be liberated by the new, lower threshold are
those who have everything investors want except experience.Market RateI once claimed that nerds were unpopular
in secondary school mainly because they had better things to do
than work full-time at being popular.  Some said I was just telling
people what they wanted to hear.  Well, I'm now about to do that
in a spectacular way: I think undergraduates are undervalued.Or more precisely, I think few realize the huge
spread in the value of 20 year olds.  Some, it's true, are not very
capable.  But others are more capable than all but a handful of 30
year olds. [1]Till now the problem has always been that it's difficult to pick
them out.  Every VC in the world, if they could go back in time,
would try to invest in Microsoft.  But which would have then?  How
many would have understood that this particular 19 year old was
Bill Gates?It's hard to judge the young because (a) they change rapidly, (b)
there is great variation between them, and (c) they're individually
inconsistent.  That last one is a big problem.  When you're young,
you occasionally say and do stupid things even when you're smart.
So if the algorithm is to filter out people who say stupid things,
as many investors and employers unconsciously do, you're going to
get a lot of false positives.Most organizations who hire people right out of college are only
aware of the average value of 22 year olds, which is not that high.  
And so the idea for most of the twentieth century was that everyone
had to begin as a trainee in some 
entry-level job.  Organizations  
realized there was a lot of variation in the incoming stream, but
instead of pursuing this thought they tended to suppress it, in the
belief that it was good for even the most promising kids to start 
at the bottom, so they didn't get swelled heads.The most productive young people will always be undervalued
by large organizations, because the young have no performance to
measure yet, and any error in guessing their ability will tend 
toward the mean.What's an especially productive 22 year old to do?  One thing you   
can do is go over the heads of organizations, directly to the users.
Any company that hires you is, economically, acting as a proxy for
the customer.  The rate at which they value you (though they may
not consciously realize it) is an attempt to guess your value to  
the user.  But there's a way to appeal their judgement.  If you
want, you can opt to be valued directly by users, by starting your
own company.The market is a lot more discerning than any employer.  And it is
completely non-discriminatory.  On the Internet, nobody knows you're
a dog.  And more to the point, nobody knows you're 22.  All users
care about is whether your site or software gives them what they
want.  They don't care if the person behind it is a high school 
kid.If you're really productive, why not make employers pay market rate
for you?  Why go work as an ordinary employee for a big
company, when you could start a startup and make them buy it to get
you?When most people hear the word ""startup,"" they think of the famous 
ones that have gone public.  But most startups that succeed do it
by getting bought.  And usually the acquirer doesn't just want the
technology, but the people who created it as well.Often big companies buy startups before they're profitable.  Obviously
in such cases they're not after revenues.  What they want is the  
development team and the software they've built so far.  When a
startup gets bought for 2 or 3 million six months in, it's really
more of a hiring bonus than an acquisition.I think this sort of thing will happen more and more, and that it 
will be better for everyone.  It's obviously better for the people
who start the startup, because they get a big chunk of money up
front.  But I think it will be better for the acquirers too.  The
central problem in big companies, and the main reason they're so 
much less productive than small companies, is the difficulty of
valuing each person's work.  Buying larval startups solves that   
problem for them: the acquirer doesn't pay till the developers have
proven themselves.  Acquirers are protected on the downside, but   
still get most of the upside.Product DevelopmentBuying startups also solves another problem afflicting big companies:
they can't do product development.  Big companies are good at
extracting the value from existing products, but bad at creating   
new ones.Why? It's worth studying this phenomenon in detail, because this  
is the raison d'etre of startups.To start with, most big companies have some kind of turf to protect,
and this tends to warp their development decisions.  For example,
Web-based applications are hot now, but
within Microsoft there must
be a lot of ambivalence about them, because the very idea of Web-based
software threatens the desktop.  So any Web-based application that  
Microsoft ends up with, will probably, like Hotmail, be something   
developed outside the company.Another reason big companies are bad at developing new products is
that the kind of people who do that tend not to have much power in
big companies (unless they happen to be the CEO).  Disruptive
technologies are developed by disruptive people.  And they either
don't work for the big company, or have been outmaneuvered by yes-men
and have comparatively little influence.Big companies also lose because they usually only build one of each
thing.  When you only have one Web browser, you can't do anything
really risky with it.  If ten different startups design ten different
Web browsers and you take the best, you'll probably get something
better.The more general version of this problem is that there are too many
new ideas for companies to explore them all.  There might be 500   
startups right now who think they're making something Microsoft
might buy.  Even Microsoft probably couldn't manage 500 development
projects in-house.Big companies also don't pay people the right way.  People developing
a new product at a big company get paid roughly the same whether
it succeeds or fails.  People at a startup expect to get rich if
the product succeeds, and get nothing if it fails. [2]  So naturally
the people at the startup work a lot harder.The mere bigness of big companies is an obstacle.  In startups,  
developers are often forced to talk directly to users, whether they
want to or not, because there is no one else to do sales and support.
It's painful doing sales, but you learn much more from
trying to sell people something than reading what   
they said in focus groups.And then of course, big companies are bad at product development 
because they're bad at everything.  Everything happens slower in
big companies than small ones, and product development is something
that has to happen fast, because you have to go through a lot of   
iterations to get something good.TrendI think the trend of big companies buying startups will only
accelerate.  One of the biggest remaining obstacles is pride.  Most  
companies, at least unconsciously, feel they ought to be able to
develop stuff in house, and that buying startups is to some degree 
an admission of failure.  And so, as people generally do with
admissions of failure, they put it off for as long as possible.
That makes the acquisition very expensive when it finally happens.What companies should do is go out and discover startups when they're
young, before VCs have puffed them up into something that costs
hundreds of millions to acquire.  Much of what VCs add, the acquirer
doesn't need anyway.Why don't acquirers try to predict the companies they're going to
have to buy for hundreds of millions, and grab them early for a     
tenth or a twentieth of that?  Because they can't predict the winners
in advance?  If they're only paying a twentieth as much, they only
have to predict a twentieth as well.  Surely they can manage that.I think companies that acquire technology will gradually learn to 
go after earlier stage startups.  They won't necessarily buy them
outright.  The solution may be some hybrid of investment and
acquisition: for example, to buy a chunk of the company and get an
option to buy the rest later.When companies buy startups, they're effectively fusing recruiting 
and product development.  And I think that's more efficient than 
doing the two separately, because you always get people who are
really committed to what they're working on.Plus this method yields teams of developers who already work well
together.  Any conflicts between them have been ironed out under   
the very hot iron of running a startup.  By the time the acquirer  
gets them, they're finishing one another's sentences.  That's  
valuable in software, because so many bugs occur at the boundaries 
between different people's code.InvestorsThe increasing cheapness of starting a company doesn't just give
hackers more power relative to employers.  It also gives them more 
power relative to investors.The conventional wisdom among VCs is that hackers shouldn't be   
allowed to run their own companies.  The founders are supposed to  
accept MBAs as their bosses, and themselves take on some title like  
Chief Technical Officer.  There may be cases where this is a good 
idea.  But I think founders will increasingly be able to push back
in the matter of control, because they just don't need the investors'
money as much as they used to.Startups are a comparatively new phenomenon.  Fairchild Semiconductor
is considered the first VC-backed startup, and they were founded   
in 1959, less than fifty years ago.  Measured on the time scale of   
social change, what we have now is pre-beta.  So we shouldn't assume
the way startups work now is the way they have to work.Fairchild needed a lot of money to get started.  They had to build
actual factories.  What does the first round of venture funding for
a Web-based startup get spent on today?  More money can't get
software written faster; it isn't needed for facilities, because
those can now be quite cheap; all money can really buy you is sales
and marketing.  A sales force is worth something, I'll admit.  But
marketing is increasingly irrelevant.  On the Internet, anything
genuinely good will spread by word of mouth.Investors' power comes from money.  When startups need less money,   
investors have less power over them.  So future founders may not
have to accept new CEOs if they don't want them.  The VCs will have 
to be dragged kicking and screaming down this road, but like many
things people have to be dragged kicking and screaming toward, it
may actually be good for them.Google is a sign of the way things are going.  As a condition of
funding, their investors insisted they hire someone old and experienced
as CEO.  But from what I've heard the founders didn't just give in
and take whoever the VCs wanted.  They delayed for an entire year,
and when they did finally take a CEO, they chose a guy with a PhD 
in computer science.It sounds to me as if the founders are still the most powerful
people in the company, and judging by Google's performance, their
youth and inexperience doesn't seem to have hurt them.  Indeed, I
suspect Google has done better than they would have if the founders
had given the VCs what they wanted, when they wanted it, and let 
some MBA take over as soon as they got their first round of funding.I'm not claiming the business guys installed by VCs have no value.
Certainly they have.  But they don't need to become the founders'
bosses, which is what that title CEO means.  I predict that in the 
future the executives installed by VCs will increasingly be COOs
rather than CEOs.  The founders will run engineering directly, and
the rest of the company through the COO.The Open CageWith both employers and investors, the balance of power is slowly
shifting towards the young.  And yet they seem the last to realize
it.  Only the most ambitious undergrads even consider starting their
own company when they graduate.  Most just want to get a job.Maybe this is as it should be.  Maybe if the idea of starting a  
startup is intimidating, you filter out the uncommitted.  But I    
suspect the filter is set a little too high.  I think there are
people who could, if they tried, start successful startups, and who
instead let themselves be swept into the intake ducts of big
companies.Have you ever noticed that when animals are let out of cages, they
don't always realize at first that the door's open?  Often they
have to be poked with a stick to get them out.  Something similar  
happened with blogs.  People could have been publishing online in    
1995, and yet blogging has only really taken off in the last couple
years.  In 1995 we thought only professional writers were entitled
to publish their ideas, and that anyone else who did was a crank.
Now publishing online is becoming so popular that everyone wants  
to do it, even print journalists.  But blogging has not taken off  
recently because of any technical innovation; it just took eight
years for everyone to realize the cage was open.I think most undergrads don't realize yet that the economic cage  
is open.  A lot have been told by their parents that the route to
success is to get a good job.  This was true when their parents
were in college, but it's less true now. The route to success is
to build something valuable, and you don't have to be working for    
an existing company to do that.  Indeed, you can often do it better
if you're not.When I talk to undergrads, what surprises me most about them is how
conservative they are.   Not politically, of course.  I mean they
don't seem to want to take risks.  This is a mistake, because the
younger you are, the more risk you can take.RiskRisk and reward are always proportionate.  For example, stocks are
riskier than bonds, and over time always have greater returns.  So
why does anyone invest in bonds?  The catch is that phrase ""over
time.""  Stocks will generate greater returns over thirty years, but
they might lose value from year to year.  So what you should invest
in depends on how soon you need the money.  If you're young, you 
should take the riskiest investments you can find.All this talk about investing may seem very theoretical.  Most
undergrads probably have more debts than assets.  They may feel
they have nothing to invest.  But that's not true: they have their
time to invest, and the same rule about risk applies there.  Your
early twenties are exactly the time to take insane career risks.The reason risk is always proportionate to reward is that market  
forces make it so.  People will pay extra for stability.  So if you
choose stability-- by buying bonds, or by going to work for a big
company-- it's going to cost you.Riskier career moves pay better on average, because there is less
demand for them.  Extreme choices like starting a startup are so  
frightening that most people won't even try.  So you don't end up   
having as much competition as you might expect, considering the
prizes at stake.The math is brutal.  While perhaps 9 out of 10 startups fail, the  
one that succeeds will pay the founders more than 10 times what
they would have made in an ordinary job. [3]
That's the sense in
which startups pay better ""on average.""Remember that.  If you start a startup, you'll probably fail.  Most
startups fail. It's the nature of the business.  But it's not
necessarily a mistake to try something that has a 90% chance of
failing, if you can afford the risk.  Failing at 40, when you have 
a family to support, could be serious.  But if you fail at 22, so    
what?  If you try to start a startup right out of college and it   
tanks, you'll end up at 23 broke and a lot smarter.  Which, if you
think about it, is roughly what you hope to get from a graduate  
program.Even if your startup does tank, you won't harm your prospects with
employers.  To make sure I asked some friends who work for big
companies.  I asked managers at Yahoo, Google, Amazon, Cisco and
Microsoft how they'd feel about two candidates, both 24, with equal
ability, one who'd tried to start a startup that tanked, and another
who'd spent the two years since college working as a developer at
a big company.  Every one responded that they'd prefer the guy who'd
tried to start his own company.  Zod Nazem, who's in charge of       
engineering at Yahoo, said:
   I actually put more value on the guy with the failed
  startup.  And you can quote me!  
So there you have it.  Want to get hired by Yahoo?  Start your own 
company.The Man is the CustomerIf even big employers think highly of young hackers who start
companies, why don't more do it?  Why are undergrads so conservative?
I think it's because they've spent so much time in institutions.The first twenty years of everyone's life consists of being piped
from one institution to another.  You probably didn't have much
choice about the secondary schools you went to.  And after high
school it was probably understood that you were supposed to go to
college.  You may have had a few different colleges to choose
between, but they were probably pretty similar.  So by this point
you've been riding on a subway line for twenty years, and the next
stop seems to be a job.Actually college is where the line ends.  Superficially, going to
work for a company may feel like just the next in a series of      
institutions, but underneath, everything is different.  The end of
school is the fulcrum of your life, the point where you go from   
net consumer to net producer.The other big change is that now, you're steering.  You can go
anywhere you want. So it may be worth standing back and understanding
what's going on, instead of just doing the default thing.All through college, and probably long before that, most undergrads 
have been thinking about what employers want.  But what really 
matters is what customers want, because they're the ones who give
employers the money to pay you.So instead of thinking about what employers want, you're probably
better off thinking directly about what users want.  To the extent 
there's any difference between the two, you can even use that to
your advantage if you start a company of your own.  For example,
big companies like docile conformists.  But this is merely an
artifact of their bigness, not something customers need.Grad SchoolI didn't consciously realize all this when I was graduating from   
college-- partly because I went straight to grad school.  Grad
school can be a pretty good deal, even if you think of one day   
starting a startup.  You can start one when you're done, or even
pull the ripcord part way through, like the founders of Yahoo and
Google.Grad school makes a good launch pad for startups, because you're
collected together with a lot of smart people, and you have bigger 
chunks of time to work on your own projects than an undergrad or
corporate employee would.  As long as you have a fairly tolerant
advisor, you can take your time developing an idea before turning   
it into a company.  David Filo and Jerry Yang started the Yahoo      
directory in February 1994 and were getting a million hits a day
by the fall, but they didn't actually drop out of grad school and
start a company till March 1995.You could also try the startup first, and if it doesn't work, then
go to grad school.  When startups tank they usually do it fairly
quickly. Within a year you'll know if you're wasting your time.If it fails, that is.  If it succeeds, you may have to delay grad
school a little longer.  But you'll have a much more enjoyable life  
once there than you would on a regular grad student stipend.ExperienceAnother reason people in their early twenties don't start startups
is that they feel they don't have enough experience.  Most investors
feel the same.I remember hearing a lot of that word ""experience"" when I was in  
college.  What do people really mean by it?  Obviously it's not the
experience itself that's valuable, but something it changes in your
brain.  What's different about your brain after you have ""experience,""
and can you make that change happen faster?I now have some data on this, and I can tell you what tends to be 
missing when people lack experience.  I've said that every 
startup needs three things: to start with good people,
to make something users want, and not to spend too much money.  It's
the middle one you get wrong when you're inexperienced.  There are   
plenty of undergrads with enough technical skill to write good
software, and undergrads are not especially prone to waste money.
If they get something wrong, it's usually not realizing they have   
to make something people want.This is not exclusively a failing of the young.  It's common for
startup founders of all ages to build things no one wants.Fortunately, this flaw should be easy to fix.  If undergrads were  
all bad programmers, the problem would be a lot harder.  It can 
take years to learn how to program.  But I don't think it takes 
years to learn how to make things people want.  My hypothesis is
that all you have to do is smack hackers on the side of the head
and tell them: Wake up.  Don't sit here making up a priori theories
about what users need.  Go find some users and see what they need.Most successful startups not only do something very specific, but  
solve a problem people already know they have.The big change that ""experience"" causes in your brain is learning
that you need to solve people's problems.  Once you grasp that, you
advance quickly to the next step, which is figuring out what those
problems are.  And that takes some effort, because the way software
actually gets used, especially by the people who pay the most for
it, is not at all what you might expect.  For example, the stated  
purpose of Powerpoint is to present ideas.  Its real role is to 
overcome people's fear of public speaking.  It allows you to give
an impressive-looking talk about nothing, and it causes the audience
to sit in a dark room looking at slides, instead of a bright one     
looking at you.This kind of thing is out there for anyone to see.  The key is to
know to look for it-- to realize that having an idea for a startup
is not like having an idea for a class project.  The goal in a
startup is not to write a cool piece of software.  It's to make 
something people want.  And to do that you have to look at users--
forget about hacking, and just look at users.  This can be quite a
mental adjustment, because little if any of the software you write
in school even has users.  A few steps before a Rubik's Cube is solved, it still looks like a
mess.  I think there are a lot of undergrads whose brains are in a  
similar position: they're only a few steps away from being able to
start successful startups, if they wanted to, but they don't realize
it.  They have more than enough technical skill.  They just haven't
realized yet that the way to create wealth is to make what users   
want, and that employers are just proxies for users in which risk  
is pooled.If you're young and smart, you don't need either of those.  You
don't need someone else to tell you what users want, because you  
can figure it out yourself.  And you don't want to pool risk, because
the younger you are, the more risk you should take.A Public Service MessageI'd like to conclude with a joint message from me and your parents.
Don't drop out of college to start a startup.  There's no rush.     
There will be plenty of time to start companies after you graduate.
In fact, it may be just as well to go work for an existing company
for a couple years after you graduate, to learn how companies work.And yet, when I think about it, I can't imagine telling Bill Gates
at 19 that he should wait till he graduated to start a company.    
He'd have told me to get lost.  And could I have honestly claimed
that he was harming his future-- that he was learning less by working
at ground zero of the microcomputer revolution than he would have
if he'd been taking classes back at Harvard?  No, probably not.And yes, while it is probably true that you'll learn some valuable
things by going to work for an existing company for a couple years
before starting your own, you'd learn a thing or two running your  
own company during that time too.The advice about going to work for someone else would get an even
colder reception from the 19 year old Bill Gates.  So I'm supposed 
to finish college, then go work for another company for two years,
and then I can start my own?  I have to wait till I'm 23?  That's  
four years.  That's more than twenty percent of my life so
far.  Plus in four years it will be way too late to make money     
writing a Basic interpreter for the Altair.And he'd be right.  The Apple II was launched just two years later.
In fact, if Bill had finished college and gone to work for another
company as we're suggesting, he might well have gone to work for
Apple.  And while that would probably have been better for all of
us, it wouldn't have been better for him.So while I stand by our responsible advice to finish college and
then go work for a while before starting a startup, I have to admit
it's one of those things the old tell the young, but don't expect
them to listen to.  We say this sort of thing mainly so we can claim
we warned you.  So don't say I didn't warn you.
Notes[1]
The average B-17 pilot in World War II was in his early twenties.
(Thanks to Tad Marko for pointing this out.)[2] If a company tried to pay employees this way, they'd be called
unfair.  And yet when they buy some startups and not others, no one
thinks of calling that unfair. 
[3] The 1/10 success rate for startups is a bit of an urban legend.
It's suspiciously neat.  My guess is the odds are slightly worse.Thanks to Jessica Livingston for reading drafts of this, to
the friends I promised anonymity to for their opinions about hiring,
and to Karen Nguyen and the Berkeley CSUA for organizing this talk.Russian TranslationRomanian TranslationJapanese Translation

If you liked this, you may also like
Hackers & Painters.

","startup advice
",human,"human
","human
"
36,36,"February 2009Hacker News was two years
old last week.  Initially it was supposed to be a side project—an
application to sharpen Arc on, and a place for current and future
Y Combinator founders to exchange news.  It's grown bigger and taken
up more time than I expected, but I don't regret that because I've
learned so much from working on it.GrowthWhen we launched in February 2007, weekday traffic was around 1600
daily uniques.  It's since grown to around 22,000.  This growth
rate is a bit higher than I'd like.  I'd like the site to grow,
since a site that isn't growing at least slowly is probably dead.
But I wouldn't want it to grow as large as Digg or Reddit—mainly
because that would dilute the character of the site, but also because
I don't want to spend all my time dealing with scaling.I already have problems enough with that.  Remember, the original
motivation for HN was to test a new programming language, and
moreover one that's focused on experimenting with language design,
not performance.  Every time the site gets slow, I fortify myself
by recalling McIlroy and Bentley's famous quote

  The key to performance is elegance, not battalions of special
  cases.

and look for the bottleneck I can remove with least code.  So far
I've been able to keep up, in the sense that performance has remained
consistently mediocre despite 14x growth. I don't know what I'll
do next, but I'll probably think of something.This is my attitude to the site generally.  Hacker News is an
experiment, and an experiment in a very young field.  Sites of this
type are only a few years old.  Internet conversation generally is
only a few decades old.  So we've probably only discovered a fraction
of what we eventually will.That's why I'm so optimistic about HN.  When a technology is this
young, the existing solutions are usually terrible; which means it
must be possible to do much better; which means many problems that
seem insoluble aren't. Including, I hope, the problem that has
afflicted so many previous communities: being ruined by growth.DilutionUsers have worried about that since the site was a few months old.
So far these alarms have been false, but they may not always be.
Dilution is a hard problem. But probably soluble; it doesn't mean
much that open conversations have ""always"" been destroyed by growth
when ""always"" equals 20 instances.But it's important to remember we're trying to solve a new problem,
because that means we're going to have to try new things, most of
which probably won't work.  A couple weeks ago I tried displaying
the names of users with the highest average comment scores in orange.
[1]
That was a mistake.  Suddenly a culture that had been more
or less united was divided into haves and have-nots.  I didn't
realize how united the culture had been till I saw it divided.  It
was painful to watch.
[2]So orange usernames won't be back.  (Sorry about that.)  But there
will be other equally broken-seeming ideas in the future, and the
ones that turn out to work will probably seem just as broken as
those that don't.Probably the most important thing I've learned about dilution is
that it's measured more in behavior than users. It's bad behavior
you want to keep out more than bad people. User behavior turns out
to be surprisingly malleable.  If people are 
expected to behave
well, they tend to; and vice versa.Though of course forbidding bad behavior does tend to keep away bad
people, because they feel uncomfortably constrained in a place where
they have to behave well.  But this way of keeping them out is
gentler and probably also more effective than overt barriers.It's pretty clear now that the broken windows theory applies to
community sites as well.  The theory is that minor forms of bad
behavior encourage worse ones: that a neighborhood with lots of
graffiti and broken windows becomes one where robberies occur.  I
was living in New York when Giuliani introduced the reforms that
made the broken windows theory famous, and the transformation was
miraculous. And I was a Reddit user when the opposite happened
there, and the transformation was equally dramatic.I'm not criticizing Steve and Alexis.  What happened to Reddit
didn't happen out of neglect.  From the start they had a policy of
censoring nothing except spam.  Plus Reddit had different goals
from Hacker News.  Reddit was a startup, not a side project; its
goal was to grow as fast as possible.  Combine rapid growth and
zero censorship, and the result is a free for all.  But I don't
think they'd do much differently if they were doing it again.
Measured by traffic, Reddit is much more successful than Hacker
News.But what happened to Reddit won't inevitably happen to HN. There
are several local maxima.  There can be places that are free for
alls and places that are more thoughtful, just as there are in the
real world; and people will behave differently depending on which
they're in, just as they do in the real world.I've observed this in the wild.  I've seen people cross-posting on
Reddit and Hacker News who actually took the trouble to write two
versions, a flame for Reddit and a more subdued version for HN.SubmissionsThere are two major types of problems a site like Hacker News needs
to avoid: bad stories and bad comments.  So far the danger of bad
stories seems smaller.  The stories on the frontpage now are still
roughly the ones that would have been there when HN started.I once thought I'd have to weight votes to keep crap off the
frontpage, but I haven't had to yet.  I wouldn't have predicted the
frontpage would hold up so well, and I'm not sure why it has.
Perhaps only the more thoughtful users care enough to submit and
upvote links, so the marginal cost of one random new user approaches
zero.  Or perhaps the frontpage protects itself, by advertising what type of submission is expected.The most dangerous thing for the frontpage is stuff that's too easy
to upvote.  If someone proves a new theorem, it takes some work by
the reader to decide whether or not to upvote it.  An amusing cartoon
takes less.  A rant with a rallying cry as the title takes zero,
because people vote it up without even reading it.Hence what I call the Fluff Principle: on a user-voted news site,
the links that are easiest to judge will take over unless you take
specific measures to prevent it.Hacker News has two kinds of protections against fluff.  The most
common types of fluff links are banned as off-topic.  Pictures of
kittens, political diatribes, and so on are explicitly banned.  This
keeps out most fluff, but not all of it.  Some links are both fluff,
in the sense of being very short, and also on topic.There's no single solution to that.  If a link is just an empty
rant, editors will sometimes kill it even if it's on topic in the
sense of being about hacking, because it's not on topic by the real
standard, which is to engage one's intellectual curiosity.  If the
posts on a site are characteristically of this type I sometimes ban
it, which means new stuff at that url is auto-killed.  If a post
has a linkbait title, editors sometimes rephrase it to be more
matter-of-fact.  This is especially necessary with links whose
titles are rallying cries, because otherwise they become implicit
""vote up if you believe such-and-such"" posts, which are the most
extreme form of fluff.The techniques for dealing with links have to evolve, because the
links do. The existence of aggregators has already affected what
they aggregate. Writers now deliberately write things to draw traffic
from aggregators—sometimes even specific ones.  (No, the irony
of this statement is not lost on me.)  Then there are the more
sinister mutations, like linkjacking—posting a paraphrase of
someone else's article and submitting that instead of the original.
These can get a lot of upvotes, because a lot of what's good in an
article often survives; indeed, the closer the paraphrase is to
plagiarism, the more survives.
[3]I think it's important that a site that kills submissions provide
a way for users to see what got killed if they want to.  That keeps
editors honest, and just as importantly, makes users confident
they'd know if the editors stopped being honest. HN users can do
this by flipping a switch called showdead in their profile.
[4]CommentsBad comments seem to be a harder problem than bad submissions.
While the quality of links on the frontpage of HN hasn't changed
much, the quality of the median comment may have decreased somewhat.There are two main kinds of badness in comments: meanness and
stupidity.  There is a lot of overlap between the two—mean
comments are disproportionately likely also to be dumb—but
the strategies for dealing with them are different.  Meanness is
easier to control.  You can have rules saying one shouldn't be mean,
and if you enforce them it seems possible to keep a lid on meanness.Keeping a lid on stupidity is harder, perhaps because stupidity is
not so easily distinguishable.  Mean people are more likely to know
they're being mean than stupid people are to know they're being
stupid.The most dangerous form of stupid comment is not the long but
mistaken argument, but the dumb joke.  Long but mistaken arguments
are actually quite rare.  There is a strong correlation between
comment quality and length; if you wanted to compare the quality
of comments on community sites, average length would be a good
predictor.  Probably the cause is human nature rather than anything
specific to comment threads. Probably it's simply that stupidity
more often takes the form of having few ideas than wrong ones.Whatever the cause, stupid comments tend to be short.  And since
it's hard to write a short comment that's distinguished for the
amount of information it conveys, people try to distinguish them
instead by being funny.  The most tempting format for stupid comments
is the supposedly witty put-down, probably because put-downs are
the easiest form of humor. 
[5]
So one advantage of forbidding
meanness is that it also cuts down on these.Bad comments are like kudzu: they take over rapidly. Comments have
much more effect on new comments than submissions have on new
submissions.  If someone submits a lame article, the other submissions
don't all become lame.  But if someone posts a stupid comment on a
thread, that sets the tone for the region around it.  People reply
to dumb jokes with dumb jokes.Maybe the solution is to add a delay before people can respond to
a comment, and make the length of the delay inversely proportional
to some prediction of its quality.  Then dumb threads would grow
slower.
[6]
PeopleI notice most of the techniques I've described are conservative:
they're aimed at preserving the character of the site rather than
enhancing it.  I don't think that's a bias of mine.  It's due to
the shape of the problem.  Hacker News had the good fortune to start
out good, so in this case it's literally a matter of preservation.
But I think this principle would also apply to sites with different
origins.The good things in a community site come from people more than
technology; it's mainly in the prevention of bad things that
technology comes into play. Technology certainly can enhance
discussion.  Nested comments do, for example.  But I'd rather use
a site with primitive features and smart, nice users than a more
advanced one whose users were idiots or trolls.So the most important thing a community site can do is attract the
kind of people it wants.  A site trying to be as big as possible
wants to attract everyone.  But a site aiming at a particular subset
of users has to attract just those—and just as importantly,
repel everyone else.  I've made a conscious effort to do this on
HN.  The graphic design is as plain as possible, and the site rules
discourage dramatic link titles.  The goal is that the only thing
to interest someone arriving at HN for the first time should be the
ideas expressed there.The downside of tuning a site to attract certain people is that,
to those people, it can be too attractive.  I'm all too aware how
addictive Hacker News can be.  For me, as for many users, it's a
kind of virtual town square.  When I want to take a break from
working, I walk into the square, just as I might into Harvard Square
or University Ave in the physical world.
[7]
But an online square is
more dangerous than a physical one.  If I spent half the day loitering
on University Ave, I'd notice.  I have to walk a mile to get there,
and sitting in a cafe feels different from working. But visiting
an online forum takes just a click, and feels superficially very
much like working.  You may be wasting your time, but you're not
idle.  Someone is wrong on the Internet, and you're fixing the
problem.Hacker News is definitely useful.  I've learned a lot from things
I've read on HN.  I've written several essays that began as comments
there.  So I wouldn't want the site to go away.  But I would like
to be sure it's not a net drag on productivity.  What a disaster
that would be, to attract thousands of smart people to a site that
caused them to waste lots of time.  I wish I could be 100% sure
that's not a description of HN.I feel like the addictiveness of games and social applications is
still a mostly unsolved problem.  The situation now is like it was
with crack in the 1980s: we've invented terribly addictive new
things, and we haven't yet evolved ways to protect ourselves from
them.  We will eventually, and that's one of the problems I hope
to focus on next.
Notes[1]
I tried ranking users by both average and median comment
score, and average (with the high score thrown out) seemed the more
accurate predictor of high quality.  Median may be the more accurate
predictor of low quality though.[2]
Another thing I learned from this experiment is that if you're
going to distinguish between people, you better be sure you do it
right.  This is one problem where rapid prototyping doesn't work.Indeed, that's the intellectually honest argument for not discriminating
between various types of people.  The reason not to do it is not
that everyone's the same, but that it's bad to do wrong and hard
to do right.[3]
When I catch egregiously linkjacked posts I replace the url
with that of whatever they copied.  Sites that habitually linkjack
get banned.[4]
Digg is notorious for its lack of transparency.  The root of
the problem is not that the guys running Digg are especially sneaky,
but that they use the wrong algorithm for generating their frontpage.
Instead of bubbling up from the bottom as they get more votes, as
on Reddit, stories start at the top and get pushed down by new
arrivals.The reason for the difference is that Digg is derived from Slashdot,
while Reddit is derived from Delicious/popular.  Digg is Slashdot
with voting instead of editors, and Reddit is Delicious/popular
with voting instead of bookmarking.  (You can still see fossils of
their origins in their graphic design.)Digg's algorithm is very vulnerable to gaming, because any story
that makes it onto the frontpage is the new top story.  Which in
turn forces Digg to respond with extreme countermeasures.  A lot
of startups have some kind of secret about the subterfuges they had
to resort to in the early days, and I suspect Digg's is the extent
to which the top stories were de facto chosen by human editors.[5]
The dialog on Beavis and Butthead was composed largely of
these, and when I read comments on really bad sites I can hear them
in their voices.[6]
I suspect most of the techniques for discouraging stupid
comments have yet to be discovered.  Xkcd implemented a particularly
clever one in its IRC channel: don't allow the same thing twice.
Once someone has said ""fail,"" no one can ever say it again.  This
would penalize short comments especially, because they have less
room to avoid collisions in.Another promising idea is the stupid 
filter, which is just like a
probabilistic spam filter, but trained on corpora of stupid and
non-stupid comments instead.You may not have to kill bad comments to solve the problem.  Comments
at the bottom of a long thread are rarely seen, so it may be enough
to incorporate a prediction of quality in the comment sorting
algorithm.[7]
What makes most suburbs so demoralizing is that there's no
center to walk to.
Thanks to Justin Kan, Jessica Livingston, Robert Morris,
Alexis Ohanian, Emmet Shear, and Fred Wilson for reading drafts of
this.
Comment on this essay.","startup advice
",human,"human
","human
"
37,37,"

Want to start a startup?  Get funded by
Y Combinator.




August 2006, rev. April 2007, September 2010In a few days it will be Demo Day, when the startups we funded
this summer present to investors.  Y Combinator funds startups twice
a year, in January and June.  Ten weeks later we invite all the
investors we know to hear them present what they've built so far.Ten weeks is not much time.  The average startup probably doesn't
have much to show for itself after ten weeks.  But the average
startup fails.  When you look at the ones that went on to do great
things, you find a lot that began with someone pounding out a
prototype in a week or two of nonstop work.  Startups are a
counterexample to the rule that haste makes waste.(Too much money seems to be as bad for startups as too much time,
so we don't give them much money either.)A week before Demo Day, we have a dress rehearsal called Rehearsal Day.
At other Y Combinator events we allow outside guests, but not at
Rehearsal Day.  No one except the other founders gets to see the rehearsals.The presentations on Rehearsal Day are often pretty rough.  But this is
to be expected.  We try to pick founders who are good at building
things, not ones who are slick presenters.  Some of the founders
are just out of college, or even still in it, and have never spoken
to a group of people they didn't already know.So we concentrate on the basics.  On Demo Day each startup will
only get ten minutes, so we encourage them to focus on just two
goals: (a) explain what you're doing, and (b) explain why users
will want it.That might sound easy, but it's not when the speakers have no
experience presenting, and they're explaining technical matters to
an audience that's mostly non-technical.This situation is constantly repeated when startups present to
investors: people who are bad at explaining, talking to people who
are bad at understanding.  Practically every successful startup,
including stars like Google, presented at some point to investors
who didn't get it and turned them down.  Was it because the founders
were bad at presenting, or because the investors were obtuse?  It's
probably always some of both.At the most recent Rehearsal Day, we four Y Combinator partners found
ourselves saying a lot of the same things we said at the last two.
So at dinner afterward we collected all our tips about presenting
to investors.  Most startups face similar challenges, so we hope
these will be useful to a wider audience.
1. Explain what you're doing.Investors' main question when judging a very early startup is whether
you've made a compelling product.  Before they can judge whether
you've built a good x, they have to understand what kind of x you've
built.  They will get very frustrated if instead of telling them
what you do, you make them sit through some kind of preamble.Say what you're doing as soon as possible, preferably in the first
sentence. ""We're Jeff and Bob and we've built an easy to use web-based
database.  Now we'll show it to you and explain why people need
this.""If you're a great public speaker you may be able to violate this
rule.  Last year one founder spent the whole first half of his talk
on a fascinating analysis of the limits of the conventional desktop
metaphor.  He got away with it, but unless you're a captivating
speaker, which most hackers aren't, it's better to play it safe.2. Get rapidly to demo.This section is now obsolete for YC founders presenting
at Demo Day, because Demo Day presentations are now so short
that they rarely include much if any demo.  They seem to work
just as well without, however, which makes me think I was
wrong to emphasize demos so much before.A demo explains what you've made more effectively than any verbal
description.  The only thing worth talking about first is the problem
you're trying to solve and why it's important.  But don't spend
more than a tenth of your time on that.  Then demo.When you demo, don't run through a catalog of features.  Instead
start with the problem you're solving, and then show how your product
solves it.  Show features in an order driven by some kind of purpose,
rather than the order in which they happen to appear on the screen.If you're demoing something web-based, assume that the network
connection will mysteriously die 30 seconds into your presentation,
and come prepared with a copy of the server software running on
your laptop.3. Better a narrow description than a vague one.One reason founders resist describing their projects concisely is
that, at this early stage, there are all kinds of possibilities.
The most concise descriptions seem misleadingly narrow.  So for
example a group that has built an easy web-based database might
resist calling their applicaton that, because it could be so much
more.  In fact, it could be anything...The problem is, as you approach (in the calculus sense) a description
of something that could be anything, the content of your description
approaches zero.  If you describe your web-based database as ""a
system to allow people to collaboratively leverage the value of
information,"" it will go in one investor ear and out the other.
They'll just discard that sentence as meaningless boilerplate, and
hope, with increasing impatience, that in the next sentence you'll
actually explain what you've made.Your primary goal is not to describe everything your system might
one day become, but simply to convince investors you're worth talking
to further.  So approach this like an algorithm that gets the right
answer by successive approximations.  Begin with a description
that's gripping but perhaps overly narrow, then flesh it out to the
extent you can.  It's the same principle as incremental development:
start with a simple prototype, then add features, but at every point
have working code.  In this case, ""working code"" means a working
description in the investor's head.4. Don't talk and drive.Have one person talk while another uses the computer.  If the same
person does both, they'll inevitably mumble downwards at the computer
screen instead of talking clearly at the audience.As long as you're standing near the audience and looking at them,
politeness (and habit) compel them to pay attention to you.  Once
you stop looking at them to fuss with something on your computer,
their minds drift off to the errands they have to run later.5. Don't talk about secondary matters at length.If you only have a few minutes, spend them explaining what your
product does and why it's great.  Second order issues like competitors
or resumes should be single slides you go through quickly at the
end.  If you have impressive resumes, just flash them on the screen
for 15 seconds and say a few words.  For competitors, list the top
3 and explain in one sentence each what they lack
that you have.  And put this kind of thing at the end, after you've
made it clear what you've built.6. Don't get too deeply into business models.It's good to talk about how you plan to make money, but mainly
because it shows you care about that and have thought about it.
Don't go into detail about your business model, because (a) that's
not what smart investors care about in a brief presentation, and
(b) any business model you have at this point is probably wrong
anyway.Recently a VC who came to speak at Y Combinator talked about a
company he just invested in.  He said their business model was wrong
and would probably change three times before they got it right.
The founders were experienced guys who'd done startups before and
who'd just succeeded in getting millions from one of the top VC
firms, and even their business model was crap.  (And yet he invested
anyway, because he expected it to be crap at this stage.)If you're solving an important problem, you're going to sound a lot
smarter talking about that than the business model.  The business
model is just a bunch of guesses, and guesses about stuff that's
probably not your area of expertise.  So don't spend your precious
few minutes talking about crap when you could be talking about
solid, interesting things you know a lot about: the problem you're
solving and what you've built so far.As well as being a bad use of time, if your business model seems
spectacularly wrong, that will push the stuff you want investors
to remember out of their heads.  They'll just remember you as the
company with the boneheaded plan for making money, rather than the
company that solved that important problem.7. Talk slowly and clearly at the audience.Everyone at Rehearsal Day could see the difference between the people
who'd been out in the world for a while and had presented to groups,
and those who hadn't.You need to use a completely different voice and manner talking to
a roomful of people than you would in conversation.  Everyday life
gives you no practice in this.  If you can't already do it, the
best solution is to treat it as a consciously artificial trick,
like juggling.However, that doesn't mean you should talk like some kind of
announcer.  Audiences tune that out.  What you need to do is talk
in this artificial way, and yet make it seem conversational.  (Writing
is the same.  Good writing is an elaborate effort to seem spontaneous.)If you want to write out your whole presentation beforehand and
memorize it, that's ok.  That has worked for some groups in the
past.  But make sure to write something that sounds like spontaneous,
informal speech, and deliver it that way too.Err on the side of speaking slowly.  At Rehearsal Day, one of the founders
mentioned a rule actors use: if you feel you're speaking too slowly,
you're speaking at about the right speed.8. Have one person talk.Startups often want to show that all the founders are equal partners.
This is a good instinct; investors dislike unbalanced teams.  But
trying to show it by partitioning the presentation is going too
far.  It's distracting.  You can demonstrate your respect
for one another in more subtle ways.  For example, when one of the
groups presented at Demo Day, the more extroverted of the two
founders did most of the talking, but he described his co-founder
as the best hacker he'd ever met, and you could tell he meant it.Pick the one or at most two best speakers, and have them do most
of the talking.Exception: If one of the founders is an expert in some specific
technical field, it can be good for them to talk about that for a
minute or so.  This kind of ""expert witness"" can add credibility,
even if the audience doesn't understand all the details.  If Jobs
and Wozniak had 10 minutes to present the Apple II, it might be a good plan
to have Jobs speak for 9 minutes and have Woz speak for a minute
in the middle about some of the technical feats he'd pulled off in
the design.  (Though of course if it were actually those two, Jobs
would speak for the entire 10 minutes.)9. Seem confident.Between the brief time available and their lack of technical
background, many in the audience will have a hard time evaluating
what you're doing.  Probably the single biggest piece of evidence,
initially, will be your own confidence in it.   You have
to show you're impressed with what you've made.And I mean show, not tell.  Never say ""we're passionate"" or ""our
product is great.""  People just ignore that—or worse, write you
off as bullshitters.  Such messages must be implicit.What you must not do is seem nervous and apologetic.  If you've
truly made something good, you're doing investors a favor by
telling them about it.  If you don't genuinely believe that, perhaps
you ought to change what your company is doing.  If you don't believe
your startup has such promise that you'd be doing them a favor by
letting them invest, why are you investing your time in it?10. Don't try to seem more than you are.Don't worry if your company is just a few months old and doesn't
have an office yet, or your founders are technical people with no
business experience.  Google was like that once, and they turned out
ok.  Smart investors can see past such superficial flaws.  They're
not looking for finished, smooth presentations.  They're looking
for raw talent.  All you need to convince them of is that you're
smart and that you're onto something good.  If you try too hard to
conceal your rawness—by trying to seem corporate, or pretending
to know about stuff you don't—you may just conceal your talent.You can afford to be candid about what you haven't figured out yet.
Don't go out of your way to bring it up (e.g. by having a slide
about what might go wrong), but don't try to pretend either that
you're further along than you are.  If you're a hacker and you're
presenting to experienced investors, they're probably better at
detecting bullshit than you are at producing it.11. Don't put too many words on slides.When there are a lot of words on a slide, people just skip reading
it.  So look at your slides and ask of each word ""could I cross
this out?""  This includes gratuitous clip art.  Try to get your
slides under 20 words if you can.Don't read your slides.  They should be something in the background
as you face the audience and talk to them, not something you face
and read to an audience sitting behind you.Cluttered sites don't do well in demos, especially when they're
projected onto a screen.  At the very least, crank up the font size
big enough to make all the text legible.  But cluttered sites are
bad anyway, so perhaps you should use this opportunity to make your
design simpler.12. Specific numbers are good.If you have any kind of data, however preliminary, tell the audience.
Numbers stick in people's heads.  If you can claim that the median
visitor generates 12 page views, that's great.But don't give them more than four or five numbers, and only give
them numbers specific to you.  You don't need to tell them the size
of the market you're in.  Who cares, really, if it's 500 million
or 5 billion a year?  Talking about that is like an actor at the
beginning of his career telling his parents how much Tom Hanks
makes.  Yeah, sure, but first you have to become Tom Hanks.  The
important part is not whether he makes ten million a year or a
hundred, but how you get there.13. Tell stories about users.The biggest fear of investors looking at early stage startups is
that you've built something based on your own a priori theories of
what the world needs, but that no one will actually want.  So it's
good if you can talk about problems specific users have and how you
solve them.Greg Mcadoo said one thing Sequoia looks for is the ""proxy for
demand.""  What are people doing now, using inadequate tools, that
shows they need what you're making?Another sign of user need is when people pay a lot for something.
It's easy to convince investors there will be demand for
a cheaper alternative to something popular, if you preserve
the qualities that made it popular.The best stories about user needs are about your own.  A remarkable
number of famous startups grew out of some need the founders had:
Apple, Microsoft, Yahoo, Google.  Experienced investors know that,
so stories of this type will get their attention.  The next best
thing is to talk about the needs of people you know personally,
like your friends or siblings.14. Make a soundbite stick in their heads.Professional investors hear a lot of pitches.  After a while they
all blur together.  The first cut is simply to be one of those
they remember.  And the way to ensure that is to create a descriptive
phrase about yourself that sticks in their heads.In Hollywood, these phrases seem to be of the form ""x meets y.""

In the startup world, they're usually ""the x of y"" or ""the x y.""
Viaweb's was ""the Microsoft Word of ecommerce.""Find one and launch it clearly (but apparently casually) in your
talk, preferably near the beginning.It's a good exercise for you, too, to sit down and try to figure
out how to describe your startup in one compelling phrase.  If you
can't, your plans may not be sufficiently focused.How to Fund a StartupHackers' Guide to InvestorsSpanish TranslationJapanese TranslationRussian Translation

Image: Casey Muller: Trevor Blackwell at Rehearsal Day, summer 2006","startup advice
",human,"human
","human
"
38,38,"July 2007An investor wants to give you money for a certain percentage of
your startup.  Should you take it?  You're about to hire your first
employee.  How much stock should you give him?These are some of the hardest questions founders face.  And yet
both have the same answer:1/(1 - n)Whenever you're trading stock in your company for anything, whether
it's money or an employee or a deal with another company, the test
for whether to do it is the same.  You should give up n% of your
company if what you trade it for improves your average outcome
enough that the (100 - n)% you have left is worth more than the
whole company was before.For example, if an investor wants to buy half your company, how
much does that investment have to improve your average outcome for
you to break even?  Obviously it has to double: if you trade half
your company for something that more than doubles the company's
average outcome, you're net ahead.  You have half as big a share
of something worth more than twice as much.In the general case, if n is the fraction of the company you're
giving up, the deal is a good one if it makes the company worth
more than 1/(1 - n).For example, suppose Y Combinator offers to fund you in return for
7% of your company.  In this case, n is .07 and 1/(1 - n) is 1.075.
So you should take the deal if you believe we can improve your
average outcome by more than 7.5%.  If we improve your outcome by
10%, you're net ahead, because the remaining .93 you hold is worth
.93 x 1.1 = 1.023.
[1]One of the things the equity equation shows us is that, financially
at least, taking money from a top VC firm can be a really good deal.
Greg Mcadoo from Sequoia recently said at a YC dinner that when
Sequoia invests alone they like to take about 30% of a company.
1/.7 = 1.43, meaning that deal is worth taking if they can improve
your outcome by more than 43%.  For the average startup, that would
be an extraordinary bargain.  It would improve the average startup's
prospects by more than 43% just to be able to say they were funded
by Sequoia, even if they never actually got the money.The reason Sequoia is such a good deal is that the percentage of
the company they take is artificially low.  They don't even try to
get market price for their investment; they limit their holdings
to leave the founders enough stock to feel the company is still
theirs.The catch is that Sequoia gets about 6000 business plans a year and
funds about 20 of them, so the odds of getting this great deal are
1 in 300. The companies that make it through are not average startups.Of course, there are other factors to consider in a VC deal.  It's
never just a straight trade of money for stock.  But if it were,
taking money from a top firm would generally be a bargain.You can use the same formula when giving stock to employees, but
it works in the other direction.  If i is the average outcome for
the company with the addition of some new person, then they're worth
n such that i = 1/(1 - n).  Which means n = (i - 1)/i.For example, suppose you're just two founders and you want to hire
an additional hacker who's so good you feel he'll increase the
average outcome of the whole company by 20%.  n = (1.2 - 1)/1.2 =
.167.  So you'll break even if you trade 16.7% of the company
for him.That doesn't mean 16.7% is the right amount of stock to give him.
Stock is not the only cost of hiring someone: there's usually salary
and overhead as well.  And if the company merely breaks even on the
deal, there's no reason to do it.I think to translate salary and overhead into stock you should
multiply the annual rate by about 1.5.  Most startups grow fast or
die; if you die you don't have to pay the guy, and if you grow fast
you'll be paying next year's salary out of next year's valuation,
which should be 3x this year's.  If your valuation grows 3x a year,
the total cost in stock of a new hire's salary and overhead is 1.5
years' cost at the present valuation.  [2]How much of an additional margin should the company need as the
""activation energy"" for the deal?  Since this is in effect the
company's profit on a hire, the market will determine that: if
you're a hot opportunity, you can charge more.Let's run through an example.  Suppose the company wants to make a
""profit"" of 50% on the new hire mentioned above.  So subtract a
third from 16.7% and we have 11.1% as his ""retail"" price.  Suppose
further that he's going to cost $60k a year in salary and overhead,
x 1.5 = $90k total.  If the company's valuation is $2 million, $90k
is 4.5%.  11.1% - 4.5% = an offer of 6.6%.Incidentally, notice how important it is for early employees to
take little salary.  It comes right out of stock that could otherwise
be given to them.Obviously there is a great deal of play in these numbers.  I'm not
claiming that stock grants can now be reduced to a formula.  Ultimately
you always have to guess.  But at least know what you're guessing.
If you choose a number based on your gut feel, or a table of typical
grant sizes supplied by a VC firm, understand what those are estimates
of.And more generally, when you make any decision involving equity,
run it through 1/(1 - n) to see if it makes sense.  You should
always feel richer after trading equity.  If the trade didn't
increase the value of your remaining shares enough to put you net
ahead, you wouldn't have (or shouldn't have) done it.Notes[1] This is why we
can't believe anyone would think Y Combinator was a bad deal.  Does
anyone really think we're so useless that in three months we can't
improve a startup's prospects by 7.5%?
[2] The obvious choice
for your present valuation is the post-money valuation of your last
funding round.  This probably undervalues the company, though,
because (a) unless your last round just happened, the company is
presumably worth more, and (b) the valuation of an early funding
round usually reflects some other contribution by the investors.Thanks to Sam Altman, Trevor Blackwell, Paul Buchheit, 
Hutch Fishman, David Hornik, Paul Kedrosky, Jessica Livingston, Gary Sabot, and 
Joshua Schachter for reading drafts of this.","startup advice
",human,"human
","human
"
39,39,"May 2006(This essay is derived from a keynote at Xtech.)Startups happen in clusters.  There are a lot of them in Silicon
Valley and Boston, and few in Chicago or Miami.  A country that
wants startups will probably also have to reproduce whatever makes
these clusters form.I've claimed that the recipe is a
great university near a town smart
people like.  If you set up those conditions within the US, startups
will form as inevitably as water droplets condense on a cold piece
of metal.  But when I consider what it would take to reproduce
Silicon Valley in another country, it's clear the US is a particularly
humid environment.  Startups condense more easily here.It is by no means a lost cause to try to create a silicon valley
in another country.  There's room not merely to equal Silicon Valley,
but to surpass it.  But if you want to do that, you have to
understand the advantages startups get from being in America.1. The US Allows Immigration.For example, I doubt it would be possible to reproduce Silicon
Valley in Japan, because one of Silicon Valley's most distinctive
features is immigration.  Half the people there speak with accents.
And the Japanese don't like immigration.  When they think about how
to make a Japanese silicon valley, I suspect they unconsciously
frame it as how to make one consisting only of Japanese people.
This way of framing the question probably guarantees failure.A silicon valley has to be a mecca for the smart and the ambitious,
and you can't have a mecca if you don't let people into it.Of course, it's not saying much that America is more open to
immigration than Japan.  Immigration policy is one area where a
competitor could do better.2. The US Is a Rich Country.I could see India one day producing a rival to Silicon Valley.
Obviously they have the right people: you can tell that by the
number of Indians in the current Silicon Valley.  The problem with
India itself is that it's still so poor.In poor countries, things we take for granted are missing.  A friend
of mine visiting India sprained her ankle falling down the steps
in a railway station.  When she turned to see what had happened,
she found the steps were all different heights.  In industrialized
countries we walk down steps our whole lives and never think about
this, because there's an infrastructure that prevents such a staircase
from being built.The US has never been so poor as some countries are now.  There
have never been swarms of beggars in the streets of American cities.
So we have no data about what it takes to get from the swarms-of-beggars
stage to the silicon-valley stage.  Could you have both at once,
or does there have to be some baseline prosperity before you get a
silicon valley?I suspect there is some speed limit to the evolution
of an economy.  Economies are made out of people, and attitudes can
only change a certain amount per generation.
[1]3. The US Is Not (Yet) a Police State.Another country I could see wanting to have a silicon valley is
China.  But I doubt they could do it yet either.  China still seems
to be a police state, and although present rulers seem enlightened
compared to the last, even enlightened despotism can probably only
get you part way toward being a great economic power.It can get you factories for building things designed elsewhere.
Can it get you the designers, though?  Can imagination flourish
where people can't criticize the government?  Imagination means
having odd ideas, and it's hard to have odd ideas about technology
without also having odd ideas about politics.  And in any case,
many technical ideas do have political implications.  So if you
squash dissent, the back pressure will propagate into technical
fields. 
[2]Singapore would face a similar problem.  Singapore seems very aware
of the importance of encouraging startups.  But while energetic
government intervention may be able to make a port run efficiently,
it can't coax startups into existence.  A state that bans chewing
gum has a long way to go before it could create a San Francisco.Do you need a San Francisco?  Might there not be an alternate route
to innovation that goes through obedience and cooperation instead
of individualism?  Possibly, but I'd bet not.  Most imaginative
people seem to share a certain prickly independence,
whenever and wherever they lived.  You see it in Diogenes telling
Alexander to get out of his light and two thousand years later in
Feynman breaking into safes at Los Alamos.
[3]
Imaginative people
don't want to follow or lead.  They're most productive when everyone
gets to do what they want.Ironically, of all rich countries the US has lost the most civil
liberties recently.  But I'm not too worried yet.  I'm hoping once
the present administration is out, the natural openness of American
culture will reassert itself.4. American Universities Are Better.You need a great university to seed a silicon valley, and so far
there are few outside the US.  I asked a handful of American computer
science professors which universities in Europe were most admired,
and they all basically said ""Cambridge"" followed by a long pause
while they tried to think of others.  There don't seem to be many
universities elsewhere that compare with the best in America, at
least in technology.In some countries this is the result of a deliberate policy.  The
German and Dutch governments, perhaps from fear of elitism, try to
ensure that all universities are roughly equal in quality.  The
downside is that none are especially good.   The best professors
are spread out, instead of being concentrated as they are in the
US.  This probably makes them less productive, because they don't
have good colleagues to inspire them.  It also means no one university
will be good enough to act as a mecca, attracting talent from abroad
and causing startups to form around it.The case of Germany is a strange one.  The Germans invented the
modern university, and up till the 1930s theirs were the best in
the world.  Now they have none that stand out.  As I was mulling
this over, I found myself thinking: ""I can understand why German
universities declined in the 1930s, after they excluded Jews.  But
surely they should have bounced back by now.""  Then I realized:
maybe not.  There are few Jews left in Germany and most Jews I know
would not want to move there.  And if you took any great American
university and removed the Jews, you'd have some pretty big gaps.
So maybe it would be a lost cause trying to create a silicon valley
in Germany, because you couldn't establish the level of university
you'd need as a seed.
[4]It's natural for US universities to compete with one another because
so many are private.  To reproduce the quality of American universities
you probably also have to reproduce this.  If universities are
controlled by the central government, log-rolling will pull them
all toward the mean: the new Institute of X will end up at the
university in the district of a powerful politician, instead of
where it should be.5. You Can Fire People in America.I think one of the biggest obstacles to creating startups in Europe
is the attitude toward employment.  The famously rigid labor laws
hurt every company, but startups especially, because startups have
the least time to spare for bureaucratic hassles.The difficulty of firing people is a particular problem for startups
because they have no redundancy.  Every person has to do their
job well.But the problem is more than just that some startup might have a
problem firing someone they needed to.  Across industries and
countries, there's a strong inverse correlation between performance
and job security.  Actors and directors are fired at the end of
each film, so they have to deliver every time.  Junior professors
are fired by default after a few years unless the university chooses
to grant them tenure.  Professional athletes know they'll be pulled
if they play badly for just a couple games.  At the other end of
the scale (at least in the US) are auto workers, New York City
schoolteachers, and civil servants, who are all nearly impossible
to fire.  The trend is so clear that you'd have to be willfully
blind not to see it.Performance isn't everything, you say?  Well, are auto workers,
schoolteachers, and civil servants happier than actors,
professors, and professional athletes?European public opinion will apparently tolerate people being fired
in industries where they really care about performance.  Unfortunately
the only industry they care enough about so far is soccer.  But
that is at least a precedent.6. In America Work Is Less Identified with Employment.The problem in more traditional places like Europe and Japan goes
deeper than the employment laws.  More dangerous is the attitude
they reflect: that an employee is a kind of servant, whom the
employer has a duty to protect.  It used to be that way in America
too.  In 1970 you were still supposed to get a job with a big
company, for whom ideally you'd work your whole career.  In return
the company would take care of you: they'd try not to fire you,
cover your medical expenses, and support you in old age.Gradually employment has been shedding such paternalistic overtones
and becoming simply an economic exchange.  But the importance of
the new model is not just that it makes it easier for startups to
grow.  More important, I think, is that it it makes it easier for
people to start startups.Even in the US most kids graduating from college still think they're
supposed to get jobs, as if you couldn't be productive without being
someone's employee.  But the less you identify work with employment,
the easier it becomes to start a startup.  When you see your career
as a series of different types of work, instead of a lifetime's
service to a single employer, there's less risk in starting your
own company, because you're only replacing one segment instead of
discarding the whole thing.The old ideas are so powerful that even the most successful startup
founders have had to struggle against them.  A year after the
founding of Apple, Steve Wozniak still hadn't quit HP.  He still
planned to work there for life.  And when Jobs found someone to
give Apple serious venture funding, on the condition that Woz quit,
he initially refused, arguing that he'd designed both the Apple I
and the Apple II while working at HP, and there was no reason he
couldn't continue.7.  America Is Not Too Fussy.If there are any laws regulating businesses, you can assume larval
startups will break most of them, because they don't know what the
laws are and don't have time to find out.For example, many startups in America begin in places where it's
not really legal to run a business.  Hewlett-Packard, Apple, and
Google were all run out of garages.  Many more startups, including
ours, were initially run out of apartments.  If the laws against
such things were actually enforced, most startups wouldn't happen.That could be a problem in fussier countries.  If Hewlett and Packard
tried running an electronics company out of their garage in
Switzerland, the old lady next door would report them to the municipal
authorities.But the worst problem in other countries is probably the effort
required just to start a company.  A friend of mine started a company
in Germany in the early 90s, and was shocked to discover, among
many other regulations, that you needed $20,000 in capital to
incorporate.  That's one reason I'm not typing this on an Apfel
laptop.  Jobs and Wozniak couldn't have come up with that kind of
money in a company financed by selling a VW bus and an HP calculator.
We couldn't have started Viaweb either.
[5]Here's a tip for governments that want to encourage startups: read
the stories of existing startups, and then try to simulate what
would have happened in your country.  When you hit something that
would have killed Apple, prune it off.Startups are marginal.  
They're started by the poor and the
timid; they begin in marginal space and spare time; they're started
by people who are supposed to be doing something else; and though
businesses, their founders often know nothing about business.  Young
startups are fragile.  A society that trims its margins sharply
will kill them all.8. America Has a Large Domestic Market.What sustains a startup in the beginning is the prospect of getting
their initial product out.  The successful ones therefore make the
first version as simple as possible.  In the US they usually begin
by making something just for the local market.This works in America, because the local market is 300 million
people.  It wouldn't work so well in Sweden.  In a small country,
a startup has a harder task: they have to sell internationally from
the start.The EU was designed partly to simulate a single, large domestic
market.  The problem is that the inhabitants still speak many
different languages.  So a software startup in Sweden is still at
a disadvantage relative to one in the US, because they have to deal
with internationalization from the beginning.  It's significant
that the most famous recent startup in Europe, Skype, worked on a
problem that was intrinsically international.However, for better or worse it looks as if Europe will in a few
decades speak a single language.  When I was a student in Italy in
1990, few Italians spoke English.  Now all educated people seem to
be expected to-- and Europeans do not like to seem uneducated.  This
is presumably a taboo subject, but if present trends continue,
French and German will eventually go the way of Irish and Luxembourgish:
they'll be spoken in homes and by eccentric nationalists.9. America Has Venture Funding.Startups are easier to start in America because funding is easier
to get.  There are now a few VC firms outside the US, but startup
funding doesn't only come from VC firms.  A more important source,
because it's more personal and comes earlier in the process, is
money from individual angel investors.  Google might never have got
to the point where they could raise millions from VC funds if they
hadn't first raised a hundred thousand from Andy Bechtolsheim.  And
he could help them because he was one of the founders of Sun.  This
pattern is repeated constantly in startup hubs.  It's this pattern
that makes them startup hubs.The good news is, all you have to do to get the process rolling is
get those first few startups successfully launched.  If they stick
around after they get rich, startup founders will almost automatically
fund and encourage new startups.The bad news is that the cycle is slow.  It probably takes five
years, on average, before a startup founder can make angel investments.
And while governments might be able to set up local VC funds
by supplying the money themselves and recruiting people from existing
firms to run them, only organic growth can produce angel investors.Incidentally, America's private universities are one reason there's
so much venture capital.  A lot of the money in VC funds comes from
their endowments.  So another advantage of private universities is
that a good chunk of the country's wealth is managed by enlightened
investors.10. America Has Dynamic Typing for Careers.Compared to other industrialized countries the US is disorganized
about routing people into careers.  For example, in America people
often don't decide to go to medical school till they've finished
college.  In Europe they generally decide in high school.The European approach reflects the old idea that each person has a
single, definite occupation-- which is not far from the idea that
each person has a natural ""station"" in life.  If this were true,
the most efficient plan would be to discover each person's station
as early as possible, so they could receive the training appropriate
to it.In the US things are more haphazard.  But that turns out to be an
advantage as an economy gets more liquid, just as dynamic typing
turns out to work better than static for ill-defined problems.  This
is particularly true with startups.  ""Startup founder"" is not the
sort of career a high school student would choose.  If you ask at
that age, people will choose conservatively.  They'll choose
well-understood occupations like engineer, or doctor, or lawyer.Startups are the kind of thing people don't plan, so you're more
likely to get them in a society where it's ok to make career decisions
on the fly.For example, in theory the purpose of a PhD program is to train you
to do research.  But fortunately in the US this is another rule
that isn't very strictly enforced.  In the US most people in CS PhD
programs are there simply because they wanted to learn more.  They
haven't decided what they'll do afterward.  So American grad schools
spawn a lot of startups, because students don't feel they're failing
if they don't go into research.Those worried about America's ""competitiveness"" often suggest
spending more on public schools.  But perhaps America's lousy public
schools have a hidden advantage.  Because they're so bad, the kids
adopt an attitude of waiting for college.  I did; I knew I was
learning so little that I wasn't even learning what the choices
were, let alone which to choose.  This is demoralizing, but it does
at least make you keep an open mind.Certainly if I had to choose between bad high schools and good
universities, like the US, and good high schools and bad universities,
like most other industrialized countries, I'd take the US system.
Better to make everyone feel like a late bloomer than a failed child
prodigy.AttitudesThere's one item conspicuously missing from this list: American
attitudes.  Americans are said to be more entrepreneurial, and less
afraid of risk.  But America has no monopoly on this.  Indians and
Chinese seem plenty entrepreneurial, perhaps more than Americans.Some say Europeans are less energetic, but I don't believe it.  I
think the problem with Europe is not that they lack balls, but that
they lack examples.Even in the US, the most successful startup founders are often
technical people who are quite timid, initially, about the idea of
starting their own company. Few are the sort of backslapping
extroverts one thinks of as typically American.  They can usually
only summon up the activation energy to start a startup when they
meet people who've done it and realize they could too.I think what holds back European hackers is simply that they don't
meet so many people who've done it. You see that variation even
within the US.  Stanford students are more entrepreneurial than
Yale students, but not because of some difference in their characters;
the Yale students just have fewer examples.I admit there seem to be different attitudes toward ambition in
Europe and the US.  In the US it's ok to be overtly ambitious, and
in most of Europe it's not.  But this can't be an intrinsically
European quality; previous generations of Europeans were as ambitious
as Americans.  What happened?  My hypothesis is that ambition was
discredited by the terrible things ambitious people did in the first
half of the twentieth century.  Now swagger is out. (Even now the
image of a very ambitious German presses a button or two, doesn't
it?)It would be surprising if European attitudes weren't affected by
the disasters of the twentieth century.  It takes a while to be
optimistic after events like that.  But ambition is human nature.
Gradually it will re-emerge.
[6]How To Do BetterI don't mean to suggest by this list that America is the perfect
place for startups.  It's the best place so far, but the sample
size is small, and ""so far"" is not very long.   On historical time 
scales, what we have now is just a
prototype.So let's look at Silicon Valley the way you'd look at a product
made by a competitor.  What weaknesses could you exploit?  How could
you make something users would like better?  The users in this case
are those critical few thousand people you'd like to move to your
silicon valley.To start with, Silicon Valley is too far from San Francisco.  Palo
Alto, the original ground zero, is about thirty miles away, and the
present center more like forty.  So people who come to work in
Silicon Valley face an unpleasant choice: either live in the boring
sprawl of the valley proper, or live in San Francisco and endure
an hour commute each way.The best thing would be if the silicon valley were not merely closer
to the interesting city, but interesting itself.  And there is a
lot of room for improvement here.  Palo Alto is not so bad, but
everything built since is the worst sort of strip development.  You
can measure how demoralizing it is by the number of people who will
sacrifice two hours a day commuting rather than live there.Another area in which you could easily surpass Silicon Valley is
public transportation.  There is a train running the length of it,
and by American standards it's not bad.  Which is to say that to
Japanese or Europeans it would seem like something out of the third
world.The kind of people you want to attract to your silicon valley like
to get around by train, bicycle, and on foot.  So if you want to
beat America, design a town that puts cars last.  It will be a while
before any American city can bring itself to do that.Capital GainsThere are also a couple things you could do to beat America at the
national level.  One would be to have lower capital gains taxes.
It doesn't seem critical to have the lowest income taxes,
because to take advantage of those, people have to move.
[7]
But
if capital gains rates vary, you move assets, not yourself, so
changes are reflected at market speeds.  The lower the rate, the
cheaper it is to buy stock in growing companies as opposed to real
estate, or bonds, or stocks bought for the dividends they pay.So if you want to encourage startups you should have a low rate on
capital gains.  Politicians are caught between a rock and a hard
place here, however: make the capital gains rate low and be accused
of creating ""tax breaks for the rich,"" or make it high and starve
growing companies of investment capital.   As Galbraith said,
politics is a matter of choosing between the unpalatable and the
disastrous.  A lot of governments experimented with the disastrous
in the twentieth century; now the trend seems to be toward the
merely unpalatable.Oddly enough, the leaders now are European countries like Belgium,
which has a capital gains tax rate of zero.ImmigrationThe other place you could beat the US would be with smarter immigration
policy.  There are huge gains to be made here.  Silicon valleys are
made of people, remember.Like a company whose software runs on Windows, those in the current
Silicon Valley are all too aware of the shortcomings of the INS,
but there's little they can do about it.  They're hostages of the
platform.America's immigration system has never been well run, and since
2001 there has been an additional admixture of paranoia.  What
fraction of the smart people who want to come to America can even
get in?  I doubt even half.  Which means if you made a competing
technology hub that let in all smart people, you'd immediately get
more than half the world's top talent, for free.US immigration policy is particularly ill-suited to startups, because
it reflects a model of work from the 1970s.  It assumes good technical
people have college degrees, and that work means working for a big
company.If you don't have a college degree you can't get an H1B visa, the
type usually issued to programmers.  But a test that excludes Steve
Jobs, Bill Gates, and Michael Dell can't be a good one.  Plus you
can't get a visa for working on your own company, only for working
as an employee of someone else's.  And if you want to apply for
citizenship you daren't work for a startup at all, because if your
sponsor goes out of business, you have to start over.American immigration policy keeps out most smart people, and channels
the rest into unproductive jobs.  It would be easy to do better.
Imagine if, instead, you treated immigration like recruiting-- if
you made a conscious effort to seek out the smartest people and get
them to come to your country.A country that got immigration right would have a huge advantage.
At this point you could become a mecca for smart people simply by
having an immigration system that let them in.A Good VectorIf you look at the kinds of things you have to do to create an
environment where startups condense, none are great sacrifices.
Great universities?  Livable towns?  Civil liberties?  Flexible
employment laws?  Immigration policies that let in smart people?
Tax laws that encourage growth?  It's not as if you have to risk
destroying your country to get a silicon valley; these are all good
things in their own right.And then of course there's the question, can you afford not to?  I
can imagine a future in which the default choice of ambitious young
people is to start their own company
rather than work for someone else's.  I'm not sure that will happen,
but it's where the trend points now.  And if that is the future,
places that don't have startups will be a whole step behind,
like those that missed the Industrial Revolution.Notes[1]
On the verge of the Industrial Revolution, England was already
the richest country in the world.  As far as such things can be
compared, per capita income in England in 1750 was higher than
India's in 1960.Deane, Phyllis, The First Industrial Revolution, Cambridge
University Press, 1965.[2]
 This has already happened once in China, during the Ming
Dynasty, when the country turned its back on industrialization at
the command of the court.  One of Europe's advantages was that it
had no government powerful enough to do that.[3]
Of course, Feynman and Diogenes were from adjacent traditions,
but Confucius, though more polite, was no more willing to be told
what to think.[4]
For similar reasons it might be a lost cause to try to establish
a silicon valley in Israel.  Instead of no Jews moving there, only
Jews would move there, and I don't think you could build a silicon
valley out of just Jews any more than you could out of just Japanese.(This is not a remark about the qualities of these groups, just their
sizes.  Japanese are only about 2% of the world population, and
Jews about .2%.)[5]
According to the World Bank, the initial capital requirement
for German companies is 47.6% of the per capita income.  Doh.World Bank, Doing Business in 2006, http://doingbusiness.org[6]
For most of the twentieth century, Europeans looked back on
the summer of 1914 as if they'd been living in a dream world.  It
seems more accurate (or at least, as accurate) to call the years
after 1914 a nightmare than to call those before a dream.  A lot
of the optimism Europeans consider distinctly American is simply
what they too were feeling in 1914.[7]
The point where things start to go wrong seems to be about
50%.  Above that people get serious about tax avoidance.  The reason
is that the payoff for avoiding tax grows hyperexponentially (x/1-x
for 0 < x < 1).  If your income tax rate is 10%, moving to Monaco
would only give you 11% more income, which wouldn't even cover the
extra cost.  If it's 90%, you'd get ten times as much income.  And
at 98%, as it was briefly in Britain in the 70s, moving to Monaco
would give you fifty times as much income.  It seems quite likely
that European governments of the 70s never drew this curve.Thanks to Trevor Blackwell, Matthias Felleisen, Jessica
Livingston, Robert Morris, Neil Rimer, Hugues Steinier, Brad 
Templeton, Fred Wilson, and Stephen Wolfram for reading
drafts of this, and to Ed Dumbill for inviting me to speak.French TranslationRussian TranslationJapanese TranslationArabic Translation","startup advice
",human,"human
","human
"
40,40,"February 2009A lot of cities look at Silicon Valley and ask ""How could we make
something like that happen here?""  The 
organic way to do it is to
establish a first-rate university in a place where rich people want
to live. That's how Silicon Valley happened.  But could you shortcut
the process by funding startups?Possibly. Let's consider what it would take.The first thing to understand is that encouraging startups is a
different problem from encouraging startups in a particular city.
The latter is much more expensive.People sometimes think they could improve the startup scene in their
town by starting something like Y 
Combinator there, but in fact it
will have near zero effect.  I know because Y Combinator itself had
near zero effect on Boston when we were based there half the year.
The people we funded came from all over the country (indeed, the
world) and afterward they went wherever they could get more
funding—which generally meant Silicon Valley.The seed funding business is not a regional business, because at
that stage startups are mobile. They're just a couple founders with
laptops. 
[1]If you want to encourage startups in a particular city, you have
to fund startups that won't leave.  There are two ways to do that:
have rules preventing them from leaving, or fund them at the point
in their life when they naturally take root.  The first approach
is a mistake, because it becomes a filter for selecting bad startups.
If your terms force startups to do things they don't want to, only
the desperate ones will take your money.Good startups will move to another city as a condition of funding.
What they won't do is agree not to move the next time they need
funding.  So the only way to get them to stay is to give them enough
that they never need to leave.___How much would that take?  If you want to keep startups from leaving
your town, you have to give them enough that they're not tempted
by an offer from Silicon Valley VCs that requires them to move.  A
startup would be able to refuse such an offer if they had grown to
the point where they were (a) rooted in your town and/or (b) so
successful that VCs would fund them even if they didn't move.How much would it cost to grow a startup to that point? A minimum
of several hundred thousand dollars.  Wufoo 
seem to have rooted
themselves in Tampa on $118k, but they're an extreme case.  On
average it would take at least half a million.So if it seems too good to be true to think you could grow a local
silicon valley by giving startups $15-20k each like Y Combinator,
that's because it is.  To make them stick around you'd have to give
them at least 20 times that much.However, even that is an interesting prospect.  Suppose to be on
the safe side it would cost a million dollars per startup. If you
could get startups to stick to your town for a million apiece, then
for a billion dollars you could bring in a thousand startups. That probably wouldn't push you past Silicon Valley itself, 
but it might get you second place.For the price of a football stadium, any town that was decent to
live in could make itself one of the biggest startup hubs in the
world.What's more, it wouldn't take very long.   You could probably do
it in five years.  During the term of one mayor. And it would get
easier over time, because the more startups you had in town, the
less it would take to get new ones to move there. By the time you
had a thousand startups in town, the VCs wouldn't be trying so hard
to get them to move to Silicon Valley; instead they'd be opening
local offices.  Then you'd really be in good shape.  You'd have
started a self-sustaining chain reaction like the one that drives
the Valley.___But now comes the hard part.  You have to pick the startups.  How
do you do that?  Picking startups is a rare and valuable skill, and
the handful of people who have it are not readily hireable.  And
this skill is so hard to measure that if a government did try to
hire people with it, they'd almost certainly get the wrong ones.For example, a city could give money to a VC fund to establish a
local branch, and let them make the choices.  But only a bad VC
fund would take that deal.  They wouldn't seem bad to the city
officials.  They'd seem very impressive.  But they'd be bad at
picking startups.  That's the characteristic failure mode of VCs.
All VCs look impressive to limited partners.  The difference between
the good ones and the bad ones only becomes visible in the other
half of their jobs: choosing and advising startups.
[2]What you really want is a pool of local angel investors—people
investing money they made from their own startups.  But unfortunately
you run into a chicken and egg problem here.  If your city isn't
already a startup hub, there won't be people there who got rich
from startups. And there is no way I can think of that a city could
attract angels from outside. By definition they're rich.  There's
no incentive that would make them move.
[3]However, a city could select startups by piggybacking on the expertise
of investors who weren't local.  It would be pretty straightforward
to make a list of the most eminent Silicon Valley angels and from
that to generate a list of all the startups they'd invested in.  If
a city offered these companies a million dollars each to move, a
lot of the earlier stage ones would probably take it.Preposterous as this plan sounds, it's probably the most efficient
way a city could select good startups.It would hurt the startups somewhat to be separated from their
original investors. On the other hand, the extra million dollars
would give them a lot more runway.___Would the transplanted startups survive?  Quite possibly. The only
way to find out would be to try it.  It would be a pretty cheap
experiment, as civil expenditures go.  Pick 30 startups that eminent
angels have recently invested in, give them each a million dollars
if they'll relocate to your city, and see what happens after a year.
If they seem to be thriving, you can try importing startups on a
larger scale.Don't be too legalistic about the conditions under which they're
allowed to leave.  Just have a gentlemen's agreement.Don't try to do it on the cheap and pick only 10 for the initial
experiment.  If you do this on too small a scale you'll just guarantee
failure. Startups need to be around other startups.  30 would be
enough to feel like a community.Don't try to make them all work in some renovated warehouse you've
made into an ""incubator.""  Real startups prefer to work in their
own spaces.In fact, don't impose any restrictions on the startups at all.
Startup founders are mostly hackers, 
and hackers are much more
constrained by gentlemen's agreements than regulations.  If they
shake your hand on a promise, they'll keep it.  But show them a
lock and their first thought is how to pick it.Interestingly, the 30-startup experiment could be done by any
sufficiently rich private citizen.  And what pressure it would 
put on the city if it worked.
[4]___Should the city take stock in return for the money?
In principle they're entitled to, but how would they choose valuations
for the startups?  You couldn't just give them all the same valuation:
that would be too low for some (who'd turn you down) and too high
for others (because it might make their next round a ""down round"").
And since we're assuming we're doing this without being able to
pick startups, we also have to assume we can't value them, since
that's practically the same thing.Another reason not to take stock in the startups is that startups
are often involved in disreputable things.  So are established
companies, but they don't get blamed for it.  If someone gets
murdered by someone they met on Facebook, the press will treat the
story as if it were about Facebook.  If someone gets murdered by
someone they met at a supermarket, the press will just treat it as
a story about a murder.  So understand that if you invest in startups,
they might build things that get used for pornography, or file-sharing,
or the expression of unfashionable opinions.  You should probably
sponsor this project jointly with your political opponents, so they
can't use whatever the startups do as a club to beat you with.It would be too much of a political liability just to give
the startups the money, though.  So the best plan would be to 
make it convertible debt, but which didn't convert except in
a really big round, like $20 million.___How well this scheme worked would depend on the 
city.  There are
some towns, like Portland, that would be easy to turn into startup
hubs, and others, like Detroit, where it would really be an uphill
battle.  So be honest with yourself about the sort of town you have
before you try this.It will be easier in proportion to how much your town resembles San
Francisco.  Do you have good weather?  Do people live downtown, or
have they abandoned the center for the suburbs?  Would the city be
described as ""hip"" and ""tolerant,"" or as reflecting ""traditional
values?""  Are there good universities nearby?  Are there walkable
neighborhoods?  Would nerds feel at home?  If you answered yes to
all these questions, you might be able not only to pull off this
scheme, but to do it for less than a million per startup.I realize the chance of any city having
the political will to carry out this plan is microscopically
small.  I just wanted to explore what it would take if one did.
How hard would it be to jumpstart a silicon valley?  It's
fascinating to think this prize might be within
the reach of so many cities.  So even though they'll all still
spend the money on the stadium, at least now someone can ask them:
why did you choose to do that instead of becoming a serious
rival to Silicon Valley?
Notes[1]
What people who start these supposedly local seed firms always
find is that (a) their applicants come from all over, not just the
local area, and (b) the local startups also apply to the other seed
firms.  So what ends up happening is that the applicant pool gets
partitioned by quality rather than geography.[2]
Interestingly, the bad VCs fail by choosing startups run by
people like them—people who are good presenters, but have no
real substance.   It's a case of the fake leading the fake.  And
since everyone involved is so plausible, the LPs who invest in these
funds have no idea what's happening till they measure their returns.[3]
Not even being a tax haven, I suspect.  That makes some rich
people move, but not the type who would make good angel investors
in startups.[4]
Thanks to Michael Keenan for pointing this out.Thanks to Trevor Blackwell, Jessica Livingston, Robert
Morris, and Fred Wilson for reading drafts of this.","startup advice
",human,"human
","human
"
41,41,"

Want to start a startup?  Get funded by
Y Combinator.




March 2007(This essay is derived from talks at the 2007 
Startup School and the Berkeley CSUA.)We've now been doing Y Combinator long enough to have some data
about success rates.  Our first batch, in the summer of 2005, had
eight startups in it.  Of those eight, it now looks as if at least
four succeeded.  Three have been acquired: 
Reddit was a merger of
two, Reddit and Infogami, and a third was acquired that we can't
talk about yet.  Another from that batch was 
Loopt, which is doing
so well they could probably be acquired in about ten minutes if
they wanted to.So about half the founders from that first summer, less than two
years ago, are now rich, at least by their standards.  (One thing
you learn when you get rich is that there are many degrees of it.)I'm not ready to predict our success rate will stay as high as 50%.
That first batch could have been an anomaly.  But we should be able
to do better than the oft-quoted (and probably made
up) standard figure of 10%.  I'd feel safe aiming at 25%.Even the founders who fail don't seem to have such a bad time.  Of
those first eight startups, three are now probably dead.  In two
cases the founders just went on to do other things at the end of
the summer.   I don't think they were traumatized by the experience.
The closest to a traumatic failure was Kiko, whose founders kept
working on their startup for a whole year before being squashed by
Google Calendar.  But they ended up happy.  They sold their software
on eBay for a quarter of a million dollars.  After they paid back
their angel investors, they had about a year's salary each.  
[1]
Then they immediately went on to start a new and much more exciting
startup, Justin.TV.So here is an even more striking statistic: 0% of that first batch
had a terrible experience.  They had ups and downs, like every
startup, but I don't think any would have traded it for a job in a
cubicle.  And that statistic is probably not an anomaly.  Whatever
our long-term success rate ends up being, I think the rate of people
who wish they'd gotten a regular job will stay close to 0%.The big mystery to me is: why don't more people start startups?  If
nearly everyone who does it prefers it to a regular job, and a
significant percentage get rich, why doesn't everyone want to do
this?  A lot of people think we get thousands of applications for
each funding cycle.  In fact we usually only get several hundred.
Why don't more people apply?  And while it must seem to anyone
watching this world that startups are popping up like crazy, the
number is small compared to the number of people with the necessary
skills.  The great majority of programmers still go straight from
college to cubicle, and stay there.It seems like people are not acting in their own interest.  What's
going on?   Well, I can answer that.  Because of Y Combinator's
position at the very start of the venture funding process, we're
probably the world's leading experts on the psychology of people
who aren't sure if they want to start a company.There's nothing wrong with being unsure.  If you're a hacker thinking
about starting a startup and hesitating before taking the leap,
you're part of a grand tradition.  Larry and Sergey seem to have
felt the same before they started Google, and so did Jerry and Filo
before they started Yahoo.  In fact, I'd guess the most successful
startups are the ones started by uncertain hackers rather than
gung-ho business guys.We have some evidence to support this.  Several of the most successful
startups we've funded told us later that they only decided to apply
at the last moment.  Some decided only hours before the deadline.The way to deal with uncertainty is to analyze it into components.
Most people who are reluctant to do something have about eight
different reasons mixed together in their heads, and don't know
themselves which are biggest.  Some will be justified and some
bogus, but unless you know the relative proportion of each, you
don't know whether your overall uncertainty is mostly justified or
mostly bogus.So I'm going to list all the components of people's reluctance to
start startups, and explain which are real.  Then would-be founders
can use this as a checklist to examine their own feelings.I admit my goal is to increase your self-confidence.  But there are
two things different here from the usual confidence-building exercise.
One is that I'm motivated to be honest.  Most people in the
confidence-building business have already achieved their goal when
you buy the book or pay to attend the seminar where they tell you
how great you are.  Whereas if I encourage people to start startups
who shouldn't, I make my own life worse.  If I encourage too many
people to apply to Y Combinator, it just means more work for me,
because I have to read all the applications.The other thing that's going to be different is my approach.  Instead
of being positive, I'm going to be negative.  Instead of telling
you ""come on, you can do it"" I'm going to consider all the reasons
you aren't doing it, and show why most (but not all) should be
ignored.  We'll start with the one everyone's born with.1. Too youngA lot of people think they're too young to start a startup.  Many
are right.  The median age worldwide is about 27, so probably a
third of the population can truthfully say they're too young.What's too young?  One of our goals with Y Combinator was to discover
the lower bound on the age of startup founders.  It always seemed
to us that investors were too conservative here—that they wanted
to fund professors, when really they should be funding grad students
or even undergrads.The main thing we've discovered from pushing the edge of this
envelope is not where the edge is, but how fuzzy it is.  The outer
limit may be as low as 16.  We don't look beyond 18 because people
younger than that can't legally enter into contracts.  But the most
successful founder we've funded so far, Sam Altman, was 19 at the
time.Sam Altman, however, is an outlying data point.  When he was 19,
he seemed like he had a 40 year old inside him.  There are other
19 year olds who are 12 inside.There's a reason we have a distinct word ""adult"" for people over a
certain age.  There is a threshold you cross.  It's conventionally
fixed at 21, but different people cross it at greatly varying ages.
You're old enough to start a startup if you've crossed this threshold,
whatever your age.How do you tell?  There are a couple tests adults use.  I realized
these tests existed after meeting Sam Altman, actually.  I noticed
that I felt like I was talking to someone much older.  Afterward I
wondered, what am I even measuring?  What made him seem older?One test adults use is whether you still have the kid flake reflex.
When you're a little kid and you're asked to do something hard, you
can cry and say ""I can't do it"" and the adults will probably let
you off.  As a kid there's a magic button you can press by saying
""I'm just a kid"" that will get you out of most difficult situations.
Whereas adults, by definition, are not allowed to flake.  They still
do, of course, but when they do they're ruthlessly pruned.The other way to tell an adult is by how they react to a challenge.
Someone who's not yet an adult will tend to respond to a challenge
from an adult in a way that acknowledges their dominance.  If an
adult says ""that's a stupid idea,"" a kid will either crawl away
with his tail between his legs, or rebel.  But rebelling presumes
inferiority as much as submission.  The adult response to
""that's a stupid idea,"" is simply to look the other person in the
eye and say ""Really?  Why do you think so?""There are a lot of adults who still react childishly to challenges,
of course.  What you don't often find are kids who react to challenges
like adults.  When you do, you've found an adult, whatever their
age.2. Too inexperiencedI once wrote that startup founders should be at least 23, and that
people should work for another company for a few years before
starting their own.  I no longer believe that, and what changed my
mind is the example of the startups we've funded.I still think 23 is a better age than 21.  But the best way to get
experience if you're 21 is to start a startup.  So, paradoxically,
if you're too inexperienced to start a startup, what you should do
is start one.  That's a way more efficient cure for inexperience
than a normal job.  In fact, getting a normal job may actually make
you less able to start a startup, by turning you into a tame animal
who thinks he needs an office to work in and a product manager to
tell him what software to write.What really convinced me of this was the Kikos.  They started a
startup right out of college.  Their inexperience caused them to
make a lot of mistakes.  But by the time we funded their second
startup, a year later, they had become extremely formidable.  They
were certainly not tame animals.  And there is no way they'd have
grown so much if they'd spent that year working at Microsoft, or
even Google.  They'd still have been diffident junior programmers.So now I'd advise people to go ahead and start startups right out
of college.  There's no better time to take risks than when you're
young.  Sure, you'll probably fail.  But even failure will get you
to the ultimate goal faster than getting a job.It worries me a bit to be saying this, because in effect we're
advising people to educate themselves by failing at our expense,
but it's the truth.3. Not determined enoughYou need a lot of determination to succeed as a startup founder.
It's probably the single best predictor of success.Some people may not be determined enough to make it.  It's
hard for me to say for sure, because I'm so determined that I can't
imagine what's going on in the heads of people who aren't.  But I
know they exist.Most hackers probably underestimate their determination.  I've seen
a lot become visibly more determined as they get used to running a 
startup.  I can think of
several we've funded who would have been delighted at first to be
bought for $2 million, but are now set on world domination.How can you tell if you're determined enough, when Larry and Sergey
themselves were unsure at first about starting a company?  I'm
guessing here, but I'd say the test is whether you're sufficiently
driven to work on your own projects.  Though they may have been
unsure whether they wanted to start a company, it doesn't seem as
if Larry and Sergey were meek little research assistants, obediently
doing their advisors' bidding.  They started projects of their own.
4. Not smart enoughYou may need to be moderately smart to succeed as a startup founder.
But if you're worried about this, you're probably mistaken.  If
you're smart enough to worry that you might not be smart enough to
start a startup, you probably are.And in any case, starting a startup just doesn't require that much
intelligence.  Some startups do.  You have to be good at math to
write Mathematica.  But most companies do more mundane stuff where
the decisive factor is effort, not brains.  Silicon Valley can warp
your perspective on this, because there's a cult of smartness here.
People who aren't smart at least try to act that way.  But if you
think it takes a lot of intelligence to get rich, try spending a
couple days in some of the fancier bits of New York or LA.If you don't think you're smart enough to start a startup doing
something technically difficult, just write enterprise software.
Enterprise software companies aren't technology companies, they're
sales companies, and sales depends mostly on effort.5. Know nothing about businessThis is another variable whose coefficient should be zero.  You
don't need to know anything about business to start a startup.  The
initial focus should be the product.  All you need to know in this
phase is how to build things people want.  If you succeed, you'll
have to think about how to make money from it.  But this is so easy
you can pick it up on the fly.I get a fair amount of flak for telling founders just to make
something great and not worry too much about making money.  And yet
all the empirical evidence points that way: pretty much 100% of
startups that make something popular manage to make money from it.
And acquirers tell me privately that revenue is not what they buy
startups for, but their strategic value.  Which means, because they
made something people want.  Acquirers know the rule holds for them
too: if users love you, you can always make money from that somehow,
and if they don't, the cleverest business model in the world won't
save you.So why do so many people argue with me?  I think one reason is that
they hate the idea that a bunch of twenty year olds could get rich
from building something cool that doesn't make any money.  They
just don't want that to be possible.  But how possible it is doesn't
depend on how much they want it to be.For a while it annoyed me to hear myself described as some kind of
irresponsible pied piper, leading impressionable young hackers down
the road to ruin.  But now I realize this kind of controversy is a
sign of a good idea.The most valuable truths are the ones most people don't believe.
They're like undervalued stocks.  If you start with them, you'll
have the whole field to yourself.  So when you find an idea you
know is good but most people disagree with, you should not
merely ignore their objections, but push aggressively in that
direction.  In this case, that means you should seek out ideas that
would be popular but seem hard to make money from.We'll bet a seed round you can't make something popular that we
can't figure out how to make money from.6. No cofounderNot having a cofounder is a real problem.  A startup is too much
for one person to bear.  And though we differ from other investors
on a lot of questions, we all agree on this.  All investors, without
exception, are more likely to fund you with a cofounder than without.We've funded two single founders, but in both cases we suggested
their first priority should be to find a cofounder.  Both did.  But
we'd have preferred them to have cofounders before they applied.
It's not super hard to get a cofounder for a project that's just
been funded, and we'd rather have cofounders committed enough to
sign up for something super hard.If you don't have a cofounder, what should you do?  Get one.  It's
more important than anything else.  If there's no one where you
live who wants to start a startup with you, move where there are
people who do.  If no one wants to work with you on your current
idea, switch to an idea people want to work on.If you're still in school, you're surrounded by potential cofounders.
A few years out it gets harder to find them.  Not only do you have
a smaller pool to draw from, but most already have jobs, and perhaps
even families to support.  So if you had friends in college you
used to scheme about startups with, stay in touch with them as well
as you can.  That may help keep the dream alive.It's possible you could meet a cofounder through something like a
user's group or a conference.  But I wouldn't be too optimistic.
You need to work with someone to know whether you want them as a
cofounder.  
[2]The real lesson to draw from this is not how to find a cofounder,
but that you should start startups when you're young and there are
lots of them around.7. No ideaIn a sense, it's not a problem if you don't have a good idea, because
most startups change their idea anyway.  In the average Y Combinator
startup, I'd guess 70% of the idea is new at the end of the
first three months.  Sometimes it's 100%.In fact, we're so sure the founders are more important than the
initial idea that we're going to try something new this funding
cycle. We're going to let people apply with no idea at all.  If you
want, you can answer the question on the application form that asks
what you're going to do with ""We have no idea.""  If you seem really
good we'll accept you anyway.  We're confident we can sit down with
you and cook up some promising project.Really this just codifies what we do already.  We put little weight
on the idea.  We ask mainly out of politeness.  The kind of question
on the application form that we really care about is the one where
we ask what cool things you've made.  If what you've made is version
one of a promising startup, so much the better, but the main thing
we care about is whether you're good at making things.  Being lead
developer of a popular open source project counts almost as much.That solves the problem if you get funded by Y Combinator.  What
about in the general case?  Because in another sense, it is a problem
if you don't have an idea.  If you start a startup with no idea,
what do you do next?So here's the brief recipe for getting startup ideas.  Find something
that's missing in your own life, and supply that need—no matter
how specific to you it seems.  Steve Wozniak built himself a computer;
who knew so many other people would want them?  A need that's narrow
but genuine is a better starting point than one that's broad but
hypothetical.  So even if the problem is simply that you don't have
a date on Saturday night, if you can think of a way to fix that by
writing software, you're onto something, because a lot of other
people have the same problem.8. No room for more startupsA lot of people look at the ever-increasing number of startups and
think ""this can't continue.""  Implicit in their thinking is a
fallacy: that there is some limit on the number of startups there
could be.  But this is false.  No one claims there's any limit on
the number of people who can work for salary at 1000-person companies.
Why should there be any limit on the number who can work for equity
at 5-person companies? 
[3]Nearly everyone who works is satisfying some kind of need.  Breaking
up companies into smaller units doesn't make those needs go away.
Existing needs would probably get satisfied more efficiently by a
network of startups than by a few giant, hierarchical organizations,
but I don't think that would mean less opportunity, because satisfying
current needs would lead to more.  Certainly this tends to be the
case in individuals.  Nor is there anything wrong with that.  We
take for granted things that medieval kings would have considered
effeminate luxuries, like whole buildings heated to spring temperatures
year round.  And if things go well, our descendants will take for
granted things we would consider shockingly luxurious.  There is
no absolute standard for material wealth.  Health care is a component
of it, and that alone is a black hole.  For the foreseeable future,
people will want ever more material wealth, so there is no limit
to the amount of work available for companies, and for startups in
particular.Usually the limited-room fallacy is not expressed directly.  Usually
it's implicit in statements like ""there are only so many startups
Google, Microsoft, and Yahoo can buy.""  Maybe, though the list of
acquirers is a lot longer than that.  And whatever you think of
other acquirers, Google is not stupid.  The reason big companies
buy startups is that they've created something valuable.  And why
should there be any limit to the number of valuable startups companies
can acquire, any more than there is a limit to the amount of wealth
individual people want?  Maybe there would be practical limits on
the number of startups any one acquirer could assimilate, but if
there is value to be had, in the form of upside that founders are
willing to forgo in return for an immediate payment, acquirers will
evolve to consume it.  Markets are pretty smart that way.9. Family to supportThis one is real.  I wouldn't advise anyone with a family to start
a startup.  I'm not saying it's a bad idea, just that I don't want
to take responsibility for advising it.  I'm willing to take
responsibility for telling 22 year olds to start startups.  So what
if they fail?  They'll learn a lot, and that job at Microsoft will
still be waiting for them if they need it.  But I'm not prepared
to cross moms.What you can do, if you have a family and want to start a startup,
is start a consulting business you can then gradually turn into a
product business.  Empirically the chances of pulling that off seem
very small. You're never going to produce Google this way.  But at
least you'll never be without an income.Another way to decrease the risk is to join an existing startup
instead of starting your own.  Being one of the first employees of
a startup is a lot like being a founder, in both the good ways and
the bad.  You'll be roughly 1/n^2 founder, where n is your employee
number.As with the question of cofounders, the real lesson here is to start
startups when you're young.10. Independently wealthyThis is my excuse for not starting a startup.  Startups are stressful.
Why do it if you don't need the money?  For every ""serial entrepreneur,""
there are probably twenty sane ones who think ""Start another
company?  Are you crazy?""I've come close to starting new startups a couple times, but I
always pull back because I don't want four years of my life to be
consumed by random schleps.  I know this business well enough to
know you can't do it half-heartedly.  What makes a good startup
founder so dangerous is his willingness to endure infinite schleps.There is a bit of a problem with retirement, though.  Like a lot
of people, I like to work.  And one of the many weird little problems
you discover when you get rich is that a lot of the interesting
people you'd like to work with are not rich.  They need to work at
something that pays the bills.  Which means if you want to have
them as colleagues, you have to work at something that pays the
bills too, even though you don't need to.  I think this is what
drives a lot of serial entrepreneurs, actually.That's why I love working on Y Combinator so much.  It's an excuse
to work on something interesting with people I like.11.  Not ready for commitmentThis was my reason for not starting a startup for most of my twenties.
Like a lot of people that age, I valued freedom most of all.  I was
reluctant to do anything that required a commitment of more than a
few months.  Nor would I have wanted to do anything that completely
took over my life the way a startup does.  And that's fine.  If you
want to spend your time travelling around, or playing in a band,
or whatever, that's a perfectly legitimate reason not to start a
company.If you start a startup that succeeds, it's going to consume at least
three or four years.  (If it fails, you'll be done a lot quicker.)
So you shouldn't do it if you're not ready for commitments on that
scale.  Be aware, though, that if you get a regular job, you'll
probably end up working there for as long as a startup would take,
and you'll find you have much less spare time than you might expect.
So if you're ready to clip on that ID badge and go to that orientation
session, you may also be ready to start that startup.12.  Need for structureI'm told there are people who need structure in their lives.  This
seems to be a nice way of saying they need someone to tell them
what to do.  I believe such people exist.  There's plenty of empirical
evidence: armies, religious cults, and so on.  They may even be the
majority.If you're one of these people, you probably shouldn't start a
startup.  In fact, you probably shouldn't even go to work for one.
In a good startup, you don't get told what to do very much.  There
may be one person whose job title is CEO, but till the company has
about twelve people no one should be telling anyone what to do.
That's too inefficient.  Each person should just do what they need
to without anyone telling them.If that sounds like a recipe for chaos, think about a soccer team.
Eleven people manage to work together in quite complicated ways,
and yet only in occasional emergencies does anyone tell anyone else
what to do.  A reporter once asked David Beckham if there were any
language problems at Real Madrid, since the players were from about
eight different countries.  He said it was never an issue, because
everyone was so good they never had to talk.  They all just did the
right thing.How do you tell if you're independent-minded enough to start a
startup?  If you'd bristle at the suggestion that you aren't, then
you probably are.13. Fear of uncertaintyPerhaps some people are deterred from starting startups because
they don't like the uncertainty.  If you go to work for Microsoft,
you can predict fairly accurately what the next few years will be
like—all too accurately, in fact.  If you start a startup, anything
might happen.Well, if you're troubled by uncertainty, I can solve that problem
for you: if you start a startup, it will probably fail.  Seriously, 
though, this is not a bad way to think
about the whole experience.  Hope for the best, but expect the
worst.  In the worst case, it will at least be interesting.  In the
best case you might get rich.No one will blame you if the startup tanks, so long as you made a
serious effort.  There may once have been a time when employers
would regard that as a mark against you, but they wouldn't now.  I
asked managers at big companies, and they all said they'd prefer
to hire someone who'd tried to start a startup and failed over
someone who'd spent the same time working at a big company.Nor will investors hold it against you, as long as you didn't fail
out of laziness or incurable stupidity.   I'm told there's a lot
of stigma attached to failing in other places—in Europe, for
example.  Not here.  In America, companies, like practically
everything else, are disposable.14. Don't realize what you're avoidingOne reason people who've been out in the world for a year or two
make better founders than people straight from college is that they
know what they're avoiding.  If their startup fails, they'll have
to get a job, and they know how much jobs suck.If you've had summer jobs in college, you may think you know what
jobs are like, but you probably don't.  Summer jobs at technology
companies are not real jobs.  If you get a summer job as a waiter,
that's a real job.  Then you have to carry your weight.  But software
companies don't hire students for the summer as a source of cheap
labor.  They do it in the hope of recruiting them when they graduate.
So while they're happy if you produce, they don't expect you to.That will change if you get a real job after you graduate.  Then
you'll have to earn your keep.  And since most of what big companies
do is boring, you're going to have to work on boring stuff.  Easy,
compared to college, but boring.  At first it may seem cool to get
paid for doing easy stuff, after paying to do hard stuff in college.
But that wears off after a few months.  Eventually it gets demoralizing
to work on dumb stuff, even if it's easy and you get paid a lot.And that's not the worst of it.  The thing that really sucks about
having a regular job is the expectation that you're supposed to be
there at certain times.  Even Google is afflicted with this,
apparently.  And what this means, as everyone who's had a regular
job can tell you, is that there are going to be times when you have
absolutely no desire to work on anything, and you're going to have
to go to work anyway and sit in front of your screen and pretend
to.  To someone who likes work, as most good hackers do, this is
torture.In a startup, you skip all that.  There's no concept of office hours
in most startups.  Work and life just get mixed together.  But the
good thing about that is that no one minds if you have a life at
work.  In a startup you can do whatever you want most of the time.
If you're a founder, what you want to do most of the time is work.
But you never have to pretend to.If you took a nap in your office in a big company, it would seem
unprofessional.  But if you're starting a startup and you fall
asleep in the middle of the day, your cofounders will just assume
you were tired.15. Parents want you to be a doctorA significant number of would-be startup founders are probably
dissuaded from doing it by their parents.  I'm not going to say you
shouldn't listen to them.  Families are entitled to their own
traditions, and who am I to argue with them?  But I will give you
a couple reasons why a safe career might not be what your parents
really want for you.One is that parents tend to be more conservative for their kids
than they would be for themselves.  This is actually a rational
response to their situation.  Parents end up sharing more of their
kids' ill fortune than good fortune.  Most parents don't mind this;
it's part of the job; but it does tend to make them excessively
conservative.  And erring on the side of conservatism is still
erring.  In almost everything, reward is proportionate to risk.  So
by protecting their kids from risk, parents are, without realizing
it, also protecting them from rewards.  If they saw that, they'd
want you to take more risks.The other reason parents may be mistaken is that, like generals,
they're always fighting the last war.  If they want you to be a
doctor, odds are it's not just because they want you to help the
sick, but also because it's a prestigious and lucrative career.
[4]
But not so lucrative or prestigious as it was when their
opinions were formed.  When I was a kid in the seventies, a doctor
was the thing to be.  There was a sort of golden triangle involving
doctors, Mercedes 450SLs, and tennis.  All three vertices now seem
pretty dated.The parents who want you to be a doctor may simply not realize how
much things have changed.  Would they be that unhappy if you were
Steve Jobs instead?  So I think the way to deal with your parents'
opinions about what you should do is to treat them like feature
requests.  Even if your only goal is to please them, the way to do
that is not simply to give them what they ask for.  Instead think
about why they're asking for something, and see if there's a better
way to give them what they need.16.  A job is the defaultThis leads us to the last and probably most powerful reason people
get regular jobs: it's the default thing to do.  Defaults are
enormously powerful, precisely because they operate without any
conscious choice.To almost everyone except criminals, it seems an axiom that if you
need money, you should get a job.  Actually this tradition is not
much more than a hundred years old.  Before that, the default way
to make a living was by farming.  It's a bad plan to treat something
only a hundred years old as an axiom.  By historical standards,
that's something that's changing pretty rapidly.We may be seeing another such change right now.  I've read a lot
of economic history, and I understand the startup world pretty well,
and it now seems to me fairly likely that we're seeing the beginning
of a change like the one from farming to manufacturing.And you know what?  If you'd been around when that change began
(around 1000 in Europe) it would have seemed to nearly everyone
that running off to the city to make your fortune was a crazy thing
to do.  Though serfs were in principle forbidden to leave their
manors, it can't have been that hard to run away to a city.  There
were no guards patrolling the perimeter of the village.  What
prevented most serfs from leaving was that it seemed insanely risky.
Leave one's plot of land?  Leave the people you'd spent your whole
life with, to live in a giant city of three or four thousand complete
strangers?  How would you live?  How would you get food, if you
didn't grow it?Frightening as it seemed to them, it's now the default with us to
live by our wits.  So if it seems risky to you to start a startup,
think how risky it once seemed to your ancestors to live as we do
now.  Oddly enough, the people who know this best are the very ones
trying to get you to stick to the old model.  How can Larry and
Sergey say you should come work as their employee, when they didn't
get jobs themselves?Now we look back on medieval peasants and wonder how they stood it.
How grim it must have been to till the same fields your whole life
with no hope of anything better, under the thumb of lords and priests
you had to give all your surplus to and acknowledge as your masters.
I wouldn't be surprised if one day people look back on what we
consider a normal job in the same way.  How grim it would be to
commute every day to a cubicle in some soulless office complex, and
be told what to do by someone you had to acknowledge as a boss—someone 
who could call you into their office and say ""take a seat,""
and you'd sit!  Imagine having to ask permission to release
software to users.  Imagine being sad on Sunday afternoons because
the weekend was almost over, and tomorrow you'd have to get up and
go to work.  How did they stand it?It's exciting to think we may be on the cusp of another shift like
the one from farming to manufacturing.  That's why I care about
startups.  Startups aren't interesting just because they're a way
to make a lot of money.  I couldn't care less about other ways to
do that, like speculating in securities.  At most those are interesting
the way puzzles are.  There's more going on with startups.  They
may represent one of those rare, historic shifts in the way 
wealth is created.That's ultimately what drives us to work on Y Combinator.  We want
to make money, if only so we don't have to stop doing it, but that's
not the main goal.  There have only been a handful of these great
economic shifts in human history.  It would be an amazing hack to
make one happen faster.
Notes[1]
The only people who lost were us.  The angels had convertible
debt, so they had first claim on the proceeds of the auction. Y
Combinator only got 38 cents on the dollar.[2]
The best kind of organization for that might be an open source
project, but those don't involve a lot of face to face meetings.
Maybe it would be worth starting one that did.[3]
There need to be some number of big companies to acquire the
startups, so the number of big companies couldn't decrease to zero.[4]
Thought experiment: If doctors did the same work, but as
impoverished outcasts, which parents would still want their kids
to be doctors?Thanks to Trevor Blackwell, Jessica Livingston, and Robert
Morris for reading drafts of this, to the founders of Zenter
for letting me use their web-based PowerPoint killer even though 
it isn't launched yet, and to Ming-Hay Luk
of the Berkeley CSUA for inviting me to speak.
Comment on this essay.Russian TranslationJapanese TranslationKorean Translation","startup advice
",human,"human
","human
"
42,42,"

Want to start a startup?  Get funded by
Y Combinator.




January 2012A year ago I noticed a pattern in the least successful startups
we'd funded: they all seemed hard to talk to.  It felt as if there
was some kind of wall between us.  I could never quite tell if they
understood what I was saying.This caught my attention because earlier we'd noticed a pattern
among the most successful startups, and it seemed to hinge on a
different quality.  We found the startups that did best were the
ones with the sort of founders about whom we'd say ""they can take
care of themselves.""  The startups that do best are fire-and-forget
in the sense that all you have to do is give them a lead, and they'll
close it, whatever type of lead it is.  When they're raising money,
for example, you can do the initial intros knowing that if you
wanted to you could stop thinking about it at that point.  You won't
have to babysit the round to make sure it happens.  That type of
founder is going to come back with the money; the only question is
how much on what terms.It seemed odd that the outliers at the two ends of the spectrum
could be detected by what appeared to be unrelated tests.  You'd
expect that if the founders at one end were distinguished by the
presence of quality x, at the other end they'd be distinguished by
lack of x.  Was there some kind of inverse relation between
resourcefulness and being hard to talk to?It turns out there is, and the key to the mystery is the old adage
""a word to the wise is sufficient.""   Because this phrase is not
only overused, but overused in an indirect way (by prepending the
subject to some advice), most people who've heard it don't know
what it means.  What it means is that if someone is wise, all you
have to do is say one word to them, and they'll understand immediately.
You don't have to explain in detail; they'll chase down all the
implications.In much the same way that all you have to do is give the right sort
of founder a one line intro to a VC, and he'll chase down the money.
That's the connection.  Understanding all the implications — even the
inconvenient implications — of what someone tells you is a subset of
resourcefulness.  It's conversational resourcefulness.Like real world resourcefulness, conversational resourcefulness
often means doing things you don't want to.  Chasing down all the
implications of what's said to you can sometimes lead to uncomfortable
conclusions.  The best word to describe the failure to do so is
probably ""denial,"" though that seems a bit too narrow.  A better
way to describe the situation would be to say that the unsuccessful
founders had the sort of conservatism that comes from weakness.
They traversed idea space as gingerly as a very old person
traverses the physical world.
[1]The unsuccessful founders weren't stupid.  Intellectually they
were as capable as
the successful founders of following all the implications of what
one said to them.  They just weren't eager to.So being hard to talk to was not what was killing the
unsuccessful startups.  It
was a sign of an underlying lack of resourcefulness.  That's what
was killing them.  As well as
failing to chase down the implications of what was said to them,
the unsuccessful founders would also fail to chase down funding,
and users, and sources of new ideas.  But the most immediate evidence
I had that something was amiss was that I couldn't talk to them.Notes[1]
A YC partner wrote:My feeling with the bad groups is that coming into office hours,
they've already decided what they're going to do and everything I
say is being put through an internal process in their heads, which
either desperately tries to munge what I've said into something
that conforms with their decision or just outright dismisses it and
creates a rationalization for doing so. They may not even be conscious
of this process but that's what I think is happening when you say
something to bad groups and they have that glazed over look. I don't
think it's confusion or lack of understanding per se, it's this
internal process at work.With the good groups, you can tell that everything you say is being
looked at with fresh eyes and even if it's dismissed, it's because
of some logical reason e.g. ""we already tried that"" or ""from speaking
to our users that isn't what they'd like,"" etc. Those groups never
have that glazed over look.Thanks to Sam Altman, Patrick Collison, Aaron Iba, Jessica Livingston,
Robert Morris, Harj Taggar, and Garry Tan for reading drafts of
this.","startup advice
",human,"human
","human
"
43,43,"November 2005In the next few years, venture capital funds will find themselves
squeezed from four directions.  They're already stuck with a seller's
market, because of the huge amounts they raised at the end of the
Bubble and still haven't invested.  This by itself is not the end
of the world.  In fact, it's just a more extreme version of the
norm
in the VC business: too much money chasing too few deals.Unfortunately, those few deals now want less and less money, because
it's getting so cheap to start a startup.  The four causes: open
source, which makes software free; Moore's law, which makes hardware
geometrically closer to free; the Web, which makes promotion free
if you're good; and better languages, which make development a lot
cheaper.When we started our startup in 1995, the first three were our biggest
expenses.  We had to pay $5000 for the Netscape Commerce Server,
the only software that then supported secure http connections.  We
paid $3000 for a server with a 90 MHz processor and 32 meg of
memory.  And we paid a PR firm about $30,000 to promote our launch.Now you could get all three for nothing.  You can get the software
for free; people throw away computers more powerful than our first
server; and if you make something good you can generate ten times
as much traffic by word of mouth online than our first PR firm got
through the print media.And of course another big change for the average startup is that
programming languages have improved-- or rather, the median language has.  At most startups ten years
ago, software development meant ten programmers writing code in
C++.  Now the same work might be done by one or two using Python
or Ruby.During the Bubble, a lot of people predicted that startups would
outsource their development to India.  I think a better model for
the future is David Heinemeier Hansson, who outsourced his development
to a more powerful language instead.  A lot of well-known applications
are now, like BaseCamp, written by just one programmer.  And one
guy is more than 10x cheaper than ten, because (a) he won't waste
any time in meetings, and (b) since he's probably a founder, he can
pay himself nothing.Because starting a startup is so cheap, venture capitalists now
often want to give startups more money than the startups want to
take.  VCs like to invest several million at a time.  But as one
VC told me after a startup he funded would only take about half a
million, ""I don't know what we're going to do.  Maybe we'll just
have to give some of it back."" Meaning give some of the fund back
to the institutional investors who supplied it, because it wasn't
going to be possible to invest it all.Into this already bad situation comes the third problem: Sarbanes-Oxley.
Sarbanes-Oxley is a law, passed after the Bubble, that drastically
increases the regulatory burden on public companies. And in addition
to the cost of compliance, which is at least two million dollars a
year, the law introduces frightening legal exposure for corporate
officers.  An experienced CFO I know said flatly: ""I would not
want to be CFO of a public company now.""You might think that responsible corporate governance is an area
where you can't go too far.  But you can go too far in any law, and
this remark convinced me that Sarbanes-Oxley must have.  This CFO
is both the smartest and the most upstanding money guy I know.  If
Sarbanes-Oxley deters people like him from being CFOs of public  
companies, that's proof enough that it's broken.Largely because of Sarbanes-Oxley, few startups go public now.  For
all practical purposes, succeeding now equals getting bought.  Which
means VCs are now in the business of finding promising little 2-3
man startups and pumping them up into companies that cost $100
million to acquire.   They didn't mean to be in this business; it's
just what their business has evolved into.Hence the fourth problem: the acquirers have begun to realize they
can buy wholesale.  Why should they wait for VCs to make the startups
they want more expensive?  Most of what the VCs add, acquirers don't
want anyway.  The acquirers already have brand recognition and HR
departments.  What they really want is the software and the developers,
and that's what the startup is in the early phase: concentrated
software and developers.Google, typically, seems to have been the first to figure this out.
""Bring us your startups early,"" said Google's speaker at the Startup School.  They're quite
explicit about it: they like to acquire startups at just the point
where they would do a Series A round.  (The Series A round is the
first round of real VC funding; it usually happens in the first
year.) It is a brilliant strategy, and one that other big technology
companies will no doubt try to duplicate.  Unless they want to have 
still more of their lunch eaten by Google.Of course, Google has an advantage in buying startups: a lot of the
people there are rich, or expect to be when their options vest.
Ordinary employees find it very hard to recommend an acquisition;
it's just too annoying to see a bunch of twenty year olds get rich
when you're still working for salary.  Even if it's the right thing   
for your company to do.The Solution(s)Bad as things look now, there is a way for VCs to save themselves.
They need to do two things, one of which won't surprise them, and  
another that will seem an anathema.Let's start with the obvious one: lobby to get Sarbanes-Oxley  
loosened.  This law was created to prevent future Enrons, not to
destroy the IPO market.  Since the IPO market was practically dead
when it passed, few saw what bad effects it would have.  But now 
that technology has recovered from the last bust, we can see clearly
what a bottleneck Sarbanes-Oxley has become.Startups are fragile plants—seedlings, in fact.  These seedlings
are worth protecting, because they grow into the trees of the
economy.  Much of the economy's growth is their growth.  I think
most politicians realize that.  But they don't realize just how   
fragile startups are, and how easily they can become collateral
damage of laws meant to fix some other problem.Still more dangerously, when you destroy startups, they make very
little noise.  If you step on the toes of the coal industry, you'll
hear about it.  But if you inadvertantly squash the startup industry,
all that happens is that the founders of the next Google stay in 
grad school instead of starting a company.My second suggestion will seem shocking to VCs: let founders cash  
out partially in the Series A round.  At the moment, when VCs invest
in a startup, all the stock they get is newly issued and all the 
money goes to the company.  They could buy some stock directly from
the founders as well.Most VCs have an almost religious rule against doing this.  They
don't want founders to get a penny till the company is sold or goes
public.  VCs are obsessed with control, and they worry that they'll
have less leverage over the founders if the founders have any money.This is a dumb plan.  In fact, letting the founders sell a little stock
early would generally be better for the company, because it would
cause the founders' attitudes toward risk to be aligned with the
VCs'.  As things currently work, their attitudes toward risk tend
to be diametrically opposed: the founders, who have nothing, would
prefer a 100% chance of $1 million to a 20% chance of $10 million,
while the VCs can afford to be ""rational"" and prefer the latter.Whatever they say, the reason founders are selling their companies
early instead of doing Series A rounds is that they get paid up
front.  That first million is just worth so much more than the
subsequent ones.  If founders could sell a little stock early,
they'd be happy to take VC money and bet the rest on a bigger
outcome.So why not let the founders have that first million, or at least
half million?  The VCs would get same number of shares for the   
money.  So what if some of the money would go to the  
founders instead of the company?Some VCs will say this is
unthinkable—that they want all their money to be put to work
growing the company.  But the fact is, the huge size of current VC
investments is dictated by the structure
of VC funds, not the needs of startups.  Often as not these large  
investments go to work destroying the company rather than growing
it.The angel investors who funded our startup let the founders sell
some stock directly to them, and it was a good deal for everyone. 
The angels made a huge return on that investment, so they're happy.
And for us founders it blunted the terrifying all-or-nothingness
of a startup, which in its raw form is more a distraction than a
motivator.If VCs are frightened at the idea of letting founders partially
cash out, let me tell them something still more frightening: you
are now competing directly with Google.
Thanks to Trevor Blackwell, Sarah Harlin, Jessica
Livingston, and Robert Morris for reading drafts of this.Romanian TranslationHebrew TranslationJapanese Translation

If you liked this, you may also like
Hackers & Painters.

","startup advice
",human,"human
","human
"
44,44,"

Want to start a startup?  Get funded by
Y Combinator.




March 2009A couple days ago I finally got being a good startup founder down
to two words:  relentlessly resourceful.Till then the best I'd managed was to get the opposite quality down
to one: hapless.  Most dictionaries say hapless means unlucky.  But
the dictionaries are not doing a very good job.  A team that outplays
its opponents but loses because of a bad decision by the referee
could be called unlucky, but not hapless.  Hapless implies passivity.
To be hapless is to be battered by circumstances — to let the world
have its way with you, instead of having your way with the world.

[1]Unfortunately there's no antonym of hapless, which makes it difficult
to tell founders what to aim for.  ""Don't be hapless"" is not much
of a rallying cry.It's not hard to express the quality we're looking for in metaphors.
The best is probably a running back.  A good running back is not
merely determined, but flexible as well.  They want to get downfield,
but they adapt their plans on the fly.Unfortunately this is just a metaphor, and not a useful one to most
people outside the US.   ""Be like a running back"" is no better than
""Don't be hapless.""But finally I've figured out how to express this quality directly.
I was writing a talk for 
investors, and I had to explain what to
look for in founders.  What would someone who was the opposite of
hapless be like?  They'd be relentlessly resourceful.  Not merely
relentless.  That's not enough to make things go your way except
in a few mostly uninteresting domains.  In any interesting domain,
the difficulties will be novel.  Which means you can't simply plow
through them, because you don't know initially how hard they are;
you don't know whether you're about to plow through a block of foam
or granite.  So you have to be resourceful. You have to keep
trying new things.Be relentlessly resourceful.That sounds right, but is it simply a description
of how to be successful in general?  I don't think so.  This isn't
the recipe for success in writing or painting, for example.  In
that kind of work the recipe is more to be actively curious.
Resourceful implies the obstacles are external, which they generally
are in startups. But in writing and painting they're mostly internal;
the obstacle is your own obtuseness.
[2]There probably are other fields where ""relentlessly resourceful""
is the recipe for success.  But though other fields may share it,
I think this is the best short description we'll find of what makes
a good startup founder.  I doubt it could be made more precise.Now that we know what we're looking for, that leads to other
questions.  For example, can this quality be taught?  After four
years of trying to teach it to people, I'd say that yes, surprisingly
often it can.  Not to everyone, but to many people. 
[3]
Some
people are just constitutionally passive, but others have a latent
ability to be relentlessly resourceful that only needs to be brought
out.This is particularly true of young people who have till now always
been under the thumb of some kind of authority.  Being relentlessly
resourceful is definitely not the recipe for success in big companies,
or in most schools.  I don't even want to think what the recipe is
in big companies, but it is certainly longer and messier, involving
some combination of resourcefulness, obedience, and building
alliances.Identifying this quality also brings us closer to answering a
question people often wonder about: how many startups there could
be.  There is not, as some people seem to think, any economic upper
bound on this number.  There's no reason to believe there is any
limit on the amount of newly created wealth consumers can absorb,
any more than there is a limit on the number of theorems that can
be proven.  So probably the limiting factor on the number of startups
is the pool of potential founders.  Some people would make good
founders, and others wouldn't.  And now that we can say what makes
a good founder, we know how to put an upper bound on the size of
the pool.This test is also useful to individuals.  If you want to know whether
you're the right sort of person to start a startup, ask yourself
whether you're relentlessly resourceful.  And if you want to know
whether to recruit someone as a cofounder, ask if they are.You can even use it tactically.  If I were running a startup, this
would be the phrase I'd tape to the mirror.  ""Make something people
want"" is the destination, but ""Be relentlessly resourceful"" is how
you get there.
Notes[1]
I think the reason the dictionaries are wrong is that the
meaning of the word has shifted.  No one writing a dictionary from
scratch today would say that hapless meant unlucky.  But a couple
hundred years ago they might have.  People were more at the mercy
of circumstances in the past, and as a result a lot of the words
we use for good and bad outcomes have origins in words about luck.When I was living in Italy, I was once trying to tell someone
that I hadn't had much success in doing something, but I couldn't
think of the Italian word for success.  I spent some time trying
to describe the word I meant.  Finally she said ""Ah! Fortuna!""[2]
There are aspects of startups where the recipe is to be
actively curious.  There can be times when what you're doing is
almost pure discovery.  Unfortunately these times are a small
proportion of the whole.  On the other hand, they are in research
too.[3]
I'd almost say to most people, but I realize (a) I have no
idea what most people are like, and (b) I'm pathologically optimistic
about people's ability to change.Thanks to Trevor Blackwell and Jessica Livingston for reading drafts
of this.","startup advice
",human,"human
","human
"
45,45,"June 2013(This talk was written for an audience of investors.)Y Combinator has now funded 564 startups including the current
batch, which has 53.  The total valuation of the 287 that have
valuations (either by raising an equity round, getting acquired,
or dying) is about $11.7 billion, and the 511 prior to the current
batch have collectively raised about $1.7 billion.
[1]As usual those numbers are dominated by a few big winners.  The top
10 startups account for 8.6 of that 11.7 billion.  But there is a
peloton of younger startups behind them.  There are about 40 more
that have a shot at being really big.Things got a little out of hand last summer when we had 84 companies
in the batch, so we tightened up our filter to decrease the batch
size. 
[2]
Several journalists have tried to interpret that as
evidence for some macro story they were telling, but the reason had
nothing to do with any external trend.  The reason was that we
discovered we were using an n² algorithm, and we needed to buy
time to fix it.  Fortunately we've come up with several techniques
for sharding YC, and the problem now seems to be fixed.  With a new
more scaleable model and only 53 companies, the current batch feels
like a walk in the park.  I'd guess we can grow another 2 or 3x
before hitting the next bottleneck. 
[3]One consequence of funding such a large number of startups is that
we see trends early.  And since fundraising is one of the main
things we help startups with, we're in a good position to notice
trends in investing.I'm going to take a shot at describing where these trends are
leading.  Let's start with the most basic question: will the future
be better or worse than the past?  Will investors, in the aggregate,
make more money or less?I think more.  There are multiple forces at work, some of which
will decrease returns, and some of which will increase them.  I
can't predict for sure which forces will prevail, but I'll describe
them and you can decide for yourself.There are two big forces driving change in startup funding: it's
becoming cheaper to start a startup, and startups are becoming a
more normal thing to do.When I graduated from college in 1986, there were essentially two
options: get a job or go to grad school.  Now there's a third: start
your own company.
That's a big change.  In principle it was possible to start your
own company in 1986 too, but it didn't seem like a real possibility.
It seemed possible to start a consulting company, or a niche product
company, but it didn't seem possible to start a company that would
become big.
[4]That kind of change, from 2 paths to 3, is the sort of big social
shift that only happens once every few generations.  I think we're
still at the beginning of this one.  It's hard to predict how big
a deal it will be.  As big a deal as the Industrial Revolution?
Maybe.  Probably not.  But it will be a big enough deal that it
takes almost everyone by surprise, because those big social shifts
always do.One thing we can say for sure is that there will be a lot more
startups.  The monolithic, hierarchical companies of the mid 20th
century are being replaced by networks
of smaller companies.  This process is not just something happening
now in Silicon Valley.  It started decades ago, and it's happening
as far afield as the car industry.  It has a long way to run. 
[5]
The other big driver of change is that startups are becoming cheaper
to start.  And in fact the two forces are related: the decreasing
cost of starting a startup is one of the reasons startups are
becoming a more normal thing to do.The fact that startups need less money means founders will increasingly
have the upper hand over investors.  You still need just as much
of their energy and imagination, but they don't need as much of
your money.  Because founders have the upper hand, they'll retain
an increasingly large share of the stock in, and control of, their
companies.  Which means investors will get less stock and less
control.Does that mean investors will make less money?  Not necessarily,
because there will be more good startups.  The total amount of
desirable startup stock available to investors will probably increase,
because the number of desirable startups will probably grow faster
than the percentage they sell to investors shrinks.There's a rule of thumb in the VC business that there are about 15
companies a year that will be really successful.  Although a lot
of investors unconsciously treat this number as if it were some
sort of cosmological constant, I'm certain it isn't.   There are
probably limits on the rate at which technology can develop, but
that's not the limiting factor now.  If it were, each successful
startup would be founded the month it became possible, and that is
not the case. Right now the limiting factor on the number of big
hits is the number of sufficiently good founders starting companies,
and that number can and will increase.  There are still a lot of
people who'd make great founders who never end up starting a company.
You can see that from how randomly some of the most successful
startups got started.  So many of the biggest startups almost didn't
happen that there must be a lot of equally good startups that
actually didn't happen.There might be 10x or even 50x more good founders out there.  As
more of them go ahead and start startups, those 15 big hits a year
could easily become 50 or even 100.
[6]What about returns, though?  Are we heading for a world in which
returns will be pinched by increasingly high valuations?  I think
the top firms will actually make more money than they have in the
past.  High returns don't come from investing at low valuations.
They come from investing in the companies that do really well.  So
if there are more of those to be had each year, the best pickers
should have more hits.This means there should be more variability in the VC business.
The firms that can recognize and attract the best startups will do
even better, because there will be more of them to recognize and
attract.  Whereas the bad firms will get the leftovers, as they do
now, and yet pay a higher price for them.Nor do I think it will be a problem that founders keep control of
their companies for longer.  The empirical evidence on that is
already clear: investors make more money as founders' bitches than
their bosses.  Though somewhat humiliating, this is actually good
news for investors, because it takes less time to serve founders
than to micromanage them.What about angels?  I think there is a lot of opportunity there.
It used to suck to be an angel investor.  You couldn't get access
to the best deals, unless you got lucky like Andy Bechtolsheim, and
when you did invest in a startup, VCs might try to strip you of
your stock when they arrived later.  Now an angel can go to something
like Demo Day or AngelList and have access to the same deals VCs
do.  And the days when VCs could wash angels out of the cap table
are long gone.I think one of the biggest unexploited opportunities in startup
investing right now is angel-sized investments made quickly.  Few
investors understand the cost that raising money from them imposes
on startups.  When the company consists only of the founders,
everything grinds to a halt during fundraising, which can easily
take 6 weeks.  The current high cost of fundraising means there is
room for low-cost investors to undercut the rest.  And in this
context, low-cost means deciding quickly.  If there were a reputable
investor who invested $100k on good terms and promised to decide
yes or no within 24 hours, they'd get access to almost all the best
deals, because every good startup would approach them first.  It
would be up to them to pick, because every bad startup would approach
them first too, but at least they'd see everything.  Whereas if an
investor is notorious for taking a long time to make up their mind
or negotiating a lot about valuation, founders will save them for
last.  And in the case of the most promising startups, which tend
to have an easy time raising money, last can easily become never.Will the number of big hits grow linearly with the total number of
new startups?  Probably not, for two reasons.  One is that the
scariness of starting a startup in the old days was a pretty effective
filter.  Now that the cost of failing is becoming lower, we should
expect founders to do it more.  That's not a bad thing.  It's common
in technology for an innovation that decreases the cost of failure
to increase the number of failures and yet leave you net ahead.The other reason the number of big hits won't grow proportionately
to the number of startups is that there will start to be an increasing
number of idea clashes.  Although the finiteness of the number of
good ideas is not the reason there are only 15 big hits a year, the
number has to be finite, and the more startups there are, the more
we'll see multiple companies doing the same thing at the same time.
It will be interesting, in a bad way, if idea clashes become a lot
more common. 
[7]Mostly because of the increasing number of early failures, the startup
business of the future won't simply be the same shape, scaled up.
What used to be an obelisk will become a pyramid.  It will be a
little wider at the top, but a lot wider at the bottom.What does that mean for investors?  One thing it means is that there
will be more opportunities for investors at the earliest stage,
because that's where the volume of our imaginary solid is growing
fastest.  Imagine the obelisk of investors that corresponds to
the obelisk of startups.  As it widens out into a pyramid to match
the startup pyramid, all the contents are adhering to the top,
leaving a vacuum at the bottom.That opportunity for investors mostly means an opportunity for new
investors, because the degree of risk an existing investor or firm
is comfortable taking is one of the hardest things for them to
change.  Different types of investors are adapted to different
degrees of risk, but each has its specific degree of risk deeply
imprinted on it, not just in the procedures they follow but in the
personalities of the people who work there.I think the biggest danger for VCs, and also the biggest opportunity,
is at the series A stage.  Or rather, what used to be the series A
stage before series As turned into de facto series B rounds.Right now, VCs often knowingly invest too much money at the series
A stage.  They do it because they feel they need to get a big chunk
of each series A company to compensate for the opportunity cost of
the board seat it consumes.  Which means when there is a lot of
competition for a deal, the number that moves is the valuation (and
thus amount invested) rather than the percentage of the company
being sold.  Which means, especially in the case of more promising
startups, that series A investors often make companies take more
money than they want.Some VCs lie and claim the company really needs that much.  Others
are more candid, and admit their financial models require them to
own a certain percentage of each company.  But we all know the
amounts being raised in series A rounds are not determined by asking
what would be best for the companies.  They're determined by VCs
starting from the amount of the company they want to own, and the
market setting the valuation and thus the amount invested.Like a lot of bad things, this didn't happen intentionally.  The
VC business backed into it as their initial assumptions gradually
became obsolete.  The traditions and financial models of the VC
business were established when founders needed investors more.  In
those days it was natural for founders to sell VCs a big chunk of
their company in the series A round.  Now founders would prefer to
sell less, and VCs are digging in their heels because they're not
sure if they can make money buying less than 20% of each series A
company.The reason I describe this as a danger is that series A investors
are increasingly at odds with the startups they supposedly serve,
and that tends to come back to bite you eventually.  The reason I
describe it as an opportunity is that there is now a lot of potential
energy built up, as the market has moved away from VCs' traditional
business model.  Which means the first VC to break ranks and start
to do series A rounds for as much equity as founders want to sell
(and with no ""option pool"" that comes only from the founders' shares)
stands to reap huge benefits.What will happen to the VC business when that happens?  Hell if I
know.  But I bet that particular firm will end up ahead.  If one
top-tier VC firm started to do series A rounds that started from
the amount the company needed to raise and let the percentage
acquired vary with the market, instead of the other way around,
they'd instantly get almost all the best startups.  And that's where
the money is.You can't fight market forces forever.  Over the last decade we've
seen the percentage of the company sold in series A rounds creep
inexorably downward.  40% used to be common.  Now VCs are fighting
to hold the line at 20%.  But I am daily waiting for the line to
collapse.  It's going to happen.  You may as well anticipate it,
and look bold.Who knows, maybe VCs will make more money by doing the right thing.
It wouldn't be the first time that happened.  Venture capital is a
business where occasional big successes generate hundredfold returns.
How much confidence can you really have in financial models for
something like that anyway?  The
big successes only have to get a tiny bit less occasional to
compensate for a 2x decrease in the stock sold in series A rounds.If you want to find new opportunities for investing, look for things
founders complain about.  Founders are your customers, and the
things they complain about are unsatisfied demand.  I've given two
examples of things founders complain about most—investors who
take too long to make up their minds, and excessive dilution in
series A rounds—so those are good places to look now.  But
the more general recipe is: do something founders want.
Notes[1]
I realize revenue and not fundraising is the proper test of
success for a startup.  The reason we quote statistics about
fundraising is because those are the numbers we have.  We couldn't
talk meaningfully about revenues without including the numbers from
the most successful startups, and we don't have those.  We often
discuss revenue growth with the earlier stage startups, because
that's how we gauge their progress, but when companies reach a
certain size it gets presumptuous for a seed investor to do that.In any case, companies' market caps do eventually become a function
of revenues, and post-money valuations of funding rounds are at
least guesses by pros about where those market caps will end up.The reason only 287 have valuations is that the rest have mostly
raised money on convertible notes, and although convertible notes
often have valuation caps, a valuation cap is merely an upper bound
on a valuation.[2]
We didn't try to accept a particular number.  We have no way
of doing that even if we wanted to.  We just tried to be significantly
pickier.[3]
Though you never know with bottlenecks, I'm guessing the next
one will be coordinating efforts among partners.[4]
I realize starting a company doesn't have to mean starting a
startup.  There will be lots of people starting normal companies
too.  But that's not relevant to an audience of investors.Geoff Ralston reports that in Silicon Valley it seemed thinkable
to start a startup in the mid 1980s.  It would have started there.
But I know it didn't to undergraduates on the East Coast.[5]
This trend is one of the main causes of the increase in
economic inequality in the US since the mid twentieth century.  The
person who would in 1950 have been the general manager of the x
division of Megacorp is now the founder of the x company, and owns
significant equity in it.[6]
If Congress passes the founder
visa in a non-broken form, that alone could in principle get
us up to 20x, since 95% of the world's population lives outside the
US.[7]
If idea clashes got bad enough, it could change what it means
to be a startup.  We currently advise startups mostly to ignore
competitors.  We tell them startups are competitive like running,
not like soccer; you don't have to go and steal the ball away from
the other team.  But if idea clashes became common enough, maybe
you'd start to have to.  That would be unfortunate.Thanks to Sam Altman, Paul Buchheit, Dalton Caldwell,
Patrick Collison, Jessica
Livingston, Andrew Mason, Geoff Ralston, and Garry Tan for reading
drafts of this.","startup advice
",human,"human
","human
"
46,46,"

Want to start a startup?  Get funded by
Y Combinator.




April 2001, rev. April 2003(This article is derived from a talk given at the 2001 Franz
Developer Symposium.)
In the summer of 1995, my friend Robert Morris and I
started a startup called 
Viaweb.  
Our plan was to write
software that would let end users build online stores.
What was novel about this software, at the time, was
that it ran on our server, using ordinary Web pages
as the interface.A lot of people could have been having this idea at the
same time, of course, but as far as I know, Viaweb was
the first Web-based application.  It seemed such
a novel idea to us that we named the company after it:
Viaweb, because our software worked via the Web,
instead of running on your desktop computer.Another unusual thing about this software was that it
was written primarily in a programming language called
Lisp. It was one of the first big end-user
applications to be written in Lisp, which up till then
had been used mostly in universities and research labs. [1]The Secret WeaponEric Raymond has written an essay called ""How to Become a Hacker,""
and in it, among other things, he tells would-be hackers what
languages they should learn.  He suggests starting with Python and
Java, because they are easy to learn.  The serious hacker will also
want to learn C, in order to hack Unix, and Perl for system
administration and cgi scripts.  Finally, the truly serious hacker
should consider learning Lisp:

  Lisp is worth learning for the profound enlightenment experience
  you will have when you finally get it; that experience will make
  you a better programmer for the rest of your days, even if you
  never actually use Lisp itself a lot.

This is the same argument you tend to hear for learning Latin.  It
won't get you a job, except perhaps as a classics professor, but
it will improve your mind, and make you a better writer in languages
you do want to use, like English.But wait a minute.  This metaphor doesn't stretch that far.  The
reason Latin won't get you a job is that no one speaks it.  If you
write in Latin, no one can understand you.  But Lisp is a computer
language, and computers speak whatever language you, the programmer,
tell them to.So if Lisp makes you a better programmer, like he says, why wouldn't
you want to use it? If a painter were offered a brush that would
make him a better painter, it seems to me that he would want to
use it in all his paintings, wouldn't he? I'm not trying to make
fun of Eric Raymond here.  On the whole, his advice is good.  What
he says about Lisp is pretty much the conventional wisdom.  But
there is a contradiction in the conventional wisdom:  Lisp will
make you a better programmer, and yet you won't use it.Why not?  Programming languages are just tools, after all.  If Lisp
really does yield better programs, you should use it.  And if it
doesn't, then who needs it?This is not just a theoretical question.  Software is a very
competitive business, prone to natural monopolies.  A company that
gets software written faster and better will, all other things
being equal, put its competitors out of business.  And when you're
starting a startup, you feel this very keenly.  Startups tend to
be an all or nothing proposition.  You either get rich, or you get
nothing.  In a startup, if you bet on the wrong technology, your
competitors will crush you.Robert and I both knew Lisp well, and we couldn't see any reason
not to trust our instincts and go with Lisp.  We knew that everyone
else was writing their software in C++ or Perl.  But we also knew
that that didn't mean anything.  If you chose technology that way,
you'd be running Windows.  When you choose technology, you have to
ignore what other people are doing, and consider only what will
work the best.This is especially true in a startup.  In a big company, you can
do what all the other big companies are doing.  But a startup can't
do what all the other startups do.  I don't think a lot of people
realize this, even in startups.The average big company grows at about ten percent a year.  So if
you're running a big company and you do everything the way the
average big company does it, you can expect to do as well as the
average big company-- that is, to grow about ten percent a year.The same thing will happen if you're running a startup, of course.
If you do everything the way the average startup does it, you should
expect average performance.  The problem here is, average performance
means that you'll go out of business.  The survival rate for startups
is way less than fifty percent.  So if you're running a startup,
you had better be doing something odd.  If not, you're in trouble.Back in 1995, we knew something that I don't think our competitors
understood, and few understand even now:  when you're writing
software that only has to run on your own servers, you can use
any language you want.  When you're writing desktop software,
there's a strong bias toward writing applications in the same
language as the operating system.  Ten years ago, writing applications
meant writing applications in C.  But with Web-based software,
especially when you have the source code of both the language and
the operating system, you can use whatever language you want.This new freedom is a double-edged sword, however.  Now that you
can use any language, you have to think about which one to use.
Companies that try to pretend nothing has changed risk finding that
their competitors do not.If you can use any language, which do you use?  We chose Lisp.
For one thing, it was obvious that rapid development would be
important in this market.  We were all starting from scratch, so
a company that could get new features done before its competitors
would have a big advantage.  We knew Lisp was a really good language
for writing software quickly, and server-based applications magnify
the effect of rapid development, because you can release software
the minute it's done.If other companies didn't want to use Lisp, so much the better.
It might give us a technological edge, and we needed all the help
we could get.  When we started Viaweb, we had no experience in
business.  We didn't know anything about marketing, or hiring
people, or raising money, or getting customers.  Neither of us had
ever even had what you would call a real job.  The only thing we
were good at was writing software.  We hoped that would save us.
Any advantage we could get in the software department, we would
take.So you could say that using Lisp was an experiment.  Our hypothesis
was that if we wrote our software in Lisp, we'd be able to get
features done faster than our competitors, and also to do things
in our software that they couldn't do.  And because Lisp was so
high-level, we wouldn't need a big development team, so our costs
would be lower.  If this were so, we could offer a better product
for less money, and still make a profit.  We would end up getting
all the users, and our competitors would get none, and eventually
go out of business.  That was what we hoped would happen, anyway.What were the results of this experiment?  Somewhat surprisingly,
it worked.  We eventually had many competitors, on the order of
twenty to thirty of them, but none of their software could compete
with ours.  We had a wysiwyg online store builder that ran on the
server and yet felt like a desktop application.  Our competitors
had cgi scripts.  And we were always far ahead of them in features.
Sometimes, in desperation, competitors would try to introduce
features that we didn't have.  But with Lisp our development cycle
was so fast that we could sometimes duplicate a new feature within
a day or two of a competitor announcing it in a press release.  By
the time journalists covering the press release got round to calling
us, we would have the new feature too.It must have seemed to our competitors that we had some kind of
secret weapon-- that we were decoding their Enigma traffic or
something.  In fact we did have a secret weapon, but it was simpler
than they realized.  No one was leaking news of their features to
us.   We were just able to develop software faster than anyone
thought possible.When I was about nine I happened to get hold of a copy of The Day
of the Jackal, by Frederick Forsyth.  The main character is an
assassin who is hired to kill the president of France.  The assassin
has to get past the police to get up to an apartment that overlooks
the president's route.  He walks right by them, dressed up as an
old man on crutches, and they never suspect him.Our secret weapon was similar.  We wrote our software in a weird
AI language, with a bizarre syntax full of parentheses.  For years
it had annoyed me to hear Lisp described that way.  But now it
worked to our advantage.  In business, there is nothing more valuable
than a technical advantage your competitors don't understand.  In
business, as in war, surprise is worth as much as force.And so, I'm a little embarrassed to say, I never said anything
publicly about Lisp while we were working on Viaweb.  We never
mentioned it to the press, and if you searched for Lisp on our Web
site, all you'd find were the titles of two books in my bio.  This
was no accident.  A startup should give its competitors as little
information as possible.  If they didn't know what language our
software was written in, or didn't care, I wanted to keep it that
way.[2]The people who understood our technology best were the customers.
They didn't care what language Viaweb was written in either, but
they noticed that it worked really well.  It let them build great
looking online stores literally in minutes.  And so, by word of
mouth mostly, we got more and more users.  By the end of 1996 we
had about 70 stores online.  At the end of 1997 we had 500.  Six
months later, when Yahoo bought us, we had 1070 users.  Today, as
Yahoo Store, this software continues to dominate its market.  It's
one of the more profitable pieces of Yahoo, and the stores built
with it are the foundation of Yahoo Shopping.  I left Yahoo in
1999, so I don't know exactly how many users they have now, but
the last I heard there were about 20,000.
The Blub ParadoxWhat's so great about Lisp?  And if Lisp is so great, why doesn't
everyone use it?  These sound like rhetorical questions, but actually
they have straightforward answers.  Lisp is so great not because
of some magic quality visible only to devotees, but because it is
simply the most powerful language available.  And the reason everyone
doesn't use it is that programming languages are not merely
technologies, but habits of mind as well, and nothing changes
slower.  Of course, both these answers need explaining.I'll begin with a shockingly controversial statement:  programming
languages vary in power.Few would dispute, at least, that high level languages are more
powerful than machine language.  Most programmers today would agree
that you do not, ordinarily, want to program in machine language.
Instead, you should program in a high-level language, and have a
compiler translate it into machine language for you.  This idea is
even built into the hardware now: since the 1980s, instruction sets
have been designed for compilers rather than human programmers.Everyone knows it's a mistake to write your whole program by hand
in machine language.  What's less often understood is that there
is a more general principle here: that if you have a choice of
several languages, it is, all other things being equal, a mistake
to program in anything but the most powerful one. [3]There are many exceptions to this rule.  If you're writing a program
that has to work very closely with a program written in a certain
language, it might be a good idea to write the new program in the
same language.  If you're writing a program that only has to do
something very simple, like number crunching or bit manipulation,
you may as well use a less abstract language, especially since it
may be slightly faster.  And if you're writing a short, throwaway
program, you may be better off just using whatever language has
the best library functions for the task.  But in general, for
application software, you want to be using the most powerful
(reasonably efficient) language you can get, and using anything
else is a mistake, of exactly the same kind, though possibly in a
lesser degree, as programming in machine language.You can see that machine language is very low level.  But, at least
as a kind of social convention, high-level languages are often all
treated as equivalent.  They're not.  Technically the term ""high-level
language"" doesn't mean anything very definite.  There's no dividing
line with machine languages on one side and all the high-level
languages on the other.  Languages fall along a continuum [4] of
abstractness, from the most powerful all the way down to machine
languages, which themselves vary in power.Consider Cobol.  Cobol is a high-level language, in the sense that
it gets compiled into machine language.  Would anyone seriously
argue that Cobol is equivalent in power to, say, Python?  It's
probably closer to machine language than Python.Or how about Perl 4?  Between Perl 4 and Perl 5, lexical closures
got added to the language.  Most Perl hackers would agree that Perl
5 is more powerful than Perl 4.  But once you've admitted that,
you've admitted that one high level language can be more powerful
than another.  And it follows inexorably that, except in special
cases, you ought to use the most powerful you can get.This idea is rarely followed to its conclusion, though.  After a
certain age, programmers rarely switch languages voluntarily.
Whatever language people happen to be used to, they tend to consider
just good enough.Programmers get very attached to their favorite languages, and I
don't want to hurt anyone's feelings, so to explain this point I'm
going to use a hypothetical language called Blub.  Blub falls right
in the middle of the abstractness continuum.  It is not the most
powerful language, but it is more powerful than Cobol or machine
language.And in fact, our hypothetical Blub programmer wouldn't use either
of them.  Of course he wouldn't program in machine language.  That's
what compilers are for.  And as for Cobol, he doesn't know how
anyone can get anything done with it.  It doesn't even have x (Blub
feature of your choice).As long as our hypothetical Blub programmer is looking down the
power continuum, he knows he's looking down.  Languages less powerful
than Blub are obviously less powerful, because they're missing some
feature he's used to.  But when our hypothetical Blub programmer
looks in the other direction, up the power continuum, he doesn't
realize he's looking up.  What he sees are merely weird languages.
He probably considers them about equivalent in power to Blub, but
with all this other hairy stuff thrown in as well.  Blub is good
enough for him, because he thinks in Blub.When we switch to the point of view of a programmer using any of
the languages higher up the power continuum, however, we find that
he in turn looks down upon Blub.  How can you get anything done in
Blub? It doesn't even have y.By induction, the only programmers in a position to see all the
differences in power between the various languages are those who
understand the most powerful one.  (This is probably what Eric
Raymond meant about Lisp making you a better programmer.) You can't
trust the opinions of the others, because of the Blub paradox:
they're satisfied with whatever language they happen to use, because
it dictates the way they think about programs.I know this from my own experience, as a high school kid writing
programs in Basic.  That language didn't even support recursion.
It's hard to imagine writing programs without using recursion, but
I didn't miss it at the time.  I thought in Basic.  And I was a
whiz at it.  Master of all I surveyed.The five languages that Eric Raymond recommends to hackers fall at
various points on the power continuum.  Where they fall relative
to one another is a sensitive topic.  What I will say is that I
think Lisp is at the top.  And to support this claim I'll tell you
about one of the things I find missing when I look at the other
four languages.  How can you get anything done in them, I think,
without macros? [5]Many languages have something called a macro.  But Lisp macros are
unique.  And believe it or not, what they do is related to the
parentheses.  The designers of Lisp didn't put all those parentheses
in the language just to be different.  To the Blub programmer, Lisp
code looks weird.  But those parentheses are there for a reason.
They are the outward evidence of a fundamental difference between
Lisp and other languages.Lisp code is made out of Lisp data objects.  And not in the trivial
sense that the source files contain characters, and strings are
one of the data types supported by the language.  Lisp code, after
it's read by the parser, is made of data structures that you can
traverse.If you understand how compilers work, what's really going on is
not so much that Lisp has a strange syntax as that Lisp has no
syntax.  You write programs in the parse trees that get generated
within the compiler when other languages are parsed.  But these
parse trees are fully accessible to your programs.  You can write
programs that manipulate them.  In Lisp, these programs are called
macros.  They are programs that write programs.Programs that write programs?  When would you ever want to do that?
Not very often, if you think in Cobol.  All the time, if you think
in Lisp.  It would be convenient here if I could give an example
of a powerful macro, and say there! how about that?  But if I did,
it would just look like gibberish to someone who didn't know Lisp;
there isn't room here to explain everything you'd need to know to
understand what it meant.  In 
Ansi Common Lisp I tried to move
things along as fast as I could, and even so I didn't get to macros
until page 160.But I think I can give a kind of argument that might be convincing.
The source code of the Viaweb editor was probably about 20-25%
macros.  Macros are harder to write than ordinary Lisp functions,
and it's considered to be bad style to use them when they're not
necessary.  So every macro in that code is there because it has to
be.  What that means is that at least 20-25% of the code in this
program is doing things that you can't easily do in any other
language.  However skeptical the Blub programmer might be about my
claims for the mysterious powers of Lisp, this ought to make him
curious.  We weren't writing this code for our own amusement.  We
were a tiny startup, programming as hard as we could in order to
put technical barriers between us and our competitors.A suspicious person might begin to wonder if there was some
correlation here.  A big chunk of our code was doing things that
are very hard to do in other languages.  The resulting software
did things our competitors' software couldn't do.  Maybe there was
some kind of connection.  I encourage you to follow that thread.
There may be more to that old man hobbling along on his crutches
than meets the eye.Aikido for StartupsBut I don't expect to convince anyone 
(over 25) 
to go out and learn
Lisp.  The purpose of this article is not to change anyone's mind,
but to reassure people already interested in using Lisp-- people
who know that Lisp is a powerful language, but worry because it
isn't widely used.  In a competitive situation, that's an advantage.
Lisp's power is multiplied by the fact that your competitors don't
get it.If you think of using Lisp in a startup, you shouldn't worry that
it isn't widely understood.  You should hope that it stays that
way. And it's likely to.  It's the nature of programming languages
to make most people satisfied with whatever they currently use.
Computer hardware changes so much faster than personal habits that
programming practice is usually ten to twenty years behind the
processor.  At places like MIT they were writing programs in
high-level languages in the early 1960s, but many companies continued
to write code in machine language well into the 1980s.  I bet a
lot of people continued to write machine language until the processor,
like a bartender eager to close up and go home, finally kicked them
out by switching to a risc instruction set.Ordinarily technology changes fast.  But programming languages are
different: programming languages are not just technology, but what
programmers think in.  They're half technology and half religion.[6]
And so the median language, meaning whatever language the median
programmer uses, moves as slow as an iceberg.  Garbage collection,
introduced by Lisp in about 1960, is now widely considered to be
a good thing.  Runtime typing, ditto, is growing in popularity.
Lexical closures, introduced by Lisp in the early 1970s, are now,
just barely, on the radar screen.  Macros, introduced by Lisp in the
mid 1960s, are still terra incognita.Obviously, the median language has enormous momentum.  I'm not
proposing that you can fight this powerful force.  What I'm proposing
is exactly the opposite: that, like a practitioner of Aikido, you
can use it against your opponents.If you work for a big company, this may not be easy.  You will have
a hard time convincing the pointy-haired boss to let you build
things in Lisp, when he has just read in the paper that some other
language is poised, like Ada was twenty years ago, to take over
the world.  But if you work for a startup that doesn't have
pointy-haired bosses yet, you can, like we did, turn the Blub
paradox to your advantage:  you can use technology that your
competitors, glued immovably to the median language, will never be
able to match.If you ever do find yourself working for a startup, here's a handy
tip for evaluating competitors.  Read their job listings.  Everything
else on their site may be stock photos or the prose equivalent,
but the job listings have to be specific about what they want, or
they'll get the wrong candidates.During the years we worked on Viaweb I read a lot of job descriptions.
A new competitor seemed to emerge out of the woodwork every month
or so.  The first thing I would do, after checking to see if they
had a live online demo, was look at their job listings.  After a
couple years of this I could tell which companies to worry about
and which not to.  The more of an IT flavor the job descriptions
had, the less dangerous the company was.  The safest kind were the
ones that wanted Oracle experience.  You never had to worry about
those.  You were also safe if they said they wanted C++ or Java
developers.  If they wanted Perl or Python programmers, that would
be a bit frightening-- that's starting to sound like a company
where the technical side, at least, is run by real hackers.  If I
had ever seen a job posting looking for Lisp hackers, I would have
been really worried.
Notes[1] Viaweb at first had two parts: the editor, written in Lisp,
which people used to build their sites, and the ordering system,
written in C, which handled orders.  The first version was mostly
Lisp, because the ordering system was small.  Later we added two
more modules, an image generator written in C, and a back-office
manager written mostly in Perl.In January 2003, Yahoo released a new version of the editor 
written in C++ and Perl.  It's hard to say whether the program is no
longer written in Lisp, though, because to translate this program
into C++ they literally had to write a Lisp interpreter: the source
files of all the page-generating templates are still, as far as I
know,  Lisp code.  (See Greenspun's Tenth Rule.)[2] Robert Morris says that I didn't need to be secretive, because
even if our competitors had known we were using Lisp, they wouldn't
have understood why:  ""If they were that smart they'd already be
programming in Lisp.""[3] All languages are equally powerful in the sense of being Turing
equivalent, but that's not the sense of the word programmers care
about. (No one wants to program a Turing machine.)  The kind of
power programmers care about may not be formally definable, but
one way to explain it would be to say that it refers to features
you could only get in the less powerful language by writing an
interpreter for the more powerful language in it. If language A
has an operator for removing spaces from strings and language B
doesn't, that probably doesn't make A more powerful, because you
can probably write a subroutine to do it in B.  But if A supports,
say, recursion, and B doesn't, that's not likely to be something
you can fix by writing library functions.[4] Note to nerds: or possibly a lattice, narrowing toward the top;
it's not the shape that matters here but the idea that there is at
least a partial order.[5] It is a bit misleading to treat macros as a separate feature.
In practice their usefulness is greatly enhanced by other Lisp
features like lexical closures and rest parameters.[6] As a result, comparisons of programming languages either take
the form of religious wars or undergraduate textbooks so determinedly
neutral that they're really works of anthropology.  People who
value their peace, or want tenure, avoid the topic.  But the question
is only half a religious one; there is something there worth
studying, especially if you want to design new languages.More Technical DetailsJapanese TranslationTurkish TranslationUzbek TranslationOrbitz Uses Lisp TooHow To Become A HackerA Scheme StoryItalian Translation


You'll find this essay and 14 others in
Hackers & Painters.

","startup advice
",human,"human
","human
"
47,47,"

Want to start a startup?  Get funded by
Y Combinator.




November 2012The way to get startup ideas is not to try to think of startup
ideas.  It's to look for problems, preferably problems you have
yourself.The very best startup ideas tend to have three things in common:
they're something the founders themselves want, that they themselves
can build, and that few others realize are worth doing.  Microsoft,
Apple, Yahoo, Google, and Facebook all began this way.
ProblemsWhy is it so important to work on a problem you have?  Among other
things, it ensures the problem really exists.  It sounds obvious
to say you should only work on problems that exist.  And yet by far
the most common mistake startups make is to solve problems no one
has.I made it myself.  In 1995 I started a company to put art galleries
online.  But galleries didn't want to be online.  It's not how the
art business works.  So why did I spend 6 months working on this
stupid idea?  Because I didn't pay attention to users.  I invented
a model of the world that didn't correspond to reality, and worked
from that.  I didn't notice my model was wrong until I tried
to convince users to pay for what we'd built.  Even then I took
embarrassingly long to catch on.  I was attached to my model of the
world, and I'd spent a lot of time on the software.  They had to
want it!Why do so many founders build things no one wants?  Because they
begin by trying to think of startup ideas.  That m.o. is doubly
dangerous: it doesn't merely yield few good ideas; it yields bad
ideas that sound plausible enough to fool you into working on them.At YC we call these ""made-up"" or ""sitcom"" startup ideas.  Imagine
one of the characters on a TV show was starting a startup.  The
writers would have to invent something for it to do.  But coming
up with good startup ideas is hard.  It's not something you can do
for the asking.  So (unless they got amazingly lucky) the writers
would come up with an idea that sounded plausible, but was actually
bad.For example, a social network for pet owners.  It doesn't sound
obviously mistaken.  Millions of people have pets.  Often they care
a lot about their pets and spend a lot of money on them.  Surely
many of these people would like a site where they could talk to
other pet owners.  Not all of them perhaps, but if just 2 or 3
percent were regular visitors, you could have millions of users.
You could serve them targeted offers, and maybe charge for premium
features. 
[1]The danger of an idea like this is that when you run it by your
friends with pets, they don't say ""I would never use this."" They
say ""Yeah, maybe I could see using something like that."" Even when
the startup launches, it will sound plausible to a lot of people.
They don't want to use it themselves, at least not right now, but
they could imagine other people wanting it.  Sum that reaction
across the entire population, and you have zero users. 
[2]
WellWhen a startup launches, there have to be at least some users who
really need what they're making — not just people who could see
themselves using it one day, but who want it urgently.  Usually
this initial group of users is small, for the simple reason that
if there were something that large numbers of people urgently needed
and that could be built with the amount of effort a startup usually
puts into a version one, it would probably already exist.  Which
means you have to compromise on one dimension: you can either build
something a large number of people want a small amount, or something
a small number of people want a large amount.  Choose the latter.
Not all ideas of that type are good startup ideas, but nearly all
good startup ideas are of that type.Imagine a graph whose x axis represents all the people who might
want what you're making and whose y axis represents how much they
want it.  If you invert the scale on the y axis, you can envision
companies as holes.  Google is an immense crater: hundreds of
millions of people use it, and they need it a lot.  A startup just
starting out can't expect to excavate that much volume.  So you
have two choices about the shape of hole you start with.  You can
either dig a hole that's broad but shallow, or one that's narrow
and deep, like a well.Made-up startup ideas are usually of the first type.  Lots of people
are mildly interested in a social network for pet owners.Nearly all good startup ideas are of the second type.  Microsoft
was a well when they made Altair Basic.  There were only a couple
thousand Altair owners, but without this software they were programming
in machine language.  Thirty years later Facebook had the same
shape.  Their first site was exclusively for Harvard students, of
which there are only a few thousand, but those few thousand users
wanted it a lot.When you have an idea for a startup, ask yourself: who wants this
right now?  Who wants this so much that they'll use it even when
it's a crappy version one made by a two-person startup they've never
heard of?  If you can't answer that, the idea is probably bad. 
[3]You don't need the narrowness of the well per se.  It's depth you
need; you get narrowness as a byproduct of optimizing for depth
(and speed).  But you almost always do get it.  In practice the
link between depth and narrowness is so strong that it's a good
sign when you know that an idea will appeal strongly to a specific
group or type of user.But while demand shaped like a well is almost a necessary condition
for a good startup idea, it's not a sufficient one.  If Mark
Zuckerberg had built something that could only ever have appealed
to Harvard students, it would not have been a good startup idea.
Facebook was a good idea because it started with a small market
there was a fast path out of.  Colleges are similar enough that if
you build a facebook that works at Harvard, it will work at any
college. So you spread rapidly through all the colleges.  Once you
have all the college students, you get everyone else simply by
letting them in.Similarly for Microsoft: Basic for the Altair; Basic for other
machines; other languages besides Basic; operating systems;
applications; IPO.
SelfHow do you tell whether there's a path out of an idea?  How do you
tell whether something is the germ of a giant company, or just a
niche product?  Often you can't. The founders of Airbnb didn't
realize at first how big a market they were tapping.  Initially
they had a much narrower idea.  They were going to let hosts rent
out space on their floors during conventions.  They didn't foresee
the expansion of this idea; it forced itself upon them gradually.
All they knew at first is that they were onto something.  That's
probably as much as Bill Gates or Mark Zuckerberg knew at first.Occasionally it's obvious from the beginning when there's a path
out of the initial niche.  And sometimes I can see a path that's
not immediately obvious; that's one of our specialties at YC.  But
there are limits to how well this can be done, no matter how much
experience you have.  The most important thing to understand about
paths out of the initial idea is the meta-fact that these are hard
to see.So if you can't predict whether there's a path out of an idea, how
do you choose between ideas?  The truth is disappointing but
interesting: if you're the right sort of person, you have the right
sort of hunches.  If you're at the leading edge of a field that's
changing fast, when you have a hunch that something is worth doing,
you're more likely to be right.In Zen and the Art of Motorcycle Maintenance, Robert Pirsig says:

  You want to know how to paint a perfect painting? It's easy.  Make
  yourself perfect and then just paint naturally.

I've wondered about that passage since I read it in high school.
I'm not sure how useful his advice is for painting specifically,
but it fits this situation well.  Empirically, the way to have good
startup ideas is to become the sort of person who has them.Being at the leading edge of a field doesn't mean you have to be
one of the people pushing it forward.  You can also be at the leading
edge as a user.  It was not so much because he was a programmer
that Facebook seemed a good idea to Mark Zuckerberg as because he
used computers so much.  If you'd asked most 40 year olds in 2004
whether they'd like to publish their lives semi-publicly on the
Internet, they'd have been horrified at the idea.  But Mark already
lived online; to him it seemed natural.Paul Buchheit says that people at the leading edge of a rapidly
changing field ""live in the future.""  Combine that with Pirsig and
you get:

  Live in the future, then build what's missing.

That describes the way many if not most of the biggest startups got
started.  Neither Apple nor Yahoo nor Google nor Facebook were even
supposed to be companies at first.  They grew out of things their
founders built because there seemed a gap in the world.If you look at the way successful founders have had their ideas,
it's generally the result of some external stimulus hitting a
prepared mind.  Bill Gates and Paul Allen hear about the Altair and
think ""I bet we could write a Basic interpreter for it."" Drew Houston
realizes he's forgotten his USB stick and thinks ""I really need to
make my files live online."" Lots of people heard about the Altair.
Lots forgot USB sticks.  The reason those stimuli caused those
founders to start companies was that their experiences had prepared
them to notice the opportunities they represented.The verb you want to be using with respect to startup ideas is not
""think up"" but ""notice."" At YC we call ideas that grow naturally
out of the founders' own experiences ""organic"" startup ideas.  The
most successful startups almost all begin this way.That may not have been what you wanted to hear.  You may have
expected recipes for coming up with startup ideas, and instead I'm
telling you that the key is to have a mind that's prepared in the
right way.  But disappointing though it may be, this is the truth.
And it is a recipe of a sort, just one that in the worst case takes
a year rather than a weekend.If you're not at the leading edge of some rapidly changing field,
you can get to one.  For example, anyone reasonably smart can
probably get to an edge of programming (e.g. building mobile apps)
in a year.  Since a successful startup will consume at least 3-5
years of your life, a year's preparation would be a reasonable
investment.  Especially if you're also looking for a cofounder.
[4]You don't have to learn programming to be at the leading edge of a
domain that's changing fast.  Other domains change fast.  But while
learning to hack is not necessary, it is for the forseeable future
sufficient. As Marc Andreessen put it, software is eating the world,
and this trend has decades left to run.Knowing how to hack also means that when you have ideas, you'll be
able to implement them.  That's not absolutely necessary (Jeff Bezos
couldn't) but it's an advantage.  It's a big advantage, when you're
considering an idea like putting a college facebook online, if
instead of merely thinking ""That's an interesting idea,"" you can
think instead ""That's an interesting idea.  I'll try building an
initial version tonight.""  It's even better when you're both a
programmer and the target user, because then the cycle of generating
new versions and testing them on users can happen inside one head.
NoticingOnce you're living in the future in some respect, the way to notice
startup ideas is to look for things that seem to be missing.  If
you're really at the leading edge of a rapidly changing field, there
will be things that are obviously missing.  What won't be obvious
is that they're startup ideas.  So if you want to find startup
ideas, don't merely turn on the filter ""What's missing?"" Also turn
off every other filter, particularly ""Could this be a big company?""
There's plenty of time to apply that test later.  But if you're
thinking about that initially, it may not only filter out lots
of good ideas, but also cause you to focus on bad ones.Most things that are missing will take some time to see.  You almost
have to trick yourself into seeing the ideas around you.But you know the ideas are out there.  This is not one of those
problems where there might not be an answer.  It's impossibly
unlikely that this is the exact moment when technological progress
stops.  You can be sure people are going to build things in the
next few years that will make you think ""What did I do before x?""And when these problems get solved, they will probably seem flamingly
obvious in retrospect.  What you need to do is turn off the filters
that usually prevent you from seeing them.  The most powerful is
simply taking the current state of the world for granted.  Even the
most radically open-minded of us mostly do that.  You couldn't get
from your bed to the front door if you stopped to question everything.But if you're looking for startup ideas you can sacrifice some of
the efficiency of taking the status quo for granted and start to
question things.  Why is your inbox overflowing?  Because you get
a lot of email, or because it's hard to get email out of your inbox?
Why do you get so much email?  What problems are people trying to
solve by sending you email?  Are there better ways to solve them?
And why is it hard to get emails out of your inbox?  Why do you
keep emails around after you've read them?  Is an inbox the optimal
tool for that?Pay particular attention to things that chafe you.  The advantage
of taking the status quo for granted is not just that it makes life
(locally) more efficient, but also that it makes life more tolerable.
If you knew about all the things we'll get in the next 50 years but
don't have yet, you'd find present day life pretty constraining,
just as someone from the present would if they were sent back 50
years in a time machine.  When something annoys you, it could be
because you're living in the future.When you find the right sort of problem, you should probably be
able to describe it as obvious, at least to you.  When we started
Viaweb, all the online stores were built by hand, by web designers
making individual HTML pages.  It was obvious to us as programmers
that these sites would have to be generated by software.
[5]Which means, strangely enough, that coming up with startup ideas
is a question of seeing the obvious.  That suggests how weird this
process is: you're trying to see things that are obvious, and yet
that you hadn't seen.Since what you need to do here is loosen up your own mind, it may
be best not to make too much of a direct frontal attack on the
problem — i.e. to sit down and try to think of ideas.  The best
plan may be just to keep a background process running, looking for
things that seem to be missing.  Work on hard problems, driven
mainly by curiosity, but have a second self watching over your
shoulder, taking note of gaps and anomalies.  
[6]Give yourself some time.  You have a lot of control over the rate
at which you turn yours into a prepared mind, but you have less
control over the stimuli that spark ideas when they hit it.  If
Bill Gates and Paul Allen had constrained themselves to come up
with a startup idea in one month, what if they'd chosen a month
before the Altair appeared?  They probably would have worked on a
less promising idea.  Drew Houston did work on a less promising
idea before Dropbox: an SAT prep startup.  But Dropbox was a much
better idea, both in the absolute sense and also as a match for his
skills.
[7]A good way to trick yourself into noticing ideas is to work on
projects that seem like they'd be cool.  If you do that, you'll
naturally tend to build things that are missing.  It wouldn't seem
as interesting to build something that already existed.Just as trying to think up startup ideas tends to produce bad ones,
working on things that could be dismissed as ""toys"" often produces
good ones.  When something is described as a toy, that means it has
everything an idea needs except being important.  It's cool; users
love it; it just doesn't matter.  But if you're living in the future
and you build something cool that users love, it may matter more
than outsiders think.  Microcomputers seemed like toys when Apple
and Microsoft started working on them.  I'm old enough to remember
that era; the usual term for people with their own microcomputers
was ""hobbyists.""  BackRub seemed like an inconsequential science
project.  The Facebook was just a way for undergrads to stalk one
another.At YC we're excited when we meet startups working on things that
we could imagine know-it-alls on forums dismissing as toys.  To us
that's positive evidence an idea is good.If you can afford to take a long view (and arguably you can't afford
not to), you can turn ""Live in the future and build what's missing""
into something even better:

  Live in the future and build what seems interesting.

SchoolThat's what I'd advise college students to do, rather than trying
to learn about ""entrepreneurship.""  ""Entrepreneurship"" is something
you learn best by doing it.  The examples of the most successful
founders make that clear.  What you should be spending your time
on in college is ratcheting yourself into the future.  College is
an incomparable opportunity to do that.  What a waste to sacrifice
an opportunity to solve the hard part of starting a startup — becoming 
the sort of person who can have organic startup ideas — by 
spending time learning about the easy part.  Especially since
you won't even really learn about it, any more than you'd learn
about sex in a class.  All you'll learn is the words for things.The clash of domains is a particularly fruitful source of ideas.
If you know a lot about programming and you start learning about
some other field, you'll probably see problems that software could
solve.  In fact, you're doubly likely to find good problems in
another domain: (a) the inhabitants of that domain are not as likely
as software people to have already solved their problems with
software, and (b) since you come into the new domain totally ignorant,
you don't even know what the status quo is to take it for granted.So if you're a CS major and you want to start a startup, instead
of taking a class on entrepreneurship you're better off taking a
class on, say, genetics.  Or better still, go work for a biotech
company.  CS majors normally get summer jobs at computer hardware
or software companies.  But if you want to find startup ideas, you
might do better to get a summer job in some unrelated field. 
[8]Or don't take any extra classes, and just build things.  It's no
coincidence that Microsoft and Facebook both got started in January.
At Harvard that is (or was) Reading Period, when students have no
classes to attend because they're supposed to be studying for finals.
[9]But don't feel like you have to build things that will become startups.  That's
premature optimization. Just build things.  Preferably with other
students.  It's not just the classes that make a university such a
good place to crank oneself into the future.  You're also surrounded
by other people trying to do the same thing.  If you work together
with them on projects, you'll end up producing not just organic
ideas, but organic ideas with organic founding teams — and that,
empirically, is the best combination.Beware of research.  If an undergrad writes something all his friends
start using, it's quite likely to represent a good startup idea.
Whereas a PhD dissertation is extremely unlikely to.  For some
reason, the more a project has to count as research, the less likely
it is to be something that could be turned into a startup.
[10]
I think the reason is that the subset of ideas that count as research
is so narrow that it's unlikely that a project that satisfied that
constraint would also satisfy the orthogonal constraint of solving
users' problems.  Whereas when students (or professors) build
something as a side-project, they automatically gravitate toward
solving users' problems — perhaps even with an additional energy
that comes from being freed from the constraints of research.
CompetitionBecause a good idea should seem obvious, when you have one you'll
tend to feel that you're late.  Don't let that deter you.  Worrying
that you're late is one of the signs of a good idea.  Ten minutes
of searching the web will usually settle the question.  Even if you
find someone else working on the same thing, you're probably not
too late.  It's exceptionally rare for startups to be killed by
competitors — so rare that you can almost discount the possibility.
So unless you discover a competitor with the sort of lock-in that
would prevent users from choosing you, don't discard the idea.If you're uncertain, ask users.  The question of whether you're too
late is subsumed by the question of whether anyone urgently needs
what you plan to make.  If you have something that no competitor
does and that some subset of users urgently need, you have a
beachhead.  
[11]The question then is whether that beachhead is big enough. Or more
importantly, who's in it: if the beachhead consists of people doing
something lots more people will be doing in the future, then it's
probably big enough no matter how small it is.  For example, if
you're building something differentiated from competitors by the
fact that it works on phones, but it only works on the newest phones,
that's probably a big enough beachhead.Err on the side of doing things where you'll face competitors.
Inexperienced founders usually give competitors more credit than
they deserve.  Whether you succeed depends far more on you than on
your competitors.  So better a good idea with competitors than a
bad one without.You don't need to worry about entering a ""crowded market"" so long
as you have a thesis about what everyone else in it is overlooking.
In fact that's a very promising starting point.  Google was that
type of idea.  Your thesis has to be more precise than ""we're going
to make an x that doesn't suck"" though. You have to be able to
phrase it in terms of something the incumbents are overlooking.
Best of all is when you can say that they didn't have the courage
of their convictions, and that your plan is what they'd have done
if they'd followed through on their own insights.  Google was that
type of idea too.  The search engines that preceded them shied away
from the most radical implications of what they were doing — particularly 
that the better a job they did, the faster users would
leave.A crowded market is actually a good sign, because it means both
that there's demand and that none of the existing solutions are
good enough.  A startup can't hope to enter a market that's obviously
big and yet in which they have no competitors.  So any startup that
succeeds is either going to be entering a market with existing
competitors, but armed with some secret weapon that will get them
all the users (like Google), or entering a market that looks small
but which will turn out to be big (like Microsoft).  
[12]
FiltersThere are two more filters you'll need to turn off if you want to
notice startup ideas: the unsexy filter and the schlep filter.Most programmers wish they could start a startup by just writing
some brilliant code, pushing it to a server, and having users pay
them lots of money.  They'd prefer not to deal with tedious problems
or get involved in messy ways with the real world.  Which is a
reasonable preference, because such things slow you down.  But this
preference is so widespread that the space of convenient startup
ideas has been stripped pretty clean.  If you let your mind wander
a few blocks down the street to the messy, tedious ideas, you'll
find valuable ones just sitting there waiting to be implemented.The schlep filter is so dangerous that I wrote a separate essay
about the condition it induces, which I called 
schlep blindness.
I gave Stripe as an example of a startup that benefited from turning
off this filter, and a pretty striking example it is.  Thousands
of programmers were in a position to see this idea; thousands of
programmers knew how painful it was to process payments before
Stripe.  But when they looked for startup ideas they didn't see
this one, because unconsciously they shrank from having to deal
with payments.  And dealing with payments is a schlep for Stripe,
but not an intolerable one.  In fact they might have had net less
pain; because the fear of dealing with payments kept most people
away from this idea, Stripe has had comparatively smooth sailing
in other areas that are sometimes painful, like user acquisition.
They didn't have to try very hard to make themselves heard by users,
because users were desperately waiting for what they were building.The unsexy filter is similar to the schlep filter, except it keeps
you from working on problems you despise rather than ones you fear.
We overcame this one to work on Viaweb. There were interesting
things about the architecture of our software, but we weren't
interested in ecommerce per se.  We could see the problem was one
that needed to be solved though.Turning off the schlep filter is more important than turning off
the unsexy filter, because the schlep filter is more likely to be
an illusion.  And even to the degree it isn't, it's a worse form
of self-indulgence.  Starting a successful startup is going to be
fairly laborious no matter what.  Even if the product doesn't entail
a lot of schleps, you'll still have plenty dealing with investors,
hiring and firing people, and so on.  So if there's some idea you
think would be cool but you're kept away from by fear of the schleps
involved, don't worry: any sufficiently good idea will have as many.The unsexy filter, while still a source of error, is not as entirely
useless as the schlep filter.  If you're at the leading edge of a
field that's changing rapidly, your ideas about what's sexy will
be somewhat correlated with what's valuable in practice.  Particularly
as you get older and more experienced.  Plus if you find an idea
sexy, you'll work on it more enthusiastically. 
[13]
RecipesWhile the best way to discover startup ideas is to become the sort
of person who has them and then build whatever interests you,
sometimes you don't have that luxury.  Sometimes you need an idea
now.  For example, if you're working on a startup and your initial
idea turns out to be bad.For the rest of this essay I'll talk about tricks for coming up
with startup ideas on demand.  Although empirically you're better
off using the organic strategy, you could succeed this way. You
just have to be more disciplined.  When you use the organic method,
you don't even notice an idea unless it's evidence that something
is truly missing.  But when you make a conscious effort to think
of startup ideas, you have to replace this natural constraint with
self-discipline.  You'll see a lot more ideas, most of them bad,
so you need to be able to filter them.One of the biggest dangers of not using the organic method is the
example of the organic method.  Organic ideas feel like inspirations.
There are a lot of stories about successful startups that began
when the founders had what seemed a crazy idea but ""just knew"" it
was promising.  When you feel that about an idea you've had while
trying to come up with startup ideas, you're probably mistaken.When searching for ideas, look in areas where you have some expertise.
If you're a database expert, don't build a chat app for teenagers
(unless you're also a teenager).  Maybe it's a good idea, but you
can't trust your judgment about that, so ignore it.  There have to
be other ideas that involve databases, and whose quality you can
judge.  Do you find it hard to come up with good ideas involving
databases?  That's because your expertise raises your standards.
Your ideas about chat apps are just as bad, but you're giving
yourself a Dunning-Kruger pass in that domain.The place to start looking for ideas is things you need.  There
must be things you need.
[14]One good trick is to ask yourself whether in your previous job you
ever found yourself saying ""Why doesn't someone make x?  If someone
made x we'd buy it in a second."" If you can think of any x people
said that about, you probably have an idea.  You know there's demand,
and people don't say that about things that are impossible to build.More generally, try asking yourself whether there's something unusual
about you that makes your needs different from most other people's.
You're probably not the only one.  It's especially good if you're
different in a way people will increasingly be.If you're changing ideas, one unusual thing about you is the idea
you'd previously been working on.  Did you discover any needs while
working on it?  Several well-known startups began this way.  Hotmail
began as something its founders wrote to talk about their previous
startup idea while they were working at their day jobs. 
[15]A particularly promising way to be unusual is to be young.  Some
of the most valuable new ideas take root first among people in their
teens and early twenties.  And while young founders are at a
disadvantage in some respects, they're the only ones who really
understand their peers.  It would have been very hard for someone
who wasn't a college student to start Facebook.  So if you're a
young founder (under 23 say), are there things you and your friends
would like to do that current technology won't let you?The next best thing to an unmet need of your own is an unmet need
of someone else.  Try talking to everyone you can about the gaps
they find in the world.  What's missing?  What would they like to
do that they can't?  What's tedious or annoying, particularly in
their work?  Let the conversation get general; don't be trying too
hard to find startup ideas.  You're just looking for something to
spark a thought.  Maybe you'll notice a problem they didn't consciously
realize they had, because you know how to solve it.When you find an unmet need that isn't your own, it may be somewhat
blurry at first.  The person who needs something may not know exactly
what they need.  In that case I often recommend that founders act
like consultants — that they do what they'd do if they'd been
retained to solve the problems of this one user.  People's problems
are similar enough that nearly all the code you write this way will
be reusable, and whatever isn't will be a small price to start out
certain that you've reached the bottom of the well.
[16]One way to ensure you do a good job solving other people's problems
is to make them your own.  When Rajat Suri of E la Carte decided
to write software for restaurants, he got a job as a waiter to learn
how restaurants worked.  That may seem like taking things to extremes,
but startups are extreme.  We love it when founders do such things.In fact, one strategy I recommend to people who need a new idea is
not merely to turn off their schlep and unsexy filters, but to seek
out ideas that are unsexy or involve schleps.  Don't try to start
Twitter.  Those ideas are so rare that you can't find them by looking
for them.  Make something unsexy that people will pay you for.A good trick for bypassing the schlep and to some extent the unsexy
filter is to ask what you wish someone else would build, so that
you could use it.  What would you pay for right now?Since startups often garbage-collect broken companies and industries,
it can be a good trick to look for those that are dying, or deserve
to, and try to imagine what kind of company would profit from their
demise.  For example, journalism is in free fall at the moment.
But there may still be money to be made from something like journalism.
What sort of company might cause people in the future to say ""this
replaced journalism"" on some axis?But imagine asking that in the future, not now.  When one company
or industry replaces another, it usually comes in from the side.
So don't look for a replacement for x; look for something that
people will later say turned out to be a replacement for x.  And
be imaginative about the axis along which the replacement occurs.
Traditional journalism, for example, is a way for readers to get
information and to kill time, a way for writers to make money and
to get attention, and a vehicle for several different types of
advertising.  It could be replaced on any of these axes (it has
already started to be on most).When startups consume incumbents, they usually start by serving
some small but important market that the big players ignore.  It's
particularly good if there's an admixture of disdain in the big
players' attitude, because that often misleads them.  For example,
after Steve Wozniak built the computer that became the Apple I, he
felt obliged to give his then-employer Hewlett-Packard the option
to produce it.  Fortunately for him, they turned it down, and one
of the reasons they did was that it used a TV for a monitor, which
seemed intolerably déclassé to a high-end hardware company like HP
was at the time. 
[17]Are there groups of 
scruffy 
but sophisticated users like the early
microcomputer ""hobbyists"" that are currently being ignored by the
big players?  A startup with its sights set on bigger things can
often capture a small market easily by expending an effort that
wouldn't be justified by that market alone.Similarly, since the most successful startups generally ride some
wave bigger than themselves, it could be a good trick to look for
waves and ask how one could benefit from them.  The prices of gene
sequencing and 3D printing are both experiencing Moore's Law-like
declines.  What new things will we be able to do in the new world
we'll have in a few years?  What are we unconsciously ruling out
as impossible that will soon be possible?
OrganicBut talking about looking explicitly for waves makes it clear that
such recipes are plan B for getting startup ideas.  Looking for
waves is essentially a way to simulate the organic method.  If
you're at the leading edge of some rapidly changing field, you don't
have to look for waves; you are the wave.Finding startup ideas is a subtle business, and that's why most
people who try fail so miserably.  It doesn't work well simply to
try to think of startup ideas.  If you do that, you get bad ones
that sound dangerously plausible.  The best approach is more indirect:
if you have the right sort of background, good startup ideas will
seem obvious to you.  But even then, not immediately.  It takes
time to come across situations where you notice something missing.
And often these gaps won't seem to be ideas for companies, just
things that would be interesting to build.  Which is why it's good
to have the time and the inclination to build things just because
they're interesting.Live in the future and build what seems interesting.  Strange as
it sounds, that's the real recipe.
Notes[1]
This form of bad idea has been around as long as the web.  It
was common in the 1990s, except then people who had it used to say
they were going to create a portal for x instead of a social network
for x.  Structurally the idea is stone soup: you post a sign saying
""this is the place for people interested in x,"" and all those people
show up and you make money from them.  What lures founders into
this sort of idea are statistics about the millions of people who
might be interested in each type of x.  What they forget is that
any given person might have 20 affinities by this standard, and no
one is going to visit 20 different communities regularly.[2]
I'm not saying, incidentally, that I know for sure a social
network for pet owners is a bad idea.  I know it's a bad idea the
way I know randomly generated DNA would not produce a viable organism.
The set of plausible sounding startup ideas is many times larger
than the set of good ones, and many of the good ones don't even
sound that plausible.  So if all you know about a startup idea is
that it sounds plausible, you have to assume it's bad.[3]
More precisely, the users' need has to give them sufficient
activation energy to start using whatever you make, which can vary
a lot.  For example, the activation energy for enterprise software
sold through traditional channels is very high, so you'd have to
be a lot better to get users to switch.  Whereas the activation
energy required to switch to a new search engine is low.  Which in
turn is why search engines are so much better than enterprise
software.[4]
This gets harder as you get older.  While the space of ideas
doesn't have dangerous local maxima, the space of careers does.
There are fairly high walls between most of the paths people take
through life, and the older you get, the higher the walls become.[5]
It was also obvious to us that the web was going to be a big
deal.  Few non-programmers grasped that in 1995, but the programmers
had seen what GUIs had done for desktop computers.[6]
Maybe it would work to have this second self keep a journal,
and each night to make a brief entry listing the gaps and anomalies
you'd noticed that day.  Not startup ideas, just the raw gaps and
anomalies.[7]
Sam Altman points out that taking time to come up with an
idea is not merely a better strategy in an absolute sense, but also
like an undervalued stock in that so few founders do it.There's comparatively little competition for the best ideas, because
few founders are willing to put in the time required to notice them.
Whereas there is a great deal of competition for mediocre ideas,
because when people make up startup ideas, they tend to make up the
same ones.[8]
For the computer hardware and software companies, summer jobs
are the first phase of the recruiting funnel.  But if you're good
you can skip the first phase.  If you're good you'll have no trouble
getting hired by these companies when you graduate, regardless of
how you spent your summers.[9]
The empirical evidence suggests that if colleges want to help
their students start startups, the best thing they can do is leave
them alone in the right way.[10]
I'm speaking here of IT startups; in biotech things are different.[11]
This is an instance of a more general rule: focus on users,
not competitors.  The most important information about competitors
is what you learn via users anyway.[12]
In practice most successful startups have elements of both.
And you can describe each strategy in terms of the other by adjusting
the boundaries of what you call the market.  But it's useful to
consider these two ideas separately.[13]
I almost hesitate to raise that point though.  Startups are
businesses; the point of a business is to make money; and with that
additional constraint, you can't expect you'll be able to spend all
your time working on what interests you most.[14]
The need has to be a strong one.  You can retroactively
describe any made-up idea as something you need.  But do you really
need that recipe site or local event aggregator as much as Drew
Houston needed Dropbox, or Brian Chesky and Joe Gebbia needed Airbnb?Quite often at YC I find myself asking founders ""Would you use this
thing yourself, if you hadn't written it?"" and you'd be surprised
how often the answer is no.[15]
Paul Buchheit points out that trying to sell something bad
can be a source of better ideas:""The best technique I've found for dealing with YC companies that
have bad ideas is to tell them to go sell the product ASAP (before
wasting time building it). Not only do they learn that nobody
wants what they are building, they very often come back with a
real idea that they discovered in the process of trying to sell
the bad idea.""[16]
Here's a recipe that might produce the next Facebook, if
you're college students.  If you have a connection to one of the
more powerful sororities at your school, approach the queen bees
thereof and offer to be their personal IT consultants, building
anything they could imagine needing in their social lives that
didn't already exist.  Anything that got built this way would be
very promising, because such users are not just the most demanding
but also the perfect point to spread from.I have no idea whether this would work.[17]
And the reason it used a TV for a monitor is that Steve Wozniak
started out by solving his own problems.  He, like most of his
peers, couldn't afford a monitor.Thanks to Sam Altman, Mike Arrington, Paul Buchheit, John Collison,
Patrick Collison, Garry Tan, and Harj Taggar for reading drafts of
this, and Marc Andreessen, Joe Gebbia, Reid Hoffman, Shel Kaphan,
Mike Moritz and Kevin Systrom for answering my questions about
startup history.Japanese TranslationItalian TranslationSpanish Translation","startup advice
",human,"human
","human
"
48,48,"November 2008One of the differences between big companies and startups is that
big companies tend to have developed procedures to protect themselves
against mistakes.  A startup walks like a toddler, bashing
into things and falling over all the time.  A big company is more
deliberate.The gradual accumulation of checks in an organization is a kind of
learning, based on disasters that have happened to it or others
like it.  After giving a contract to a supplier who goes bankrupt
and fails to deliver, for example, a company might require all
suppliers to prove they're solvent before submitting bids.As companies grow they invariably get more such checks, either in
response to disasters they've suffered, or (probably more often)
by hiring people from bigger companies who bring with them customs
for protecting against new types of disasters.It's natural for organizations to learn from mistakes.  The problem
is, people who propose new checks almost never consider that the
check itself has a cost.Every check has a cost. For example, consider the case of making
suppliers verify their solvency.  Surely that's mere prudence?  But
in fact it could have substantial costs.  There's obviously the
direct cost in time of the people on both sides who supply and check
proofs of the supplier's solvency.  But the real costs are the ones
you never hear about: the company that would be the best supplier,
but doesn't bid because they can't spare the effort to get verified.
Or the company that would be the best supplier, but falls just short
of the threshold for solvency—which will of course have been set
on the high side, since there is no apparent cost of increasing it.Whenever someone in an organization proposes to add a new check,
they should have to explain not just the benefit but the cost.  No
matter how bad a job they did of analyzing it, this meta-check would
at least remind everyone there had to be a cost, and send them
looking for it.If companies started doing that, they'd find some surprises.  Joel
Spolsky recently spoke at Y Combinator about selling software to
corporate customers.  He said that in most companies software costing
up to about $1000 could be bought by individual managers without
any additional approvals.  Above that threshold, software purchases
generally had to be approved by a committee.  But babysitting this
process was so expensive for software vendors that it didn't make
sense to charge less than $50,000.  Which means if you're making
something you might otherwise have charged $5000 for, you have to
sell it for $50,000 instead.The purpose of the committee is presumably to ensure that the company
doesn't waste money. And yet the result is that the company pays
10 times as much.Checks on purchases will always be expensive, because the harder
it is to sell something to you, the more it has to cost.  And not
merely linearly, either.  If you're hard enough to sell to, the
people who are best at making things don't want to bother.  The
only people who will sell to you are companies that specialize in
selling to you.  Then you've sunk to a whole new level of inefficiency.
Market mechanisms no longer protect you, because the good suppliers
are no longer in the market.Such things happen constantly to the biggest organizations of all,
governments.  But checks instituted by governments can cause much
worse problems than merely overpaying.  Checks instituted by
governments can cripple a country's whole economy.  Up till about
1400, China was richer and more technologically advanced than Europe.
One reason Europe pulled ahead was that the Chinese government
restricted long trading voyages.  So it was left to the Europeans
to explore and eventually to dominate the rest of the world, including
China.In more recent times, Sarbanes-Oxley has practically destroyed the
US IPO market.  That wasn't the intention of the legislators who
wrote it.  They just wanted to add a few more checks on public
companies.  But they forgot to consider the cost.  They forgot that
companies about to go public are usually rather stretched, and that
the weight of a few extra checks that might be easy for General
Electric to bear are enough to prevent younger companies from being
public at all.Once you start to think about the cost of checks, you can start to
ask other interesting questions. Is the cost increasing or decreasing?
Is it higher in some areas than others?  Where does it increase
discontinuously?  If large organizations started to ask questions
like that, they'd learn some frightening things.I think the cost of checks may actually be increasing.  The reason
is that software plays an increasingly important role in companies,
and the people who write software are particularly harmed by checks.Programmers are unlike many types of workers in that the best ones
actually prefer to work hard.  This doesn't seem to be the case in
most types of work.  When I worked in fast food, we didn't prefer
the busy times.  And when I used to mow lawns, I definitely didn't
prefer it when the grass was long after a week of rain.Programmers, though, like it better when they write more code.  Or
more precisely, when they release more code.  Programmers like to
make a difference.  Good ones, anyway.For good programmers, one of the best things about working for a
startup is that there are few checks on releases.  In true startups,
there are no external checks at all.  If you have an idea for a new
feature in the morning, you can write it and push it to the production
servers before lunch.  And when you can do that, you have more
ideas.At big companies, software has to go through various approvals
before it can be launched.  And the cost of doing this can be
enormous—in fact, discontinuous.  I was talking recently to a
group of three programmers whose startup had been acquired a few
years before by a big company.  When they'd been independent, they
could release changes instantly.  Now, they said, the absolute
fastest they could get code released on the production servers was
two weeks.This didn't merely make them less productive.  It made them hate
working for the acquirer.Here's a sign of how much programmers like to be able to work hard:
these guys would have paid to be able to release code immediately,
the way they used to.  I asked them if they'd trade 10% of the
acquisition price for the ability to release code immediately, and
all three instantly said yes.  Then I asked what was the maximum
percentage of the acquisition price they'd trade for it.  They said
they didn't want to think about it, because they didn't want to
know how high they'd go, but I got the impression it might be as
much as half.They'd have sacrificed hundreds of thousands of dollars, perhaps
millions, just to be able to deliver more software to users.  And
you know what?  It would have been perfectly safe to let them.  In
fact, the acquirer would have been better off; not only wouldn't
these guys have broken anything, they'd have gotten a lot more done.
So the acquirer is in fact getting worse performance at greater
cost.  Just like the committee approving software purchases.And just as the greatest danger of being hard to sell to is not
that you overpay but that the best suppliers won't even sell to
you, the greatest danger of applying too many checks to your
programmers is not that you'll make them unproductive, but that
good programmers won't even want to work for you.Steve Jobs's famous maxim ""artists ship"" works both ways.  Artists
aren't merely capable of shipping.  They insist on it.  So if you
don't let people ship, you won't have any artists.","startup advice
",human,"human
","human
"
49,49,"January 2015Corporate Development, aka corp dev, is the group within companies
that buys other companies. If you're talking to someone from corp
dev, that's why, whether you realize it yet or not.It's usually a mistake to talk to corp dev unless (a) you want to
sell your company right now and (b) you're sufficiently likely to
get an offer at an acceptable price.  In practice that means startups
should only talk to corp dev when they're either doing really well
or really badly.  If you're doing really badly, meaning the company
is about to die, you may as well talk to them, because you have
nothing to lose. And if you're doing really well, you can safely
talk to them, because you both know the price will have to be high,
and if they show the slightest sign of wasting your time, you'll
be confident enough to tell them to get lost.The danger is to companies in the middle.  Particularly to young
companies that are growing fast, but haven't been doing it for long
enough to have grown big yet.  It's usually a mistake for a promising
company less than a year old even to talk to corp dev.But it's a mistake founders constantly make.  When someone from
corp dev wants to meet, the founders tell themselves they should
at least find out what they want.  Besides, they don't want to
offend Big Company by refusing to meet.Well, I'll tell you what they want.  They want to talk about buying
you.  That's what the title ""corp dev"" means.   So before agreeing
to meet with someone from corp dev, ask yourselves, ""Do we want to
sell the company right now?""  And if the answer is no, tell them
""Sorry, but we're focusing on growing the company.""  They won't be
offended.  And certainly the founders of Big Company won't be
offended. If anything they'll think more highly of you.  You'll
remind them of themselves.  They didn't sell either; that's why
they're in a position now to buy other companies.
[1]Most founders who get contacted by corp dev already know what it
means.  And yet even when they know what corp dev does and know
they don't want to sell, they take the meeting.  Why do they do it?
The same mix of denial and wishful thinking that underlies most
mistakes founders make. It's flattering to talk to someone who wants
to buy you.  And who knows, maybe their offer will be surprisingly
high.  You should at least see what it is, right?No.  If they were going to send you an offer immediately by email,
sure, you might as well open it.  But that is not how conversations
with corp dev work.  If you get an offer at all, it will be at the
end of a long and unbelievably distracting process.  And if the
offer is surprising, it will be surprisingly low.Distractions are the thing you can least afford in a startup.  And
conversations with corp dev are the worst sort of distraction,
because as well as consuming your attention they undermine your
morale.  One of the tricks to surviving a grueling process is not
to stop and think how tired you are.  Instead you get into a sort
of flow. 
[2]
Imagine what it would do to you if at mile 20 of a
marathon, someone ran up beside you and said ""You must feel really
tired.  Would you like to stop and take a rest?""  Conversations
with corp dev are like that but worse, because the suggestion of
stopping gets combined in your mind with the imaginary high price
you think they'll offer.And then you're really in trouble.  If they can, corp dev people
like to turn the tables on you. They like to get you to the point
where you're trying to convince them to buy instead of them trying
to convince you to sell.  And surprisingly often they succeed.This is a very slippery slope, greased with some of the most powerful
forces that can work on founders' minds, and attended by an experienced
professional whose full time job is to push you down it.Their tactics in pushing you down that slope are usually fairly
brutal. Corp dev people's whole job is to buy companies, and they
don't even get to choose which.  The only way their performance is
measured is by how cheaply they can buy you, and the more ambitious
ones will stop at nothing to achieve that. For example, they'll
almost always start with a lowball offer, just to see if you'll
take it. Even if you don't, a low initial offer will demoralize you
and make you easier to manipulate.And that is the most innocent of their tactics. Just wait till
you've agreed on a price and think you have a done deal, and then
they come back and say their boss has vetoed the deal and won't do
it for more than half the agreed upon price. Happens all the time.
If you think investors can behave badly, it's nothing compared to
what corp dev people can do.  Even corp dev people at companies
that are otherwise benevolent.I remember once complaining to a
friend at Google about some nasty trick their corp dev people had
pulled on a YC startup.""What happened to Don't be Evil?"" I asked.""I don't think corp dev got the memo,"" he replied.The tactics you encounter in M&A conversations can be like nothing
you've experienced in the otherwise comparatively 
upstanding world
of Silicon Valley.  It's as if a chunk of genetic material from the
old-fashioned robber baron business world got incorporated into the
startup world.
[3]The simplest way to protect yourself is to use the trick that John
D. Rockefeller, whose grandfather was an alcoholic, used to protect
himself from becoming one.  He once told a Sunday school class

  Boys, do you know why I never became a drunkard?  Because I never
  took the first drink.

Do you want to sell your company right now?  Not eventually, right
now.  If not, just don't take the first meeting.  They won't be
offended.  And you in turn will be guaranteed to be spared one of
the worst experiences that can happen to a startup.If you do want to sell, there's another set of 
techniques
 for doing
that.  But the biggest mistake founders make in dealing with corp
dev is not doing a bad job of talking to them when they're ready
to, but talking to them before they are.  So if you remember only
the title of this essay, you already know most of what you need to
know about M&A in the first year.Notes[1]
I'm not saying you should never sell.  I'm saying you should
be clear in your own mind about whether you want to sell or not,
and not be led by manipulation or wishful thinking into trying to
sell earlier than you otherwise would have.[2]
In a startup, as in most competitive sports, the task at hand
almost does this for you; you're too busy to feel tired.  But when
you lose that protection, e.g. at the final whistle, the fatigue
hits you like a wave.  To talk to corp dev is to let yourself feel
it mid-game.[3]
To be fair, the apparent misdeeds of corp dev people are magnified
by the fact that they function as the face of a large organization
that often doesn't know its own mind.  Acquirers can be surprisingly
indecisive about acquisitions, and their flakiness is indistinguishable
from dishonesty by the time it filters down to you.Thanks to Marc Andreessen, Jessica Livingston, Geoff
Ralston, and Qasar Younis for reading drafts of this.","startup advice
",human,"human
","human
"
50,50,"March 2021The secret curse of the nonprofit world is restricted donations.
If you haven't been involved with nonprofits, you may never have
heard this phrase before. But if you have been, it probably made
you wince.Restricted donations mean donations where the donor limits what can
be done with the money. This is common with big donations, perhaps
the default. And yet it's usually a bad idea. Usually the way the
donor wants the money spent is not the way the nonprofit would have
chosen. Otherwise there would have been no need to restrict the
donation. But who has a better understanding of where money needs
to be spent, the nonprofit or the donor?If a nonprofit doesn't understand better than its donors where money
needs to be spent, then it's incompetent and you shouldn't be
donating to it at all.Which means a restricted donation is inherently suboptimal. It's
either a donation to a bad nonprofit, or a donation for the wrong
things.There are a couple exceptions to this principle. One is when the
nonprofit is an umbrella organization. It's reasonable to make a
restricted donation to a university, for example, because a university
is only nominally a single nonprofit. Another exception is when the
donor actually does know as much as the nonprofit about where money
needs to be spent. The Gates Foundation, for example, has specific
goals and often makes restricted donations to individual nonprofits
to accomplish them. But unless you're a domain expert yourself or
donating to an umbrella organization, your donation would do more
good if it were unrestricted.If restricted donations do less good than unrestricted ones, why
do donors so often make them? Partly because doing good isn't donors'
only motive. They often have other motives as well — to make a mark,
or to generate good publicity
[1],
or to comply with regulations
or corporate policies. Many donors may simply never have considered
the distinction between restricted and unrestricted donations. They
may believe that donating money for some specific purpose is just
how donation works. And to be fair, nonprofits don't try very hard
to discourage such illusions. They can't afford to. People running
nonprofits are almost always anxious about money. They can't afford
to talk back to big donors.You can't expect candor in a relationship so asymmetric. So I'll
tell you what nonprofits wish they could tell you. If you want to
donate to a nonprofit, donate unrestricted. If you trust them to
spend your money, trust them to decide how.
Note[1]
Unfortunately restricted donations tend to generate more
publicity than unrestricted ones. ""X donates money to build a school
in Africa"" is not only more interesting than ""X donates money to Y
nonprofit to spend as Y chooses,"" but also focuses more attention
on X.
Thanks to Chase Adam, Ingrid Bassett, Trevor Blackwell, and Edith
Elliot for reading drafts of this.","startup advice
",human,"human
","human
"
51,51,"October 2005The first Summer Founders Program has just finished.  We were
surprised how well it went.  Overall only about 10% of startups   
succeed, but if I had to guess now, I'd predict three or four of  
the eight startups we funded will make it.Of the startups that needed further funding, I believe all have
either closed a round or are likely to soon.  Two have already
turned down (lowball) acquisition offers.We would have been happy if just one of the eight seemed promising
by the end of the summer.  What's going on?  Did some kind of anomaly
make this summer's applicants especially good?  We worry about that,
but we can't think of one.  We'll find out this winter.The whole summer was full of surprises.  The best was that the hypothesis we were testing seems to be
correct.  Young hackers can start viable companies.  This is good
news for two reasons: (a) it's an encouraging thought, and (b) it 
means that Y Combinator, which is predicated on the idea, is not
hosed.AgeMore precisely, the hypothesis was that success in a startup depends
mainly on how smart and energetic you are, and much less on how old
you are or how much business experience you have.  The results so
far bear this out.  The 2005 summer founders ranged in age from 18  
to 28 (average 23), and there is no correlation between their ages
and how well they're doing.This should not really be surprising. Bill Gates and Michael Dell 
were both 19 when they started the companies that made them famous.
Young founders are not a new phenomenon: the trend began as soon
as computers got cheap enough for college kids to afford them.Another of our hypotheses was that you can start a startup on less
money than most people think.  Other investors were surprised to
hear the most we gave any group was $20,000.  But we knew it was
possible to start on that little because we started Viaweb on
$10,000.And so it proved this summer.  Three months' funding is enough to
get into second gear.  We had a demo day for potential investors
ten weeks in, and seven of the eight groups had a prototype ready
by that time.  One, Reddit, had
already launched, and were able to give a demo of their live site.A researcher who studied the SFP startups said the one thing they 
had in common was that they all worked ridiculously hard.  People
this age are commonly seen as lazy.  I think in some cases it's not
so much that they lack the appetite for work, but that the work
they're offered is unappetizing.The experience of the SFP suggests that if you let motivated people
do real work, they work hard, whatever their age.  As one of the
founders said ""I'd read that starting a startup consumed your life,  
but I had no idea what that meant until I did it.""I'd feel guilty if I were a boss making people work this hard.  But
we're not these people's bosses.  They're working on their own
projects.  And what makes them work is not us but their competitors.
Like good athletes, they don't work hard because the coach yells
at them, but because they want to win.We have less power than bosses, and yet the founders work harder  
than employees.  It seems like a win for everyone.  The only catch
is that we get on average only about 5-7% of the upside, while an
employer gets nearly all of it.  (We're counting on it being 5-7%
of a much larger number.)As well as working hard, the groups all turned out to be extraordinarily
responsible.  I can't think of a time when one failed to do something
they'd promised to, even by being late for an appointment.  This
is another lesson the world has yet to learn.  One of the founders
discovered that the hardest part of arranging a meeting with
executives at a big cell phone carrier was getting a rental company
to rent him a car, because he was too young.I think the problem here is much the same as with the apparent
laziness of people this age.  They seem lazy because the work they're
given is pointless, and they act irresponsible because they're not
given any power.  Some of them, anyway.  We only have a sample size
of about twenty, but it seems so far that if you let people in their
early twenties be their own bosses, they rise to the occasion.MoraleThe summer founders were as a rule very idealistic.  They also  
wanted very much to get rich.  These qualities might seem incompatible,
but they're not.  These guys want to get rich, but they want to do
it by changing the world.  They wouldn't (well, seven of the eight
groups wouldn't) be interested in making money by speculating in
stocks.  They want to make something people use.I think this makes them more effective as founders.  As hard as  
people will work for money, they'll work harder for a cause.  And   
since success in a startup depends so much on motivation, the
paradoxical result is that the people likely to make the most money
are those who aren't in it just for the money.The founders of Kiko, for example,   
are working on an Ajax calendar.  They want to get rich, but they
pay more attention to design than they would if that were their
only motivation.  You can tell just by looking at it.I never considered it till this summer, but this might be another
reason startups run by hackers tend to do better than those run by
MBAs.  Perhaps it's not just that hackers understand technology
better, but that they're driven by more powerful motivations.
Microsoft, as I've said before, is a dangerously misleading example.  
Their mean corporate culture only works for monopolies.   
Google is a better model.Considering that the summer founders are the sharks in this ocean,
we were surprised how frightened most of them were of competitors.
But now that I think of it, we were just as frightened when we
started Viaweb.  For the first year, our initial reaction to news 
of a competitor was always: we're doomed.  Just as a hypochondriac
magnifies his symptoms till he's convinced he has some terrible
disease, when you're not used to competitors you magnify them into
monsters.Here's a handy rule for startups: competitors are rarely as dangerous
as they seem.  Most will self-destruct before you can destroy them.
And it certainly doesn't matter how many of them there are, any
more than it matters to the winner of a marathon how many runners
are behind him.""It's a crowded market,"" I remember one founder saying worriedly.""Are you the current leader?"" I asked.""Yes.""""Is anyone able to develop software faster than you?""""Probably not.""""Well, if you're ahead now, and you're the fastest, then you'll
stay ahead.  What difference does it make how many others there
are?""Another group was worried when they realized they had to rewrite
their software from scratch.  I told them it would be a bad sign
if they didn't.  The main function of your initial version is to  
be rewritten.That's why we advise groups to ignore issues like scalability,
internationalization, and heavy-duty security at first. [1] I can
imagine an advocate of ""best practices"" saying these ought to be
considered from the start.  And he'd be right, except that they
interfere with the primary function of software in a startup: to  
be a vehicle for experimenting with its own design.  Having to
retrofit internationalization or scalability is a pain, certainly.  
The only bigger pain is not needing to, because your initial version
was too big and rigid to evolve into something users wanted.I suspect this is another reason startups beat big companies.
Startups can be irresponsible and release version 1s that are light
enough to evolve.  In big companies, all the pressure is in the   
direction of over-engineering.What Got LearnedOne thing we were curious about this summer was where these groups  
would need help.  That turned out to vary a lot.  Some we helped
with technical advice-- for example, about how to set up an application
to run on multiple servers.  Most we helped with strategy questions,
like what to patent, and what to charge for and what to give away.
Nearly all wanted advice about dealing with future investors: how  
much money should they take and what kind of terms should they
expect?However, all the groups quickly learned how to deal with stuff like
patents and investors.  These problems aren't intrinsically difficult,
just unfamiliar.It was surprising-- slightly frightening even-- how fast they
learned.  The weekend before the demo day for investors, we had a  
practice session where all the groups gave their presentations.  
They were all terrible.  We tried to explain how to make them better,
but we didn't have much hope.  So on demo day I told the assembled
angels and VCs that these guys were hackers, not MBAs, and so while
their software was good, we should not expect slick presentations 
from them.The groups then proceeded to give fabulously slick presentations. 
Gone were the mumbling recitations of lists of features.   It was
as if they'd spent the past week at acting school.  I still don't 
know how they did it.Perhaps watching each others' presentations helped them see what
they'd been doing wrong.  Just as happens in college, the summer   
founders learned a lot from one another-- maybe more than they
learned from us.  A lot of the problems they face are the same,  
from dealing with investors to hacking Javascript.I don't want to give the impression there were no problems this  
summer.  A lot went wrong, as usually happens with startups.  One
group got an ""exploding
term-sheet"" from some VCs.  Pretty much all the groups who had
dealings with big companies found that big companies do everything
infinitely slowly.  (This is to be expected.  If big companies
weren't incapable, there would be no room for startups to exist.)
And of course there were the usual nightmares associated with
servers.  In short, the disasters this summer were just the usual childhood
diseases.  Some of this summer's eight startups will   
probably die eventually; it would be extraordinary if all eight 
succeeded.  But what kills them will not be dramatic, external    
threats, but a mundane, internal one: not getting enough done.So far, though, the news is all good.  In fact, we were surprised
how much fun the summer was for us.  The main reason was how much
we liked the founders.  They're so earnest and hard-working.  They
seem to like us too.  And this illustrates another advantage of
investing over hiring: our relationship with them is way better   
than it would be between a boss and an employee.  Y Combinator ends
up being more like an older brother than a parent.I was surprised how much time I spent making introductions.
Fortunately I discovered that when a startup needed to talk to
someone, I could usually get to the right person by at most one
hop.  I remember wondering, how did my friends get to be so eminent?
and a second later realizing: shit, I'm forty.Another surprise was that the three-month batch format,
which we were forced into by the constraints of the summer, turned
out to be an advantage.  When we started Y Combinator, we planned
to invest the way other venture firms do: as proposals came in,     
we'd evaluate them and decide yes or no.  The SFP
was just an experiment to get things started.  But it worked so
well that we plan to do 
all 
our investing this way, one cycle in
the summer and one in winter.  It's more efficient for us, and
better for the startups too.Several groups said our weekly dinners saved them from a common
problem afflicting startups: working so hard that one has no social
life.  (I remember that part all too well.)  This way, they were
guaranteed a social event at least once a week.IndependenceI've heard Y Combinator described as an ""incubator.""  Actually we're
the opposite: incubators exert more control than ordinary VCs, and
we make a point of exerting less.  Among other things, incubators
usually make you work in their office-- that's where the 
word ""incubator"" comes from.  That seems the wrong model.  If
investors get too involved, they smother one of the most powerful 
forces in a startup: the feeling that it's your own company.Incubators were conspicuous failures during the Bubble.  There's  
still debate about whether this was because of the Bubble, or because
they're a bad idea.  My vote is they're a bad idea.  I think they 
fail because they select for the wrong people.  When we were starting
a startup, we would never have taken funding from an ""incubator.""
We can find office space, thanks; just give us the money.  And  
people with that attitude are the ones likely to succeed in startups.Indeed, one quality all the founders shared this summer was a spirit
of independence.  I've been wondering about that.  Are some people
just a lot more independent than others, or would everyone be this
way if they were allowed to?As with most nature/nurture questions, the answer is probably: some
of each.  But my main conclusion from the summer is that there's
more environment in the mix than most people realize.  I could see
that from how the founders' attitudes changed during the   
summer.  Most were emerging from twenty or so years of being told
what to do.  They seemed a little surprised at having total freedom.
But they grew into it really quickly; some of these guys now seem
about four inches taller (metaphorically) than they did at the
beginning of the summer.When we asked the summer founders what surprised them most about
starting a company, one said ""the most shocking thing is that it 
worked.""It will take more experience to know for sure, but my guess is that
a lot of hackers could do this-- that if you put people in a position
of independence, they develop the qualities they need.  Throw them
off a cliff, and most will find on the way down that they have   
wings.The reason this is news to anyone is that the same forces work in
the other direction too. Most hackers are 
employees, and this molds
you into someone to whom starting a startup seems impossible as
surely as starting a startup molds you into someone who can handle
it.If I'm right, ""hacker"" will mean something different in twenty years
than it does now. Increasingly it will mean the people who run the
company.  Y Combinator is just accelerating a process that would
have happened anyway.  Power is shifting from the people who deal
with money to the people who create technology, and if our experience
this summer is any guide, this will be a good thing.Notes[1] By heavy-duty security I mean efforts to protect against truly
determined attackers.The image
shows us, the 2005 summer founders, and Smartleaf
co-founders Mark Nitzberg and Olin Shivers at the 30-foot table 
Kate Courteau designed for us. Photo by Alex Lewin.Thanks to Sarah Harlin, Steve Huffman, Jessica Livingston,
Zak Stone, and Aaron Swartz for reading drafts of this.
Romanian TranslationJapanese Translation","startup advice
",human,"human
","human
"
52,52,"

Want to start a startup?  Get funded by
Y Combinator.




May 2004
(This essay was originally published in Hackers 
& Painters.)
If you wanted to get rich, how would you do it? I think your best
bet would be to start or join a startup.  That's been a 
reliable way to get rich for hundreds of years.  The word ""startup"" 
dates from the 1960s, but what happens in one is 
very similar to the venture-backed trading voyages of the
Middle Ages.Startups usually involve technology, so much so that the phrase
""high-tech startup"" is almost redundant.  A startup is a small
company that takes on a hard technical problem.Lots of people get rich knowing nothing more than that.
You don't have to know physics to be a good pitcher.  But
I think it could give you an edge to understand the underlying principles.
Why do startups have to be small?  
Will a startup inevitably stop being a startup as it
grows larger?  
And why do they so often work on
developing new technology?   Why are there so many startups
selling new drugs or computer software, and none selling corn oil
or laundry detergent?The PropositionEconomically, you can think of a startup as a way to 
compress your whole working life into a few years.  Instead
of working at a low intensity for forty years, you work as
hard as you possibly can for four.  This pays especially well
in technology, where you earn a premium for working fast.Here is a brief sketch of the economic proposition.  If you're
a good hacker in your mid twenties, you can
get a job paying about $80,000 per year.  So on average 
such a hacker must be
able to do at least $80,000 worth of work per year for the 
company just to break even.  You could probably
work twice as many hours as a corporate employee, and if
you focus you can probably get three times as much done in
an hour. 
[1]
You should get another multiple of two, at
least, by eliminating the drag 
of the pointy-haired middle
manager who would be your boss in a big company.
Then there is one more multiple: how much smarter are you
than your job description expects you to be?
Suppose another multiple of three.  Combine all these multipliers, and I'm
claiming you could be 36 times more 
productive than you're expected to be in a random corporate
job. 
[2]
  If a fairly good hacker is worth $80,000 a year at a 
big company, then a smart
hacker working very hard without any corporate
bullshit to slow him down should be able to do work worth about
$3 million a year.Like all back-of-the-envelope calculations, this one
has a lot of wiggle room.  I wouldn't try to
defend the actual numbers.  But I stand by the 
structure of the calculation.  I'm not claiming
the multiplier is precisely 36, but it is certainly more
than 10, and probably rarely as high as 100.If $3 million a year seems
high, remember that we're talking about the limit case:
the case where you not only have zero leisure time
but indeed work so hard that you endanger your health.Startups are not magic.  They don't change the laws of
wealth creation.  They just represent a point at the far end of the curve.
There is a conservation law at work here: if
you want to make a million dollars, you have to endure a 
million dollars' worth of pain.  
For example, one way to
make a million dollars would be to work for the 
Post Office your whole life, and save every penny of your 
salary.  Imagine the stress of working for the Post 
Office for fifty years.   In a startup you compress all
this stress into three or four years.  You do tend to get a 
certain 
bulk discount if you buy the economy-size pain,
but you can't evade the fundamental conservation law.
If starting a startup were easy, everyone would do it.Millions, not BillionsIf $3 million a year seems high to some people, it will seem
low to others.  Three million? 
How do I get to be a billionaire, like Bill Gates?So let's get Bill Gates out of the way right now.  It's not
a good idea to use famous rich people 
as examples, because the press only 
write about the very richest, and these tend to be outliers.
Bill Gates is a smart, determined, and hardworking man,
but you need more than
that to make as much money as he has.  You also need to be
very lucky.There is a large random
factor in the success of any company.  So the guys you end 
up reading about in the papers are the ones who are very 
smart, totally dedicated, and win the lottery.
Certainly Bill is smart and dedicated, but Microsoft also 
happens to have been the beneficiary of one of the most spectacular
blunders in the history of business: the licensing deal for
DOS.  No doubt Bill did 
everything he could to steer IBM into making that blunder, 
and he has done an excellent job of exploiting it, but if
there had been one person with a brain on IBM's side,
Microsoft's future would have been very different.
Microsoft at that stage had little leverage over IBM.
They were effectively a component supplier.  If IBM had 
required an exclusive license, as they should have, Microsoft
would still have signed the deal.  It would still have
meant a lot of money for them,  and IBM
could easily have gotten an operating system elsewhere.Instead IBM ended up using all its power in the market
to give Microsoft control of the PC standard.  From 
that point, all Microsoft had to do was execute.  They
never had to bet the company on a bold decision.  All they
had to do was play hardball with licensees and copy more
innovative products reasonably promptly.If IBM hadn't made this mistake, Microsoft would
still have been a successful company, but it
could not have grown so big so fast. 
Bill Gates would be rich, but he'd be somewhere
near the bottom of the Forbes 400 with the other guys his age.There are a lot of ways to get
rich, and this essay is about only one of them.  This
essay is about how to make money by creating wealth and
getting paid for it.  There are plenty of other ways to 
get money, including chance, speculation, marriage, inheritance, 
theft, extortion, fraud, monopoly,
graft, lobbying,
counterfeiting, and prospecting.  Most of the greatest fortunes
have probably involved several of these.The advantage of creating wealth, as a way to get rich,
is not just that it's more legitimate 
(many of the other methods are now illegal) 
but that it's more
straightforward.  You just have to do something people want.Money Is Not WealthIf you want to create wealth, it will help to understand what it is.  
Wealth is not the same thing as money. 
[3]
  Wealth is as old as
human history.  Far older, in fact; ants have wealth. 
Money is a comparatively recent invention.Wealth is the fundamental thing.  Wealth is stuff we want: food, 
clothes, houses, cars, gadgets, travel to interesting places,
and so on.  You can have wealth without
having money.  If you had a magic machine that
could on command make you a car or cook you dinner or do your
laundry, or do anything else you wanted, you wouldn't need money.
Whereas if you were in the middle of Antarctica, where there is
nothing to buy, it wouldn't matter how much money you had.Wealth is what you want, not money.  But if wealth is the important
thing, why does everyone talk about making money?   It is
a kind of shorthand: money is a way of moving wealth, and in practice
they are usually interchangeable.  But they are not the same thing,
and unless you plan to get rich by counterfeiting, talking about
making money can make it harder to understand how to 
make money.Money is a side effect of specialization.
In a specialized society, most of the
things you need, you can't make for yourself.  If you want a potato
or a pencil or a place to live, you have to get it from someone
else.How do you get the person who grows the potatoes to give you some?
By giving him something he wants in return.  But you can't get
very far by trading things directly with the people who
need them.  If you make violins, and none of the local
farmers wants one, how will you eat?The solution societies find, as they get more specialized, is to
make the trade into a two-step process.  Instead of trading violins
directly for potatoes, you trade violins for, say, silver, 
which you can then trade again for anything else you need.  The
intermediate stuff-- the medium of exchange-- can be anything that's
rare and portable.  Historically metals have been the most common,
but recently we've been using a medium of exchange, called the dollar,
that doesn't physically exist.  It works as a medium of exchange,
however, because its rarity 
is guaranteed by the U.S. Government.The advantage of a medium of exchange is that it makes trade work.
The disadvantage is that it tends to obscure what trade really
means.  People think that what a business does is make money.
But money is just the intermediate stage-- just
a shorthand-- for whatever people want.
What most businesses really do is make  
wealth.  They do something people want. 
[4]The Pie FallacyA surprising number of people retain from childhood the idea
that there is a fixed amount of wealth in the world. 
There is, in any normal family, a fixed amount of money at 
any moment.  But that's not the same thing.When wealth is talked about in this context, it is often
described as a pie.  ""You can't make the pie larger,""
say politicians.
When you're
talking about the amount of money in one family's bank
account, or the amount available to a government from one
year's tax revenue, this is true.  
If one person gets more, someone else has to get less.I can remember believing, as a child, that if a few
rich people had all the money, it left less for everyone else.
Many people seem to continue to believe something like this
well into adulthood.  This fallacy is usually there in the 
background when you hear someone talking about how x percent
of the population have y percent of the wealth.  If you plan
to start a startup, then whether you realize it or not, you're
planning to disprove the Pie Fallacy.What leads people astray here is the abstraction of
money.  Money is not wealth.  It's
just something we use to move wealth around.
So although there may be, in certain specific moments (like
your family, this month) a fixed amount of money available to
trade with other people for things you want,
there is not a fixed amount of wealth in the world.  
You can make more wealth.  Wealth has been getting created and
destroyed (but on balance, created) for all of human history.Suppose you own a beat-up old car. 
Instead of sitting on your butt next
summer, you could spend the time restoring your car to pristine condition.
In doing so you create wealth.  The world is-- and
you specifically are-- one pristine old car the richer.  And not
just in some metaphorical way.  If you sell your car,
you'll get more for it.In restoring your old car you have made yourself
richer.  You haven't made anyone else poorer.  So there is
obviously not a fixed pie.  And in fact, when you look at 
it this way, you wonder why anyone would think there was. 
[5]Kids know, without knowing they know, that they can create
wealth.  If you need to give someone a present and don't
have any money, you make one.  But kids are so bad at making
things that they consider home-made presents to be a distinct,
inferior, sort of thing to store-bought ones-- a mere expression
of the proverbial thought that counts. 
And indeed, the lumpy ashtrays
we made for our parents did not have much of a resale market.CraftsmenThe people most likely to grasp that wealth can be
created are the ones who are good at making things, the craftsmen.
Their hand-made objects become store-bought ones. 
But with the rise of industrialization there are fewer and
fewer craftsmen.  One of the biggest remaining groups  is
computer programmers.A programmer can sit down in front of a computer and
create wealth.  A good piece of software is, in itself, 
a valuable thing.
There is no manufacturing to confuse the issue.  Those
characters you type 
are a complete, finished product.
If someone sat down and wrote a web
browser that didn't suck (a fine idea, by the way), the world
would be that much richer.
[5b]Everyone in a company works together to create
wealth, in the sense of making more things people want.
Many of the employees (e.g. the people in the mailroom or
the personnel department) work at one remove from the 
actual making of stuff.  Not the programmers.  They
literally think the product, one line at a time.
And so it's clearer to programmers that wealth is something
that's made, rather than being distributed, like slices of a
pie, by some imaginary Daddy.It's also obvious to programmers that there are huge variations
in the rate at which wealth is created.  At Viaweb we had one
programmer who was a sort of monster of productivity.  
I remember watching what he did one long day and estimating that
he had added several hundred thousand dollars
to the market value of the company. 
A great programmer, on a roll, could 
create a million dollars worth of wealth in a couple weeks.
A mediocre programmer over the same period will generate zero or
even negative wealth (e.g. by introducing bugs).This is
why so many of the best programmers are libertarians.
In our world, you sink or swim, and there are no excuses.
When those far removed from the creation of wealth-- undergraduates,
reporters, politicians-- hear
that the richest 5% of the people have 
half the total wealth, they tend to think injustice!
An experienced programmer would be more likely to think
is that all?  The top 5% of programmers
probably write 99% of the good software.Wealth can be created without being sold.  Scientists, till
recently at least, effectively donated the wealth they 
created.  We are all richer for knowing about penicillin,
because we're less likely to die from infections.  Wealth
is whatever people want, and not dying is certainly something
we want.  Hackers often donate their work by 
writing open source software that anyone can use for free.
I am much the richer for the operating system
FreeBSD, which I'm running on the computer I'm using now,
and so is Yahoo, which runs it on all their servers.What a Job IsIn industrialized countries, people belong to one institution or
another at least until their twenties.  After all those years you get
used to the idea of belonging to a group of people who all get up
in the morning, go to some set of buildings, and do things that they
do not, ordinarily, enjoy doing.  Belonging to such a group becomes
part of your identity: name, age, role, institution.
If you have to introduce yourself, or
someone else describes you, it will be as something like, John
Smith, age 10, a student at such and such elementary school, or
John Smith, age 20, a student at such and such college.When John Smith finishes school he is expected to get a job.  And
what getting a job seems to mean is joining another institution.
Superficially it's a lot like college.  You pick the companies you
want to work for and apply to join them.  If one likes you, you
become a member of this new group.  You get up in the morning and
go to a new set of buildings, and do things that you do not, ordinarily,
enjoy doing.  There are a few differences: life is not as much fun,
and you get paid, instead of paying, as you did in college.  But
the similarities feel greater than the differences.  John Smith is
now John Smith, 22, a software developer at such and such corporation.In fact John Smith's
life has changed more than he realizes.  Socially, a company
looks much like college, but the deeper you go into the
underlying reality, the more different it gets.What a company does, and has to do if it wants to continue to
exist, is earn money.  And the way most companies make money
is by creating wealth.  Companies can be so specialized that this
similarity is concealed, but it is not only manufacturing 
companies that create wealth.  A big component of wealth is
location. 
Remember that magic machine that could
make you cars and cook you dinner and so on?  It would not be
so useful if it delivered your dinner to a random location
in central Asia.  
If wealth means what people want, companies that move
things also create wealth.  Ditto for
many other kinds of companies that don't make anything
physical.  Nearly all companies exist to do something people
want.And that's what you do, as well, when you go to work for a company.
But here there is another layer that tends to obscure the underlying
reality.  In a company, the work you do is averaged together with
a lot of other people's.  
You may not even be aware you're doing something people
want.  Your contribution may be indirect.  But the company as a
whole must be giving people something they want, or they won't make
any money.  And if they are paying you x dollars a year, then on
average you must be contributing at least x dollars a year worth
of work, or the company will be spending more than it makes,
and will go out of business.Someone graduating from college thinks, and is told, that he needs
to get a job, as if the important thing were becoming a member of 
an institution.  A more direct way to put it would be: you need to
start doing something people want.  You don't
need to
join a company to do that.  All a company is is a group of people
working together to do something people want.  It's doing something people
want that matters, not joining the group. 
[6]For most people the   
best plan probably is to go to work for some existing
company.  But it is a good idea to understand what's happening   
when you do this.  A job means doing something people want,
averaged together with everyone else in that company.Working HarderThat averaging gets to be a problem.
I think the single biggest problem afflicting large companies is the   
difficulty of assigning a value to each person's work. 
For the most part they punt.  In a
big company you get paid a fairly predictable salary for working 
fairly hard.  You're expected not to be obviously incompetent or
lazy, but you're not expected to devote your whole life to your
work.It turns out, though, that there are economies of scale in how much of your
life you devote to your work.  In the right kind of business,  
someone who really devoted himself to work could generate ten or
even a hundred times as much wealth as an average
employee.  A programmer, for example, instead of chugging along
maintaining and updating an existing piece of software, could write
a whole new piece of software, and with it create a new source of
revenue.Companies are not set up to reward people who want to do this. 
You can't go to your boss and say, I'd like to start working ten
times as hard, so will you please pay me ten times as much? For
one thing, the official fiction is that you are already working as
hard as you can.  But a more serious problem is that the company
has no way of measuring the value of your work.Salesmen are an exception.  It's easy 
to measure how much revenue they generate, and they're
usually paid a percentage of it.  If a salesman wants to work harder,
he can just start doing it, and he will automatically
get paid proportionally more.There is one other job besides sales where big companies can
hire first-rate people: in the top management jobs. 
And for the same reason: their performance can
be measured.  The top managers are
held responsible for the performance of the entire company.
Because an ordinary employee's performance can't usually
be measured, he is not expected to do
more than put in a solid effort.  Whereas top management, like
salespeople, have to actually come up with the numbers.
The CEO of a company that tanks cannot plead that he put in  
a solid effort.  If the company does badly, he's done badly.A company that could pay all its employees so straightforwardly   
would be enormously successful.  Many employees would work harder
if they could get paid for it.  More importantly,
such a company would attract people who wanted to work
especially hard. 
It would crush its competitors.Unfortunately, companies can't pay everyone like salesmen.  Salesmen
work alone.  Most employees' work is tangled together.  Suppose
a company makes some kind of consumer gadget.  The 
engineers build a reliable gadget with all kinds of new features;
the industrial designers design a beautiful case for it; and then
the marketing people convince everyone that
it's something they've got to have.  How do you know how much of the
gadget's sales are due to each group's efforts?  Or, for that
matter, how much is due to the creators of past gadgets that gave
the company a reputation for quality?  There's no way to  
untangle all their contributions.  Even if you could read the minds
of the consumers, you'd find these factors were all blurred together.If you want to go faster, it's a problem to have your work
tangled together with a large number of other people's.  In a  
large group, your performance is not separately measurable-- and 
the rest of the group slows you down.Measurement and LeverageTo get rich you need to get yourself in a situation with two
things, measurement and leverage.  You need to be in a
position where your performance can be measured, or there is
no way to get paid more by doing more.  And you have to
have leverage, in the sense that the decisions you make have   
a big effect.Measurement alone is not enough.  An example of a job with
measurement but not leverage is doing piecework in a
sweatshop.  Your performance is measured and you get paid  
accordingly, but you have no scope for decisions.  The only
decision you get to make is how fast you work, and that
can probably only increase your earnings by a factor
of two or three.An example of a job with both measurement and leverage would
be lead actor in a movie.  Your performance can be measured in the
gross of the movie.  And you have leverage in the sense that your
performance can make or break it.CEOs also have both measurement and leverage.  They're measured,
in that the performance of the company is their performance.
And they have leverage in that their decisions
set the whole company moving in one direction or another.I think everyone who gets rich by their own efforts will be
found to be in a situation with measurement and leverage.    
Everyone I can think of does: CEOs, movie stars, 
hedge fund managers, professional athletes.  A good hint to the
presence of leverage is the possibility of failure.
Upside must be balanced by downside, so if there is 
big potential for gain there must also be a terrifying
possibility of loss.  CEOs, stars, fund managers, and athletes
all live with the sword hanging over their heads;
the moment they start to suck, they're out.  If you're in
a job that feels safe, you are not going to get rich,
because if there is no danger there is almost certainly no leverage.But you don't have to become a CEO or a movie star to
be in a situation with measurement and leverage.  All you        
need to do is be part of a small group working on a
hard problem.Smallness = MeasurementIf you can't measure the value of the work done by individual  
employees, you can get close.  You can measure the value
of the work done by small groups.One level at which you can accurately measure the revenue
generated by employees is at the level of the whole company.   
When the company is small, you are thereby fairly close to 
measuring the contributions of individual employees.  A viable
startup might only have ten employees, which puts you within a
factor of ten of measuring individual effort.Starting or joining a startup is thus as close as most
people can get to saying to one's boss, I want to work ten times
as hard, so please pay me ten times as much.  There are two
differences: you're not saying it to your boss, but directly to the
customers (for whom your boss is only a proxy after all), and
you're not doing it individually, but along with a small group
of other ambitious people.It will, ordinarily, be a group.  Except in a few unusual kinds
of work, like acting or writing books, you can't be a company 
of one person.  
And the people you work with had better be good, because it's their work that
yours is going to be averaged with.A big company is like a giant galley driven by a thousand rowers.
Two things keep the speed of the
galley down.  One is that individual rowers don't see any
result from working harder. 
The other is that, in a group of a
thousand people, the average rower is likely  to be
pretty average.If you took ten people at random out of the big galley and
put them in a boat by themselves, they could probably go  
faster.  They would have both carrot and stick to motivate   
them.  An energetic rower would be encouraged by the thought
that he could have a visible effect on the speed of
the boat.  And if someone was lazy, the others would be more likely
to notice and complain.But the real advantage of the ten-man boat shows when 
you take the ten best rowers out of the big galley
and put them in a boat together.  They will have all
the extra motivation that comes from being in a small group.
But more importantly, by selecting that small a group
you can get the best rowers.  Each one will be in
the top 1%.  It's a much better deal for them to average  
their work together with a small group of their peers than to    
average it with everyone.That's the real point of startups.  Ideally, you are getting
together with a group of other people who also want to work
a lot harder, and get paid a lot more, than they would in
a big company.  And because startups tend to get founded 
by self-selecting groups of ambitious people who already 
know one another (at least by reputation), the level of 
measurement is more precise than you get from smallness alone.
A startup is not merely ten people, but ten people like you.Steve Jobs once said that the success or failure of a startup
depends on the first ten employees.  I agree. If 
anything, it's more like the first five.
Being small is not, in itself, what makes startups kick butt,   
but rather that small groups can be select.
You don't want small in the sense of a
village, but small in the sense of an all-star team.The larger a group, the closer its average member will be to the average
for the population as a whole.   So all other things being
equal, a very able person in a big company is probably
getting a bad deal, because his performance is dragged down by
the overall lower performance of the others.  Of course,
all other things often are not equal: the able person may 
not care about money, or may prefer the stability of a large
company.  But a very able person who does care about money
will ordinarily do better to go off and work with a small
group of peers.Technology = LeverageStartups offer anyone a way to be in a situation with
measurement and leverage.
They allow measurement because they're small,
and they offer leverage because they
make money by inventing new technology.What is technology?  It's technique. It's the way  
we all do things.  And when
you discover a new way to do things, its value is multiplied
by all the people who use it.  It is the proverbial fishing
rod, rather than the fish.  That's the difference between a
startup and a restaurant or a barber shop.  You fry eggs or cut 
hair one customer at a time.  Whereas if 
you solve a technical problem that a lot of people care about,
you help everyone who uses your solution.  
That's leverage.If you look at history, it seems that most people
who got rich by creating wealth did it by developing
new technology.  You just can't fry eggs or cut hair fast enough.
What made the Florentines rich in 1200 
was the discovery of new techniques for making the high-tech 
product of the time, fine woven cloth.  What made the
Dutch rich in 1600 was the discovery of shipbuilding and
navigation techniques that enabled them to dominate the seas
of the Far East.Fortunately there is a natural fit between smallness and
solving hard problems.  The leading edge of technology moves
fast.  Technology that's valuable today could be worthless
in a couple years.  Small companies are more at home in this
world, because they don't have layers of bureaucracy to
slow them down.
Also, technical advances tend to come from unorthodox approaches,
and small companies are less constrained by convention.Big companies can develop technology.  They just can't do it
quickly.  Their size makes them slow and prevents
them from rewarding employees for the extraordinary
effort required.  So in practice big companies only get to develop 
technology in fields where large capital requirements prevent startups from
competing with them, like microprocessors, power plants, 
or passenger aircraft.  And even in those fields they depend heavily
on startups for components and ideas.It's obvious that biotech or software startups exist to solve
hard technical problems, but 
I think it will also be found to be true 
in businesses that don't seem to be about technology.  McDonald's,
for example, grew big by designing a system, the McDonald's 
franchise, that could then be reproduced at will all over the 
face of the earth.  A McDonald's franchise is controlled by rules
so precise that it is practically
a piece of software.  Write once, run everywhere.
Ditto for Wal-Mart.  Sam Walton got rich not by being a 
retailer, but by designing a new kind of store.Use difficulty as a guide not just in selecting the overall
aim of your company, but also at decision points along the way.
At Viaweb one of our rules of thumb was run upstairs.
Suppose you are a little, nimble guy being chased by a big,
fat, bully.  You open a door and find yourself in a    
staircase.  Do you go up or down?  I say up.  The
bully can probably run downstairs as fast as you can.
Going upstairs his bulk will be more of a disadvantage.
Running upstairs is hard for you but even harder for him.What this meant in practice was that we deliberately sought      
hard problems.  If there were two features we could add to our
software, both equally valuable in proportion to their difficulty,
we'd always take the harder one.  Not just because it was 
more valuable, but because it was harder.
We delighted in forcing bigger, slower competitors
to follow us over difficult ground.
Like guerillas, startups prefer the difficult terrain of the
mountains, where the troops of the central government
can't follow.  I can remember times when we were just
exhausted after wrestling all day with some horrible technical
problem.  And I'd be delighted, because something that was 
hard for us would be impossible for our competitors.This is not just a good way to run a startup.  It's what
a startup is.
Venture capitalists know about this and have a phrase for it:
barriers to entry.  If you go to a VC with a new 
idea and ask him to invest in it, one of the first things
he'll ask is, how hard would this be for someone else to  
develop?  That is, how much difficult ground
have you put between yourself and potential pursuers? 
[7]
And you had better have a convincing explanation of why 
your technology would be hard to duplicate.  Otherwise as
soon as some big company becomes aware of it, they'll make
their own, and with their brand name, capital, and
distribution clout, they'll take away your market overnight.
You'd be like guerillas caught in the open field by regular
army forces.One way to put up barriers to entry is through patents. 
But patents may not provide much protection. 
Competitors commonly find ways to work around a patent.
And if they can't, they 
may simply violate it and invite you to sue them.
A big company is not afraid to be sued; it's an everyday thing
for them.  They'll make sure that suing them is expensive and
takes a long time.
Ever heard of Philo Farnsworth?  He invented
television.  The reason you've never
heard of him is that his company was not the one to make
money from it. 
[8]
The company that did was RCA, and
Farnsworth's reward for his efforts was a decade of
patent litigation.Here, as so often, the best defense is a good offense.  If
you can develop technology that's simply too hard for
competitors to duplicate, you don't need to rely on other
defenses.  Start by picking a hard problem, and
then at every decision point, take the harder choice. 
[9]The Catch(es)If it were simply a matter of working harder than 
an ordinary employee and getting paid proportionately, it would
obviously be a good deal to start a startup.  Up to a point it
would be more fun. I don't think many people 
like the slow pace of big companies, the interminable meetings,
the water-cooler conversations, the clueless middle managers,
and so on.Unfortunately there are a couple catches.  One is that you
can't choose the point on the curve that you want to inhabit.
You can't decide, for example, that you'd like to work just
two or three times as hard, and get paid that much more.  When
you're running a startup, your competitors decide how
hard you work.  And they pretty much all make the same decision:
as hard as you possibly can.The other catch is that the payoff is only on average proportionate
to your productivity.  There is, as I said before, a large
random multiplier in the success of any company.  So in
practice the deal is not that you're 30 times as productive and get 
paid 30 times as much.  It is that you're 30 times as productive,
and get paid between zero and a thousand times as much.
If the mean is 30x, the median is probably zero.
Most startups tank, and not just the dogfood 
portals we all heard about during
the Internet Bubble.  It's common for a startup
to be developing a genuinely good product, take slightly
too long to do it, run out of money, and have to shut down.A startup is like a mosquito.  A bear can absorb a hit and a crab
is armored against one, but a mosquito is designed for one thing:
to score.  No energy is wasted on defense.  The defense of mosquitos, 
as a species, is that there are a lot of them, but this is little 
consolation to the individual mosquito.Startups, like mosquitos, tend to be an all-or-nothing proposition.
And you don't generally know which of the two you're going to
get till the last minute. 
Viaweb came close to tanking several times. Our trajectory
was like a sine wave.  Fortunately we got bought at
the top of the cycle, but it was damned close.   While we were
visiting Yahoo in California to talk about selling the company
to them, we had to borrow a conference room to reassure
an investor who was about to back out of a new round of funding 
that we needed to stay alive.The all-or-nothing aspect of startups was not something we wanted.
Viaweb's hackers were all extremely risk-averse.
If there had been some way just to work super hard and get
paid for it, without having a lottery mixed in, we would have
been delighted.  We would have much preferred a 100% chance of
$1 million to a 20% chance of $10 million, even though 
theoretically the second is worth twice as much.   Unfortunately,
there is not currently any space in the business world where
you can get the first deal.The closest you can get is by
selling your startup in the early stages, giving up upside  
(and risk) for a smaller but guaranteed payoff.  We had a 
chance to do this, and stupidly, as we then thought, let it slip by.
After that we became comically eager to sell.
For the next year or so,
if anyone expressed the slightest curiosity about Viaweb
we would try to sell them the company.  But there were no takers,
so we had to keep going.It would have been a bargain to 
buy us at an early stage, but companies doing acquisitions are not
looking for bargains.  A company big enough to acquire 
startups will be big enough to be fairly conservative, and 
within the company the people in charge of acquisitions will
be among the more conservative, because they are likely to be
business school types who joined the company late.  
They would rather overpay for a safe choice.  So
it is easier to sell an established startup, even at a large
premium, than an early-stage one.Get UsersI think it's a good idea to get bought, if you can.  Running a
business is different from growing one.
It is just as well to let a big company take over once you reach 
cruising altitude.  It's
also financially wiser, because selling allows you to diversify.
What would you think of a financial advisor who put all his
client's assets into one volatile stock?How do you get bought?  Mostly by doing the same things 
you'd do if you didn't intend to sell the company.  Being 
profitable, for example.   But getting bought is also an art
in its own right, and one that we spent a lot of time trying
to master.Potential buyers will
always delay if they can.  The hard part about getting
bought is getting them to act.  For most people, the most powerful motivator
is not the hope of gain, but the fear of loss.  For potential
acquirers, the most powerful motivator is the prospect that 
one of their competitors will buy you.  This, as we found, 
causes CEOs to take red-eyes.  
The second biggest is the worry that, if they don't buy you 
now, you'll continue to grow rapidly and will cost more to
acquire later, or even become a competitor.In both cases, what it all comes down to is users.  
You'd think that a company about to buy you would do a lot of
research and decide for themselves how valuable your technology
was.  Not at all.  What they go by is the number of users you
have.In effect, acquirers assume the customers know who has the
best technology.  And this is not as stupid as it sounds.  Users 
are the only real proof that you've created wealth.  Wealth is 
what people want, and if people aren't using your software,
maybe it's not just because you're bad at marketing.  Maybe it's
because you haven't made what they want.Venture capitalists have a list of danger signs to watch out for.
Near the top is the company run by techno-weenies who are 
obsessed with solving interesting technical problems, instead
of making users happy.  In a startup, you're not just trying to
solve problems.  You're trying to solve problems that 
users care about.So I think you should make users the test, just as 
acquirers do.  Treat a startup as an optimization problem 
in which performance is measured by number of users.  As anyone
who has tried to optimize software knows, the key is measurement.
When you try to guess where your program is slow, and what would
make it faster, you almost always guess wrong.Number of users may not be the perfect test, but it will 
be very close.  It's what acquirers care about.  It's what 
revenues depend on.  
It's what makes competitors unhappy.
It's what impresses reporters, and potential
new users.  Certainly it's a better test than your a priori
notions of what problems are important to solve, no matter how
technically adept you are.Among other things, treating a startup as an optimization
problem will help you avoid another
pitfall that VCs worry about, and rightly-- taking a long time
to develop a product.  Now we can recognize this as something
hackers already know to avoid: premature optimization.  Get a version 
1.0 out there as soon as you can.  Until you have some users to
measure, you're optimizing based on guesses.The ball you need to keep your eye on here is the underlying
principle that wealth is what people want.  If you plan to get 
rich by creating wealth, you have to know what people want.  
So few businesses really pay attention to making customers happy.
How often do you walk into a store, or call a company on the
phone, with a feeling of dread in the back of your mind?
When you hear ""your call is important to us, please stay on
the line,"" do you think, oh good, now everything will be all right?A restaurant can afford to serve the occasional burnt dinner.
But in technology, you cook one thing and that's what everyone
eats.  So any difference between what people want and what
you deliver is multiplied.  
You please or annoy
customers wholesale.  The closer you can get to what they want,
the more wealth you generate.Wealth and PowerMaking wealth is not the only way to get rich.  For most of
human history it has not even been the most common.  Until
a few centuries ago,
the main sources of wealth were mines, slaves and serfs,
land, and cattle,
and the only ways to acquire these rapidly were by inheritance,
marriage, conquest, or confiscation.  
Naturally wealth had a bad reputation.Two things changed.  The first was the rule of law.  For most of the world's
history, if you did somehow accumulate a fortune, the ruler or his 
henchmen 
would find a way to steal it.
But in medieval Europe something new happened.
A new class of merchants and manufacturers
began to collect in towns. 
[10]
Together they were able to withstand the local feudal
lord.  So 
for the first time in our history, the bullies stopped stealing the
nerds' lunch money.
This was naturally a great incentive,
and possibly indeed the main cause of the second big change,
industrialization.A great deal has been written about the causes of the Industrial 
Revolution.  But surely a necessary, if not sufficient, condition
was that people who made fortunes be able to enjoy them in peace.
[11]
One piece of evidence is what happened to countries
that tried to return to the old model, like the Soviet
Union, and to a lesser extent Britain under the labor
governments of the 1960s and early 1970s.  Take away the incentive
of wealth, and technical innovation grinds to a halt.Remember what a startup is, economically: 
a way of saying, I want to work faster.  Instead of accumulating
money slowly by being paid a regular wage for fifty years, I 
want to get it over with as soon as possible.  So governments
that forbid you to accumulate wealth are in effect decreeing
that you work slowly.  They're willing to let you earn $3 million over
fifty years, but they're not willing to let you work so hard that
you can do it in two.  They are like
the corporate boss that you can't go to and say, I want to work
ten times as hard, so please pay me ten times a much.
Except this is not a boss you can escape by starting your own
company.The problem with working slowly is not just that technical
innovation happens slowly.  It's that it tends not to happen at all.
It's only when you're deliberately looking for hard problems,
as a way to use speed to the greatest advantage, that you take
on this kind of project.  Developing new technology is a 
pain in the ass. It is, as Edison said, one percent 
inspiration and ninety-nine percent perspiration.  
Without the incentive of wealth, no one wants to do it.
Engineers will work on sexy projects like fighter planes and moon
rockets for ordinary salaries, but more mundane technologies
like light bulbs or semiconductors have to be developed by entrepreneurs.Startups
are not just something that happened in Silicon Valley in 
the last couple decades.  Since it became possible to
get rich by creating wealth, everyone who has done it has
used essentially the same recipe: measurement and leverage,
where measurement comes from working with a small
group, and leverage from developing new techniques.
The recipe was the same in Florence in 1200 as it is 
in Santa Clara today.Understanding this may help to answer an important question:
why Europe grew so powerful.
Was it something about the geography of 
Europe?  Was it that Europeans are somehow racially superior?
Was it their religion?  The answer (or at least
the proximate cause) may be that the
Europeans 
rode on the crest of a powerful new idea: allowing those who
made a lot of money to keep it.Once you're allowed to do that, 
people who want to get rich can do it by generating
wealth instead of stealing it.
The resulting technological growth translates not only 
into wealth but into military power.  The theory that led to
the stealth plane was developed by a Soviet mathematician.
But because the Soviet Union didn't have a computer industry,
it remained for them a theory;
they didn't have hardware capable of executing the calculations
fast enough to design an actual airplane.In that respect the Cold War teaches the same lesson as
World War II and, for that matter, most wars in recent history.
Don't let a ruling
class of warriors and politicians squash the entrepreneurs.
The same recipe that makes individuals rich
makes countries powerful.  Let the nerds keep their lunch
money, and you rule the world.Notes[1]
One valuable thing you tend to get only in startups is
uninterruptability.  Different kinds of
work have different time quanta.  Someone proofreading a
manuscript
could probably be interrupted every fifteen minutes
with little loss of productivity.  But the time quantum for
hacking is very long: it might take an hour just to load
a problem into your head.  So the
cost of having someone from personnel
call you about a form you forgot to fill out can be huge.This is why hackers give you such a baleful stare as they
turn from their screen to answer your question.  Inside
their heads a giant house of cards is tottering.The mere possibility of being interrupted deters hackers
from starting hard projects.  This is why they
tend to work late at night, and why it's next to impossible
to write great software in a cubicle (except late at night).One great advantage of startups is that they don't yet have
any of the people who interrupt you.  There is no personnel
department, and thus no form nor anyone to call you about it.[2]
Faced with the idea that people working for startups might be
20 or 30 times as productive as those working for large companies,
executives at large companies will naturally wonder, how could
I get the people working for me to do that?  The answer is
simple: pay them to.Internally most companies are run like Communist states.
If you believe in free markets, why not turn your company into one?Hypothesis: A company will be maximally profitable when each
employee is paid in proportion to the wealth they generate.[3]
Until recently even governments sometimes didn't grasp the
distinction between money and wealth.  Adam
Smith (Wealth of Nations, v:i) mentions several
that tried to preserve their
""wealth"" by forbidding the export of gold or silver.
But having more of the medium of exchange would not make
a country richer; if you have more money chasing the same
amount of material wealth, the only result is higher prices.[4]
There are many senses of the word ""wealth,"" not all of
them material.  I'm not trying to make a deep philosophical
point here about which
is the true kind.  I'm writing about one specific,
rather technical sense of the word ""wealth.""  What
people will give you money for.
This is an interesting sort of wealth to study, because
it is the kind that prevents you from starving.
And what people will give you money for depends on them,
not you.When you're starting a business,
it's easy to slide into thinking that customers
want what you do.  During the Internet Bubble I talked
to a woman who, because she liked the outdoors, was
starting an ""outdoor portal."" You know what
kind of business you should start if you like
the outdoors?  One to recover data from crashed hard disks.What's the connection?  None at all.  Which is precisely my point.
If you want
to create wealth (in the narrow technical sense of not
starving) then you should be especially skeptical about any
plan that centers on things you like doing.
That is where your idea of what's valuable is least
likely to coincide with other people's.[5]
In the average car restoration you probably do make everyone
else microscopically poorer, by doing a small amount of damage to
the environment.  While environmental costs should be taken
into account, they don't
make wealth a zero-sum game.  For example, if you repair
a machine that's broken because a part has come unscrewed,
you create wealth with no environmental cost.[5b]
This essay was written before Firefox.[6]
Many people feel confused and depressed in
their early twenties.  Life seemed so much more fun in college.
Well, of course it was.  Don't be fooled by the surface similarities.
You've gone from guest to servant.
It's possible to have fun in this new world. 
Among other things, you now get to go behind the doors that say
""authorized personnel only.""
But the change is a shock at first, and all the worse
if you're not consciously aware of it.[7]
When VCs asked us how long it would take another startup
to duplicate our software, we used to reply that they probably
wouldn't be able to at all. I think this made us seem naive,
or liars.[8]
Few technologies have one clear inventor.  So as
a rule, if you know the ""inventor"" of something
(the telephone, the assembly line, the airplane, 
the light bulb, the transistor) it is because their
company made money from it, and the company's PR people worked
hard to spread the story.  If you don't know who invented
something (the automobile, the television, the computer,
the jet engine, the laser), it's because other companies
made all the money.[9]
This is a good plan for life in general.
If you have two choices, choose the harder.
If you're trying to decide whether to go out running or
sit home and watch TV, go running.
Probably the reason this trick works so well is that
when you have two choices and one is harder, the
only reason you're even considering the other is laziness.
You know in the back of your mind what's the right thing
to do, and this trick merely forces you to acknowledge it.[10]
It is probably no accident that the middle class
first appeared in northern Italy and the low countries,
where there were no strong central governments.   These two
regions were the richest of their time and became the twin
centers from which Renaissance civilization radiated.
If they no longer play that role, it is because
other places, like the United States, have been truer to the
principles they discovered.[11]
It may indeed be a sufficient condition.  But if so, why didn't
the Industrial Revolution happen earlier?  Two possible (and
not incompatible) answers: (a) It did.  
The Industrial Revolution was one in a series.
(b) Because in medieval towns, monopolies
and guild regulations initially slowed the development of new means
of production.

Comment on this essay.Russian TranslationArabic TranslationSpanish Translation


You'll find this essay and 14 others in
Hackers & Painters.

","startup advice
",human,"human
","human
"
53,53,"

Want to start a startup?  Get funded by
Y Combinator.





Watch how this essay was
written.




February 2009One of the things I always tell startups is a principle I learned
from Paul Buchheit: it's better to make a few people really happy
than to make a lot of people semi-happy.  I was saying recently to
a reporter that if I could only tell startups 10 things, this would
be one of them.  Then I thought: what would the other 9 be?When I made the list there turned out to be 13:

1. Pick good cofounders.Cofounders are for a startup what location is for real estate.  You
can change anything about a house except where it is.  In a startup
you can change your idea easily, but changing your cofounders is
hard. 
[1]
And the success of a startup is almost always a function
of its founders.2. Launch fast.The reason to launch fast is not so much that it's critical to get
your product to market early, but that you haven't really started
working on it till you've launched.  Launching teaches you what you
should have been building.  Till you know that you're wasting your
time.  So the main value of whatever you launch with is as a pretext
for engaging users.3. Let your idea evolve.This is the second half of launching fast. Launch fast and iterate.
It's a big mistake to treat a startup as if it were merely a matter
of implementing some brilliant initial idea. As in an essay, most
of the ideas appear in the implementing.4. Understand your users.You can envision the wealth created by a startup as a rectangle,
where one side is the number of users and the other is how much you
improve their lives.
[2]
The second dimension is the one you have
most control over.  And indeed, the growth in the first will be
driven by how well you do in the second.  As in science, the hard
part is not answering questions but asking them: the hard part is
seeing something new that users lack. The better you understand
them the better the odds of doing that. That's why so many successful
startups make something the founders needed.5. Better to make a few users love you than a lot ambivalent.Ideally you want to make large numbers of users love you, but you
can't expect to hit that right away.  Initially you have to choose
between satisfying all the needs of a subset of potential users,
or satisfying a subset of the needs of all potential users.  Take
the first. It's easier to expand userwise than satisfactionwise.
And perhaps more importantly, it's harder to lie to yourself.  If
you think you're 85% of the way to a great product, how do you know
it's not 70%?  Or 10%?  Whereas it's easy to know how many users
you have.6. Offer surprisingly good customer service.Customers are used to being maltreated.  Most of the companies they
deal with are quasi-monopolies that get away with atrocious customer
service. Your own ideas about what's possible have been unconsciously
lowered by such experiences.  Try making your customer service not
merely good, but 
surprisingly good.  Go out of your way to make
people happy.  They'll be overwhelmed; you'll see.  In the earliest
stages of a startup, it pays to offer customer service on a level
that wouldn't scale, because it's a way of learning about your
users.7. You make what you measure.I learned this one from Joe Kraus. 
[3]
Merely measuring something
has an uncanny tendency to improve it.  If you want to make your
user numbers go up, put a big piece of paper on your wall and every
day plot the number of users.  You'll be delighted when it goes up
and disappointed when it goes down.  Pretty soon you'll start
noticing what makes the number go up, and you'll start to do more
of that.  Corollary: be careful what you measure.8. Spend little.I can't emphasize enough how important it is for a startup to be cheap.
Most startups fail before they make something people want, and the
most common form of failure is running out of money.  So being cheap
is (almost) interchangeable with iterating rapidly.
[4]
But it's
more than that.  A culture of cheapness keeps companies young in
something like the way exercise keeps people young.9. Get ramen profitable.""Ramen profitable"" means a startup makes just enough to pay the
founders' living expenses.  It's not rapid prototyping for business
models (though it can be), but more a way of hacking the investment
process.  Once you cross over into ramen profitable, it completely
changes your relationship with investors.  It's also great for
morale.10. Avoid distractions.Nothing kills startups like distractions.  The worst type are those
that pay money: day jobs, consulting, profitable side-projects.
The startup may have more long-term potential, but you'll always
interrupt working on it to answer calls from people paying you now.
Paradoxically, fundraising is this type of distraction, so try to
minimize that too.11. Don't get demoralized.Though the immediate cause of death in a startup tends to be running
out of money, the underlying cause is usually lack of focus.  Either
the company is run by stupid people (which can't be fixed with
advice) or the people are smart but got demoralized.  Starting a
startup is a huge moral weight.  Understand this and make a conscious
effort not to be ground down by it, just as you'd be careful to
bend at the knees when picking up a heavy box.12. Don't give up.Even if you get demoralized, don't give up.  You can get surprisingly
far by just not giving up.  This isn't true in all fields.  There
are a lot of people who couldn't become good mathematicians no
matter how long they persisted.  But startups aren't like that.
Sheer effort is usually enough, so long as you keep morphing your
idea.13. Deals fall through.One of the most useful skills we learned from Viaweb was not getting
our hopes up.  We probably had 20 deals of various types fall
through.  After the first 10 or so we learned to treat deals as
background processes that we should ignore till they terminated.
It's very dangerous to morale to start to depend on deals closing,
not just because they so often don't, but because it makes them
less likely to.

Having gotten it down to 13 sentences, I asked myself which I'd
choose if I could only keep one.Understand your users.  That's the key.  The essential task in a
startup is to create wealth; the dimension of wealth you have most
control over is how much you improve users' lives; and the hardest
part of that is knowing what to make for them.  Once you know what
to make, it's mere effort to make it, and most decent hackers are
capable of that.Understanding your users is part of half the principles in this
list.  That's the reason to launch early, to understand your users.
Evolving your idea is the embodiment of understanding your users.
Understanding your users well will tend to push you toward making
something that makes a few people deeply happy.  The most important
reason for having surprisingly good customer service is that it
helps you understand your users.  And understanding your users will
even ensure your morale, because when everything else is collapsing
around you, having just ten users who love you will keep you going.Notes[1]
Strictly speaking it's impossible without a time machine.[2]
In practice it's more like a ragged comb.[3]
Joe thinks one of the founders of Hewlett Packard said it first,
but he doesn't remember which.[4]
They'd be interchangeable if markets stood still.  Since they
don't, working twice as fast is better than having twice as much
time.Turkish TranslationSpanish TranslationBulgarian TranslationJapanese TranslationPersian Translation","startup advice
",human,"human
","human
"
54,54,"March 2012I'm not a very good speaker.  I say ""um"" a lot. Sometimes I have
to pause when I lose my train of thought.  I wish I were a better
speaker.  But I don't wish I were a better speaker like I wish I
were a better writer.  What I really want is to have good ideas,
and that's a much bigger part of being a good writer than being a
good speaker.Having good ideas is most of writing well.  If you know what you're
talking about, you can say it in the plainest words and you'll be
perceived as having a good style.  With speaking it's the opposite:
having good ideas is an alarmingly small component of being a good
speaker.I first noticed this at a conference several years ago.
There was another speaker who was much better than me.
He had all of us roaring with laughter.  I seemed awkward and
halting by comparison.  Afterward I put my talk online like I usually
do.  As I was doing it I tried to imagine what a transcript of the
other guy's talk would be like, and it was only then I realized he
hadn't said very much.Maybe this would have been obvious to someone who knew more about
speaking, but it was a revelation to me how much less ideas mattered
in speaking than writing.
[1]A few years later I heard a talk by someone who was not merely a
better speaker than me, but a famous speaker.  Boy was he good.  So
I decided I'd pay close attention to what he said, to learn how he
did it.  After about ten sentences I found myself thinking ""I don't
want to be a good speaker.""Being a really good speaker is not merely orthogonal to having good ideas,
but in many ways pushes you in the opposite direction.  For example,
when I give a talk, I usually write it out beforehand.  I know that's
a mistake; I know delivering a 
prewritten 
talk makes it harder to
engage with an audience.  The way to get the attention of an audience
is to give them your full attention, and when you're delivering
a prewritten talk, your attention is always divided between the
audience and the talk — even if you've memorized it.  If you want
to engage an audience, it's better to start with no more than an outline
of what you want to say and 
ad lib the individual sentences.  But
if you do that, you might spend no more time thinking about each
sentence than it takes to say it.
[2]
Occasionally the stimulation
of talking to a live audience makes you think of new things, but
in general this is not going to generate ideas as well as writing
does, where you can spend as long on each sentence as you want.If you rehearse a prewritten speech enough, you can get
asymptotically close to the sort of engagement you get when speaking
ad lib.  Actors do.  But here again there's a tradeoff between
smoothness and ideas.  All the time you spend practicing a talk,
you could instead spend making it better.  Actors don't face
that temptation, except in the rare cases where they've written the
script, but any speaker does.  Before I give a talk I can usually
be found sitting in a corner somewhere with a copy printed out on
paper, trying to rehearse it in my head.  But I always end up
spending most of the time rewriting it instead.  Every talk I give
ends up being given from a manuscript full of things crossed out
and rewritten.  Which of course makes me um even more, because I
haven't had any time to practice the new bits.
[3]Depending on your audience, there are even worse tradeoffs than
these.  Audiences like to be flattered; they like jokes; they like
to be swept off their feet by a vigorous stream of words.  As you
decrease the intelligence of the audience, being a good speaker is
increasingly a matter of being a good bullshitter.  That's true in
writing too of course, but the descent is steeper with talks.  Any
given person is dumber as a member of an audience than as a reader.
Just as a speaker ad libbing can only spend as long thinking about
each sentence as it takes to say it, a person hearing a talk can
only spend as long thinking about each sentence as it takes to hear
it.  Plus people in an audience are always affected by the reactions
of those around them, and the reactions that spread from person to
person in an audience are disproportionately the more brutish sort,
just as low notes travel through walls better than high ones.  Every
audience is an incipient mob, and a good speaker uses that.  Part
of the reason I laughed so much at the talk by the good speaker at
that conference was that everyone else did.
[4]So are talks useless?  They're certainly inferior to the written
word as a source of ideas.  But that's not all talks are good for.
When I go to a talk, it's usually because I'm interested in the
speaker.  Listening to a talk is the closest most of us can get to
having a conversation with someone like the president, who doesn't
have time to meet individually with all the people who want to meet
him.Talks are also good at motivating me to do things.  It's probably
no coincidence that so many famous speakers are described as
motivational speakers.  That may be what public speaking is really
for.  It's probably what it was originally for.  The emotional
reactions you can elicit with a talk can be a powerful force.
I wish I could say that this force was more often used for good than
ill, but I'm not sure.Notes[1]
I'm not talking here about academic talks, which are a 
different type of thing.  While the
audience at an academic talk might appreciate a joke, they will (or
at least should) make a conscious effort to see what new ideas
you're presenting.[2]
That's the lower bound.  In practice you can often do better,
because talks are usually about things you've written or talked
about before, and when you ad lib, you end up reproducing some of
those sentences.  Like early medieval architecture, impromptu talks
are made of spolia.  Which feels a bit dishonest, incidentally,
because you have to deliver these sentences as if you'd just thought
of them.[3]
Robert Morris points out that there is a way in which practicing
talks makes them better: reading a talk out loud can expose awkward
parts.  I agree and in fact I read most things I write out loud at
least once for that reason.[4]
For sufficiently small audiences, it may not be true that being
part of an audience makes people dumber.  The real decline seems
to set in when the audience gets too big for the talk to feel like
a conversation — maybe around 10 people.
Thanks to Sam Altman and Robert Morris for reading drafts
of this.","writing advice
",human,"human
","human
"
55,55,"September 2009I bet you the current issue of Cosmopolitan has an article
whose title begins with a number. ""7 Things He Won't Tell You about
Sex,"" or something like that.  Some popular magazines
feature articles of this type on the cover of every
issue.  That can't be happening by accident.  Editors must know
they attract readers.Why do readers like the list of n things so much?   Mainly because
it's easier to read than a regular article.  
[1]
Structurally, the list of n things is a degenerate case of essay.
An essay can go anywhere the writer wants.  In a list of n things
the writer agrees to constrain himself to a collection of points
of roughly equal importance, and he tells the reader explicitly
what they are.Some of the work of reading an article is understanding its
structure—figuring out what in high school we'd have called
its ""outline."" Not explicitly, of course, but someone who really
understands an article probably has something in his brain afterward
that corresponds to such an outline.  In a list of n things, this
work is done for you.  Its structure is an exoskeleton.As well as being explicit, the structure is guaranteed to be of the
simplest possible type: a few main points with few to no subordinate
ones, and no particular connection between them.Because the main points are unconnected, the list of n things is
random access.  There's no thread of reasoning you have to follow.  You could
read the list in any order.  And because the points are independent
of one another, they work like watertight compartments in an
unsinkable ship.  If you get bored with, or can't understand, or
don't agree with one point, you don't have to give up on the article.
You can just abandon that one and skip to the next.  A list of n
things is parallel and therefore fault tolerant.There are times when this format is what a writer wants.  One, obviously,
is when what you have to say actually is a list of n
things.  I once wrote an essay about the mistakes that kill startups, and a few people made fun of me
for writing something whose title began with a number.  But in that
case I really was trying to make a complete catalog of a number of
independent things.  In fact, one of the questions I was trying to
answer was how many there were.There are other less legitimate reasons for using this format.  For
example, I use it when I get close to a deadline.  If I have to
give a talk and I haven't started it a few days beforehand, I'll
sometimes play it safe and make the talk a list of n things.The list of n things is easier for writers as well as readers.  When
you're writing a real essay, there's always a chance you'll hit a
dead end.  A real essay is a train of thought, and some trains of
thought just peter out.  That's an alarming possibility when you
have to give a talk in a few days.  What if you run out of ideas?
The compartmentalized structure of the list of n things protects
the writer from his own stupidity in much the same way it protects
the reader.  If you run out of ideas on one point, no problem: it
won't kill the essay.  You can take out the whole point if you need
to, and the essay will still survive.Writing a list of n things is so relaxing.  You think of n/2 of
them in the first 5 minutes.  So bang, there's the structure, and
you just have to fill it in.  As you think of more points, you just
add them to the end.  Maybe you take out or rearrange or combine a
few, but at every stage you have a valid (though initially low-res)
list of n things.  It's like the sort of programming where you write
a version 1 very quickly and then gradually modify it, but at every
point have working code—or the style of painting where you begin
with a complete but very blurry sketch done in an hour, then spend
a week cranking up the resolution.Because the list of n things is easier for writers too, it's not
always a damning sign when readers prefer it.  It's not necessarily
evidence readers are lazy; it could also mean they don't have
much confidence in the writer.  The list of n things is in that
respect the cheeseburger of essay forms.  If you're eating at a
restaurant you suspect is bad, your best bet is to order the
cheeseburger.  Even a bad cook can make a decent cheeseburger.  And
there are pretty strict conventions about what a cheeseburger should
look like.  You can assume the cook isn't going to try something
weird and artistic.  The list of n things similarly limits the
damage that can be done by a bad writer.  You know it's going to
be about whatever the title says, and the format prevents the writer
from indulging in any flights of fancy.Because the list of n things is the easiest essay form, it should
be a good one for beginning writers.  And in fact it is what most
beginning writers are taught.  The classic 5 paragraph essay is
really a list of n things for n = 3.  But the students writing them
don't realize they're using the same structure as the articles they
read in Cosmopolitan. They're not allowed to include the numbers,
and they're expected to spackle over the gaps with gratuitous
transitions (""Furthermore..."") and cap the thing at either end with
introductory and concluding paragraphs so it will look superficially
like a real essay.
[2]It seems a fine plan to start students off with the list of n things.
It's the easiest form.  But if we're going to do that, why not do
it openly?  Let them write lists of n things like the pros, with
numbers and no transitions or ""conclusion.""There is one case where the list of n things is a dishonest format:
when you use it to attract attention by falsely claiming the list
is an exhaustive one.  I.e. if you write an article that purports
to be about the 7 secrets of success.  That kind of title is the
same sort of reflexive challenge as a whodunit. You have to at least
look at the article to check whether they're the same 7 you'd list.
Are you overlooking one of the secrets of success?  Better check.It's fine to put ""The"" before the number if you really believe
you've made an exhaustive list.  But evidence suggests most things
with titles like this are linkbait.The greatest weakness of the list of n things is that there's so
little room for new thought.  The main point of essay writing, when
done right, is the new ideas you have while doing it.  A real essay,
as the name implies, is 
dynamic: you don't know what you're going
to write when you start.  It will be about whatever you discover
in the course of writing it.This can only happen in a very limited way in a list of n things.
You make the title first, and that's what it's going to be about.
You can't have more new ideas in the writing than will fit in the
watertight compartments you set up initially.  And your brain seems
to know this: because you don't have room for new ideas, you don't
have them.Another advantage of admitting to beginning writers that the 5
paragraph essay is really a list of n things is that we can warn
them about this.  It only lets you experience the defining
characteristic of essay writing on a small scale: in thoughts of a
sentence or two.  And it's particularly dangerous that the 5 paragraph
essay buries the list of n things within something that looks like
a more sophisticated type of essay.  If you don't know you're using
this form, you don't know you need to escape it.Notes[1]
Articles of this type are also startlingly popular on Delicious,
but I think that's because 
delicious/popular 
is driven by bookmarking,
not because Delicious users are stupid.  Delicious users are
collectors, and a list of n things seems particularly collectible
because it's a collection itself.[2]
Most ""word problems"" in school math textbooks are similarly
misleading.  They look superficially like the application of math
to real problems, but they're not.  So if anything they reinforce
the impression that math is merely a complicated but pointless
collection of stuff to be memorized.Russian Translation","writing advice
",human,"human
","human
"
56,56,"July 2006
When I was in high school I spent a lot of time imitating bad
writers.  What we studied in English classes was mostly fiction,
so I assumed that was the highest form of writing.  Mistake number
one.  The stories that seemed to be most admired were ones in which
people suffered in complicated ways.  Anything funny or
gripping was ipso facto suspect, unless it was old enough to be hard to
understand, like Shakespeare or Chaucer.  Mistake number two.  The
ideal medium seemed the short story, which I've since learned had
quite a brief life, roughly coincident with the peak of magazine
publishing.  But since their size made them perfect for use in
high school classes, we read a lot of them, which gave us the
impression the short story was flourishing.  Mistake number three.
And because they were so short, nothing really had to happen; you
could just show a randomly truncated slice of life, and that was
considered advanced.  Mistake number four.  The result was that I
wrote a lot of stories in which nothing happened except that someone
was unhappy in a way that seemed deep.For most of college I was a philosophy major.  I was very impressed
by the papers published in philosophy journals.  They were so
beautifully typeset, and their tone was just captivating—alternately
casual and buffer-overflowingly technical.  A fellow would be walking
along a street and suddenly modality qua modality would spring upon
him.  I didn't ever quite understand these papers, but I figured
I'd get around to that later, when I had time to reread them more
closely.  In the meantime I tried my best to imitate them.  This
was, I can now see, a doomed undertaking, because they weren't
really saying anything.  No philosopher ever refuted another, for
example, because no one said anything definite enough to refute.
Needless to say, my imitations didn't say anything either.In grad school I was still wasting time imitating the wrong things.
There was then a fashionable type of program called an expert system,
at the core of which was something called an inference engine.  I
looked at what these things did and thought ""I could write that in
a thousand lines of code.""  And yet eminent professors were writing
books about them, and startups were selling them for a year's salary
a copy.  What an opportunity, I thought; these impressive things
seem easy to me; I must be pretty sharp.  Wrong.  It was simply a
fad.  The books the professors wrote about expert systems are now
ignored.  They were not even on a path to anything interesting.
And the customers paying so much for them were largely the same
government agencies that paid thousands for screwdrivers and toilet
seats.How do you avoid copying the wrong things?  Copy only what you
genuinely like.  That would have saved me in all three cases.  I
didn't enjoy the short stories we had to read in English classes;
I didn't learn anything from philosophy papers; I didn't use expert
systems myself.  I believed these things were good because they
were admired.It can be hard to separate the things you like from the things
you're impressed with.  One trick is to ignore presentation.  Whenever
I see a painting impressively hung in a museum, I ask myself: how
much would I pay for this if I found it at a garage sale, dirty and
frameless, and with no idea who painted it?  If you walk around a
museum trying this experiment, you'll find you get some truly
startling results.  Don't ignore this data point just because it's
an outlier.Another way to figure out what you like is to look at what you enjoy
as guilty pleasures.  Many things people like, especially if they're
young and ambitious, they like largely for the feeling of virtue
in liking them.  99% of people reading Ulysses are thinking
""I'm reading Ulysses"" as they do it. A guilty pleasure is
at least a pure one.  What do you read when you don't feel up to being
virtuous?  What kind of book do you read and feel sad that there's
only half of it left, instead of being impressed that you're half
way through?  That's what you really like.Even when you find genuinely good things to copy, there's another
pitfall to be avoided.  Be careful to copy what makes them good,
rather than their flaws.  It's easy to be drawn into imitating
flaws, because they're easier to see, and of course easier to copy
too.  For example, most painters in the eighteenth and nineteenth
centuries used brownish colors.  They were imitating the great
painters of the Renaissance, whose paintings by that time were brown
with dirt.  Those paintings have since been cleaned, revealing
brilliant colors; their imitators are of course still brown.It was painting, incidentally, that cured me of copying the wrong
things.  Halfway through grad school I decided I wanted to try being
a painter, and the art world was so manifestly corrupt that it
snapped the leash of credulity.  These people made philosophy
professors seem as scrupulous as mathematicians.  It was so clearly
a choice of doing good work xor being an insider that I was forced
to see the distinction.  It's there to some degree in almost every
field, but I had till then managed to avoid facing it.That was one of the most valuable things I learned from painting:
you have to figure out for yourself what's 
good.  You can't trust
authorities. They'll lie to you on this one.

Comment on this essay.Chinese TranslationRomanian TranslationSpanish TranslationRussian Translation","writing advice
",human,"human
","human
"
57,57,"October 2015Here's a simple trick for getting more people to read what you
write: write in spoken language.Something comes over most people when they start writing. They write
in a different language than they'd use if they were talking to a
friend. The sentence structure and even the words are different.
No one uses ""pen"" as a verb in spoken English. You'd feel like an
idiot using ""pen"" instead of ""write"" in a conversation with a friend.The last straw for me was a sentence I read a couple days ago:

  The mercurial Spaniard himself declared: ""After Altamira, all is
  decadence.""

It's from Neil Oliver's A History of Ancient Britain. I feel bad
making an example of this book, because it's no worse than lots of
others.  But just imagine calling Picasso ""the mercurial Spaniard"" when
talking to a friend.  Even one
sentence of this would raise eyebrows in conversation.  And yet
people write whole books of it.Ok, so written and spoken language are different. Does that make
written language worse?If you want people to read and understand what you write, yes.
Written language is more complex, which makes it more work to read.
It's also more formal and distant, which gives the reader's attention
permission to drift.  But perhaps worst of all, the complex sentences
and fancy words give you, the writer, the false impression that
you're saying more than you actually are.You don't need complex sentences to express complex ideas.  When
specialists in some abstruse topic talk to one another about ideas
in their field, they don't use sentences any more complex than they
do when talking about what to have for lunch.  They use different
words, certainly.  But even those they use no more than necessary.
And in my experience, the harder the subject, the more informally
experts speak. Partly, I think, because they have less to prove,
and partly because the harder the ideas you're talking about, the
less you can afford to let language get in the way.Informal language is the athletic clothing of ideas.I'm not saying spoken language always works best. Poetry is as much
music as text, so you can say things you wouldn't say in conversation.
And there are a handful of writers who can get away with using fancy
language in prose. And then of course there are cases where writers
don't want to make it easy to understand what they're saying—in
corporate announcements of bad news, for example, or at the more
bogus end of the humanities.  But for nearly everyone else, spoken
language is better.It seems to be hard for most people to write in spoken language.
So perhaps the best solution is to write your first draft the way
you usually would, then afterward look at each sentence and ask ""Is
this the way I'd say this if I were talking to a friend?"" If it
isn't, imagine what you would say, and use that instead.  After a
while this filter will start to operate as you write. When you write
something you wouldn't say, you'll hear the clank as it hits the
page.Before I publish a new essay, I read it out loud and fix everything
that doesn't sound like conversation. I even fix bits that are
phonetically awkward; I don't know if that's necessary, but it
doesn't cost much.This trick may not always be enough.  I've seen writing so far
removed from spoken language that it couldn't be fixed sentence by
sentence.  For cases like that there's a more drastic solution.
After writing the first draft, try explaining to a friend what you
just wrote. Then replace the draft with what you said to your friend.People often tell me how much my essays sound like me talking.
The fact that this seems worthy of comment shows how rarely people
manage to write in spoken language.  Otherwise everyone's writing
would sound like them talking.If you simply manage to write in spoken language, you'll be ahead
of 95% of writers.  And it's so easy to do: just don't let a sentence
through unless it's the way you'd say it to a friend.Thanks to Patrick Collison and Jessica Livingston for reading drafts of this.Japanese TranslationArabic Translation","writing advice
",human,"human
","human
"
58,58,"February 2022Writing about something, even something you know well, usually shows
you that you didn't know it as well as you thought. Putting ideas
into words is a severe test. The first words you choose are usually
wrong; you have to rewrite sentences over and over  to
get them exactly right. And your ideas won't just be imprecise, but
incomplete too. Half the ideas that end up in an essay will be ones
you thought of while you were writing it. Indeed, that's why I write
them.Once you publish something, the convention is that whatever you
wrote was what you thought before you wrote it. These were your
ideas, and now you've expressed them. But you know this isn't true.
You know that putting your ideas into words changed them. And not
just the ideas you published. Presumably there were others that
turned out to be too broken to fix, and those you discarded instead.It's not just having to commit your ideas to specific words that
makes writing so exacting. The real test is reading what you've
written. You have to pretend to be a neutral reader who knows nothing
of what's in your head, only what you wrote. When he reads what you
wrote, does it seem correct? Does it seem complete? If you make an
effort, you can read your writing as if you were a complete stranger,
and when you do the news is usually bad. It takes me many cycles
before I can get an essay past the stranger. But the stranger is
rational, so you always can, if you ask him what he needs. If he's
not satisfied because you failed to mention x or didn't qualify
some sentence sufficiently, then you mention x or add more
qualifications. Happy now? It may cost you some nice sentences, but
you have to resign yourself to that. You just have to make them as
good as you can and still satisfy the stranger.This much, I assume, won't be that controversial. I think it will
accord with the experience of anyone who has tried to write about
anything nontrivial. There may exist people whose thoughts are so
perfectly formed that they just flow straight into words. But I've
never known anyone who could do this, and if I met someone who said
they could, it would seem evidence of their limitations rather than
their ability. Indeed, this is a trope in movies: the guy who claims
to have a plan for doing some difficult thing, and who when questioned
further, taps his head and says ""It's all up here."" Everyone watching
the movie knows what that means. At best the plan is vague and
incomplete. Very likely there's some undiscovered flaw that invalidates
it completely. At best it's a plan for a plan.In precisely defined domains it's possible to form complete ideas
in your head. People can play chess in their heads, for example.
And mathematicians can do some amount of math in their heads, though
they don't seem to feel sure of a proof over a certain length till
they write it down. But this only seems possible with ideas you can
express in a formal language.  [1] Arguably what such people are
doing is putting ideas into words in their heads. I can to some
extent write essays in my head. I'll sometimes think of a paragraph
while walking or lying in bed that survives nearly unchanged in the
final version. But really I'm writing when I do this. I'm doing the
mental part of writing; my fingers just aren't moving as I do it.
[2]You can know a great deal about something without writing about it.
Can you ever know so much that you wouldn't learn more from trying
to explain what you know? I don't think so. I've written about at
least two subjects I know well — Lisp hacking and startups
— and in both cases I learned a lot from writing about them.
In both cases there were things I didn't consciously realize till
I had to explain them. And I don't think my experience was anomalous.
A great deal of knowledge is unconscious, and experts have if
anything a higher proportion of unconscious knowledge than beginners.I'm not saying that writing is the best way to explore all ideas.
If you have ideas about architecture, presumably the best way to
explore them is to build actual buildings. What I'm saying is that
however much you learn from exploring ideas in other ways, you'll
still learn new things from writing about them.Putting ideas into words doesn't have to mean writing, of course.
You can also do it the old way, by talking. But in my experience,
writing is the stricter test. You have to commit to a single, optimal
sequence of words. Less can go unsaid when you don't have tone of
voice to carry meaning. And you can focus in a way that would seem
excessive in conversation. I'll often spend 2 weeks on an essay and
reread drafts 50 times. If you did that in conversation
it would seem evidence of some kind of
mental disorder. 
If you're lazy,
of course, writing and talking are equally useless. But if you want
to push yourself to get things right, writing is the steeper hill.
[3]The reason I've spent so long establishing this rather obvious point
is that it leads to another that many people will find shocking.
If writing down your ideas always makes them more precise and more
complete, then no one who hasn't written about a topic has fully
formed ideas about it. And someone who never writes has no fully
formed ideas about anything nontrivial.It feels to them as if they do, especially if they're not in the
habit of critically examining their own thinking. Ideas can feel
complete. It's only when you try to put them into words that you
discover they're not. So if you never subject your ideas to that
test, you'll not only never have fully formed ideas, but also never
realize it.Putting ideas into words is certainly no guarantee that they'll be
right. Far from it. But though it's not a sufficient condition, it
is a necessary one.Notes[1] Machinery and
circuits are formal languages.[2] I thought of this
sentence as I was walking down the street in Palo Alto.[3] There are two
senses of talking to someone: a strict sense in which the conversation
is verbal, and a more general sense in which it can take any form,
including writing. In the limit case (e.g. Seneca's letters),
conversation in the latter sense becomes essay writing.It can be very useful to talk (in either sense) with other people
as you're writing something. But a verbal conversation will never
be more exacting than when you're talking about something you're
writing. Thanks to Trevor Blackwell, Patrick
Collison, and Robert Morris for reading drafts of this.  French Translation","writing advice
",human,"human
","human
"
59,59,"March 2005
(In the process
of answering an email, I accidentally wrote a tiny essay about writing.
I usually spend weeks on an essay.  This one took  67 minutes—23
of writing, and  44 of rewriting.)I think it's far more important to write well than most people
realize.  Writing doesn't just communicate ideas; it generates them.
If you're bad at writing and don't like to do it, you'll miss out
on most of the ideas writing would have generated.As for how to write well, here's the short version: 
Write a bad version
1 as fast as you can; rewrite it over and over; cut out everything
unnecessary; write in a conversational tone; develop a nose for
bad writing, so you can see and fix it in yours; imitate writers
you like; if you can't get started, tell someone what you plan to
write about, then write down what you said; expect
80% of the ideas in an essay to happen after you start writing it,
and 50% of those you start with to be wrong; be confident enough
to cut; have friends you trust read your stuff and tell you which
bits are confusing or drag; don't (always) make detailed outlines;
mull ideas over for a few days before
writing; carry a small notebook or scrap paper with you; start writing 
when you think of the first 
sentence; if a deadline
forces you to start before that, just say the most important sentence
first; write about stuff you like; don't try to sound impressive; don't hesitate to change the topic on the fly;
use footnotes to contain digressions; use anaphora to knit
sentences together; read your essays out loud to see (a) where you stumble
over awkward phrases and (b) which bits are boring (the
paragraphs you dread reading); try to tell the
reader something new and useful; work in fairly big quanta of time;
when you restart, begin by rereading what you have so far; when you
finish, leave yourself something easy to start with; accumulate
notes for topics you plan to cover at the bottom of the file; don't
feel obliged to cover any of them; write for a reader who won't
read the essay as carefully as you do, just as pop songs are
designed to sound ok on crappy car radios; 
if you say anything mistaken, fix it immediately;
ask friends which sentence you'll regret most; go back and tone
down harsh remarks; publish stuff online, because
an audience makes you write more, and thus generate more
ideas; print out drafts instead of just looking at them
on the screen; use simple, germanic words; learn to distinguish
surprises from digressions; learn to recognize the approach of an
ending, and when one appears, grab it.Russian TranslationJapanese TranslationRomanian TranslationSpanish TranslationGerman TranslationChinese TranslationHungarian TranslationCatalan TranslationDanish TranslationArabic Translation","writing advice
",human,"human
","human
"
60,60,"April 2006, rev August 2009Plato quotes Socrates as saying ""the unexamined life is not worth
living.""  Part of what he meant was that the proper role of humans is to
think, just as the proper role of anteaters is to poke their noses
into anthills.A lot of ancient philosophy had the quality — and I
don't mean this in an insulting way — of the kind of conversations
freshmen have late at night in common rooms:

What is our purpose?  Well, we humans are
as conspicuously different from other animals as the anteater.
In our case the distinguishing feature is the ability to reason.
So obviously that is what we should be doing, and a human who
doesn't is doing a bad job of being human — is no better than an
animal.

Now we'd give a different answer.  At least, someone Socrates's age
would.  We'd ask why we even suppose we have a ""purpose"" in life.
We may be better adapted for some things than others; we
may be happier doing things we're adapted for; but why assume
purpose?The history of ideas
is a history of gradually discarding the assumption that it's all
about us.  No, it turns out, the earth is not the center of the
universe — not even the center of the solar system.  No, it turns
out, humans are not created by God in his own image; they're just
one species among many, descended not merely from apes, but from
microorganisms.  Even the concept of ""me"" turns out to be fuzzy
around the edges if you examine it closely.The idea that we're the center of things is difficult to discard.
So difficult that there's probably room to discard more.  Richard
Dawkins made another step in that direction only in the last several
decades, with the idea of the 
selfish gene.   
No, it turns
out, we're not even the protagonists: we're just the latest model
vehicle our genes have constructed to travel around in.  And having
kids is our genes heading for the lifeboats.  Reading
that book snapped my brain out of its previous way of thinking the
way Darwin's must have when it first appeared.(Few people can experience now what Darwin's contemporaries did
when The Origin of Species was first published, because everyone
now is raised either to take evolution for granted, or to regard
it as a heresy. No one encounters the idea of natural selection for
the first time as an adult.)So if you want to discover things that have been overlooked till
now, one really good place to look is in our blind spot: in our
natural, naive belief that it's all about us.  And expect to encounter
ferocious opposition if you do.Conversely, if you have to choose between two theories, prefer the
one that doesn't center on you.This principle isn't only for big ideas.  It works in everyday life,
too.  For example, suppose you're saving a piece of cake in the fridge, and you
come home one day to find your housemate has eaten
it.  Two possible theories:

a) Your housemate did it deliberately to upset you.  He knew
you were saving that piece of cake.b) Your housemate was hungry.

I say pick b.  No one knows who said ""never attribute to malice what
can be explained by incompetence,"" but it is a powerful idea.
Its more general version is our answer to the Greeks:
Don't see purpose where there isn't.
Or better still, the positive version:
See randomness.Korean Translation","philosophical reflection
",human,"human
","human
"
61,61,"July 2024Successful people tend to be persistent. New ideas often don't work
at first, but they're not deterred. They keep trying and eventually
find something that does.Mere obstinacy, on the other hand, is a recipe for failure. Obstinate
people are so annoying. They won't listen. They beat their heads
against a wall and get nowhere.But is there any real difference between these two cases? Are
persistent and obstinate people actually behaving differently? Or
are they doing the same thing, and we just label them later as
persistent or obstinate depending on whether they turned out to be
right or not?If that's the only difference then there's nothing to be learned
from the distinction. Telling someone to be persistent rather than
obstinate would just be telling them to be right rather than wrong,
and they already know that. Whereas if persistence and obstinacy
are actually different kinds of behavior, it would be worthwhile
to tease them apart.
[1]I've talked to a lot of determined people, and it seems to me that
they're different kinds of behavior. I've often walked away from a
conversation thinking either ""Wow, that guy is determined"" or ""Damn,
that guy is stubborn,"" and I don't think I'm just talking about
whether they seemed right or not. That's part of it, but not all
of it.There's something annoying about the obstinate that's not simply
due to being mistaken. They won't listen. And that's not true of
all determined people. I can't think of anyone more determined than
the Collison brothers, and when you point out a problem to them,
they not only listen, but listen with an almost predatory intensity.
Is there a hole in the bottom of their boat? Probably not, but if
there is, they want to know about it.It's the same with most successful people. They're never more
engaged than when you disagree with them. Whereas the obstinate
don't want to hear you. When you point out problems, their eyes
glaze over, and their replies sound like ideologues talking about
matters of doctrine.
[2]The reason the persistent and the obstinate seem similar is that
they're both hard to stop. But they're hard to stop in different
senses. The persistent are like boats whose engines can't be throttled
back. The obstinate are like boats whose rudders can't be turned.
[3]In the degenerate case they're indistinguishable: when there's only
one way to solve a problem, your only choice is whether to give up
or not, and persistence and obstinacy both say no. This is presumably
why the two are so often conflated in popular culture. It assumes
simple problems. But as problems get more complicated, we can see
the difference between them. The persistent are much more attached
to points high in the decision tree than to minor ones lower down,
while the obstinate spray ""don't give up"" indiscriminately over the
whole tree.The persistent are attached to the goal. The obstinate are attached
to their ideas about how to reach it.Worse still, that means they'll tend to be attached to their first
ideas about how to solve a problem, even though these are the least
informed by the experience of working on it. So the obstinate aren't
merely attached to details, but disproportionately likely to be
attached to wrong ones.Why are they like this? Why are the obstinate obstinate? One
possibility is that they're overwhelmed. They're not very capable.
They take on a hard problem. They're immediately in over their head.
So they grab onto ideas the way someone on the deck of a rolling
ship might grab onto the nearest handhold.That was my initial theory, but on examination it doesn't hold up.
If being obstinate were simply a consequence of being in over one's
head, you could make persistent people become obstinate by making
them solve harder problems. But that's not what happens. If you
handed the Collisons an extremely hard problem to solve, they
wouldn't become obstinate. If anything they'd become less obstinate.
They'd know they had to be open to anything.Similarly, if obstinacy were caused by the situation, the obstinate
would stop being obstinate when solving easier problems. But they
don't. And if obstinacy isn't caused by the situation, it must come
from within. It must be a feature of one's personality.Obstinacy is a reflexive resistance to changing one's ideas. This
is not identical with stupidity, but they're closely related. A
reflexive resistance to changing one's ideas becomes a sort of
induced stupidity as contrary evidence mounts. And obstinacy is a
form of not giving up that's easily practiced by the stupid. You
don't have to consider complicated tradeoffs; you just dig in your
heels. It even works, up to a point.The fact that obstinacy works for simple problems is an important
clue. Persistence and obstinacy aren't opposites. The relationship
between them is more like the relationship between the two kinds
of respiration we can do: aerobic respiration, and the anaerobic
respiration we inherited from our most distant ancestors. Anaerobic
respiration is a more primitive process, but it has its uses. When
you leap suddenly away from a threat, that's what you're using.The optimal amount of obstinacy is not zero. It can be good if your
initial reaction to a setback is an unthinking ""I won't give up,""
because this helps prevent panic. But unthinking only gets you so
far. The further someone is toward the obstinate end of the continuum,
the less likely they are to succeed in solving hard problems.
[4]Obstinacy is a simple thing. Animals have it. But persistence turns
out to have a fairly complicated internal structure.One thing that distinguishes the persistent is their energy. At the
risk of putting too much weight on words, they persist rather than
merely resisting. They keep trying things. Which means the persistent
must also be imaginative. To keep trying things, you have to keep
thinking of things to try.Energy and imagination make a wonderful combination. Each gets the
best out of the other. Energy creates demand for the ideas produced
by imagination, which thus produces more, and imagination gives
energy somewhere to go.
[5]Merely having energy and imagination is quite rare. But to solve
hard problems you need three more qualities: resilience, good
judgement, and a focus on some kind of goal.Resilience means not having one's morale destroyed by setbacks.
Setbacks are inevitable once problems reach a certain size, so if
you can't bounce back from them, you can only do good work on a
small scale. But resilience is not the same as obstinacy. Resilience
means setbacks can't change your morale, not that they can't change
your mind.Indeed, persistence often requires that one change one's mind.
That's where good judgement comes in. The persistent are quite
rational. They focus on expected value. It's this, not recklessness,
that lets them work on things that are unlikely to succeed.There is one point at which the persistent are often irrational
though: at the very top of the decision tree. When they choose
between two problems of roughly equal expected value, the choice
usually comes down to personal preference. Indeed, they'll often
classify projects into deliberately wide bands of expected value
in order to ensure that the one they want to work on still qualifies.Empirically this doesn't seem to be a problem. It's ok to be
irrational near the top of the decision tree. One reason is that
we humans will work harder on a problem we love. But there's another
more subtle factor involved as well: our preferences among problems
aren't random. When we love a problem that other people don't, it's
often because we've unconsciously noticed that it's more important
than they realize.Which leads to our fifth quality: there needs to be some overall
goal. If you're like me you began, as a kid, merely with the desire
to do something great. In theory that should be the most powerful
motivator of all, since it includes everything that could possibly
be done. But in practice it's not much use, precisely because it
includes too much. It doesn't tell you what to do at this moment.So in practice your energy and imagination and resilience and good
judgement have to be directed toward some fairly specific goal. Not
too specific, or you might miss a great discovery adjacent to what
you're searching for, but not too general, or it won't work to
motivate you.
[6]When you look at the internal structure of persistence, it doesn't
resemble obstinacy at all. It's so much more complex. Five distinct
qualities — energy, imagination, resilience, good judgement, and
focus on a goal — combine to produce a phenomenon that seems a bit
like obstinacy in the sense that it causes you not to give up. But
the way you don't give up is completely different. Instead of merely
resisting change, you're driven toward a goal by energy and resilience,
through paths discovered by imagination and optimized by judgement.
You'll give way on any point low down in the decision tree, if its
expected value drops sufficiently, but energy and resilience keep
pushing you toward whatever you chose higher up.Considering what it's made of, it's not surprising that the right
kind of stubbornness is so much rarer than the wrong kind, or that
it gets so much better results. Anyone can do obstinacy. Indeed,
kids and drunks and fools are best at it. Whereas very few people
have enough of all five of the qualities that produce the right kind
of stubbornness, but when they do the results are magical.
Notes[1]
I'm going to use ""persistent"" for the good kind of stubborn
and ""obstinate"" for the bad kind, but I can't claim I'm simply
following current usage. Conventional opinion barely distinguishes
between good and bad kinds of stubbornness, and usage is correspondingly
promiscuous. I could have invented a new word for the good kind,
but it seemed better just to stretch ""persistent.""[2]
There are some domains where one can succeed by being obstinate.
Some political leaders have been notorious for it. But it won't
work in situations where you have to pass external tests. And indeed
the political leaders who are famous for being obstinate are famous
for getting power, not for using it well.[3]
There will be some resistance to turning the rudder of a
persistent person, because there's some cost to changing direction.[4]
The obstinate do sometimes succeed in solving hard problems.
One way is through luck: like the stopped clock that's right twice
a day, they seize onto some arbitrary idea, and it turns out to be
right. Another is when their obstinacy cancels out some other form
of error. For example, if a leader has overcautious subordinates,
their estimates of the probability of success will always be off
in the same direction. So if he mindlessly says ""push ahead regardless""
in every borderline case, he'll usually turn out to be right.[5]
If you stop there, at just energy and imagination, you get
the conventional caricature of an artist or poet.[6]
Start by erring on the small side. If you're inexperienced
you'll inevitably err on one side or the other, and if you err on
the side of making the goal too broad, you won't get anywhere.
Whereas if you err on the small side you'll at least be moving
forward. Then, once you're moving, you expand the goal.Thanks to Trevor Blackwell, 
Jessica Livingston, Jackie McDonough,
Courtenay Pipkin, Harj Taggar, and Garry Tan for reading drafts of
this.","philosophical reflection
",human,"human
","human
"
62,62,"September 2017The most valuable insights are both general and surprising. 
F = ma for example. But general and surprising is a hard
combination to achieve. That territory tends to be picked
clean, precisely because those insights are so valuable.Ordinarily, the best that people can do is one without the
other: either surprising without being general (e.g.
gossip), or general without being surprising (e.g.
platitudes).Where things get interesting is the moderately valuable
insights.  You get those from small additions of whichever
quality was missing.  The more common case is a small
addition of generality: a piece of gossip that's more than
just gossip, because it teaches something interesting about
the world. But another less common approach is to focus on
the most general ideas and see if you can find something new
to say about them. Because these start out so general, you
only need a small delta of novelty to produce a useful
insight.A small delta of novelty is all you'll be able to get most
of the time. Which means if you take this route, your ideas
will seem a lot like ones that already exist. Sometimes
you'll find you've merely rediscovered an idea that did
already exist.  But don't be discouraged.  Remember the huge
multiplier that kicks in when you do manage to think of
something even a little new.Corollary: the more general the ideas you're talking about,
the less you should worry about repeating yourself.  If you
write enough, it's inevitable you will.  Your brain is much
the same from year to year and so are the stimuli that hit
it. I feel slightly bad when I find I've said something
close to what I've said before, as if I were plagiarizing
myself. But rationally one shouldn't.  You won't say
something exactly the same way the second time, and that
variation increases the chance you'll get that tiny but
critical delta of novelty.And of course, ideas beget ideas.  (That sounds 
familiar.)
An idea with a small amount of novelty could lead to one
with more. But only if you keep going. So it's doubly
important not to let yourself be discouraged by people who
say there's not much new about something you've discovered.
""Not much new"" is a real achievement when you're talking
about the most general ideas. It's not true that there's nothing new under the sun.  There
are some domains where there's almost nothing new.  But
there's a big difference between nothing and almost nothing,
when it's multiplied by the area under the sun.
Thanks to Sam Altman, Patrick Collison, and Jessica
Livingston for reading drafts of this.Japanese Translation","philosophical reflection
",human,"human
","human
"
63,63,"November 2022Since I was about 9 I've been puzzled by the apparent contradiction
between being made of matter that behaves in a predictable way, and
the feeling that I could choose to do whatever I wanted. At the
time I had a self-interested motive for exploring the question. At
that age (like most succeeding ages) I was always in trouble with
the authorities, and it seemed to me that there might possibly be
some way to get out of trouble by arguing that I wasn't responsible
for my actions. I gradually lost hope of that, but the puzzle
remained: How do you reconcile being a machine made of matter with
the feeling that you're free to choose what you do?
[1]The best way to explain the answer may be to start with a slightly
wrong version, and then fix it. The wrong version is: You can do
what you want, but you can't want what you want. Yes, you can control
what you do, but you'll do what you want, and you can't control
that.The reason this is mistaken is that people do sometimes change what
they want. People who don't want to want something — drug addicts,
for example — can sometimes make themselves stop wanting it. And
people who want to want something — who want to like classical
music, or broccoli — sometimes succeed.So we modify our initial statement: You can do what you want, but
you can't want to want what you want.That's still not quite true. It's possible to change what you want
to want. I can imagine someone saying ""I decided to stop wanting
to like classical music."" But we're getting closer to the truth.
It's rare for people to change what they want to want, and the more
""want to""s we add, the rarer it gets.We can get arbitrarily close to a true statement by adding more ""want
to""s in much the same way we can get arbitrarily close to 1 by adding
more 9s to a string of 9s following a decimal point. In practice
three or four ""want to""s must surely be enough. It's hard even to
envision what it would mean to change what you want to want to want
to want, let alone actually do it.So one way to express the correct answer is to use a regular
expression. You can do what you want, but there's some statement
of the form ""you can't (want to)* want what you want"" that's true.
Ultimately you get back to a want that you don't control.
[2]
Notes[1]
I didn't know when I was 9 that matter might behave randomly,
but I don't think it affects the problem much. Randomness destroys
the ghost in the machine as effectively as determinism.[2]
If you don't like using an expression, you can make the same
point using higher-order desires: There is some n such that you
don't control your nth-order desires.
Thanks to Trevor Blackwell,
Jessica Livingston, Robert Morris, and
Michael Nielsen for reading drafts of this.Irish Translation","philosophical reflection
",human,"human
","human
"
64,64,"October 2021If you asked people what was special about Einstein, most would say
that he was really smart. Even the ones who tried to give you a
more sophisticated-sounding answer would probably think this first.
Till a few years ago I would have given the same answer myself. But
that wasn't what was special about Einstein. What was special about
him was that he had important new ideas. Being very smart was a
necessary precondition for having those ideas, but the two are not
identical.It may seem a hair-splitting distinction to point out that intelligence
and its consequences are not identical, but it isn't. There's a big
gap between them. Anyone who's spent time around universities and
research labs knows how big. There are a lot of genuinely smart
people who don't achieve very much.I grew up thinking that being smart was the thing most to be desired.
Perhaps you did too. But I bet it's not what you really want. Imagine
you had a choice between being really smart but discovering nothing
new, and being less smart but discovering lots of new ideas. Surely
you'd take the latter. I would. The choice makes me uncomfortable,
but when you see the two options laid out explicitly like that,
it's obvious which is better.The reason the choice makes me uncomfortable is that being smart
still feels like the thing that matters, even though I know
intellectually that it isn't. I spent so many years thinking it
was. The circumstances of childhood are a perfect storm for fostering
this illusion. Intelligence is much easier to measure than the value
of new ideas, and you're constantly being judged by it. Whereas
even the kids who will ultimately discover new things aren't usually
discovering them yet. For kids that way inclined, intelligence is
the only game in town.There are more subtle reasons too, which persist long into adulthood.
Intelligence wins in conversation, and thus becomes the basis of
the dominance hierarchy.
[1]
Plus having new ideas is such a new
thing historically, and even now done by so few people, that society
hasn't yet assimilated the fact that this is the actual destination,
and intelligence merely a means to an end.
[2]Why do so many smart people fail to discover anything new? Viewed
from that direction, the question seems a rather depressing one.
But there's another way to look at it that's not just more optimistic,
but more interesting as well. Clearly intelligence is not the only
ingredient in having new ideas. What are the other ingredients?
Are they things we could cultivate?Because the trouble with intelligence, they say, is that it's mostly
inborn. The evidence for this seems fairly convincing, especially
considering that most of us don't want it to be true, and the
evidence thus has to face a stiff headwind. But I'm not going
to get into that question here, because it's the other ingredients
in new ideas that I care about, and it's clear that many of them
can be cultivated.That means the truth is excitingly different from the story I got
as a kid. If intelligence is what matters, and also mostly inborn,
the natural consequence is a sort of Brave New World fatalism. The
best you can do is figure out what sort of work you have an ""aptitude""
for, so that whatever intelligence you were born with will at least
be put to the best use, and then work as hard as you can at it.
Whereas if intelligence isn't what matters, but only one of several
ingredients in what does, and many of those aren't inborn, things
get more interesting. You have a lot more control, but the problem
of how to arrange your life becomes that much more complicated.So what are the other ingredients in having new ideas? The fact
that I can even ask this question proves the point I raised earlier
— that society hasn't assimilated the fact that it's this and not
intelligence that matters. Otherwise we'd all know the answers
to such a fundamental question.
[3]I'm not going to try to provide a complete catalogue of the other
ingredients here. This is the first time I've posed
the question to myself this way, and I think it may take a while
to answer. But I wrote recently about one of the most important:
an obsessive interest in a particular topic. 
And this can definitely be cultivated.Another quality you need in order to discover new ideas is
independent-mindedness. I wouldn't want to 
claim that this is
distinct from intelligence — I'd be reluctant to call someone smart
who wasn't independent-minded — but though largely inborn, this
quality seems to be something that can be cultivated to some extent.There are general techniques for having new ideas — for example,
for working on your own projects
and
for overcoming the obstacles you face with early work
— and these
can all be learned. Some of them can be learned by societies. And
there are also collections of techniques for generating specific types
of new ideas, like startup ideas and 
essay topics.And of course there are a lot of fairly mundane ingredients in
discovering new ideas, like working hard, 
getting enough sleep, avoiding certain
kinds of stress, having the right colleagues, and finding tricks
for working on what you want even when it's not what you're supposed
to be working on. Anything that prevents people from doing great
work has an inverse that helps them to. And this class of ingredients
is not as boring as it might seem at first. For example, having new
ideas is generally associated with youth. But perhaps it's not youth
per se that yields new ideas, but specific things that come with
youth, like good health and lack of responsibilities. Investigating
this might lead to strategies that will help people of any age to
have better ideas.One of the most surprising ingredients in having new ideas is writing
ability. There's a class of new ideas that are best discovered by
writing essays and books. And that ""by"" is deliberate: you don't
think of the ideas first, and then merely write them down. There
is a kind of thinking that one does by writing, and if you're clumsy
at writing, or don't enjoy doing it, that will get in your way if
you try to do this kind of thinking.
[4]I predict the gap between intelligence and new ideas will turn out
to be an interesting place. If we think of this gap merely as a measure
of unrealized potential, it becomes a sort of wasteland that we try to
hurry through with our eyes averted. But if we flip the question,
and start inquiring into the other ingredients in new ideas that
it implies must exist, we can mine this gap for discoveries about
discovery.
Notes[1]
What wins in conversation depends on who with. It ranges from
mere aggressiveness at the bottom, through quick-wittedness in the
middle, to something closer to actual intelligence at the top,
though probably always with some component of quick-wittedness.[2]
Just as intelligence isn't the only ingredient in having new
ideas, having new ideas isn't the only thing intelligence is useful
for. It's also useful, for example, in diagnosing problems and figuring
out how to fix them. Both overlap with having new ideas, but both
have an end that doesn't.Those ways of using intelligence are much more common than having
new ideas. And in such cases intelligence is even harder to distinguish
from its consequences.[3]
Some would attribute the difference between intelligence and
having new ideas to ""creativity,"" but this doesn't seem a very
useful term. As well as being pretty vague, it's shifted half a frame
sideways from what we care about: it's neither separable from
intelligence, nor responsible for all the difference between
intelligence and having new ideas.[4]
Curiously enough, this essay is an example. It started out
as an essay about writing ability. But when I came to the distinction
between intelligence and having new ideas, that seemed so much more
important that I turned the original essay inside out, making that
the topic and my original topic one of the points in it. As in many
other fields, that level of reworking is easier to contemplate once
you've had a lot of practice.
Thanks to Trevor Blackwell, Patrick Collison, Jessica Livingston,
Robert Morris, Michael Nielsen, and Lisa Randall for reading drafts
of this.
","philosophical reflection
",human,"human
","human
"
65,65,"April 2007There are two different ways people judge you.  Sometimes judging
you correctly is the end goal.  But there's a second much more
common type of judgement where it isn't.  We tend to regard all
judgements of us as the first type.  We'd probably be happier if
we realized which are and which aren't.The first type of judgement, the type where judging you is the end
goal, include court cases, grades in classes, and most competitions.
Such judgements can of course be mistaken, but because the goal is
to judge you correctly, there's usually some kind of appeals process.
If you feel you've been misjudged, you can protest that you've been
treated unfairly.Nearly all the judgements made on children are of this type, so we
get into the habit early in life of thinking that all judgements
are.But in fact there is a second much larger class of judgements where
judging you is only a means to something else.  These include college
admissions, hiring and investment decisions, and of course the
judgements made in dating.  This kind of judgement is not really
about you.Put yourself in the position of someone selecting players for a
national team.  Suppose for the sake of simplicity that this is a
game with no positions, and that you have to select 20 players.
There will be a few stars who clearly should make the team, and
many players who clearly shouldn't.  The only place your judgement
makes a difference is in the borderline cases.  Suppose you screw
up and underestimate the 20th best player, causing him not to make
the team, and his place to be taken by the 21st best.  You've still
picked a good team.  If the players have the usual distribution of
ability, the 21st best player will be only slightly worse than the
20th best.  Probably the difference between them will be less than
the measurement error.The 20th best player may feel he has been misjudged.  But your goal
here wasn't to provide a service estimating people's ability.  It
was to pick a team, and if the difference between the 20th and 21st
best players is less than the measurement error, you've still done
that optimally.It's a false analogy even to use the word unfair to describe this
kind of misjudgement.  It's not aimed at producing a correct estimate
of any given individual, but at selecting a reasonably optimal set.One thing that leads us astray here is that the selector seems to
be in a position of power.  That makes him seem like a judge.  If
you regard someone judging you as a customer instead of a judge,
the expectation of fairness goes away.  The author of a good novel
wouldn't complain that readers were unfair for preferring a
potboiler with a racy cover.  Stupid, perhaps, but not unfair.Our early training and our self-centeredness combine to make us
believe that every judgement of us is about us.  In fact most aren't.
This is a rare case where being less self-centered will make people
more confident.  Once you realize how little most people judging
you care about judging you accurately—once you realize that because
of the normal distribution of most applicant pools, it matters least
to judge accurately in precisely the cases where judgement has the
most effect—you won't take rejection so personally.And curiously enough, taking rejection less personally may help you
to get rejected less often.  If you think someone judging you will
work hard to judge you correctly, you can afford to be passive.
But the more you realize that most judgements are greatly influenced
by random, extraneous factors—that most people judging you are
more like a fickle novel buyer than a wise and perceptive 
magistrate—the more you realize you can do things to influence the
outcome.One good place to apply this principle is in college applications.
Most high school students applying to college do it with the usual
child's mix of inferiority and self-centeredness: inferiority in
that they assume that admissions committees must be all-seeing;
self-centeredness in that they assume admissions committees care
enough about them to dig down into their application and figure out
whether they're good or not.  These combine to make applicants
passive in applying and hurt when they're rejected.  If college
applicants realized how quick and impersonal most selection processes
are, they'd make more effort to sell themselves, and take the outcome
less personally.Spanish TranslationRussian TranslationArabic Translation","life advice
",human,"human
","human
"
66,66,"January 2005(I wrote this talk for a
high school.  I never actually 
gave it, because the school authorities vetoed the plan to invite me.)When I said I was speaking at a high school, my friends were curious.
What will you say to high school students?  So I asked them, what
do you wish someone had told you in high school?  Their answers
were remarkably similar.  So I'm going to tell you what we all wish
someone had told us.I'll start by telling you something you don't have to know in high
school: what you want to do with your life.  People are always
asking you this, so you think you're supposed to have an answer.
But adults ask this mainly as a conversation starter.   They want
to know what sort of person you are, and this question is just to
get you talking.  They ask it the way you might poke a hermit crab
in a tide pool, to see what it does.If I were back in high school and someone asked about my plans, I'd
say that my first priority was to learn what the options were.  You
don't need to be in a rush to choose your life's work.  What you   
need to do is discover what you like.  You have to work on stuff  
you like if you want to be good at what you do.It might seem that nothing would be easier than deciding what you
like, but it turns out to be hard, partly because it's hard to get
an accurate picture of most jobs.  Being a doctor is not the way
it's portrayed on TV.  Fortunately you can also watch real doctors,
by volunteering in hospitals. [1]But there are other jobs you can't learn about, because no one is
doing them yet.  Most of the work I've done in the last ten years
didn't exist when I was in high school.  The world changes fast,  
and the rate at which it changes is itself speeding up.  In such a
world it's not a good idea to have fixed plans.And yet every May, speakers all over the country fire up the Standard 
Graduation Speech, the theme of which is: don't give up on your
dreams.  I know what they mean, but this is a bad way to put it,
because it implies you're supposed to be bound by some plan you
made early on.  The computer world has a name for this: premature
optimization.  And it is synonymous with disaster.  These speakers
would do better to say simply, don't give up.What they really mean is, don't get demoralized.  Don't think that   
you can't do what other people can.   And I agree you shouldn't  
underestimate your potential.  People who've done great things tend
to seem as if they were a race apart.  And most biographies only   
exaggerate this illusion, partly due to the worshipful attitude   
biographers inevitably sink into, and partly because, knowing how
the story ends, they can't help streamlining the plot till it seems
like the subject's life was a matter of destiny, the mere unfolding
of some innate genius.   In fact I suspect if you had the sixteen
year old Shakespeare or Einstein in school with you, they'd seem
impressive, but not totally unlike your other friends.Which is an uncomfortable thought.  If they were just like us, then
they had to work very hard to do what they did.  And that's one  
reason we like to believe in genius.  It gives us an excuse for
being lazy.  If these guys were able to do what they did only because
of some magic Shakespeareness or Einsteinness, then it's not our
fault if we can't do something as good.I'm not saying there's no such thing as genius.  But if you're
trying to choose between two theories and one gives you an excuse 
for being lazy, the other one is probably right.So far we've cut the Standard Graduation Speech down from ""don't
give up on your dreams"" to ""what someone else can do, you can do.""
But it needs to be cut still further.  There is some variation
in natural ability.  Most people overestimate its role, but it does  
exist.  If I were talking to a guy four feet tall whose ambition 
was to play in the NBA, I'd feel pretty stupid saying, you can
do anything if you really try. [2]We need to cut the Standard Graduation Speech down to, ""what someone
else with your abilities can do, you can do; and don't underestimate
your abilities.""  But as so often happens, the closer you get to
the truth, the messier your sentence gets.  We've taken a nice, 
neat (but wrong) slogan, and churned it up like a mud puddle.  It
doesn't make a very good speech anymore.  But worse still, it doesn't
tell you what to do anymore.  Someone with your abilities?  What  
are your abilities?UpwindI think the solution is to work in the other direction.  Instead
of working back from a goal, work forward from promising situations.
This is what most successful people actually do anyway.In the graduation-speech approach, you decide where you want to be
in twenty years, and then ask: what should I do now to get there?  
I propose instead that you don't commit to anything in the future,
but just look at the options available now, and choose those that
will give you the most promising range of options afterward.It's not so important what you work on, so long as you're not wasting
your time.  Work on things that interest you and increase your
options, and worry later about which you'll take.Suppose you're a college freshman deciding whether to major in math   
or economics.  Well, math will give you more options: you can go into
almost any field from math.  If you major in math it will be easy
to get into grad school in economics, but if you major in economics
it will be hard to get into grad school in math.Flying a glider is a good metaphor here.  Because a glider doesn't
have an engine, you can't fly into the wind without losing a lot
of altitude.  If you let yourself get far downwind of good places    
to land, your options narrow uncomfortably.  As a rule you want to
stay upwind.  So I propose that as a replacement for ""don't give   
up on your dreams.""  Stay upwind.How do you do that, though?  Even if math is upwind of economics,
how are you supposed to know that as a high school student?Well, you don't, and that's what you need to find out.   Look for smart people
and hard problems.  Smart people tend to clump together, and if you
can find such a clump, it's probably worthwhile to join it.  But
it's not straightforward to find these, because there is a lot of  
faking going on.To a newly arrived undergraduate, all university departments look
much the same.  The professors all seem forbiddingly intellectual
and publish papers unintelligible to outsiders.  But while in some
fields the papers are unintelligible because they're full of hard
ideas, in others they're deliberately written in an obscure way to
seem as if they're saying something important.  This may seem a   
scandalous proposition, but it has been experimentally verified,
in the famous Social Text affair.  Suspecting that the papers
published by literary theorists were often just intellectual-sounding
nonsense, a physicist deliberately wrote a paper full of
intellectual-sounding nonsense, and submitted it to a literary
theory journal, which published it.The best protection is always to be working on hard problems.   
Writing novels is hard.  Reading novels isn't. 
Hard means worry: if you're not worrying that
something you're making will come out badly, or that you won't be 
able to understand something you're studying, then it isn't hard
enough.  There has to be suspense.Well, this seems a grim view of the world, you may think.  What I'm
telling you is that you should worry?  Yes, but it's not as bad as
it sounds.  It's exhilarating to overcome worries.  You don't see
faces much happier than people winning gold medals.  And you know
why they're so happy?  Relief.I'm not saying this is the only way to be happy.  Just that some
kinds of worry are not as bad as they sound.AmbitionIn practice, ""stay upwind"" reduces to ""work on hard problems.""  And  
you can start today.  I wish I'd grasped that in
high school.Most people like to be good at what they do.  In the so-called real  
world this need is a powerful force.  But high school students
rarely benefit from it, because they're given a fake thing to do.  
When I was in high school, I let myself believe that my job was to 
be a high school student.  And so I let my need to be good at what 
I did be satisfied by merely doing well in school.If you'd asked me in high school what the difference was between
high school kids and adults, I'd have said it was that adults had 
to earn a living.  Wrong.  It's that adults take responsibility for
themselves.  Making a living is only a small part of it.
Far more important is to take intellectual responsibility for oneself.If I had to go through high school again, I'd treat it like a day
job.  I don't mean that I'd slack in school.  Working at something
as a day job doesn't mean doing it badly.  It means not being defined
by it.  I mean I wouldn't think of myself as a high school student,
just as a musician with a day job as a waiter doesn't think of   
himself as a waiter. [3]   And when I wasn't working at my day job
I'd start trying to do real work.When I ask people what they regret most about high school, they
nearly all say the same thing: that they wasted so much time.  If
you're wondering what you're doing now that you'll regret most
later, that's probably it. [4]Some people say this is inevitable — that high school students
aren't capable of getting anything done yet.  But I don't think
this is true.  And the proof is that you're bored.  You probably
weren't bored when you were eight.  When you're eight it's called
""playing"" instead of ""hanging out,"" but it's the same thing.  And
when I was eight, I was rarely bored.  Give me a back yard and a
few other kids and I could play all day.The reason this got stale in middle school and high school, I now
realize, is that I was ready for something else.  Childhood was
getting old.I'm not saying you shouldn't hang out with your friends — that you
should all become humorless little robots who do nothing but work.
Hanging out with friends is like chocolate cake.  You enjoy it more
if you eat it occasionally than if you eat nothing but chocolate  
cake for every meal.  No matter how much you like chocolate cake,
you'll be pretty queasy after the third meal of it.  And that's  
what the malaise one feels in high school is: mental queasiness.
[5]You may be thinking, we have to do more than get good grades.  We
have to have extracurricular activities.  But you know
perfectly well how bogus most of these are.  Collecting donations
for a charity is an admirable thing to do, but it's not hard.
It's not getting something done.  What I mean by getting something
done is learning how to write well, or how to program computers,
or what life was really like in preindustrial societies, or how to   
draw the human face from life.  This sort of thing rarely translates
into a line item on a college application.CorruptionIt's dangerous to design your life around getting into college,  
because the people you have to impress to get into college are not 
a very discerning audience.  At most colleges, it's not the professors
who decide whether you get in, but admissions officers, and they
are nowhere near as smart.  They're the NCOs of the intellectual
world. They can't tell how smart you are.
The mere existence of prep schools is proof of that.Few parents
would pay so much for their kids to go to a school that didn't 
improve their admissions prospects.  Prep schools openly say this
is one of their aims.  But what that means, if you stop to 
think about it, is that they can
hack the admissions process: that they can take the very same kid
and make him seem a more appealing candidate than he would if he  
went to the local public school. [6]Right now most of you feel your job in life is to be a promising
college applicant.  But that means you're designing your life to
satisfy a process so mindless that there's a whole industry devoted
to subverting it.  No wonder you become cynical.  The malaise you
feel is the same that a producer of reality TV shows or a tobacco 
industry executive feels.  And you don't even get paid a lot.So what do you do?  What you should not do is rebel.  That's what
I did, and it was a mistake.  I didn't realize exactly what was   
happening to us, but I smelled a major rat.  And so I just gave up.
Obviously the world sucked, so why bother?When I discovered that one of our teachers was herself using Cliff's
Notes, it seemed par for the course.  Surely it meant nothing to
get a good grade in such a class.In retrospect this was stupid.  It was like someone getting fouled
in a soccer game and saying, hey, you fouled me, that's against the
rules, and walking off the field in indignation.  Fouls happen. 
The thing to do when you get fouled is not to lose your cool.  Just
keep playing.  By putting you in this situation, society has fouled you.   Yes, 
as you suspect, a lot of the stuff you learn in your classes is   
crap.  And yes, as you suspect, the college admissions process is
largely a charade.  But like many fouls, this one was unintentional.
[7] So just keep playing.Rebellion is almost as stupid as obedience.  In either case you let
yourself be defined by what they tell you to do.  The best plan, I
think, is to step onto an orthogonal vector.  Don't just do what 
they tell you, and don't just refuse to.  Instead treat school as
a day job.  As day jobs go, it's pretty sweet.  You're done at 3
o'clock, and you can even work on your own stuff while you're there.CuriosityAnd what's your real job supposed to be?  Unless you're Mozart,   
your first task is to figure that out.  What are the great things
to work on?  Where are the imaginative people?  And most importantly,
what are you interested in?  The word ""aptitude"" is misleading,
because it implies something innate.  The most powerful sort of
aptitude is a consuming interest in some question, and such interests
are often acquired tastes.A distorted version of this idea has filtered into popular culture
under the name ""passion.""  I recently saw an ad for waiters saying
they wanted people with a ""passion for service.""  The real thing 
is not something one could have for waiting on tables.  And passion
is a bad word for it. A better name would be curiosity.Kids are curious, but the curiosity I mean has a different shape from kid
curiosity.  Kid curiosity is broad and shallow; they ask why at
random about everything.  In most adults this curiosity dries up
entirely.  It has to: you can't get anything done if you're always
asking why about everything.  But in ambitious adults, instead of
drying up, curiosity becomes narrow and deep.  The mud flat morphs
into a well.Curiosity turns work into play.  For Einstein, relativity wasn't a
book full of hard stuff he had to learn for an exam.  It was a
mystery he was trying to solve.  So it probably felt like less work
to him to invent it than it would seem to someone now to learn it
in a class.One of the most dangerous illusions you get from school is the idea
that doing great things requires a lot of discipline.  Most subjects
are taught in such a boring way that it's only by discipline that
you can flog yourself through them.  So I was surprised when, early
in college, I read a quote by Wittgenstein saying that he had no 
self-discipline and had never been able to deny himself anything, 
not even a cup of coffee.Now I know a number of people who do great work, and it's the same
with all of them.  They have little discipline.  They're all terrible
procrastinators and find it almost impossible to make themselves
do anything they're not interested in.  One still hasn't sent out
his half of the thank-you notes from his wedding, four years ago.
Another has 26,000 emails in her inbox.I'm not saying you can get away with zero self-discipline.  You 
probably need about the amount you need to go running.  I'm often  
reluctant to go running, but once I do, I enjoy it.  And if I don't  
run for several days, I feel ill.  It's the same with people who 
do great things.  They know they'll feel bad if they don't work,
and they have enough discipline to get themselves to their desks
to start working.  But once they get started, interest takes over,
and discipline is no longer necessary.Do you think Shakespeare was gritting his teeth and diligently
trying to write Great Literature?  Of course not.  He was having
fun.  That's why he's so good.If you want to do good work, what you need is a great curiosity   
about a promising question.   The critical moment for Einstein
was when he looked at Maxwell's equations and said, what the hell
is going on here?It can take years to zero in on a productive question, because it
can take years to figure out what a subject is really about.  To
take an extreme example, consider math.  Most people think they
hate math, but the boring stuff you do in school under the name
""mathematics"" is not at all like what mathematicians do.The great mathematician G. H.  Hardy said he didn't like math in 
high school either.  He only took it up because he was better at
it than the other students.  Only later did he realize math was
interesting — only later did he start to ask questions instead of
merely answering them correctly.When a friend of mine used to grumble because he had to write a
paper for school, his mother would tell him: find a way to make it
interesting.  That's what you need to do: find a question that makes
the world interesting.  People who do great things look at the same
world everyone else does, but notice some odd detail that's
compellingly mysterious.And not only in intellectual matters.  Henry Ford's great question 
was, why do cars have to be a luxury item?  What would happen if
you treated them as a commodity?  Franz Beckenbauer's was, in effect,
why does everyone have to stay in his position?  Why can't defenders
score goals too?NowIf it takes years to articulate great questions, what do you do now,
at sixteen?  Work toward finding one.  Great questions don't appear
suddenly.  They gradually congeal in your head.  And what makes
them congeal is experience.  So the way to find great questions is
not to search for them — not to wander about thinking, what great  
discovery shall I make?  You can't answer that; if you could, you'd 
have made it.The way to get a big idea to appear in your head is not to hunt for
big ideas, but to put in a lot of time on work that interests you,
and in the process keep your mind open enough that a big idea can
take roost.  Einstein, Ford, and Beckenbauer all used this recipe.
They all knew their work like a piano player knows the keys.  So  
when something seemed amiss to them, they had the confidence to
notice it.Put in time how and on what?  Just pick a project that seems
interesting: to master some chunk of material, or to make something,
or to answer some question.  Choose a project that will take less
than a month, and make it something you have the means to finish.
Do something hard enough to stretch you, but only just, especially 
at first.  If you're deciding between two projects, choose whichever 
seems most fun. If one blows up in your face, start another.  Repeat
till, like an internal combustion engine, the process becomes   
self-sustaining, and each project generates the next one.  (This
could take years.)It may be just as well not to do a project ""for school,"" if that
will restrict you or make it seem like work.  Involve your friends
if you want, but not too many, and only if they're not flakes.  
Friends offer moral support (few startups are started by one person),
but secrecy also has its advantages.  There's something pleasing
about a secret project.  And you can take more risks, because no  
one will know if you fail.Don't worry if a project doesn't seem to be on the path to some
goal you're supposed to have. Paths can bend a lot more than you
think.  So let the path grow out the project.  The most important
thing is to be excited about it, because it's by doing that you 
learn.Don't disregard unseemly motivations.  One of the most powerful is
the desire to be better than other people at something.  Hardy said
that's what got him started, and I think the only unusual thing  
about him is that he admitted it.  Another powerful motivator is
the desire to do, or know, things you're not supposed to.  Closely
related is the desire to do something audacious.  Sixteen year olds
aren't supposed to write novels.  So if you try, anything you achieve
is on the plus side of the ledger; if you fail utterly, you're doing
no worse than expectations.  [8]Beware of bad models.  Especially when they excuse laziness.  When  
I was in high school I used to write ""existentialist"" short stories
like ones I'd seen by famous writers.  My stories didn't have a lot
of plot, but they were very deep.  And they were less work to write
than entertaining ones would have been.  I should have known that
was a danger sign.  And in fact I found my stories pretty boring;  
what excited me was the idea of writing serious, intellectual stuff
like the famous writers.Now I have enough experience to realize that those famous writers
actually sucked.  Plenty of famous people do; in the short term,
the quality of one's work is only a small component of fame.  
I should have been less worried about doing something
that seemed cool, and just done something I liked.  That's the
actual road to coolness anyway.A key ingredient in many projects, almost a project on its own, is 
to find good books.  Most books are bad.  Nearly all textbooks are
bad. [9]  So don't assume a subject is to be learned from whatever
book on it happens to be closest.  You have to search actively for 
the tiny number of good books.The important thing is to get out there and do stuff.  Instead of
waiting to be taught, go out and learn.Your life doesn't have to be shaped by admissions officers.  It  
could be shaped by your own curiosity.  It is for all ambitious   
adults.  And you don't have to wait to start.  In fact, you don't 
have to wait to be an adult.  There's no switch inside you that
magically flips when you turn a certain age or graduate from some
institution.  You start being an adult when you decide to take
responsibility for your life.  You can do that at any age.  [10]This may sound like bullshit.  I'm just a minor, you may think, I
have no money, I have to live at home, I have to do what adults  
tell me all day long.  Well, most adults labor under restrictions  
just as cumbersome, and they manage to get things done.  If you
think it's restrictive being a kid, imagine having kids.The only real difference between adults and high school kids is
that adults realize they need to get things done, and high school
kids don't.  That realization hits most people around 23.  But I'm
letting you in on the secret early.  So get to work.  Maybe you can
be the first generation whose greatest regret from high school isn't
how much time you wasted.
Notes[1] A doctor friend warns that even this can give an inaccurate 
picture. ""Who knew how much time it would take up, how little
autonomy one would have for endless years of training, and how
unbelievably annoying it is to carry a beeper?""[2] His best bet would probably be to become dictator and intimidate
the NBA into letting him play.  So far the closest anyone has come
is Secretary of Labor.[3] A day job is one you take to pay the bills so you can do what
you really want, like play in a band, or invent relativity.Treating high school as a day job might actually make it easier for
some students to get good grades.  If you treat your classes
as a game, you won't be demoralized if they seem pointless.However bad your classes, you need to get good grades in them to   
get into a decent college.  And that is worth doing, because
universities are where a lot of the clumps of smart people are these
days.[4] The second biggest regret was caring so much about unimportant
things.  And especially about what other people thought of them.I think what they really mean, in the latter case, is caring what
random people thought of them.  Adults care just as much what other
people think, but they get to be more selective about the other
people.I have about thirty friends whose opinions I care about,
and the opinion of the rest of the world barely affects me.  The
problem in high school is that your peers are chosen for you by    
accidents of age and geography, rather than by you based on respect
for their judgement.[5] The key to wasting time is distraction.  Without distractions
it's too obvious to your brain that you're not doing anything with
it, and you start to feel uncomfortable.  If you want to measure 
how dependent you've become on distractions, try this experiment:
set aside a chunk of time on a weekend and sit alone and think.
You can have a notebook to write your thoughts down in, but nothing
else: no friends, TV, music, phone, IM, email, Web, games, books, 
newspapers, or magazines.  Within an hour most people will feel a 
strong craving for distraction.[6] I don't mean to imply that the only function of prep schools
is to trick admissions officers.  They also generally provide a 
better education.  But try this thought experiment: suppose prep
schools supplied the same superior education but had a tiny (.001)
negative effect on college admissions.  How many parents would still
send their kids to them?It might also be argued that kids who went to prep schools, because
they've learned more, are better college candidates.  But
this seems empirically false.  What you learn in even the best high
school is rounding error compared to what you learn in college.  
Public school kids arrive at college with a slight disadvantage,  
but they start to pull ahead in the sophomore year.(I'm not saying public school kids are smarter than preppies, just
that they are within any given college.  That follows necessarily
if you agree prep schools improve kids' admissions prospects.)[7] Why does society foul you?  Indifference, mainly.  There are
simply no outside forces pushing high school to be good.  The air
traffic control system works because planes would crash otherwise.
Businesses have to deliver because otherwise competitors would take
their customers.  But no planes crash if your school sucks, and it
has no competitors.  High school isn't evil; it's random; but random
is pretty bad.[8] And then of course there is money.  It's not a big factor in
high school, because you can't do much that anyone wants.  But a
lot of great things were created mainly to make money.  Samuel
Johnson said ""no man but a blockhead ever wrote except for money.""
(Many hope he was exaggerating.)[9] Even college textbooks are bad.  When you get to college,
you'll find that (with a few stellar exceptions) the textbooks are
not written by the leading scholars in the field they describe.
Writing college textbooks is unpleasant work, done mostly by people
who need the money.  It's unpleasant because the publishers exert
so much control, and there are few things worse than close supervision
by someone who doesn't understand what you're doing.  This phenomenon
is apparently 
even worse in the production of high school textbooks.[10] Your teachers are always telling you to behave like adults.
I wonder if they'd like it if you did.  You may be loud and
disorganized, but you're very docile compared to adults.  If you
actually started acting like adults, it would be just as if a bunch
of adults had been transposed into your bodies.  Imagine the reaction
of an FBI agent or taxi driver or reporter to being told they had
to ask permission to go the bathroom, and only one person could go
at a time.  To say nothing of the things you're taught.  If a bunch
of actual adults suddenly found themselves trapped in high school,
the first thing they'd do is form a union and renegotiate all the
rules with the administration.Thanks to Ingrid Bassett, Trevor Blackwell, 
Rich Draves, Dan Giffin, Sarah
Harlin, Jessica Livingston, Jackie McDonough, Robert Morris, Mark Nitzberg, Lisa 
Randall, and Aaron Swartz for reading drafts of this, and to many
others for talking to me about high school.Why Nerds are UnpopularJapanese TranslationRussian TranslationGeorgian Translation","life advice
",human,"human
","human
"
67,67,"November 2019Everyone knows that to do great work you need both natural ability
and determination. But there's a third ingredient that's not as
well understood: an obsessive interest in a particular topic.To explain this point I need to burn my reputation with some group
of people, and I'm going to choose bus ticket collectors.  There
are people who collect old bus tickets. Like many collectors, they
have an obsessive interest in the minutiae of what they collect.
They can keep track of distinctions between different types of bus
tickets that would be hard for the rest of us to remember.  Because
we don't care enough. What's the point of spending so much time
thinking about old bus tickets?Which leads us to the second feature of this kind of obsession:
there is no point. A bus ticket collector's love is disinterested.
They're not doing it to impress us or to make themselves rich, but
for its own sake.When you look at the lives of people who've done great work, you
see a consistent pattern. They often begin with a bus ticket
collector's obsessive interest in something that would have seemed
pointless to most of their contemporaries. One of the most striking
features of Darwin's book about his voyage on the Beagle is the
sheer depth of his interest in natural history. His curiosity seems
infinite. Ditto for Ramanujan, sitting by the hour working out on
his slate what happens to series.It's a mistake to think they were ""laying the groundwork"" for the
discoveries they made later. There's too much intention in that
metaphor. Like bus ticket collectors, they were doing it
because they liked it.But there is a difference between Ramanujan and a bus ticket
collector. Series matter, and bus tickets don't.If I had to put the recipe for genius into one sentence, that might
be it: to have a disinterested obsession with something that matters.Aren't I forgetting about the other two ingredients? Less than you
might think. An obsessive interest in a topic is both a proxy for
ability and a substitute for determination.  Unless you have
sufficient mathematical aptitude, you won't find series interesting.
And when you're obsessively interested in something, you don't need
as much determination: you don't need to push yourself as hard when
curiosity is pulling you.An obsessive interest will even bring you luck, to the extent
anything can. Chance, as Pasteur said, favors the prepared mind,
and if there's one thing an obsessed mind is, it's prepared.The disinterestedness of this kind of obsession is its most important
feature. Not just because it's a filter for earnestness, but because
it helps you discover new ideas.The paths that lead to new ideas tend to look unpromising. If they
looked promising, other people would already have explored them.
How do the people who do great work discover these paths that others
overlook?  The popular story is that they simply have better vision:
because they're so talented, they see paths that others miss.  But
if you look at the way great discoveries are made, that's not what
happens. Darwin didn't pay closer attention to individual species
than other people because he saw that this would lead to great
discoveries, and they didn't. He was just really, really interested
in such things.Darwin couldn't turn it off. Neither could Ramanujan.  They didn't
discover the hidden paths that they did because they seemed promising,
but because they couldn't help it. That's what allowed them to
follow paths that someone who was merely ambitious would have
ignored.What rational person would decide that the way to write great novels
was to begin by spending several years creating an imaginary elvish
language, like Tolkien, or visiting every household in southwestern
Britain, like Trollope? No one, including Tolkien and Trollope.The bus ticket theory is similar to Carlyle's famous definition of
genius as an infinite capacity for taking pains. But there are two
differences. The bus ticket theory makes it clear that the source
of this infinite capacity for taking pains is not infinite diligence,
as Carlyle seems to have meant, but the sort of infinite interest
that collectors have. It also adds an important qualification: an
infinite capacity for taking pains about something that matters.So what  matters? You can never be sure. It's precisely because no
one can tell in advance which paths are promising that you can
discover new ideas by working on what you're interested in.But there are some heuristics you can use to guess whether an
obsession might be one that matters. For example, it's more promising
if you're creating something, rather than just consuming something
someone else creates. It's more promising if something you're
interested in is difficult, especially if it's more difficult for
other people than it is for you. And the obsessions of talented
people are more likely to be promising. When talented people become
interested in random things, they're not truly random.But you can never be sure. In fact, here's an interesting idea
that's also rather alarming if it's true: it may be that to do great
work, you also have to waste a lot of time.In many different areas, reward is proportionate to risk. If that
rule holds here, then the way to find paths that lead to truly great
work is to be willing to expend a lot of effort on things that turn
out to be every bit as unpromising as they seem.I'm not sure if this is true. On one hand, it seems surprisingly
difficult to waste your time so long as you're working hard on
something interesting. So much of what you do ends up being useful.
But on the other hand, the rule about the relationship between risk
and reward is so powerful that it seems to hold wherever risk occurs.
Newton's case, at least, suggests that the risk/reward rule holds
here. He's famous for one particular obsession of his that turned
out to be unprecedentedly fruitful: using math to describe the
world. But he had two other obsessions, alchemy and theology, that
seem to have been complete wastes of time. He ended up net ahead.
His bet on what we now call physics paid off so well that it more
than compensated for the other two. But were the other two necessary,
in the sense that he had to take big risks to make such big
discoveries? I don't know.Here's an even more alarming idea: might one make all bad bets?  It
probably happens quite often. But we don't know how often, because
these people don't become famous.It's not merely that the returns from following a path are hard to
predict. They change dramatically over time. 1830 was a really good
time to be obsessively interested in natural history. If Darwin had
been born in 1709 instead of 1809, we might never have heard of
him.What can one do in the face of such uncertainty?  One solution is
to hedge your bets, which in this case means to follow the obviously
promising paths instead of your own private obsessions. But as with
any hedge, you're decreasing reward when you decrease risk.  If you
forgo working on what you like in order to follow some more
conventionally ambitious path, you might miss something wonderful
that you'd otherwise have discovered. That too must happen all the
time, perhaps even more often than the genius whose bets all fail.The other solution is to let yourself be interested in lots of
different things. You don't decrease your upside if you switch
between equally genuine interests based on which seems to be working
so far. But there is a danger here too: if you work on too many
different projects, you might not get deeply enough into any of
them.One interesting thing about the bus ticket theory is that it may
help explain why different types of people excel at different kinds
of work. Interest is much more unevenly distributed than ability.
If natural ability is all you need to do great work, and natural
ability is evenly distributed, you have to invent elaborate theories
to explain the skewed distributions we see among those who actually
do great work in various fields. But it may be that much of the
skew has a simpler explanation: different people are interested in
different things.The bus ticket theory also explains why people are less likely to
do great work after they have children. Here interest has to compete
not just with external obstacles, but with another interest, and
one that for most people is extremely powerful. It's harder to find
time for work after you have kids, but that's the easy part. The
real change is that you don't want to.But the most exciting implication of the bus ticket theory is that
it suggests ways to encourage great work. If the recipe for genius
is simply natural ability plus hard work, all we can do is hope we
have a lot of ability, and work as hard as we can. But if interest
is a critical ingredient in genius, we may be able, by cultivating
interest, to cultivate genius.For example, for the very ambitious, the bus ticket theory suggests
that the way to do great work is to relax a little. Instead of
gritting your teeth and diligently pursuing what all your peers
agree is the most promising line of research, maybe you should try
doing something just for fun. And if you're stuck, that may be the
vector along which to break out.I've always liked Hamming's famous double-barrelled question: what
are the most important problems in your field, and why aren't you
working on one of them? It's a great way to shake yourself up. But
it may be overfitting a bit. It might be at least as useful to ask
yourself: if you could take a year off to work on something that
probably wouldn't be important but would be really interesting,
what would it be?The bus ticket theory also suggests a way to avoid slowing down as
you get older. Perhaps the reason people have fewer new ideas as
they get older is not simply that they're losing their edge. It may
also be because once you become established, you can no longer mess
about with irresponsible side projects the way you could when you
were young and no one cared what you did.The solution to that is obvious: remain irresponsible. It will be
hard, though, because the apparently random projects you take up
to stave off decline will read to outsiders as evidence of it.  And
you yourself won't know for sure that they're wrong. But it will
at least be more fun to work on what you want.It may even be that we can cultivate a habit of intellectual bus
ticket collecting in kids. The usual plan in education is to start
with a broad, shallow focus, then gradually become more specialized.
But I've done the opposite with my kids. I know I can count on their
school to handle the broad, shallow part, so I take them deep.When they get interested in something, however random, I encourage
them to go preposterously, bus ticket collectorly, deep.  I don't
do this because of the bus ticket theory. I do it because I want
them to feel the joy of learning, and they're never going to feel
that about something I'm making them learn. It has to be something
they're interested in. I'm just following the path of least resistance;
depth is a byproduct. But if in trying to show them the joy of
learning I also end up training them to go deep, so much the better.Will it have any effect? I have no idea. But that uncertainty may
be the most interesting point of all. There is so much more to learn
about how to do great work. As old as human civilization feels,
it's really still very young if we haven't nailed something so
basic. It's exciting to think there are still discoveries to make
about discovery. If that's the sort of thing you're interested in.
Notes[1] There are other types of collecting that illustrate this point
better than bus tickets, but they're also more popular. It seemed
just as well to use an inferior example rather than offend more
people by telling them their hobby doesn't matter.[2] I worried a little about using the word ""disinterested,"" since
some people mistakenly believe it means not interested. But anyone
who expects to be a genius will have to know the meaning of such a
basic word, so I figure they may as well start now.[3] Think how often genius must have been nipped in the bud by
people being told, or telling themselves, to stop messing about and
be responsible. Ramanujan's mother was a huge enabler. Imagine if
she hadn't been. Imagine if his parents had made him go out and get
a job instead of sitting around at home doing math.On the other hand, anyone quoting the preceding paragraph to justify
not getting a job is probably mistaken.[4] 1709 Darwin is to time what the Milanese Leonardo is to space.[5] ""An infinite capacity for taking pains"" is a paraphrase of what
Carlyle wrote. What he wrote, in his History of Frederick the Great,
was ""... it is the fruit of 'genius' (which means transcendent
capacity of taking trouble, first of all)...."" Since the paraphrase
seems the name of the idea at this point, I kept it.Carlyle's History was published in 1858. In 1785 Hérault de Séchelles
quoted Buffon as saying ""Le génie n'est qu'une plus grande aptitude
ŕ la patience."" (Genius is only a greater aptitude for patience.)[6] Trollope was establishing the system of postal routes. He himself
sensed the obsessiveness with which he pursued this goal.

   It is amusing to watch how a passion will grow upon a man. During
   those two years it was the ambition of my life to cover the
   country with rural letter-carriers.

Even Newton occasionally sensed the degree of his obsessiveness.
After computing pi to 15 digits, he wrote in a letter to a friend:

   I am ashamed to tell you to how many figures I carried these
   computations, having no other business at the time.

Incidentally, Ramanujan was also a compulsive calculator. As Kanigel
writes in his excellent biography:

  One Ramanujan scholar, B. M. Wilson, later told how Ramanujan's
  research into number theory was often ""preceded by a table of
  numerical results, carried usually to a length from which most
  of us would shrink.""

[7] Working to understand the natural world counts as creating
rather than consuming.Newton tripped over this distinction when he chose
to work on theology. His beliefs did not allow him to see it, but
chasing down paradoxes in nature is fruitful in a way that chasing
down paradoxes in sacred texts is not.[8] How much of people's propensity to become interested in a topic
is inborn?  My experience so far suggests the answer is: most of
it. Different kids get interested in different things, and it's
hard to make a child interested in something they wouldn't otherwise
be. Not in a way that sticks.  The most you can do on behalf of a
topic is to make sure it gets a fair showing — to make it clear to
them, for example, that there's more to math than the dull drills
they do in school. After that it's up to the child.Thanks to Marc Andreessen, Trevor Blackwell, Patrick Collison, Kevin
Lacker, Jessica Livingston, Jackie McDonough, Robert Morris, Lisa
Randall, Zak Stone, and my 7 year old for reading drafts of this.Spanish TranslationRussian TranslationKorean TranslationArmenian Translation","life advice
",human,"human
","human
"
68,68,"January 2016Life is short, as everyone knows. When I was a kid I used to wonder
about this. Is life actually short, or are we really complaining
about its finiteness?  Would we be just as likely to feel life was
short if we lived 10 times as long?Since there didn't seem any way to answer this question, I stopped
wondering about it.  Then I had kids.  That gave me a way to answer
the question, and the answer is that life actually is short.Having kids showed me how to convert a continuous quantity, time,
into discrete quantities. You only get 52 weekends with your 2 year
old.  If Christmas-as-magic lasts from say ages 3 to 10, you only
get to watch your child experience it 8 times.  And while it's
impossible to say what is a lot or a little of a continuous quantity
like time, 8 is not a lot of something.  If you had a handful of 8
peanuts, or a shelf of 8 books to choose from, the quantity would
definitely seem limited, no matter what your lifespan was.Ok, so life actually is short.  Does it make any difference to know
that?It has for me.  It means arguments of the form ""Life is too short
for x"" have great force.  It's not just a figure of speech to say
that life is too short for something.  It's not just a synonym for
annoying.  If you find yourself thinking that life is too short for
something, you should try to eliminate it if you can.When I ask myself what I've found life is too short for, the word
that pops into my head is ""bullshit."" I realize that answer is
somewhat tautological.  It's almost the definition of bullshit that
it's the stuff that life is too short for.  And yet bullshit does
have a distinctive character.  There's something fake about it.
It's the junk food of experience.
[1]If you ask yourself what you spend your time on that's bullshit,
you probably already know the answer.  Unnecessary meetings, pointless
disputes, bureaucracy, posturing, dealing with other people's
mistakes, traffic jams, addictive but unrewarding pastimes.There are two ways this kind of thing gets into your life: it's
either forced on you, or it tricks you.  To some extent you have to
put up with the bullshit forced on you by circumstances.  You need
to make money, and making money consists mostly of errands.  Indeed,
the law of supply and demand ensures that: the more rewarding some
kind of work is, the cheaper people will do it.  It may be that
less bullshit is forced on you than you think, though.  There has
always been a stream of people who opt out of the default grind and
go live somewhere where opportunities are fewer in the conventional
sense, but life feels more authentic.  This could become more common.You can do it on a smaller scale without moving.  The amount of
time you have to spend on bullshit varies between employers.  Most
large organizations (and many small ones) are steeped in it.  But
if you consciously prioritize bullshit avoidance over other factors
like money and prestige, you can probably find employers that will
waste less of your time.If you're a freelancer or a small company, you can do this at the
level of individual customers.  If you fire or avoid toxic customers,
you can decrease the amount of bullshit in your life by more than
you decrease your income.But while some amount of bullshit is inevitably forced on you, the
bullshit that sneaks into your life by tricking you is no one's
fault but your own.  And yet the bullshit you choose may be harder
to eliminate than the bullshit that's forced on you.  Things that
lure you into wasting your time have to be really good at
tricking you.  An example that will be familiar to a lot of people
is arguing online.  When someone
contradicts you, they're in a sense attacking you. Sometimes pretty
overtly.  Your instinct when attacked is to defend yourself.  But
like a lot of instincts, this one wasn't designed for the world we
now live in.  Counterintuitive as it feels, it's better most of
the time not to defend yourself.  Otherwise these people are literally
taking your life.
[2]Arguing online is only incidentally addictive. There are more
dangerous things than that. As I've written before, one byproduct
of technical progress is that things we like tend to become more
addictive.  Which means we will increasingly have to make a conscious
effort to avoid addictions — to stand outside ourselves and ask ""is
this how I want to be spending my time?""As well as avoiding bullshit, one should actively seek out things
that matter.  But different things matter to different people, and
most have to learn what matters to them.  A few are lucky and realize
early on that they love math or taking care of animals or writing,
and then figure out a way to spend a lot of time doing it.  But
most people start out with a life that's a mix of things that
matter and things that don't, and only gradually learn to distinguish
between them.For the young especially, much of this confusion is induced by the
artificial situations they find themselves in. In middle school and
high school, what the other kids think of you seems the most important
thing in the world.  But when you ask adults what they got wrong
at that age, nearly all say they cared too much what other kids
thought of them.One heuristic for distinguishing stuff that matters is to ask
yourself whether you'll care about it in the future.  Fake stuff
that matters usually has a sharp peak of seeming to matter.  That's
how it tricks you.  The area under the curve is small, but its shape
jabs into your consciousness like a pin.The things that matter aren't necessarily the ones people would
call ""important.""  Having coffee with a friend matters.  You won't
feel later like that was a waste of time.One great thing about having small children is that they make you
spend time on things that matter: them. They grab your sleeve as
you're staring at your phone and say ""will you play with me?"" And
odds are that is in fact the bullshit-minimizing option.If life is short, we should expect its shortness to take us by
surprise. And that is just what tends to happen.  You take things
for granted, and then they're gone.  You think you can always write
that book, or climb that mountain, or whatever, and then you realize
the window has closed.  The saddest windows close when other people
die. Their lives are short too.  After my mother died, I wished I'd
spent more time with her.  I lived as if she'd always be there.
And in her typical quiet way she encouraged that illusion.  But an
illusion it was. I think a lot of people make the same mistake I
did.The usual way to avoid being taken by surprise by something is to
be consciously aware of it.  Back when life was more precarious,
people used to be aware of death to a degree that would now seem a
bit morbid.  I'm not sure why, but it doesn't seem the right answer
to be constantly reminding oneself of the grim reaper hovering at
everyone's shoulder.  Perhaps a better solution is to look at the
problem from the other end. Cultivate a habit of impatience about
the things you most want to do. Don't wait before climbing that
mountain or writing that book or visiting your mother.  You don't
need to be constantly reminding yourself why you shouldn't wait.
Just don't wait.I can think of two more things one does when one doesn't have much
of something: try to get more of it, and savor what one has.  Both
make sense here.How you live affects how long you live.  Most people could do better.
Me among them.But you can probably get even more effect by paying closer attention
to the time you have.  It's easy to let the days rush by.  The
""flow"" that imaginative people love so much has a darker cousin
that prevents you from pausing to savor life amid the daily slurry
of errands and alarms.  One of the most striking things I've read
was not in a book, but the title of one: James Salter's Burning
the Days.It is possible to slow time somewhat. I've gotten better at it.
Kids help.  When you have small children, there are a lot of moments
so perfect that you can't help noticing.It does help too to feel that you've squeezed everything out of
some experience.  The reason I'm sad about my mother is not just
that I miss her but that I think of all the things we could have
done that we didn't.  My oldest son will be 7 soon.  And while I
miss the 3 year old version of him, I at least don't have any regrets
over what might have been.  We had the best time a daddy and a 3
year old ever had.Relentlessly prune bullshit, don't wait to do things that matter,
and savor the time you have.  That's what you do when life is short.Notes[1]
At first I didn't like it that the word that came to mind was
one that had other meanings.  But then I realized the other meanings
are fairly closely related.  Bullshit in the sense of things you
waste your time on is a lot like intellectual bullshit.[2]
I chose this example deliberately as a note to self.  I get
attacked a lot online.  People tell the craziest lies about me.
And I have so far done a pretty mediocre job of suppressing the
natural human inclination to say ""Hey, that's not true!""Thanks to Jessica Livingston and Geoff Ralston for reading drafts
of this.Korean TranslationJapanese TranslationChinese Translation","life advice
",human,"human
","human
"
69,69,"July 2007I have too much stuff.  Most people in America do.  In fact, the
poorer people are, the more stuff they seem to have.  Hardly anyone
is so poor that they can't afford a front yard full of old cars.It wasn't always this way.  Stuff used to be rare and valuable.
You can still see evidence of that if you look for it.  For example,
in my house in Cambridge, which was built in 1876, the bedrooms
don't have closets.  In those days people's stuff fit in a chest
of drawers.  Even as recently as a few decades ago there was a lot
less stuff.  When I look back at photos from the 1970s, I'm surprised
how empty houses look.  As a kid I had what I thought was a huge
fleet of toy cars, but they'd be dwarfed by the number of toys my
nephews have.  All together my Matchboxes and Corgis took up about
a third of the surface of my bed.  In my nephews' rooms the bed is
the only clear space.Stuff has gotten a lot cheaper, but our attitudes toward it haven't
changed correspondingly.  We overvalue stuff.That was a big problem
for me when I had no money.  I felt poor, and stuff seemed valuable,
so almost instinctively I accumulated it.  Friends would leave
something behind when they moved, or I'd see something as I was
walking down the street on trash night (beware of anything you find
yourself describing as ""perfectly good""), or I'd find something in
almost new condition for a tenth its retail price at a garage sale.
And pow, more stuff.In fact these free or nearly free things weren't bargains, because
they were worth even less than they cost.  Most of the stuff I
accumulated was worthless, because I didn't need it.What I didn't understand was that the value of some new acquisition
wasn't the difference between its retail price and what I paid for
it.  It was the value I derived from it.  Stuff is an extremely
illiquid asset.  Unless you have some plan for selling that valuable
thing you got so cheaply, what difference does it make what it's
""worth?""  The only way you're ever going to extract any value from
it is to use it.  And if you don't have any immediate use for it,
you probably never will.Companies that sell stuff have spent huge sums training us to think
stuff is still valuable.  But it would be closer to the truth to
treat stuff as worthless.In fact, worse than worthless, because once you've accumulated a
certain amount of stuff, it starts to own you rather than the other
way around.  I know of one couple who couldn't retire to the town
they preferred because they couldn't afford a place there big enough
for all their stuff.  Their house isn't theirs; it's their stuff's.And unless you're extremely organized, a house full of stuff can
be very depressing. A cluttered room saps one's spirits.  One
reason, obviously, is that there's less room for people in a room
full of stuff.  But there's more going on than that.  I think humans
constantly scan their environment to build a mental model of what's
around them.  And the harder a scene is to parse, the less energy
you have left for conscious thoughts.  A cluttered room is literally
exhausting.(This could explain why clutter doesn't seem to bother kids as much
as adults. Kids are less perceptive.  They build a coarser model
of their surroundings, and this consumes less energy.)I first realized the worthlessness of stuff when I lived in Italy
for a year.  All I took with me was one large backpack of stuff.
The rest of my stuff I left in my landlady's attic back in the US.
And you know what?  All I missed were some of the books.  By the
end of the year I couldn't even remember what else I had stored in
that attic.And yet when I got back I didn't discard so much as a box of it.
Throw away a perfectly good rotary telephone?  I might need that
one day.The really painful thing to recall is not just that I accumulated
all this useless stuff, but that I often spent money I desperately
needed on stuff that I didn't.Why would I do that?  Because the people whose job is to sell you
stuff are really, really good at it.  The average 25 year old is
no match for companies that have spent years figuring out how to
get you to spend money on stuff.  They make the experience of buying
stuff so pleasant that ""shopping"" becomes a leisure activity.How do you protect yourself from these people?  It can't be easy.
I'm a fairly skeptical person, and their tricks worked on me well
into my thirties.  But one thing that might work is to ask yourself,
before buying something, ""is this going to make my life noticeably
better?""A friend of mine cured herself of a clothes buying habit by asking
herself before she bought anything ""Am I going to wear this all the
time?""  If she couldn't convince herself that something she was
thinking of buying would become one of those few things she wore
all the time, she wouldn't buy it.  I think that would work for any
kind of purchase.  Before you buy anything, ask yourself: will this
be something I use constantly?  Or is it just something nice?  Or
worse still, a mere bargain?The worst stuff in this respect may be stuff you don't use much
because it's too good.  Nothing owns you like fragile stuff.  For
example, the ""good china"" so many households have, and whose defining
quality is not so much that it's fun to use, but that one must be
especially careful not to break it.Another way to resist acquiring stuff is to think of the overall
cost of owning it.  The purchase price is just the beginning.  You're
going to have to think about that thing for years—perhaps for
the rest of your life.  Every thing you own takes energy away from
you.  Some give more than they take.  Those are the only things
worth having.I've now stopped accumulating stuff.  Except books—but books are
different.  Books are more like a fluid than individual objects.
It's not especially inconvenient to own several thousand books,
whereas if you owned several thousand random possessions you'd be
a local celebrity.  But except for books, I now actively avoid
stuff.  If I want to spend money on some kind of treat, I'll take
services over 
goods any day.I'm not claiming this is because I've achieved some kind of zenlike
detachment from material things.  I'm talking about something more
mundane.  A historical change has taken place, and I've now realized
it.  Stuff used to be valuable, and now it's not.In industrialized countries the same thing happened with food in
the middle of the twentieth century.  As food got cheaper (or we
got richer; they're indistinguishable), eating too much started to
be a bigger danger than eating too little.   We've now reached that
point with stuff.  For most people, rich or poor, stuff has become
a burden.The good news is, if you're carrying a burden without knowing it,
your life could be better than you realize.  Imagine walking around
for years with five pound ankle weights, then suddenly having them
removed.
Spanish TranslationRussian TranslationItalian TranslationPolish TranslationTurkish TranslationFrench TranslationSlovak TranslationRomanian TranslationGerman Translation","life advice
",human,"human
","human
"
70,70,"Note: The strategy described at the end of this essay didn't work.
It would work for a while, and then I'd gradually find myself
using the Internet on my work computer.  I'm trying other
strategies now, but I think this time I'll wait till I'm sure
they work before writing about them.May 2008Procrastination feeds on distractions.  Most people find it
uncomfortable just to sit and do nothing; you avoid work by doing
something else.So one way to beat procrastination is to starve it of distractions.
But that's not as straightforward as it sounds, because there are
people working hard to distract you.  Distraction is not a static
obstacle that you avoid like you might avoid a rock in the road.
Distraction seeks you out.Chesterfield described dirt as matter out of place.  Distracting
is, similarly, desirable at the wrong time.  And technology is
continually being refined to produce more and more desirable things.
Which means that as we learn to avoid one class of distractions,
new ones constantly appear, like drug-resistant bacteria.Television, for example, has after 50 years of refinement reached
the point where it's like visual crack.  I realized when I was 13
that TV was addictive, so I stopped watching it.  But I read recently
 that the average American watches 
4 hours 
of TV a day.  A quarter
of their life.TV is in decline now, but only because people have found even more
addictive ways of wasting time.  And what's especially dangerous
is that many happen at your computer.  This is no accident.  An
ever larger percentage of office workers sit in front of computers
connected to the Internet, and distractions always evolve toward
the procrastinators.I remember when computers were, for me at least, exclusively for
work.  I might occasionally dial up a server to get mail or ftp
files, but most of the time I was offline.  All I could do was write
and program.  Now I feel as if someone snuck a television onto my
desk.  Terribly addictive things are just a click away.  Run into
an obstacle in what you're working on?  Hmm, I wonder what's new
online.  Better check.After years of carefully avoiding classic time sinks like TV, games,
and Usenet, I still managed to fall prey to distraction, because
I didn't realize that it evolves.  Something that used to be safe,
using the Internet, gradually became more and more dangerous.  Some
days I'd wake up, get a cup of tea and check the news, then check
email, then check the news again, then answer a few emails, then
suddenly notice it was almost lunchtime and I hadn't gotten any real
work done.  And this started to happen more and more often.It took me surprisingly long to realize how distracting the Internet
had become, because the problem was intermittent.  I ignored it the
way you let yourself ignore a bug that only appears intermittently.  When
I was in the middle of a project, distractions weren't really a
problem.  It was when I'd finished one project and was deciding
what to do next that they always bit me.Another reason it was hard to notice the danger of this new type
of distraction was that social customs hadn't yet caught up with
it.  If I'd spent a whole morning sitting on a sofa watching TV,
I'd have noticed very quickly.  That's a known danger sign, like
drinking alone.  But using the Internet still looked and felt a 
lot like work.Eventually, though, it became clear that the Internet had become so much
more distracting that I had to start treating it differently.
Basically, I had to add a new application to my list of known time
sinks: Firefox.* * *The problem is a hard one to solve because most people still need
the Internet for some things.  If you drink too much, you can solve
that problem by stopping entirely.  But you can't solve the problem
of overeating by stopping eating.  I couldn't simply avoid the 
Internet entirely, as I'd done with previous time sinks.At first I tried rules.  For example, I'd tell myself I was only
going to use the Internet twice a day.  But these schemes never
worked for long.  Eventually something would come up that required
me to use it more than that.  And then I'd gradually slip back
into my old ways.Addictive things have to be treated as if they were sentient
adversaries—as if there were a little man in your head always
cooking up the most plausible arguments for doing whatever you're
trying to stop doing.  If you leave a path to it, he'll find it.The key seems to be visibility.  The biggest ingredient in most bad habits
is denial.  So you have to make it so that you can't merely slip
into doing the thing you're trying to avoid.  It has to set off
alarms.Maybe in the long term the right answer for dealing with Internet
distractions will be 
software that watches and controls them.  But
in the meantime I've found a more drastic solution that definitely
works: to set up a separate computer for using the Internet.I now leave wifi turned off on my main computer except when I need
to transfer a file or edit a web page, and I have a separate laptop 
on the other side
of the room that I use to check mail or browse the web.  (Irony of
ironies, it's the computer Steve Huffman wrote Reddit on.  When
Steve and Alexis auctioned off their old laptops for charity, I
bought them for the Y Combinator museum.)My rule is that I can spend as much time online as I want, as long
as I do it on that computer.  And this turns out to be enough.  When
I have to sit on the other side of the room to check email or browse
the web, I become much more aware of it.  Sufficiently aware, in
my case at least, that it's hard to spend more than about an hour
a day online.And my main computer is now freed for work.  If you try this trick,
you'll probably be struck by how different it feels when your
computer is disconnected from the Internet.  It was alarming to me
how foreign it felt to sit in front of a computer that could
only be used for work, because that showed how much time I must
have been wasting.Wow.  All I can do at this computer is work.  Ok, I better work
then.That's the good part.  Your old bad habits now help you to work.
You're used to sitting in front of that computer for hours at a
time.   But you can't browse the web or check email now.  What are
you going to do?  You can't just sit there.  So you start working.Good and Bad ProcrastinationSpanish TranslationArabic TranslationCatalan TranslationRussian TranslationSpanish Translation","life advice
",human,"human
","human
"
71,71,"April 2020I recently saw a 
video 
of TV journalists and politicians confidently
saying that the coronavirus would be no worse than the flu. What
struck me about it was not just how mistaken they seemed, but how
daring. How could they feel safe saying such things?The answer, I realized, is that they didn't think they could get
caught. They didn't realize there was any danger in making false
predictions. These people constantly make false predictions, and
get away with it, because the things they make predictions about
either have mushy enough outcomes that they can bluster their way
out of trouble, or happen so far in the future that few remember
what they said.An epidemic is different. It falsifies your predictions rapidly and
unequivocally.But epidemics are rare enough that these people clearly
didn't realize this was even a possibility. Instead they just
continued to use their ordinary m.o., which, as the epidemic has
made clear, is to talk confidently about things they don't
understand.An event like this is thus a uniquely powerful way of taking people's
measure. As Warren Buffett said, ""It's only when the tide goes out
that you learn who's been swimming naked."" And the tide has just
gone out like never before.Now that we've seen the results, let's remember what we saw, because
this is the most accurate test of credibility we're ever likely to have. I hope.Finnish TranslationGerman TranslationFrench Translation","social commentary
",human,"human
","human
"
72,72,"January 2004
Have you ever seen an old photo of yourself and
been embarrassed at the way you looked?   Did we actually
dress like that?  We did.  And we had no idea how
silly we looked.
It's the nature of fashion to be invisible, in the
same way the movement of the earth is invisible to all
of us riding on it.What scares me is that there are moral fashions too.
They're just as arbitrary, and just as invisible to most people.
But they're much more dangerous.
Fashion is mistaken for good design; 
moral fashion is mistaken for good.
Dressing oddly gets you laughed at.  Violating
moral fashions can get you fired, ostracized, imprisoned, or
even killed.If you could travel back in a time machine, one thing
would be true no matter where you went: you'd have to watch
what you said.  
Opinions we consider harmless could have 
gotten you in big trouble.
I've already said at least one thing that would have gotten me in big
trouble in most of Europe in the seventeenth century,
and did get Galileo in big trouble when he said
it — that the earth moves. [1]
It seems to be a constant throughout history: In every
period, people believed things that were just ridiculous,
and believed them so strongly that you would have gotten in
terrible trouble for saying otherwise.Is our time any different?
To anyone who has read any amount of history, the answer is
almost certainly no.   It would be a remarkable coincidence if ours
were the first era to get everything just right.It's tantalizing to think we believe
things that people in the future will find ridiculous.
What would someone coming back to visit us in a time machine
have to be careful not to say?
That's what I want to study here.
But
I want to do more than just shock everyone with
the heresy du jour.  I want to find general
recipes for discovering what you can't say, in any era.The Conformist TestLet's start with a test:  
Do you have any opinions that you would be reluctant to express
in front of a group of your peers?If the answer is no,
you might want to stop and think about that.  If everything
you believe is something you're supposed to believe, could
that possibly be a coincidence?  Odds are it isn't.  Odds are
you just think what you're told.The other alternative would be that you independently considered
every question and came up with the exact same answers that
are now considered acceptable.  That seems unlikely, because
you'd also have to make the same mistakes.  Mapmakers
deliberately put slight mistakes in their maps so they can
tell when someone copies them.  If another map has the same
mistake, that's very convincing evidence.Like every other era in history, our moral map almost certainly
contains a few mistakes.  And anyone who makes the same mistakes
probably didn't do it by accident.  It would be
like someone claiming they had independently decided in
1972 that bell-bottom jeans were a good idea.If you believe everything you're supposed to now, how can
you be sure you wouldn't also have believed everything you
were supposed to if you had grown up among the plantation
owners of the pre-Civil War South, or in Germany in the 1930s — or
among the Mongols in 1200, for that matter?  Odds are you
would have.Back in the era of terms like ""well-adjusted,"" the idea
seemed to be that there was something wrong with
you if you thought things you didn't dare say out loud.
This seems backward.  Almost certainly, there
is something wrong with you if you don't think things
you don't dare say out loud.TroubleWhat can't we say?  One way to find these ideas is simply to look
at things people do say, and get in trouble for.  [2]Of course, we're not just looking for things we can't say.   
We're looking for things we can't say that are true, or at least
have enough chance of being true that the question
should remain open.  But many of the
things people get in trouble for saying probably
do make it over this second, lower threshold.  No one
gets in trouble for saying
that 2 + 2 is 5, or that people in Pittsburgh are ten feet tall.
Such obviously false statements might be treated as jokes, or
at worst as evidence of insanity, but they are not likely to
make anyone mad.  The statements that make people mad are
the ones they worry might be believed.
I suspect the statements that make people maddest
are those they worry might be true.If Galileo had said that people in Padua were ten feet tall,
he would have been regarded as a harmless eccentric.  Saying   
the earth orbited the sun was another matter.  The church knew
this would set people thinking.Certainly, as we look back on the past, this rule of thumb works
well.  A lot of the statements people got in trouble for seem
harmless now.  So it's likely that visitors from the
future would agree with at least some of the statements that
get people in trouble today.  Do we have no Galileos?  Not
likely.To find them,
keep track of opinions that get
people in trouble, and start asking, could this be true?
Ok, it may be heretical (or whatever modern equivalent), but
might it also be true?HeresyThis won't get us all the answers, though.  What if no one
happens to have gotten in trouble for a particular idea yet?
What if some idea would be so radioactively controversial that
no one would dare express it in public?   How can we find these too?Another approach is to follow that word, heresy.  In every period
of history, there seem to have been labels that got applied to 
statements to shoot them down before anyone had a chance to ask
if they were true or not.  ""Blasphemy"", ""sacrilege"", and ""heresy""
were such
labels for a good part of western history, as in more recent times
""indecent"", ""improper"", and ""unamerican"" have been.   By now these
labels have lost their sting.  They always do.
By now they're mostly used ironically.
But in their time,
they had real force.The word ""defeatist"", for example, has no particular political
connotations now.
But in Germany in 1917 it was a weapon, used by Ludendorff in
a purge of those who favored a negotiated peace.
At the start of World War II it was used
extensively by Churchill and his supporters to silence their
opponents.
In 1940, any argument against Churchill's aggressive policy was ""defeatist"".
Was it right or wrong?  Ideally, no one got far enough to ask
that.
We have such labels today, of course, quite a lot of them,
from the all-purpose ""inappropriate"" to the dreaded ""divisive.""
In any period, it should be easy to figure out what such labels are,
simply by looking at what people call ideas they disagree
with besides untrue.  When a politician says his opponent is
mistaken, that's a straightforward criticism, but when he
attacks a statement as ""divisive"" or ""racially insensitive""
instead of arguing that it's false, we should start paying
attention.So another way to figure out which of our taboos future generations
will laugh at is to start with the
labels.  Take a label — ""sexist"", for example — and try to think
of some ideas that would be called that.  Then for each ask, might
this be true?Just start listing ideas at random?  Yes, because they
won't really be random.  The ideas that come to mind first
will be the most plausible ones.  They'll be things you've already  
noticed but didn't let yourself think.In 1989 some clever researchers tracked
the eye movements of radiologists as they scanned chest images for
signs of lung cancer. [3]  They found that even when the radiologists
missed a cancerous lesion, their eyes had usually paused at the site of it.
Part of their brain knew there was something there; it just
didn't percolate all the way up into conscious knowledge. 
I think many interesting heretical thoughts are already mostly   
formed in our minds.  If we turn off our self-censorship
temporarily, those will be the first to emerge.Time and SpaceIf we could look into the future it would be obvious which
of our taboos they'd laugh at.
We can't do that, but we can do something almost as good: we can
look into the past.  Another way to figure out what we're
getting wrong is to look at what used to be acceptable
and is now unthinkable.Changes between the past and the present sometimes do represent
progress.  In a field like physics,
if we disagree with past generations it's because we're
right and they're wrong.  But this becomes rapidly less true as   
you move away from the certainty of the hard sciences.  By the time
you get to social questions, many changes are just fashion.
The age of consent fluctuates like hemlines.We may imagine that we are a great deal smarter and more virtuous than
past generations, but the more history you read, the less likely
this seems.  People in past times were much like us.  Not heroes,
not barbarians.  Whatever their ideas were, they were ideas
reasonable people could believe.So here is another source of interesting heresies.  Diff present
ideas against those of various past cultures, and see what you   
get. [4]
Some will be
shocking by present standards.  Ok, fine; but which might also be true?You don't have to look into the past to find big differences.   
In our own time, different societies have wildly varying ideas
of what's ok and what isn't.
So you can try diffing other cultures' ideas against ours as well.
(The best way to do that is to visit them.)

Any idea that's considered harmless in a significant
percentage of times and places, and yet is taboo in ours,
is a  candidate for something we're mistaken
about.For example, at the high water mark of political correctness
in the early 1990s, Harvard distributed to its
faculty and staff a brochure saying, among other things, that it
was inappropriate to compliment a colleague or student's
clothes.  No more ""nice shirt.""
I think this principle is rare among the world's cultures, past or present.
There are probably more where it's considered especially
polite to compliment someone's clothing than where it's considered
improper.

Odds are this is, in a mild form, an example of one of
the taboos a visitor from the future would
have to be careful to avoid if he happened to set his time machine for
Cambridge, Massachusetts, 1992. [5]PrigsOf course, if they have time machines in the future they'll
probably have a separate reference manual just for Cambridge.
This has always been a fussy place, a town of i dotters and
t crossers, where you're liable to get both your grammar and   
your ideas corrected in the same conversation.   And that
suggests another way to find taboos.  Look for prigs,
and see what's inside their heads.Kids' heads are repositories of all our taboos.
It seems fitting to us that kids' ideas should be bright and clean.
The picture we give them of the world is 
not merely simplified, to suit their developing minds, 
but sanitized as well, to suit our
ideas of what kids ought to think. [6]You can see this on a small scale in the matter of
dirty words.  A lot of my friends are starting to have children
now, and they're all trying 
not to use words like
""fuck"" and ""shit"" within baby's hearing, lest baby start using 
these words too.
But these
words are part of the language, and adults use them all the
time.  So parents are giving their kids an inaccurate idea of 
the language by not using
them.  Why do they do this?  Because they don't think it's
fitting that kids should use the whole language.  We like
children to seem innocent. [7]Most adults, likewise, deliberately give kids a misleading
view of the world.
One of the most obvious
examples is Santa Claus.  We think it's cute for little kids to
believe in Santa Claus.  I myself think it's cute for little
kids to believe in Santa Claus.  But one wonders, do we tell
them this stuff for their sake, or for ours?I'm not arguing for or against this idea here.  It is probably
inevitable that parents should want to dress up their kids'
minds in cute little baby outfits.  I'll probably do it myself.
The important thing for our purposes is that, as a result,
a well brought-up teenage kid's brain is a more
or less complete collection of all our taboos — and in mint
condition, because they're untainted by experience.  
Whatever we think that will later turn out to be ridiculous, 
it's almost certainly inside that head.How do we get at these ideas?  By the following thought experiment.
Imagine a kind of latter-day Conrad character
who has worked for a time as a mercenary in Africa, for a time
as a doctor in Nepal, for a time as the manager of a
nightclub in Miami.  The specifics don't matter — just
someone who has
seen a lot.  Now imagine comparing what's inside this guy's head
with what's inside the head
of a well-behaved sixteen year old girl from
the suburbs.  What does he think that
would shock her?
He knows the world; she knows, or at least embodies, present
taboos.  Subtract one from the other, and the result is what
we can't say.
MechanismI can think of one more way to figure out what we can't
say: to look at how taboos are created.   How do moral
fashions arise, and why are they adopted?
If we can understand this mechanism, we
may be able to see it at work in our own time.Moral fashions don't seem to be created the way ordinary
fashions are.  Ordinary fashions seem to arise by accident when
everyone imitates the whim of some influential person.
The fashion for broad-toed shoes in
late fifteenth century Europe began because Charles VIII of
France had six toes on one foot.  The fashion for the
name Gary began when the actor Frank Cooper adopted the name
of a tough mill town in Indiana.  Moral fashions more often
seem to be created deliberately.  When there's something we
can't say, it's often because some group doesn't want us to.The prohibition will be strongest when the group is nervous. 
The irony of Galileo's situation was that he got in trouble
for repeating Copernicus's ideas.  Copernicus himself didn't.
In fact, Copernicus was a canon of a cathedral, and dedicated his
book to the pope.  But by Galileo's time the church was in
the throes of the Counter-Reformation and was much more
worried about unorthodox ideas.To launch a taboo, a group has to be poised halfway between
weakness and power.  A confident group doesn't need taboos
to protect it.  It's not considered improper to
make disparaging remarks about Americans, or the English.
And yet a group has to be powerful enough to enforce a
taboo.  Coprophiles, as of this writing, don't seem to be
numerous or energetic enough to have had their
interests promoted to a lifestyle.I suspect the biggest source of moral taboos will turn out to
be power struggles in which one side only barely has
the upper hand.  That's where you'll find a group
powerful enough to enforce taboos, but weak enough to need them.Most struggles, whatever they're really about, will be cast
as struggles between competing ideas.
The English Reformation was at bottom a struggle for wealth and power,
but it ended up being
cast as a struggle to preserve the souls
of Englishmen from the corrupting influence of Rome.
It's easier to get people to fight for an idea.
And whichever side wins, their
ideas will also be considered to have triumphed, as if God
wanted to signal his agreement by selecting that side as the victor.We often like to think of World War II as a triumph
of freedom over totalitarianism.  We conveniently forget that
the Soviet Union was also one of the winners.I'm not saying that struggles are never about ideas,
just that they will always be made to seem to be about
ideas, whether they are or not.   And just as there is nothing
so unfashionable as the last, discarded fashion, there is
nothing so wrong as the principles of the most recently
defeated opponent.

Representational art is only now
recovering from the approval of both Hitler and Stalin. [8]Although moral fashions tend to arise from different sources
than fashions in clothing, the mechanism of their adoption seems
much the same.  The early adopters will be driven by ambition:
self-consciously cool people who want to distinguish themselves
from the common herd.  As the fashion becomes established they'll
be joined by a second, much larger group, driven by fear. [9] This
second group adopt the fashion not because they want to stand
out but because they are afraid of standing out.So if you want to figure out what we can't say, look at the
machinery of fashion and try to predict what it would make
unsayable.  What groups are powerful but nervous, and what
ideas would they like to suppress?  What ideas were tarnished by
association when they ended up on the losing side of a recent
struggle?  If a self-consciously cool person wanted to differentiate
himself from preceding fashions (e.g. from his parents), 
which of their ideas would he tend to reject?
What are conventional-minded people afraid of saying?This technique won't find us all the things we can't say.
I can think of some that aren't the result of
any recent struggle. Many of our taboos are rooted
deep in the past.  But this approach, combined with the
preceding four, will turn up a good number of unthinkable
ideas.WhySome would ask, why would one want to do this?  Why deliberately
go poking around among nasty, disreputable ideas?  Why look 
under rocks?I do it, first of all, for the same reason I did look under
rocks as a kid: plain curiosity.  And I'm especially curious about
anything that's forbidden.  Let me see and decide for myself.Second, I do it because I don't like the idea of being mistaken.
If, like other eras, we believe things that will later seem ridiculous,
I want to know what they are so that I, at least, can avoid
believing them.Third, I do it because it's good for the brain.  To do good work
you need a brain that can go anywhere.  And you especially need a
brain that's in the habit of going where it's not supposed to.Great work tends to grow out of ideas
that others have overlooked, and no idea is so overlooked as one that's
unthinkable.
Natural selection, for example.
It's so simple.  Why didn't anyone think of it before?  Well,
that is all too obvious.  Darwin himself was careful to tiptoe
around the implications of his theory.  He wanted to spend his
time thinking about biology, not arguing with people who accused
him of being an atheist.In the sciences, especially, it's a great advantage to be able to
question assumptions.
The m.o. of scientists, or at least of the
good ones, is precisely that: look for places where
conventional wisdom is broken, and then try to pry apart the
cracks and see what's underneath.  That's where new theories come
from.A good scientist, in other words, does not merely ignore
conventional wisdom, but makes a special effort to break it.
Scientists go looking for trouble.
This should be the m.o. of any scholar, but 
scientists seem much more willing to look under rocks. [10]Why?  It could
be that the scientists are simply smarter; most physicists could,
if necessary, make it through a PhD program in French literature,
but few professors of French literature could make it through
a PhD program in physics.  Or it could be because it's clearer
in the sciences whether theories are true or false, and this
makes scientists bolder.  (Or it could be that, because it's
clearer in the sciences whether theories are true or false, you
have to be smart to get jobs as a scientist, rather than just a
good politician.)Whatever the reason, there seems a clear correlation between
intelligence and willingness to consider shocking ideas.
This isn't just because smart people actively work to find holes in
conventional thinking.  I think conventions also have
less hold over them to start with.
You can see that in the
way they dress.It's not only in the sciences that heresy pays off.
In any competitive field, you can
win big by seeing things that others daren't.  
And in every
field there are probably heresies few dare utter.  Within
the US car industry there is a lot of hand-wringing now
about declining market share.
Yet the cause is so obvious that any observant outsider could
explain it in a second: they make bad cars.  And they have for
so long that by now the US car brands are antibrands — something
you'd buy a car despite, not because of.  Cadillac stopped
being the Cadillac of cars in about 1970.  And yet I suspect
no one dares say this. [11]  Otherwise these companies would have
tried to fix the problem.Training yourself to think unthinkable thoughts has advantages
beyond the thoughts themselves.  It's like stretching.
When you stretch before running, you put your body into positions
much more extreme
than any it will assume during the run.
If you can think things
so outside the box that they'd make people's hair stand on end,
you'll have no trouble with the small trips outside the box that
people call innovative.Pensieri StrettiWhen you find something you can't say, what do you do with it?
My advice is, don't say it.   Or at least, pick your battles.Suppose in the future there is a movement to ban
the color yellow. Proposals to paint anything yellow are
denounced as ""yellowist"", as is anyone suspected of liking the  
color.  People who like orange are tolerated but viewed with
suspicion.  Suppose you realize there is nothing
wrong with yellow.  If you go around saying this, you'll be
denounced as a yellowist too, and you'll find yourself having a   
lot of arguments with anti-yellowists.
If your aim in life is to rehabilitate the color yellow, that may
be what you want.
But if you're mostly interested in
other questions, being labelled as a yellowist will just be
a distraction.  Argue with idiots, and you become an idiot.The most important thing is to be able to think what you
want, not to say what you want.  And if you feel you have to
say everything you think, it may inhibit you from thinking  
improper thoughts.  I think it's better to follow the opposite
policy.  Draw a sharp line between your thoughts and your
speech.  Inside your head, anything is allowed.
Within my head I make a point of encouraging the most outrageous
thoughts I can imagine. 
But, as in
a secret society, nothing that happens within the building
should be told to outsiders.  The first rule of Fight
Club is, you do not talk about Fight Club.When Milton was going to visit Italy in the 1630s,
Sir Henry Wootton, who had been ambassador to Venice, told him
his motto should be
""i pensieri stretti & il viso sciolto.""  Closed thoughts
and an open face.  Smile at everyone, and don't tell them
what you're thinking.   This was wise advice.
Milton was an argumentative fellow, and the Inquisition
was a bit restive at that time.  But I think the difference 
between Milton's situation and ours is only a matter of
degree.
Every era has its heresies, and if you don't get imprisoned for them you
will at least get in enough trouble that it becomes a complete
distraction.I admit it seems cowardly to keep quiet.
When I read about the harassment to which
the Scientologists subject their critics [12], or that pro-Israel groups
are ""compiling dossiers"" on those who speak out against Israeli
human rights abuses [13], or about people being sued for
violating the DMCA [14], part of me wants
to say, ""All right, you bastards, bring it on.""
The problem is, there are so many things you can't say.
If you said them all you'd 
have no time left for your real work.
You'd have to turn into Noam Chomsky.  [15]The trouble with keeping your thoughts secret, though,
is that you lose the advantages of discussion.  Talking
about an idea leads to more ideas.
So the optimal plan, if you can manage it,
is to have a few trusted
friends you can speak openly to.  This is not just a
way to develop ideas; it's also a good
rule of thumb for choosing friends.  The people
you can say heretical things to without getting jumped on
are also the most interesting to know.Viso Sciolto?I don't think we need
the viso sciolto so much as the pensieri stretti.
Perhaps the best policy is to make it plain that you don't
agree with whatever zealotry is current in your time, but
not to be too specific about what you disagree with.  Zealots
will try to draw you out, but you don't have to answer them.
If they try to force you to treat a question on their
terms by asking ""are you with us or against us?"" you can
always just answer ""neither"".Better still, answer ""I haven't decided.""
That's what Larry Summers
did when a group tried to put
him in this position.  Explaining himself later, he said
""I don't do litmus tests."" [16]
A lot of the
questions people get hot about are actually quite complicated.
There is no prize for getting the answer quickly.If the anti-yellowists seem to be getting out of hand and
you want to fight back, there are ways
to do it without getting yourself accused of being a
yellowist.  Like skirmishers in
an ancient army, you want to avoid directly engaging the
main body of the enemy's troops.  Better to harass them
with arrows from a distance.One way to do this is to ratchet the debate up one level of
abstraction.
If you argue against censorship in general, you can avoid being
accused of whatever heresy is contained
in the book or film that someone is trying to censor.
You can attack labels with meta-labels: labels that refer
to the use of labels to prevent discussion.
The spread of the term ""political correctness"" meant the beginning of
the end of political correctness, because it enabled one to
attack the phenomenon as a whole without being accused of any
of the specific heresies it sought to suppress.Another way to counterattack is with metaphor.  Arthur Miller
undermined the House Un-American Activities Committee
by writing a play, ""The Crucible,"" about the Salem witch trials.
He never referred directly to the committee and so gave them
no way to reply.
What could HUAC do, defend the Salem witch trials?  And yet
Miller's metaphor stuck so well that to this day the activities
of the committee are often described as a ""witch-hunt.""Best of all, probably, is humor.  Zealots, whatever their   
cause, invariably lack a sense of humor.
They can't reply in kind to jokes.
They're as unhappy on the territory of
humor as a mounted knight on a skating rink.
Victorian prudishness, for example, seems to have been defeated
mainly by treating it as a joke.  Likewise its reincarnation as
political correctness.
""I am glad that I
managed to write 'The Crucible,'"" Arthur Miller wrote,
""but looking back I have often wished I'd
had the temperament to do an absurd comedy, which is what the
situation deserved."" [17]ABQA Dutch friend says
I should use Holland as an example of a tolerant society.
It's true they have a long tradition of
comparative open-mindedness.  For centuries the low countries were the place
to go to say things you couldn't say anywhere else,
and this helped to make the region a center of scholarship and industry
(which have been closely tied for longer than most people realize).
Descartes, though claimed by the French, did much of his thinking in
Holland.And yet, I wonder.  The Dutch seem to live their lives up to their
necks in rules and regulations.  There's so much you can't do there;
is there really nothing
you can't say?Certainly the fact that they value open-mindedness is no guarantee.
Who thinks they're not open-minded?  Our hypothetical prim miss from
the suburbs thinks she's open-minded.  Hasn't she been
taught to be?  Ask anyone, and they'll say the same thing: they're
pretty open-minded, though they draw the line at things that are really
wrong.  (Some tribes
may avoid ""wrong"" as
judgemental, and may instead use a more neutral sounding euphemism
like ""negative"" or ""destructive"".)When people are bad at math, they know it, because they get the
wrong answers on tests.  But when people are bad at open-mindedness
they don't know it.  In fact they tend to think the opposite.
Remember, it's the nature of fashion to be invisible.  It wouldn't
work otherwise.  Fashion doesn't
seem like fashion to someone in the grip of it.  It just seems like
the right thing to do.  It's only by looking from a distance that
we see oscillations in people's idea of the right thing to do, and
can identify them as fashions.Time gives us such distance for free.  Indeed, the arrival of new
fashions makes old fashions easy to see, because they
seem so ridiculous by contrast.  From one end of a pendulum's
swing, the other end seems especially far away.To see fashion in your own time, though, requires a conscious effort.
Without time to give you distance, you have to create distance yourself.
Instead of being part of the mob, stand
as far away from it as you can and watch what it's
doing.  And pay especially close attention whenever an idea is being
suppressed.  Web filters for children and employees often ban
sites containing pornography, violence, and hate speech.  What
counts as pornography and violence?  And what, exactly, is
""hate speech?"" This sounds like a phrase out of 1984.Labels like that are probably the biggest external clue.
If a statement is false,
that's the worst thing you can say about it.  You don't
need to say that it's heretical.  And if it isn't false, it
shouldn't be suppressed.  So when you see statements being
attacked as x-ist or y-ic (substitute your current values of
x and y), whether in 1630 or 2030, that's a sure sign that
something is wrong.  When you hear such labels being used,
ask why.Especially if you hear yourself using them.  It's not just
the mob you need to learn to watch from a distance.  You need to be
able to watch your own thoughts from a distance.  That's not
a radical idea, by the way; it's the main difference between
children and adults.  When a child gets angry because he's
tired, he doesn't know what's happening.  An adult can
distance himself enough from the
situation to say ""never mind, I'm just tired.""  I don't
see why one couldn't, by a similar process, learn to
recognize and discount the effects of moral fashions.You have to take that extra step if you want to think clearly.
But it's harder, because now you're working against social customs 
instead of with them.  Everyone encourages you to grow up to the 
point where you can discount your own bad moods.  Few encourage   
you to continue to the point where you can discount society's bad
moods.How can you see the wave, when you're the water?  Always be
questioning.  That's the only defence.  What can't you say?  And why?NotesThanks to Sarah Harlin, Trevor Blackwell, Jessica Livingston,
Robert Morris, Eric Raymond and Bob van der Zwaan for reading drafts of this
essay, and to Lisa Randall, Jackie McDonough, Ryan Stanley and Joel Rainey 
for conversations about heresy.
Needless to say they bear no blame for opinions
expressed in it, and especially for opinions not
expressed in it.Re: What You Can't SayLabelsJapanese TranslationFrench TranslationGerman TranslationDutch TranslationRomanian TranslationHebrew TranslationTurkish TranslationChinese TranslationButtonsA Civic Duty to AnnoyThe Perils of ObedienceAliens Cause Global WarmingHays CodeStratagem 32Conspiracy TheoriesMark Twain: Corn-pone OpinionsA Blacklist for ""Excuse Makers""What You Can't Say Will Hurt You","social commentary
",human,"human
","human
"
73,73,"May 2008Adults lie constantly to kids.  I'm not saying we should stop, but
I think we should at least examine which lies we tell and why.There may also be a benefit to us.  We were all lied to as kids,
and some of the lies we were told still affect us.  So by studying
the ways adults lie to kids, we may be able to clear our heads of
lies we were told.I'm using the word ""lie"" in a very general sense: not just overt
falsehoods, but also all the more subtle ways we mislead kids.
Though ""lie"" has negative connotations, I don't mean to suggest we
should never do this—just that we should pay attention when
we do.
[1]One of the most remarkable things about the way we lie to kids is
how broad the conspiracy is.  All adults know what their culture 
lies to kids about: they're the questions you answer ""Ask
your parents.""  If a kid asked who won the World Series in 1982
or what the atomic weight of carbon was, you could just tell him.
But if a kid asks you ""Is there a God?"" or ""What's a prostitute?""
you'll probably say ""Ask your parents.""Since we all agree, kids see few cracks in the view of the world
presented to them.  The biggest disagreements are between parents
and schools, but even those are small. Schools are careful what
they say about controversial topics, and if they do contradict what
parents want their kids to believe, parents either pressure the
school into keeping 
quiet or move their kids to a new school.The conspiracy is so thorough that most kids who discover it do so
only by discovering internal contradictions in what they're told.
It can be traumatic for the ones who wake up during the operation.
Here's what happened to Einstein:

  Through the reading of popular scientific books I soon reached
  the conviction that much in the stories of the Bible could not
  be true.  The consequence was a positively fanatic freethinking
  coupled with the impression that youth is intentionally being
  deceived by the state through lies: it was a crushing impression.
  [2]

I remember that feeling.  By 15 I was convinced the world was corrupt
from end to end.  That's why movies like The Matrix have such
resonance.  Every kid grows up in a fake world.  In a way it would
be easier if the forces behind it were as clearly differentiated
as a bunch of evil machines, and one could make a clean break just by
taking a pill.
ProtectionIf you ask adults why they lie to kids, the most common reason they
give is to protect them.  And kids do need protecting.  The environment
you want to create for a newborn child will be quite unlike the
streets of a big city.That seems so obvious it seems wrong to call it a lie.  It's certainly
not a bad lie to tell, to give a baby the impression the world is
quiet and warm and safe.  But this harmless type of lie can turn
sour if left unexamined.Imagine if you tried to keep someone in as protected an environment
as a newborn till age 18.  To mislead someone so grossly about the
world would seem not protection but abuse.  That's an extreme
example, of course; when parents do that sort of thing it becomes
national news.  But you see the same problem on a smaller scale in
the malaise teenagers feel in suburbia.The main purpose of suburbia is to provide a protected environment
for children to grow up in.  And it seems great for 10 year olds.
I liked living in suburbia when I was 10.  I didn't notice how
sterile it was.  My whole world was no bigger than a few friends'
houses I bicycled to and some woods I ran around in.  On a log scale
I was midway between crib and globe.  A suburban street was just
the right size.  But as I grew older, suburbia started to feel
suffocatingly fake.Life can be pretty good at 10 or 20, but it's often frustrating at
15.  This is too big a problem to solve here, but certainly one
reason life sucks at 15 is that kids are trapped in a world designed
for 10 year olds.What do parents hope to protect their children from by raising them
in suburbia?  A friend who moved out of Manhattan said merely that
her 3 year old daughter ""saw too much.""  Off the top of my head,
that might include: people who are high or drunk, poverty, madness,
gruesome medical conditions, sexual behavior of various degrees of
oddness, and violent anger.I think it's the anger that would worry me most if I had a 3 year
old.  I was 29 when I moved to New York and I was surprised even
then.  I wouldn't want a 3 year old to see some of the disputes I
saw.  It would be too frightening.  A lot of the things adults
conceal from smaller children, they conceal because they'd be
frightening, not because they want to conceal the existence of such
things.  Misleading the child is just a byproduct.This seems one of the most justifiable types of lying adults do to
kids.  But because the lies are indirect we don't keep a very strict
accounting of them.  Parents know they've concealed the facts about
sex, and many at some point sit their kids down and explain more.
But few tell their kids about the differences between the real world
and the cocoon they grew up in.  Combine this with the confidence
parents try to instill in their kids, and every year you get a new
crop of 18 year olds who think they know how to run the world.Don't all 18 year olds think they know how to run the world?  Actually
this seems to be a recent innovation, no more than about 100 years old.
In preindustrial times teenage kids were junior members of the adult
world and comparatively well aware of their shortcomings.  They
could see they weren't as strong or skillful as the village smith.
In past times people lied to kids about some things more than we
do now, but the lies implicit in an artificial, protected environment
are a recent invention.  Like a lot of new inventions, the rich got
this first.  Children of kings and great magnates were the first
to grow up out of touch with the world.  Suburbia means half the
population can live like kings in that respect.
Sex (and Drugs)I'd have different worries about raising teenage kids in New York.
I'd worry less about what they'd see, and more about what they'd
do.  I went to college with a lot of kids who grew up in Manhattan,
and as a rule they seemed pretty jaded.  They seemed to have lost
their virginity at an average of about 14 and by college had tried
more drugs than I'd even heard of.The reasons parents don't want their teenage kids having sex are
complex.  There are some obvious dangers: pregnancy and sexually
transmitted diseases.  But those aren't the only reasons parents
don't want their kids having sex.  The average parents of a 14 year
old girl would hate the idea of her having sex even if there were
zero risk of pregnancy or sexually transmitted diseases.Kids can probably sense they aren't being told the whole story.
After all, pregnancy and sexually transmitted diseases are just as
much a problem for adults, and they have sex.What really bothers parents about their teenage kids having sex?
Their dislike of the idea is so visceral it's probably inborn.  But
if it's inborn it should be universal, and there are plenty of
societies where parents don't mind if their teenage kids have
sex—indeed, where it's normal for 14 year olds to become
mothers.  So what's going on?  There does seem to be a universal
taboo against sex with prepubescent children.  One can imagine
evolutionary reasons for that.  And I think this is the main reason
parents in industrialized societies dislike teenage kids having
sex.  They still think of them as children, even though biologically
they're not, so the taboo against child sex still has force.One thing adults conceal about sex they also conceal about drugs:
that it can cause great pleasure.  That's what makes sex and drugs
so dangerous. The desire for them can cloud one's judgement—which
is especially frightening when the judgement being clouded is the
already wretched judgement of a teenage kid.Here parents' desires conflict.  Older societies told kids they had
bad judgement, but modern parents want their children to be confident.
This may well be a better plan than the old one of putting them in
their place, but it has the side effect that after having implicitly
lied to kids about how good their judgement is, we then have to lie
again about all the things they might get into trouble with if they
believed us.If parents told their kids the truth about sex and drugs, it would
be: the reason you should avoid these things is that you have lousy
judgement.  People with twice your experience still get burned by
them.  But this may be one of those cases where the truth wouldn't
be convincing, because one of the symptoms of bad judgement is
believing you have good judgement.  When you're too weak to lift
something, you can tell, but when you're making a decision impetuously,
you're all the more sure of it.
InnocenceAnother reason parents don't want their kids having sex is that
they want to keep them innocent.  Adults have a certain model of
how kids are supposed to behave, and it's different from what they
expect of other adults.One of the most obvious differences is the words kids are allowed
to use.  Most parents use words when talking to other adults that
they wouldn't want their kids using.  They try to hide even the
existence of these words for as long as they can.  And this is
another of those conspiracies everyone participates in: everyone
knows you're not supposed to swear in front of kids.I've never heard more different explanations for anything parents
tell kids than why they shouldn't swear.  Every parent I know forbids
their children to swear, and yet no two of them have the same
justification.  It's clear most start with not wanting kids to
swear, then make up the reason afterward.So my theory about what's going on is that the function of
swearwords is to mark the speaker as an adult.  There's no difference
in the meaning of ""shit"" and ""poopoo.""  So why should one be ok for
kids to say and one forbidden?  The only explanation is: by definition.
[3]Why does it bother adults so much when kids do things reserved for
adults?   The idea of a foul-mouthed, cynical 10 year old leaning
against a lamppost with a cigarette hanging out of the corner of
his mouth is very disconcerting.  But why?One reason we want kids to be innocent is that we're programmed to
like certain kinds of helplessness.  I've several times heard mothers
say they deliberately refrained from correcting their young children's
mispronunciations because they were so cute.  And if you think about
it, cuteness is helplessness. Toys and cartoon characters meant to
be cute always have clueless expressions and stubby, ineffectual
limbs.It's not surprising we'd have an inborn desire to love and protect
helpless creatures, considering human offspring are so helpless for
so long.  Without the helplessness that makes kids cute, they'd be
very annoying.  They'd merely seem like incompetent adults.  But
there's more to it than that.  The reason our hypothetical jaded
10 year old bothers me so much is not just that he'd be annoying,
but that he'd have cut off his prospects for growth so early.  To
be jaded you have to think you know how the world works, and any
theory a 10 year old had about that would probably be a pretty
narrow one.Innocence is also open-mindedness.  We want kids to be innocent so
they can continue to learn.  Paradoxical as it sounds, there are
some kinds of knowledge that get in the way of other kinds of
knowledge.  If you're going to learn that the world is a brutal
place full of people trying to take advantage of one another, you're
better off learning it last.  Otherwise you won't bother learning
much more.Very smart adults often seem unusually innocent, and I don't think
this is a coincidence.  I think they've deliberately avoided learning
about certain things.  Certainly I do.  I used to think I wanted
to know everything.  Now I know I don't.
DeathAfter sex, death is the topic adults lie most conspicuously about
to kids.  Sex I believe they conceal because of deep taboos.  But
why do we conceal death from kids?   Probably because small children
are particularly horrified by it.  They want to feel safe, and death
is the ultimate threat.One of the most spectacular lies our parents told us was about the
death of our first cat.  Over the years, as we asked for more
details, they were compelled to invent more, so the story grew quite
elaborate.  The cat had died at the vet's office.  Of what?  Of the
anaesthesia itself.  Why was the cat at the vet's office?  To be
fixed.  And why had such a routine operation killed it?  It wasn't
the vet's fault; the cat had a congenitally weak heart; the anaesthesia
was too much for it; but there was no way anyone could have
known this in advance.  It was not till we were in our twenties
that the truth came out: my sister, then about three, had accidentally
stepped on the cat and broken its back.They didn't feel the need to tell us the cat was now happily in cat
heaven.  My parents never claimed that people or animals who died
had ""gone to a better place,"" or that we'd meet them again.  It
didn't seem to harm us.My grandmother told us an edited version of the death of my
grandfather.  She said they'd been sitting reading one day, and
when she said something to him, he didn't answer.  He seemed to be
asleep, but when she tried to rouse him, she couldn't.  ""He was
gone."" Having a heart attack sounded like falling asleep.  Later I
learned it hadn't been so neat, and the heart attack had taken most
of a day to kill him.Along with such outright lies, there must have been a lot of changing
the subject when death came up.  I can't remember that, of course,
but I can infer it from the fact that I didn't really grasp I was
going to die till I was about 19.  How could I have missed something
so obvious for so long?  Now that I've seen parents managing the
subject, I can see how: questions about death are gently but firmly
turned aside.On this topic, especially, they're met half-way by kids.  Kids often
want to be lied to.  They want to believe they're living in a
comfortable, safe world as much as their parents want them to believe
it.
[4]
IdentitySome parents feel a strong adherence to an ethnic or religious group
and want their kids to feel it too.  This usually requires two
different kinds of lying: the first is to tell the child that he
or she is an X, and the second is whatever specific lies Xes
differentiate themselves by believing.
[5]Telling a child they have a particular ethnic or religious identity
is one of the stickiest things you can tell them.  Almost anything
else you tell a kid, they can change their mind about later when
they start to think for themselves.  But if you tell a kid they're
a member of a certain group, that seems nearly impossible to shake.This despite the fact that it can be one of the most premeditated
lies parents tell.  When parents are of different religions, they'll
often agree between themselves that their children will be ""raised
as Xes.""  And it works. The kids obligingly grow up considering
themselves as Xes, despite the fact that if their parents had chosen
the other way, they'd have grown up considering themselves as Ys.One reason this works so well is the second kind of lie involved.
The truth is common property.  You can't distinguish your group by
doing things that are rational, and believing things that are true.
If you want to set yourself apart from other people, you have to
do things that are arbitrary, and believe things that are false.
And after having spent their whole lives doing things that are arbitrary
and believing things that are false, and being regarded as odd by
""outsiders"" on that account, the cognitive dissonance pushing
children to regard themselves as Xes must be enormous.  If they
aren't an X, why are they attached to all these arbitrary beliefs
and customs?  If they aren't an X, why do all the non-Xes call them
one?This form of lie is not without its uses.  You can use it to carry
a payload of beneficial beliefs, and they will also become part of
the child's identity.  You can tell the child that in addition to
never wearing the color yellow, believing the world was created by
a giant rabbit, and always snapping their fingers before eating
fish, Xes are also particularly honest and industrious.  Then X
children will grow up feeling it's part of their identity to be
honest and industrious.This probably accounts for a lot of the spread of modern religions,
and explains why their doctrines are a combination of the useful
and the bizarre.  The bizarre half is what makes the religion stick,
and the useful half is the payload.
[6]
AuthorityOne of the least excusable reasons adults lie to kids is to maintain
power over them.  Sometimes these lies are truly sinister, like a
child molester telling his victims they'll get in trouble if they
tell anyone what happened to them.  Others seem more innocent; it
depends how badly adults lie to maintain their power, and what they
use it for.Most adults make some effort to conceal their flaws from children.
Usually their motives are mixed.  For example, a father who has an
affair generally conceals it from his children.  His motive is
partly that it would worry them, partly that this would introduce
the topic of sex, and partly (a larger part than he would admit)
that he doesn't want to tarnish himself in their eyes.If you want to learn what lies are told to kids, read almost any
book written to teach them about ""issues.""
[7]
Peter Mayle wrote
one called Why Are We Getting a Divorce?  It begins with the three
most important things to remember about divorce, one of which is:

  You shouldn't put the blame on one parent, because divorce is
  never only one person's fault.
  [8]

Really?  When a man runs off with his secretary, is it always partly
his wife's fault?  But I can see why Mayle might have said this.
Maybe it's more important for kids to respect their parents than
to know the truth about them.But because adults conceal their flaws, and at the same time insist
on high standards of behavior for kids, a lot of kids grow up feeling
they fall hopelessly short.  They walk around feeling horribly evil
for having used a swearword, while in fact most of the adults around
them are doing much worse things.This happens in intellectual as well as moral questions.  The more
confident people are, the more willing they seem to be to answer a
question ""I don't know.""  Less confident people feel they have to
have an answer or they'll look bad.  My parents were pretty good
about admitting when they didn't know things, but I must have been
told a lot of lies of this type by teachers, because I rarely heard
a teacher say ""I don't know"" till I got to college.  I remember
because it was so surprising to hear someone say that in front of
a class.The first hint I had that teachers weren't omniscient came in sixth
grade, after my father contradicted something I'd learned in school.
When I protested that the teacher had said the opposite, my father
replied that the guy had no idea what he was talking about—that
he was just an elementary school teacher, after all.Just a teacher?  The phrase seemed almost grammatically ill-formed.
Didn't teachers know everything about the subjects they taught?
And if not, why were they the ones teaching us?The sad fact is, US public school teachers don't generally understand
the stuff they're teaching very well.  There are some sterling
exceptions, but as a rule people planning to go into teaching rank
academically near the bottom of the college population.  So the
fact that I still thought at age 11 that teachers were infallible
shows what a job the system must have done on my brain.
SchoolWhat kids get taught in school is a complex mix of lies.  The most
excusable are those told to simplify ideas to make them easy to
learn.  The problem is, a lot of propaganda gets slipped into the
curriculum in the name of simplification.Public school textbooks represent a compromise between what various
powerful groups want kids to be told.  The lies are rarely overt.
Usually they consist either of omissions or of over-emphasizing
certain topics at the expense of others.  The view of history we
got in elementary school was a crude hagiography, with at least one
representative of each powerful group.The famous scientists I remember were Einstein, Marie Curie, and
George Washington Carver.   Einstein was a big deal because his
work led to the atom bomb.  Marie Curie was involved with X-rays.
But I was mystified about Carver.  He seemed to have done stuff
with peanuts.It's obvious now that he was on the list because he was black (and
for that matter that Marie Curie was on it because she was a woman),
but as a kid I was confused for years about him.  I wonder if it
wouldn't have been better just to tell us the truth: that there
weren't any famous black scientists.  Ranking George Washington
Carver with Einstein misled us not only about science, but about
the obstacles blacks faced in his time.As subjects got softer, the lies got more frequent.  By the time
you got to politics and recent history, what we were taught was
pretty much pure propaganda.  For example, we were taught to regard
political leaders as saints—especially the recently martyred
Kennedy and King.  It was astonishing to learn later that they'd
both been serial womanizers, and that Kennedy was a speed freak to
boot.  (By the time King's plagiarism emerged, I'd lost the ability
to be surprised by the misdeeds of famous people.)I doubt you could teach kids recent history without teaching them
lies, because practically everyone who has anything to say about
it has some kind of spin to put on it.  Much recent history consists
of spin.  It would probably be better just to teach them metafacts
like that.Probably the biggest lie told in schools, though, is that the way
to succeed is through following ""the rules.""  In fact most such
rules are just hacks to manage large groups efficiently.
PeaceOf all the reasons we lie to kids, the most powerful is probably
the same mundane reason they lie to us.Often when we lie to people it's not part of any conscious strategy,
but because they'd react violently to the truth.  Kids, almost by
definition, lack self-control.  They react violently to things—and
so they get lied to a lot. 
[9]A few Thanksgivings ago, a friend of mine found himself in a situation
that perfectly illustrates the complex motives we have when we lie
to kids.  As the roast turkey appeared on the table, his alarmingly
perceptive 5 year old son suddenly asked if the turkey had wanted
to die.  Foreseeing disaster, my friend and his wife rapidly
improvised: yes, the turkey had wanted to die, and in fact had lived
its whole life with the aim of being their Thanksgiving dinner.
And that (phew) was the end of that.Whenever we lie to kids to protect them, we're usually also lying
to keep the peace.One consequence of this sort of calming lie is that we grow up
thinking horrible things are normal.  It's hard for us to feel a
sense of urgency as adults over something we've literally been
trained not to worry about.  When I was about 10 I saw a documentary
on pollution that put me into a panic.  It seemed the planet was
being irretrievably ruined.  I went to my mother afterward to ask
if this was so.  I don't remember what she said, but she made me
feel better, so I stopped worrying about it.That was probably the best way to handle a frightened 10 year old.
But we should understand the price.  This sort of lie is one of the
main reasons bad things persist: we're all trained to ignore them.
DetoxA sprinter in a race almost immediately enters a state called ""oxygen
debt.""  His body switches to an emergency source of energy that's
faster than regular aerobic respiration.  But this process builds
up waste products that ultimately require extra oxygen to break
down, so at the end of the race he has to stop and pant for a while
to recover.We arrive at adulthood with a kind of truth debt.  We were told a
lot of lies to get us (and our parents) through our childhood.  Some
may have been necessary.  Some probably weren't.  But we all arrive
at adulthood with heads full of lies.There's never a point where the adults sit you down and explain all
the lies they told you.  They've forgotten most of them.  So if
you're going to clear these lies out of your head, you're going to
have to do it yourself.Few do.  Most people go through life with bits of packing material
adhering to their minds and never know it.  You probably never can
completely undo the effects of lies you were told as a kid, but
it's worth trying.  I've found that whenever I've been able to undo
a lie I was told, a lot of other things fell into place.Fortunately, once you arrive at adulthood you get a valuable new
resource you can use to figure out what lies you were told.  You're
now one of the liars.  You get to watch behind the scenes as adults
spin the world for the next generation of kids.The first step in clearing your head is to realize how far you are
from a neutral observer.  When I left high school I was, I thought,
a complete skeptic.  I'd realized high school was crap.  I thought
I was ready to question everything I knew.  But among the many other
things I was ignorant of was how much debris there already was in
my head.  It's not enough to consider your mind a blank slate.  You
have to consciously erase it.
Notes[1]
One reason I stuck with such a brutally simple word is that
the lies we tell kids are probably not quite as harmless as we
think.  If you look at what adults told children in the past, it's
shocking how much they lied to them.  Like us, they did it with the
best intentions.  So if we think we're as open as one could reasonably
be with children, we're probably fooling ourselves.  Odds are people
in 100 years will be as shocked at some of the lies we tell as we
are at some of the lies people told 100 years ago.I can't predict which these will be, and I don't want to write an
essay that will seem dumb in 100 years.  So instead of using special
euphemisms for lies that seem excusable according to present fashions,
I'm just going to call all our lies lies.(I have omitted one type: lies told to play games with kids'
credulity.  These range from ""make-believe,"" which is not really a
lie because it's told with a wink, to the frightening lies told by
older siblings.   There's not much to say about these: I wouldn't
want the first type to go away, and wouldn't expect the second type
to.)[2]
Calaprice, Alice (ed.), The Quotable Einstein, Princeton
University Press, 1996.[3]
If you ask parents why kids shouldn't swear, the less educated
ones usually reply with some question-begging answer like ""it's
inappropriate,"" while the more educated ones come up with elaborate
rationalizations.  In fact the less educated parents seem closer
to the truth.[4]
As a friend with small children pointed out, it's easy for small
children to consider themselves immortal, because time seems to
pass so slowly for them.  To a 3 year old, a day feels like a month
might to an adult.  So 80 years sounds to him like 2400 years would
to us.[5]
I realize I'm going to get endless grief for classifying religion
as a type of lie.  Usually people skirt that issue with some
equivocation implying that lies believed for a sufficiently long
time by sufficiently large numbers of people are immune to the usual
standards for truth.  But because I can't predict which lies future
generations will consider inexcusable, I can't safely omit any type
we tell.  Yes, it seems unlikely that religion will be out of fashion
in 100 years, but no more unlikely than it would have seemed to
someone in 1880 that schoolchildren in 1980 would be taught that
masturbation was perfectly normal and not to feel guilty about it.[6]
Unfortunately the payload can consist of bad customs as well
as good ones.  For example, there are certain qualities that some
groups in America consider ""acting white.""  In fact most of them
could as accurately be called ""acting Japanese.""  There's nothing
specifically white about such customs. They're common to all cultures
with long traditions of living in cities.  So it is probably a
losing bet for a group to consider behaving the opposite way as
part of its identity.[7]
In this context, ""issues"" basically means ""things we're going
to lie to them about.""  That's why there's a special name for these
topics.[8]
Mayle, Peter, Why Are We Getting a Divorce?, Harmony, 1988.[9]
The ironic thing is, this is also the main reason kids lie to
adults.  If you freak out when people tell you alarming things,
they won't tell you them.  Teenagers don't tell their parents what
happened that night they were supposed to be staying at a friend's
house for the same reason parents don't tell 5 year olds the truth
about the Thanksgiving turkey.  They'd freak if they knew.
Thanks to Sam Altman, Marc Andreessen, Trevor Blackwell,
Patrick Collison, Jessica Livingston, Jackie McDonough, Robert
Morris, and David Sloo for reading drafts of this.  And since there
are some controversial ideas here, I should add that none of them
agreed with everything in it.German TranslationFrench TranslationRussian Translation","social commentary
",human,"human
","human
"
74,74,"May 2008
Great cities attract ambitious people.  You can sense it when you
walk around one.  In a hundred subtle ways, the city sends you a
message: you could do more; you should try harder.The surprising thing is how different these messages can be.  New
York tells you, above all: you should make more money.  There are
other messages too, of course.  You should be hipper.  You should
be better looking.  But the clearest message is that you should be
richer.What I like about Boston (or rather Cambridge) is that the message
there is: you should be smarter.  You really should get around to
reading all those books you've been meaning to.When you ask what message a city sends, you sometimes get surprising
answers.  As much as they respect brains in Silicon Valley, the
message the Valley sends is: you should be more powerful.That's not quite the same message New York sends.  Power matters
in New York too of course, but New York is pretty impressed by a
billion dollars even if you merely inherited it.  In Silicon Valley
no one would care except a few real estate agents.  What matters
in Silicon Valley is how much effect you have on the world.  The
reason people there care about Larry and Sergey is not their wealth
but the fact that they control Google, which affects practically
everyone._____How much does it matter what message a city sends?  Empirically,
the answer seems to be: a lot. You might think that if you had
enough strength of mind to do great things, you'd be able to transcend
your environment.  Where you live should make at most a couple
percent difference.  But if you look at the historical evidence,
it seems to matter more than that.  Most people who did great things
were clumped together in a few places where that sort of thing was
done at the time.You can see how powerful cities are from something I wrote about
earlier: the case of the Milanese Leonardo.  
Practically every
fifteenth century Italian painter you've heard of was from Florence,
even though Milan was just as big.  People in Florence weren't
genetically different, so you have to assume there was someone born
in Milan with as much natural ability as Leonardo.  What happened
to him?If even someone with the same natural ability as Leonardo
couldn't beat the force of environment, do you suppose you can?I don't.  I'm fairly stubborn, but I wouldn't try to fight this
force.  I'd rather use it.  So I've thought a lot about where to
live.I'd always imagined Berkeley would be the ideal place — that
it would basically be Cambridge with good weather.  But when I
finally tried living there a couple years ago, it turned out not
to be.  The message Berkeley sends is: you should live better.  Life
in Berkeley is very civilized.  It's probably the place in America
where someone from Northern Europe would feel most at home.  But
it's not humming with ambition.In retrospect it shouldn't have been surprising that a place so
pleasant would attract people interested above all in quality of
life.  Cambridge with good weather, it turns out, is not Cambridge.
The people you find in Cambridge are not there by accident.  You
have to make sacrifices to live there.  It's expensive and somewhat
grubby, and the weather's often bad.  So the kind of people you
find in Cambridge are the kind of people who want to live where the
smartest people are, even if that means living in an expensive,
grubby place with bad weather.As of this writing, Cambridge seems to be the intellectual capital
of the world.  I realize that seems a preposterous claim.  What
makes it true is that it's more preposterous to claim about anywhere
else.  American universities currently seem to be the best, judging
from the flow of ambitious students.  And what US city has a stronger
claim?  New York?  A fair number of smart people, but diluted by a
much larger number of neanderthals in suits.  The Bay Area has a
lot of smart people too, but again, diluted;  there are two great
universities, but they're far apart.  Harvard and MIT are practically
adjacent by West Coast standards, and they're surrounded by about
20 other colleges and universities.
[1]Cambridge as a result feels like a town whose main industry is
ideas, while New York's is finance and Silicon Valley's is startups._____When you talk about cities in the sense we are, what you're really
talking about is collections of people.  For a long time cities
were the only large collections of people, so you could use the two
ideas interchangeably.  But we can see how much things are changing
from the examples I've mentioned.  New York is a classic great city.
But Cambridge is just part of a city, and Silicon Valley is not
even that.  (San Jose is not, as it sometimes claims, the capital
of Silicon Valley.  It's just 178 square miles at one end of it.)Maybe the Internet will change things further.  Maybe one day the
most important community you belong to will be a virtual one, and
it won't matter where you live physically.  But I wouldn't bet on
it.  The physical world is very high bandwidth, and some of the
ways cities send you messages are quite subtle.One of the exhilarating things about coming back to Cambridge every
spring is walking through the streets at dusk, when you can see
into the houses.  When you walk through Palo Alto in the evening,
you see nothing but the blue glow of TVs.  In Cambridge you see
shelves full of promising-looking books.  Palo Alto was probably
much like Cambridge in 1960, but you'd never guess now that there
was a university nearby.  Now it's just one of the richer neighborhoods
in Silicon Valley. 
[2]A city speaks to you mostly by accident — in things you see
through windows, in conversations you overhear.  It's not something
you have to seek out, but something you can't turn off.  One of the
occupational hazards of living in Cambridge is overhearing the
conversations of people who use interrogative intonation in declarative
sentences.  But on average I'll take Cambridge conversations over
New York or Silicon Valley ones.A friend who moved to Silicon Valley in the late 90s said the worst
thing about living there was the low quality of the eavesdropping.
At the time I thought she was being deliberately eccentric. Sure,
it can be interesting to eavesdrop on people, but is good quality
eavesdropping so important that it would affect where you chose to
live?  Now I understand what she meant.  The conversations you
overhear tell you what sort of people you're among._____No matter how determined you are, it's hard not to be influenced
by the people around you.  It's not so much that you do whatever a
city expects of you, but that you get discouraged when no one around
you cares about the same things you do.There's an imbalance between encouragement and discouragement like
that between gaining and losing money.  Most people overvalue
negative amounts of money: they'll work much harder to avoid losing
a dollar than to gain one.  Similarly, although there are plenty of
people strong enough to resist doing something just because that's
what one is supposed to do where they happen to be, there are few
strong enough to keep working on something no one around them cares
about.Because ambitions are to some extent incompatible and admiration
is a zero-sum game, each city tends to focus on one type of ambition.
The reason Cambridge is the intellectual capital is not just that
there's a concentration of smart people there, but that there's
nothing else people there care about more.  Professors in
New York and the Bay area are second class citizens — till they
start hedge funds or startups respectively.This suggests an answer to a question people in New York have
wondered about since the Bubble: whether New York could grow into
a startup hub to rival Silicon Valley.  One reason that's unlikely
is that someone starting a startup in New York would feel like a
second class citizen. 
[3]
There's already something else people in New York admire more.In the long term, that could be a bad thing for New York.  The power
of an important new technology does eventually convert to money.
So by caring more about money and less about power than Silicon
Valley, New York is recognizing the same thing, but slower.
[4]
And in fact it has been losing to Silicon Valley at its own game:
the ratio of New York to California residents in the Forbes 400 has
decreased from 1.45 (81:56) when the list was first published in
1982 to .83 (73:88) in 2007._____Not all cities send a message.  Only those that are centers for
some type of ambition do.  And it can be hard to tell exactly what
message a city sends without living there.  I understand the messages
of New York, Cambridge, and Silicon Valley because I've lived for
several years in each of them.  DC and LA seem to send messages
too, but I haven't spent long enough in either to say for sure what
they are.The big thing in LA seems to be fame.  There's an A List of people
who are most in demand right now, and what's most admired is to be
on it, or friends with those who are.  Beneath that, the message is
much like New York's, though perhaps with more emphasis on physical
attractiveness.In DC the message seems to be that the most important thing is who
you know.  You want to be an insider.  In practice this seems to
work much as in LA.  There's an A List and you want to be on it or
close to those who are.  The only difference is how the A List is
selected.  And even that is not that different.At the moment, San Francisco's message seems to be the same as
Berkeley's: you should live better.  But this will change if enough
startups choose SF over the Valley.  During the Bubble that was a
predictor of failure — a self-indulgent choice, like buying
expensive office furniture.  Even now I'm suspicious when startups
choose SF.  But if enough good ones do, it stops being a self-indulgent
choice, because the center of gravity of Silicon Valley will shift
there.I haven't found anything like Cambridge for intellectual ambition.
Oxford and Cambridge (England) feel like Ithaca or Hanover: the
message is there, but not as strong.Paris was once a great intellectual center.  If you went there in
1300, it might have sent the message Cambridge does now.  But I
tried living there for a bit last year, and the ambitions of the
inhabitants are not intellectual ones.  The message Paris sends now
is: do things with style.  I liked that, actually.  Paris is the
only city I've lived in where people genuinely cared about art.  In
America only a few rich people buy original art, and even the more
sophisticated ones rarely get past judging it by the brand name of
the artist.  But looking through windows at dusk in Paris you can
see that people there actually care what paintings look like.
Visually, Paris has the best eavesdropping I know. 
[5]There's one more message I've heard from cities: in London you can
still (barely) hear the message that one should be more aristocratic.
If you listen for it you can also hear it in Paris, New York, and
Boston.  But this message is everywhere very faint.  It would have
been strong 100 years ago, but now I probably wouldn't have picked
it up at all if I hadn't deliberately tuned in to that wavelength
to see if there was any signal left._____So far the complete list of messages I've picked up from cities is:
wealth, style, hipness, physical attractiveness, fame, political
power, economic power, intelligence, social class, and quality of
life.My immediate reaction to this list is that it makes me slightly
queasy.  I'd always considered ambition a good thing, but I realize
now that was because I'd always implicitly understood it to mean
ambition in the areas I cared about.  When you list everything
ambitious people are ambitious about, it's not so pretty.On closer examination I see a couple things on the list that are
surprising in the light of history.  For example, physical
attractiveness wouldn't have been there 100 years ago (though it
might have been 2400 years ago).  It has always mattered for women,
but in the late twentieth century it seems to have started to matter
for men as well.  I'm not sure why — probably some combination
of the increasing power of women, the increasing influence of actors
as models, and the fact that so many people work in offices now:
you can't show off by wearing clothes too fancy to wear in a factory,
so you have to show off with your body instead.Hipness is another thing you wouldn't have seen on the list 100
years ago.  Or wouldn't you?  What it means is to know what's what.
So maybe it has simply replaced the component of social class that
consisted of being ""au fait.""  That could explain why hipness seems
particularly admired in London: it's version 2 of the traditional
English delight in obscure codes that only insiders understand.Economic power would have been on the list 100 years ago, but what
we mean by it is changing.  It used to mean the control of vast
human and material resources.  But increasingly it means the ability
to direct the course of technology, and some of the people in a
position to do that are not even rich — leaders of important
open source projects, for example.  The Captains of Industry of
times past had laboratories full of clever people cooking up new
technologies for them.  The new breed are themselves those people.As this force gets more attention, another is dropping off the list:
social class.  I think the two changes are related.  Economic power,
wealth, and social class are just names for the same thing at
different stages in its life: economic power converts to wealth,
and wealth to social class.  So the focus of admiration is simply
shifting upstream._____Does anyone who wants to do great work have to live in a great city?
No; all great cities inspire some sort of ambition, but they aren't
the only places that do.  For some kinds of work, all you need is
a handful of talented colleagues.What cities provide is an audience, and a funnel for peers.  These
aren't so critical in something like math or physics, where no
audience matters except your peers, and judging ability is sufficiently
straightforward that hiring and admissions committees can do it
reliably.  In a field like math or physics all you need is a
department with the right colleagues in it.  It could be anywhere — in
Los Alamos, New Mexico, for example.It's in fields like the arts or writing or technology that the
larger environment matters.  In these the best practitioners aren't
conveniently collected in a few top university departments and
research labs — partly because talent is harder to judge, and
partly because people pay for these things, so one doesn't need to
rely on teaching or research funding to support oneself.  It's in
these more chaotic fields that it helps most to be in a great city:
you need the encouragement of feeling that people around you care
about the kind of work you do, and since you have to find peers for
yourself, you need the much larger intake mechanism of a great city.You don't have to live in a great city your whole life to benefit
from it.  The critical years seem to be the early and middle ones
of your career.  Clearly you don't have to grow up in a great city.
Nor does it seem to matter if you go to college in one.  To most
college students a world of a few thousand people seems big enough.
Plus in college you don't yet have to face the hardest kind of
work — discovering new problems to solve.It's when you move on to the next and much harder step that it helps
most to be in a place where you can find peers and encouragement.
You seem to be able to leave, if you want, once you've found both.
The Impressionists show the typical pattern: they were born all
over France (Pissarro was born in the Carribbean) and died all over
France, but what defined them were the years they spent together
in Paris._____Unless you're sure what you want to do and where the leading center
for it is, your best bet is probably to try living in several
places when you're young.  You can never tell what message a city
sends till you live there, or even whether it still sends one.
Often your information will be wrong: I tried living in Florence
when I was 25, thinking it would be an art center, but it turned
out I was 450 years too late.Even when a city is still a live center of ambition, you won't know
for sure whether its message will resonate with you till you hear
it.  When I moved to New York, I was very excited at first.  It's
an exciting place.  So it took me quite a while to realize I just
wasn't like the people there.  I kept searching for the Cambridge
of New York.  It turned out it was way, way uptown: an hour uptown
by air.Some people know at 16 what sort of work they're going to do, but
in most ambitious kids, ambition seems to precede anything specific
to be ambitious about.  They know they want to do something great.
They just haven't decided yet whether they're going to be a rock
star or a brain surgeon.  There's nothing wrong with that.  But it
means if you have this most common type of ambition, you'll probably
have to figure out where to live by trial and error.  You'll
probably have to find the city where you feel at home to know what sort of
ambition you have.Notes[1]
This is one of the advantages of not having the universities
in your country controlled by the government.  When governments
decide how to allocate resources, political deal-making causes
things to be spread out geographically.  No central goverment would
put its two best universities in the same town, unless it was the
capital (which would cause other problems).  But scholars seem to
like to cluster together as much as people in any other field, and
when given the freedom to they derive the same advantages from it.[2]
There are still a few old professors in Palo Alto, but one by
one they die and their houses are transformed by developers into
McMansions and sold to VPs of Bus Dev.[3]
How many times have you read about startup founders who continued
to live inexpensively as their companies took off?  Who continued
to dress in jeans and t-shirts, to drive the old car they had in
grad school, and so on?  If you did that in New York, people would
treat you like shit.  If you walk into a fancy restaurant in San
Francisco wearing a jeans and a t-shirt, they're nice to you; who
knows who you might be?  Not in New York.One sign of a city's potential as a technology center is the number
of restaurants that still require jackets for men.  According to
Zagat's there are none in San Francisco, LA, Boston, or Seattle, 
4 in DC, 6 in Chicago, 8 in London, 13 in New York, and 20 in Paris.(Zagat's lists the Ritz Carlton Dining Room in SF as requiring jackets
but I couldn't believe it, so I called to check and in fact they
don't. Apparently there's only one restaurant left on the entire West
Coast that still requires jackets: The French Laundry in Napa Valley.)[4]
Ideas are one step upstream from economic power, so it's
conceivable that intellectual centers like Cambridge will one day
have an edge over Silicon Valley like the one the Valley has over
New York.This seems unlikely at the moment; if anything Boston is falling
further and further behind.  The only reason I even mention the
possibility is that the path from ideas to startups has recently
been getting smoother.  It's a lot easier now for a couple of hackers
with no business experience to start a startup than it was 10 years
ago.  If you extrapolate another 20 years, maybe the balance of
power will start to shift back.  I wouldn't bet on it, but I wouldn't
bet against it either.[5]
If Paris is where people care most about art, why is New York
the center of gravity of the art business?  Because in the twentieth
century, art as brand split apart from art as stuff.  New York is
where the richest buyers are, but all they demand from art is brand,
and since you can base brand on anything with a sufficiently
identifiable style, you may as well use the local stuff.Thanks to Trevor Blackwell, Sarah Harlin, Jessica Livingston,
Jackie McDonough, Robert Morris, and David Sloo for reading drafts
of this.Italian TranslationPortuguese TranslationChinese TranslationKorean Translation","social commentary
",human,"human
","human
"
75,75,"July 2020One of the most revealing ways to classify people is by the degree
and aggressiveness of their conformism. Imagine a Cartesian coordinate
system whose horizontal axis runs from conventional-minded on the
left to independent-minded on the right, and whose vertical axis
runs from passive at the bottom to aggressive at the top. The
resulting four quadrants define four types of people. Starting in
the upper left and going counter-clockwise: aggressively
conventional-minded, passively conventional-minded, passively
independent-minded, and aggressively independent-minded.I think that you'll find all four types in most societies, and that
which quadrant people fall into depends more on their own personality
than the beliefs prevalent in their society.
[1]Young children offer some of the best evidence for both points.
Anyone who's been to primary school has seen the four types, and
the fact that school rules are so arbitrary is strong evidence that
which quadrant people fall into depends more on them than the rules.The kids in the upper left quadrant, the aggressively conventional-minded
ones, are the tattletales. They believe not only that rules must
be obeyed, but that those who disobey them must be punished.The kids in the lower left quadrant, the passively conventional-minded,
are the sheep. They're careful to obey the rules, but when other
kids break them, their impulse is to worry that those kids will be
punished, not to ensure that they will.The kids in the lower right quadrant, the passively independent-minded,
are the dreamy ones. They don't care much about rules and probably
aren't 100% sure what the rules even are.And the kids in the upper right quadrant, the aggressively
independent-minded, are the naughty ones. When they see a rule,
their first impulse is to question it. Merely being told what to
do makes them inclined to do the opposite.When measuring conformism, of course, you have to say with respect
to what, and this changes as kids get older. For younger kids it's
the rules set by adults. But as kids get older, the source of rules
becomes their peers. So a pack of teenagers who all flout school
rules in the same way are not independent-minded; rather the opposite.In adulthood we can recognize the four types by their distinctive
calls, much as you could recognize four species of birds. The call
of the aggressively conventional-minded is ""Crush <outgroup>!"" (It's
rather alarming to see an exclamation point after a variable, but
that's the whole problem with the aggressively conventional-minded.)
The call of the passively conventional-minded is ""What will the
neighbors think?"" The call of the passively independent-minded is
""To each his own."" And the call of the aggressively independent-minded
is ""Eppur si muove.""The four types are not equally common. There are more passive people
than aggressive ones, and far more conventional-minded people than
independent-minded ones. So the passively conventional-minded are
the largest group, and the aggressively independent-minded the
smallest.Since one's quadrant depends more on one's personality than the
nature of the rules, most people would occupy the same quadrant
even if they'd grown up in a quite different society.Princeton professor Robert George recently wrote:

   I sometimes ask students what their position on slavery would
   have been had they been white and living in the South before
   abolition. Guess what? They all would have been abolitionists!
   They all would have bravely spoken out against slavery, and
   worked tirelessly against it.

He's too polite to say so, but of course they wouldn't. And indeed,
our default assumption should not merely be that his students would,
on average, have behaved the same way people did at the time, but
that the ones who are aggressively conventional-minded today would
have been aggressively conventional-minded then too. In other words,
that they'd not only not have fought against slavery, but that
they'd have been among its staunchest defenders.I'm biased, I admit, but it seems to me that aggressively
conventional-minded people are responsible for a disproportionate
amount of the trouble in the world, and that a lot of the customs
we've evolved since the Enlightenment have been designed to protect
the rest of us from them. In particular, the retirement of the
concept of heresy and its replacement by the principle of freely
debating all sorts of different ideas, even ones that are currently
considered unacceptable, without any punishment for those who try
them out to see if they work.
[2]Why do the independent-minded need to be protected, though? Because
they have all the new ideas. To be a successful scientist, for
example, it's not enough just to be right. You have to be right
when everyone else is wrong. Conventional-minded people can't do
that. For similar reasons, all successful startup CEOs are not
merely independent-minded, but aggressively so. So it's no coincidence
that societies prosper only to the extent that they have customs
for keeping the conventional-minded at bay.
[3]In the last few years, many of us have noticed that the customs
protecting free inquiry have been weakened. Some say we're overreacting
— that they haven't been weakened very much, or that they've been
weakened in the service of a greater good. The latter I'll dispose
of immediately. When the conventional-minded get the upper hand,
they always say it's in the service of a greater good.  It just
happens to be a different, incompatible greater good each time.As for the former worry, that the independent-minded are being
oversensitive, and that free inquiry hasn't been shut down that
much, you can't judge that unless you are yourself independent-minded.
You can't know how much of the space of ideas is being lopped off
unless you have them, and only the independent-minded have the ones
at the edges. Precisely because of this, they tend to be very
sensitive to changes in how freely one can explore ideas. They're
the canaries in this coalmine.The conventional-minded say, as they always do, that they don't
want to shut down the discussion of all ideas, just the bad ones.You'd think it would be obvious just from that sentence what a
dangerous game they're playing. But I'll spell it out. There are
two reasons why we need to be able to discuss even ""bad"" ideas.The first is that any process for deciding which ideas to ban is
bound to make mistakes. All the more so because no one intelligent
wants to undertake that kind of work, so it ends up being done by
the stupid. And when a process makes a lot of mistakes, you need
to leave a margin for error. Which in this case means you need to
ban fewer ideas than you'd like to. But that's hard for the
aggressively conventional-minded to do, partly because they enjoy
seeing people punished, as they have since they were children, and
partly because they compete with one another. Enforcers of orthodoxy
can't allow a borderline idea to exist, because that gives other
enforcers an opportunity to one-up them in the moral purity department,
and perhaps even to turn enforcer upon them. So instead of getting
the margin for error we need, we get the opposite: a race to the
bottom in which any idea that seems at all bannable ends up being
banned. 
[4]The second reason it's dangerous to ban the discussion of ideas is
that ideas are more closely related than they look. Which means if
you restrict the discussion of some topics, it doesn't only affect
those topics. The restrictions propagate back into any topic that
yields implications in the forbidden ones. And that is not an edge
case. The best ideas do exactly that: they have consequences
in fields far removed from their origins. Having ideas in a world
where some ideas are banned is like playing soccer on a pitch that
has a minefield in one corner. You don't just play the same game
you would have, but on a different shaped pitch. You play a much
more subdued game even on the ground that's safe.In the past, the way the independent-minded protected themselves
was to congregate in a handful of places —  first in courts, and
later in universities — where they could to some extent make their
own rules. Places where people work with ideas tend to have customs
protecting free inquiry, for the same reason wafer fabs have powerful
air filters, or recording studios good sound insulation. For the
last couple centuries at least, when the aggressively conventional-minded
were on the rampage for whatever reason, universities were the
safest places to be.That may not work this time though, due to the unfortunate fact
that the latest wave of intolerance began in universities. It began
in the mid 1980s, and by 2000 seemed to have died down, but it has
recently flared up again with the arrival of social media. This
seems, unfortunately, to have been an own goal by Silicon Valley.
Though the people who run Silicon Valley are almost all independent-minded,
they've handed the aggressively conventional-minded a tool such as
they could only have dreamed of.On the other hand, perhaps the decline in the spirit of free inquiry
within universities is as much the symptom of the departure of the
independent-minded as the cause. People who would have become
professors 50 years ago have other options now. Now they can become
quants or start startups. You have to be independent-minded to
succeed at either of those. If these people had been professors,
they'd have put up a stiffer resistance on behalf of academic
freedom. So perhaps the picture of the independent-minded fleeing
declining universities is too gloomy. Perhaps the universities are
declining because so many have already left.
[5]Though I've spent a lot of time thinking about this situation, I
can't predict how it plays out. Could some universities reverse the
current trend and remain places where the independent-minded want
to congregate? Or will the independent-minded gradually abandon
them? I worry a lot about what we might lose if that happened.But I'm hopeful long term. The independent-minded are good at
protecting themselves. If existing institutions are compromised,
they'll create new ones. That may require some imagination. But
imagination is, after all, their specialty.
Notes[1]
I realize of course that if people's personalities vary in any
two ways, you can use them as axes and call the resulting four
quadrants personality types. So what I'm really claiming is that
the axes are orthogonal and that there's significant variation in
both.[2]
The aggressively conventional-minded aren't responsible for all
the trouble in the world. Another big source of trouble is the sort
of charismatic leader who gains power by appealing to them. They
become much more dangerous when such leaders emerge.[3]
I never worried about writing things that offended the
conventional-minded when I was running Y Combinator. If YC were a
cookie company, I'd have faced a difficult moral choice.
Conventional-minded people eat cookies too. But they don't start
successful startups. So if I deterred them from applying to YC, the
only effect was to save us work reading applications.[4]
There has been progress in one area: the punishments for talking
about banned ideas are less severe than in the past. There's little
danger of being killed, at least in richer countries. The aggressively
conventional-minded are mostly satisfied with getting people fired.[5]
Many professors are independent-minded — especially in math,
the hard sciences, and engineering, where you have to be to succeed.
But students are more representative of the general population, and
thus mostly conventional-minded. So when professors and students
are in conflict, it's not just a conflict between generations but
also between different types of people.Thanks to Sam Altman, Trevor Blackwell, Nicholas Christakis, Patrick
Collison, Sam Gichuru, Jessica Livingston, Patrick McKenzie, Geoff
Ralston, and Harj Taggar for reading drafts of this.German TranslationKorean TranslationSerbian Translation","social commentary
",human,"human
","human
"
76,76,"May 2021Noora Health, a nonprofit I've 
supported for years, just launched
a new NFT. It has a dramatic name, Save Thousands of Lives,
because that's what the proceeds will do.Noora has been saving lives for 7 years. They run programs in
hospitals in South Asia to teach new mothers how to take care of
their babies once they get home. They're in 165 hospitals now. And
because they know the numbers before and after they start at a new
hospital, they can measure the impact they have. It is massive.
For every 1000 live births, they save 9 babies.This number comes from a study
of 133,733 families at 28 different
hospitals that Noora conducted in collaboration with the Better
Birth team at Ariadne Labs, a joint center for health systems
innovation at Brigham and Women’s Hospital and Harvard T.H. Chan
School of Public Health.Noora is so effective that even if you measure their costs in the
most conservative way, by dividing their entire budget by the number
of lives saved, the cost of saving a life is the lowest I've seen.
$1,235.For this NFT, they're going to issue a public report tracking how
this specific tranche of money is spent, and estimating the number
of lives saved as a result.NFTs are a new territory, and this way of using them is especially
new, but I'm excited about its potential. And I'm excited to see
what happens with this particular auction, because unlike an NFT
representing something that has already happened,
this NFT gets better as the price gets higher.The reserve price was about $2.5 million, because that's what it
takes for the name to be accurate: that's what it costs to save
2000 lives. But the higher the price of this NFT goes, the more
lives will be saved. What a sentence to be able to write.","social commentary
",human,"human
","human
"
77,77,"December 2019There are two distinct ways to be politically moderate: on purpose
and by accident. Intentional moderates are trimmers, deliberately
choosing a position mid-way between the extremes of right and left.
Accidental moderates end up in the middle, on average, because they
make up their own minds about each question, and the far right and
far left are roughly equally wrong.You can distinguish intentional from accidental moderates by the
distribution of their opinions. If the far left opinion on some
matter is 0 and the far right opinion 100, an intentional moderate's
opinion on every question will be near 50. Whereas an accidental
moderate's opinions will be scattered over a broad range, but will,
like those of the intentional moderate, average to about 50.Intentional moderates are similar to those on the far left and the
far right in that their opinions are, in a sense, not their own.
The defining quality of an ideologue, whether on the left or the
right, is to acquire one's opinions in bulk. You don't get to pick
and choose. Your opinions about taxation can be predicted from your
opinions about sex. And although intentional moderates
might seem to be the opposite of ideologues, their beliefs (though
in their case the word ""positions"" might be more accurate) are also
acquired in bulk. If the median opinion shifts to the right or left,
the intentional moderate must shift with it. Otherwise they stop
being moderate.Accidental moderates, on the other hand, not only choose their own
answers, but choose their own questions. They may not care at all
about questions that the left and right both think are terribly
important. So you can only even measure the politics of an accidental
moderate from the intersection of the questions they care about and
those the left and right care about, and this can
sometimes be vanishingly small.It is not merely a manipulative rhetorical trick to say ""if you're
not with us, you're against us,"" but often simply false.Moderates are sometimes derided as cowards, particularly by 
the extreme left. But while it may be accurate to call intentional
moderates cowards, openly being an accidental moderate requires the
most courage of all, because you get attacked from both right and
left, and you don't have the comfort of being an orthodox member
of a large group to sustain you.Nearly all the most impressive people I know are accidental moderates.
If I knew a lot of professional athletes, or people in the entertainment
business, that might be different. Being on the far left or far
right doesn't affect how fast you run or how well you sing. But
someone who works with ideas has to be independent-minded to do it
well.Or more precisely, you have to be independent-minded about the ideas
you work with. You could be mindlessly doctrinaire in your politics
and still be a good mathematician. In the 20th century, a lot of
very smart people were Marxists — just no one who was smart about
the subjects Marxism involves. But if the ideas you use in your
work intersect with the politics of your time, you have two choices:
be an accidental moderate, or be mediocre.Notes[1] It's possible in theory for one side to be entirely right and
the other to be entirely wrong. Indeed, ideologues must always
believe this is the case. But historically it rarely has been.[2] For some reason the far right tend to ignore moderates rather
than despise them as backsliders. I'm not sure why. Perhaps it
means that the far right is less ideological than the far left. Or
perhaps that they are more confident, or more resigned, or simply
more disorganized. I just don't know.[3] Having heretical opinions doesn't mean you have to express
them openly. It may be
easier to have them if you don't.
Thanks to Austen Allred, Trevor Blackwell, Patrick Collison, Jessica Livingston,
Amjad Masad, Ryan Petersen, and Harj Taggar for reading drafts of this.Japanese Translation","social commentary
",human,"human
","human
"
78,78,"May 2021Most people think of nerds as quiet, diffident people. In ordinary
social situations they are — as quiet and diffident as the star
quarterback would be if he found himself in the middle of a physics
symposium. And for the same reason: they are fish out of water.
But the apparent diffidence of nerds is an illusion due to the fact
that when non-nerds observe them, it's usually in ordinary social
situations. In fact some nerds are quite fierce.The fierce nerds are a small but interesting group. They are as a
rule extremely competitive — more competitive, I'd say, than highly
competitive non-nerds. Competition is more personal for them. Partly
perhaps because they're not emotionally mature enough to distance
themselves from it, but also because there's less randomness in the
kinds of competition they engage in, and they are thus more justified
in taking the results personally.Fierce nerds also tend to be somewhat overconfident, especially
when young. It might seem like it would be a disadvantage to be
mistaken about one's abilities, but empirically it isn't. Up to a
point, confidence is a self-fullfilling prophecy.Another quality you find in most fierce nerds is intelligence. Not
all nerds are smart, but the fierce ones are always at least
moderately so. If they weren't, they wouldn't have the confidence
to be fierce.
[1]There's also a natural connection between nerdiness and
independent-mindedness. It's hard to be 
independent-minded without
being somewhat socially awkward, because conventional beliefs are
so often mistaken, or at least arbitrary. No one who was both
independent-minded and ambitious would want to waste the effort it
takes to fit in. And the independent-mindedness of the fierce nerds
will obviously be of the aggressive 
rather than the passive type:
they'll be annoyed by rules, rather than dreamily unaware of them.I'm less sure why fierce nerds are impatient, but most seem to be.
You notice it first in conversation, where they tend to interrupt
you. This is merely annoying, but in the more promising fierce nerds
it's connected to a deeper impatience about solving problems. Perhaps
the competitiveness and impatience of fierce nerds are not separate 
qualities, but two manifestations of a single underlying drivenness.When you combine all these qualities in sufficient quantities, the
result is quite formidable. The most vivid example of fierce nerds
in action may be James Watson's The Double Helix. The first sentence
of the book is ""I have never seen Francis Crick in a modest mood,""
and the portrait he goes on to paint of Crick is the quintessential
fierce nerd: brilliant, socially awkward, competitive, independent-minded,
overconfident. But so is the implicit portrait he paints of himself.
Indeed, his lack of social awareness makes both portraits that much
more realistic, because he baldly states all sorts of opinions and
motivations that a smoother person would conceal. And moreover it's
clear from the story that Crick and Watson's fierce nerdiness was
integral to their success. Their independent-mindedness caused them
to consider approaches that most others ignored, their overconfidence
allowed them to work on problems they only half understood (they
were literally described as ""clowns"" by one eminent insider), and
their impatience and competitiveness got them to the answer ahead
of two other groups that would otherwise have found it within the
next year, if not the next several months.
[2]The idea that there could be fierce nerds is an unfamiliar one not
just to many normal people but even to some young nerds. Especially
early on, nerds spend so much of their time in ordinary social
situations and so little doing real work that they get a lot more
evidence of their awkwardness than their power. So there will be
some who read this description of the fierce nerd and realize ""Hmm,
that's me."" And it is to you, young fierce nerd, that I now turn.I have some good news, and some bad news. The good news is that
your fierceness will be a great help in solving difficult problems.
And not just the kind of scientific and technical problems that
nerds have traditionally solved. As the world progresses, the number
of things you can win at by getting the right answer increases.
Recently getting rich became 
one of them: 7 of the 8 richest people
in America are now fierce nerds.Indeed, being a fierce nerd is probably even more helpful in business
than in nerds' original territory of scholarship. Fierceness seems
optional there. Darwin for example doesn't seem to have been
especially fierce. Whereas it's impossible to be the CEO of a company
over a certain size without being fierce, so now that nerds can win
at business, fierce nerds will increasingly monopolize the really
big successes.The bad news is that if it's not exercised, your fierceness will
turn to bitterness, and you will become an intellectual playground
bully: the grumpy sysadmin, the forum troll, the 
hater, the shooter
down of new ideas.How do you avoid this fate? Work on ambitious projects. If you
succeed, it will bring you a kind of satisfaction that neutralizes
bitterness. But you don't need to have succeeded to feel this;
merely working on hard projects gives most fierce nerds some
feeling of satisfaction. And those it doesn't, it at least keeps
busy.
[3]Another solution may be to somehow turn off your fierceness, by
devoting yourself to meditation or psychotherapy or something like
that. Maybe that's the right answer for some people. I have no idea.
But it doesn't seem the optimal solution to me. If you're given a
sharp knife, it seems to me better to use it than to blunt its edge
to avoid cutting yourself.If you do choose the ambitious route, you'll have a tailwind behind
you. There has never been a better time to be a nerd. In the past
century we've seen a continuous transfer of power from dealmakers
to technicians — from the charismatic to the competent — and I
don't see anything on the horizon that will end it. At least not
till the nerds end it themselves by bringing about the singularity.Notes[1]
To be a nerd is to be socially awkward, and there are two
distinct ways to do that: to be playing the same game as everyone
else, but badly, and to be playing a different game. The smart nerds
are the latter type.[2]
The same qualities that make fierce nerds so effective can
also make them very annoying. Fierce nerds would do well to remember
this, and (a) try to keep a lid on it, and (b) seek out organizations
and types of work where getting the right answer matters more than
preserving social harmony. In practice that means small groups
working on hard problems. Which fortunately is the most fun kind
of environment anyway.[3]
If success neutralizes bitterness, why are there some people
who are at least moderately successful and yet still quite bitter?
Because people's potential bitterness varies depending on how
naturally bitter their personality is, and how ambitious they are:
someone who's naturally very bitter will still have a lot left after
success neutralizes some of it, and someone who's very ambitious
will need proportionally more success to satisfy that ambition.So the worst-case scenario is someone who's both naturally bitter
and extremely ambitious, and yet only moderately successful.
Thanks to Trevor Blackwell, Steve Blank, Patrick Collison, Jessica
Livingston, Amjad Masad, and Robert Morris for reading drafts of this.Chinese Translation","social commentary
",human,"human
","human
"
79,79,"February 2008A user on Hacker News recently posted a
comment
that set me thinking:

  Something about hacker culture that never really set well with
  me was this — the nastiness. ... I just don't understand why people
  troll like they do.

I've thought a lot over the last couple years about the problem of
trolls.  It's an old one, as old as forums, but
we're still just learning what the causes are and how to address
them.There are two senses of the word ""troll.""  In the original sense
it meant someone, usually an outsider, who deliberately stirred up
fights in a forum by saying controversial things.
[1]
For example,
someone who didn't use a certain programming language might go to
a forum for users of that language and make disparaging remarks
about it, then sit back and watch as people rose to the bait.  This
sort of trolling was in the nature of a practical joke, like letting
a bat loose in a room full of people.The definition then spread to people who behaved like assholes in
forums, whether intentionally or not.  Now when people talk about
trolls they usually mean this broader sense of the word.  Though
in a sense this is historically inaccurate, it is in other ways
more accurate, because when someone is being an asshole it's usually
uncertain even in their own mind how much is deliberate.
That is arguably one of the defining qualities of an asshole.I think trolling in the broader sense has four causes.  The most
important is distance.  People will say things in anonymous forums
that they'd never dare say to someone's face, just as they'll do
things in cars that they'd never do as pedestrians — like tailgate
people, or honk at them, or cut them off.Trolling tends to be particularly bad in forums related to computers,
and I think that's due to the kind of people you find there.  Most
of them (myself included) are more comfortable dealing with abstract
ideas than with people.  Hackers can be abrupt even in person.  Put
them on an anonymous forum, and the problem gets worse.The third cause of trolling is incompetence.  If you disagree with
something, it's easier to say ""you suck"" than to figure out and
explain exactly what you disagree with.  You're also safe that way
from refutation.  In this respect trolling is a lot like graffiti.
Graffiti happens at the intersection of ambition and incompetence:
people want to make their mark on the world, but have no other way
to do it than literally making a mark on the world.
[2]The final contributing factor is the culture of the forum.  Trolls
are like children (many are children) in that they're capable of
a wide range of behavior depending on what they think will be
tolerated.  In a place where rudeness isn't tolerated, most can be
polite.  But vice versa as well.There's a sort of Gresham's Law of trolls: trolls are willing to
use a forum with a lot of thoughtful people in it, but thoughtful
people aren't willing to use a forum with a lot of trolls in it.
Which means that once trolling takes hold, it tends to become the
dominant culture.  That had already happened to Slashdot and Digg by
the time I paid attention to comment threads there, but I watched
it happen to Reddit.News.YC is, among other things, an experiment to see if this fate
can be avoided.  The sites's guidelines
explicitly ask people not to say things they wouldn't say face to
face.  If someone starts being rude, other users will step in and
tell them to stop.  And when people seem to be deliberately trolling,
we ban them ruthlessly.Technical tweaks may also help.  On Reddit, votes on your comments
don't affect your karma score, but they do on News.YC.  And it does
seem to influence people when they can see their reputation in the
eyes of their peers drain away after making an asshole remark.
Often users have second thoughts and delete such comments.One might worry this would prevent people from expressing controversial
ideas, but empirically that doesn't seem to be what happens.  When
people say something substantial that gets modded down, they
stubbornly leave it up.  What people delete are wisecracks, because
they have less invested in them.So far the experiment seems to be working.  The level of conversation
on News.YC is as high as on any forum I've seen.  But we still only
have about 8,000 uniques a day.  The conversations on Reddit were
good when it was that small.  The challenge is whether we can keep
things this way.I'm optimistic we will.  We're not depending just on technical
tricks.  The core users of News.YC are mostly refugees from other
sites that were overrun by trolls.  They feel about trolls roughly
the way refugees from Cuba or Eastern Europe feel about dictatorships.
So there are a lot of people working to keep this from happening
again.
Notes[1]
I mean forum in the general sense of a place to exchange views.
The original Internet forums were not web sites but Usenet newsgroups.[2]
I'm talking here about everyday tagging.  Some graffiti is
quite impressive (anything becomes art if you do it well enough)
but the median tag is just visual spam.Russian Translation","social commentary
",human,"human
","human
"
80,80,"April 2021When intellectuals talk about the death penalty, they talk about
things like whether it's permissible for the state to take someone's
life, whether the death penalty acts as a deterrent, and whether
more death sentences are given to some groups than others. But in
practice the debate about the death penalty is not about whether
it's ok to kill murderers. It's about whether it's ok to kill
innocent people, because at least 4% of people on death row are
innocent.When I was a kid I imagined that it was unusual for people to be
convicted of crimes they hadn't committed, and that in murder cases
especially this must be very rare. Far from it. Now, thanks to
organizations like the
Innocence Project,
we see a constant stream
of stories about murder convictions being overturned after new
evidence emerges. Sometimes the police and prosecutors were just
very sloppy. Sometimes they were crooked, and knew full well they
were convicting an innocent person.Kenneth Adams and three other men spent 18 years in prison on a
murder conviction. They were exonerated after DNA testing implicated
three different men, two of whom later confessed. The police had
been told about the other men early in the investigation, but never
followed up the lead.Keith Harward spent 33 years in prison on a murder conviction. He
was convicted because ""experts"" said his teeth matched photos of
bite marks on one victim. He was exonerated after DNA testing showed
the murder had been committed by another man, Jerry Crotty.Ricky Jackson and two other men spent 39 years in prison after being
convicted of murder on the testimony of a 12 year old boy, who later
recanted and said he'd been coerced by police. Multiple people have
confirmed the boy was elsewhere at the time. The three men were
exonerated after the county prosecutor dropped the charges, saying
""The state is conceding the obvious.""Alfred Brown spent 12 years in prison on a murder conviction,
including 10 years on death row. He was exonerated after it was
discovered that the assistant district attorney had concealed phone
records proving he could not have committed the crimes.Glenn Ford spent 29 years on death row after having been convicted
of murder. He was exonerated after new evidence proved he was not
even at the scene when the murder occurred. The attorneys assigned
to represent him had never tried a jury case before.Cameron Willingham was actually executed in 2004 by lethal injection.
The ""expert"" who testified that he deliberately set fire to his
house has since been discredited. A re-examination of the case
ordered by the state of Texas in 2009 concluded that ""a finding of
arson could not be sustained.""Rich Glossip 
has spent 20 years on death row after being convicted
of murder on the testimony of the actual killer, who escaped with
a life sentence in return for implicating him. In 2015 he came
within minutes of execution before it emerged that Oklahoma had
been planning to kill him with an illegal combination of drugs.
They still plan to go ahead with the execution, perhaps as soon as
this summer, despite 
new 
evidence exonerating him.I could go on. There are hundreds of similar cases. In Florida
alone, 29 death row prisoners have been exonerated so far.Far from being rare, wrongful murder convictions are 
very common.
Police are under pressure to solve a crime that has gotten a lot
of attention. When they find a suspect, they want to believe he's
guilty, and ignore or even destroy evidence suggesting otherwise.
District attorneys want to be seen as effective and tough on crime,
and in order to win convictions are willing to manipulate witnesses
and withhold evidence. Court-appointed defense attorneys are
overworked and often incompetent. There's a ready supply of criminals
willing to give false testimony in return for a lighter sentence,
suggestible witnesses who can be made to say whatever police want,
and bogus ""experts"" eager to claim that science proves the defendant
is guilty. And juries want to believe them, since otherwise some
terrible crime remains unsolved.This circus of incompetence and dishonesty is the real issue with
the death penalty. We don't even reach the point where theoretical
questions about the moral justification or effectiveness of capital
punishment start to matter, because so many of the people sentenced
to death are actually innocent. Whatever it means in theory, in
practice capital punishment means killing innocent people.
Thanks to Trevor Blackwell, Jessica Livingston, and Don Knight for
reading drafts of this.Related:Will Florida Kill an Innocent Man?Was Kevin Cooper Framed for Murder?Did Texas execute an innocent man?","social commentary
",human,"human
","human
"
81,81,"December 2014I've read Villehardouin's chronicle of the Fourth Crusade at least
two times, maybe three.  And yet if I had to write down everything
I remember from it, I doubt it would amount to much more than a
page.  Multiply this times several hundred, and I get an uneasy
feeling when I look at my bookshelves. What use is it to read all
these books if I remember so little from them?A few months ago, as I was reading Constance Reid's excellent
biography of Hilbert, I figured out if not the answer to this
question, at least something that made me feel better about it.
She writes:

  Hilbert had no patience with mathematical lectures which filled
  the students with facts but did not teach them how to frame a
  problem and solve it. He often used to tell them that ""a perfect
  formulation of a problem is already half its solution.""

That has always seemed to me an important point, and I was even
more convinced of it after hearing it confirmed by Hilbert.But how had I come to believe in this idea in the first place?  A
combination of my own experience and other things I'd read.  None
of which I could at that moment remember!  And eventually I'd forget
that Hilbert had confirmed it too.  But my increased belief in the
importance of this idea would remain something I'd learned from
this book, even after I'd forgotten I'd learned it.Reading and experience train your model of the world.  And even if
you forget the experience or what you read, its effect on your model
of the world persists.  Your mind is like a compiled program you've
lost the source of.  It works, but you don't know why.The place to look for what I learned from Villehardouin's chronicle
is not what I remember from it, but my mental models of the crusades,
Venice, medieval culture, siege warfare, and so on.  Which doesn't
mean I couldn't have read more attentively, but at least the harvest
of reading is not so miserably small as it might seem.This is one of those things that seem obvious in retrospect.  But
it was a surprise to me and presumably would be to anyone else who
felt uneasy about (apparently) forgetting so much they'd read.Realizing it does more than make you feel a little better about
forgetting, though.  There are specific implications.For example, reading and experience are usually ""compiled"" at the
time they happen, using the state of your brain at that time.  The
same book would get compiled differently at different points in
your life.  Which means it is very much worth reading important
books multiple times.  I always used to feel some misgivings about
rereading books.  I unconsciously lumped reading together with work
like carpentry, where having to do something again is a sign you
did it wrong the first time.  Whereas now the phrase ""already read""
seems almost ill-formed.Intriguingly, this implication isn't limited to books.  Technology
will increasingly make it possible to relive our experiences.  When
people do that today it's usually to enjoy them again (e.g. when
looking at pictures of a trip) or to find the origin of some bug in
their compiled code (e.g. when Stephen Fry succeeded in remembering
the childhood trauma that prevented him from singing).  But as
technologies for recording and playing back your life improve, it
may become common for people to relive experiences without any goal
in mind, simply to learn from them again as one might when rereading
a book.Eventually we may be able not just to play back experiences but
also to index and even edit them. So although not knowing how you
know things may seem part of being human, it may not be.
Thanks to Sam Altman, Jessica Livingston, and Robert Morris for reading 
drafts of this.Japanese Translation","educational content
",human,"human
","human
"
82,82,"August 2005(This essay is derived from a talk at Oscon 2005.)Lately companies have been paying more attention to open source.
Ten years ago there seemed a real danger Microsoft would extend its
monopoly to servers.  It seems safe to say now that open source has
prevented that.  A recent survey found 52% of companies are replacing
Windows servers with Linux servers.
[1]More significant, I think, is which 52% they are.  At this point,
anyone proposing to run Windows on servers should be prepared to
explain what they know about servers that Google, Yahoo, and Amazon
don't.But the biggest thing business has to learn from open source is not
about Linux or Firefox, but about the forces that produced them.
Ultimately these will affect a lot more than what software you use.We may be able to get a fix on these underlying forces by triangulating
from open source and blogging.  As you've probably noticed, they
have a lot in common.Like open source, blogging is something people do themselves, for
free, because they enjoy it.  Like open source hackers, bloggers
compete with people working for money, and often win.  The method
of ensuring quality is also the same: Darwinian.  Companies ensure
quality through rules to prevent employees from screwing up.  But
you don't need that when the audience can communicate with one
another.  People just produce whatever they want; the good stuff
spreads, and the bad gets ignored.  And in both cases, feedback
from the audience improves the best work.Another thing blogging and open source have in common is the Web.
People have always been willing to do great work
for free,  but before the Web it was harder to reach an audience
or collaborate on projects.AmateursI think the most important of the new principles business has to learn is
that people work a lot harder on stuff they like.  Well, that's
news to no one.  So how can I claim business has to learn it?  When
I say business doesn't know this, I mean the structure of business
doesn't reflect it.Business still reflects an older model, exemplified by the French
word for working: travailler.  It has an English cousin, travail,
and what it means is torture.
[2]This turns out not to be the last word on work, however.
As societies get richer, they learn something about
work that's a lot like what they learn about diet.  We know now that the
healthiest diet is the one our peasant ancestors were forced to
eat because they were poor.  Like rich food, idleness
only seems desirable when you don't get enough of it.  I think we were
designed to work, just as we were designed to eat a certain amount
of fiber, and we feel bad if we don't.There's a name for people who work for the love of it: amateurs.
The word now has such bad connotations that we forget its etymology,
though it's staring us in the face.  ""Amateur"" was originally rather
a complimentary word.  But the thing to be in the twentieth century
was professional, which amateurs, by definition, are not.That's why the business world was so surprised by one lesson from
open source: that people working for love often surpass those working
for money.   Users don't switch from Explorer to Firefox because
they want to hack the source.  They switch because it's a better
browser.It's not that Microsoft isn't trying.  They know controlling the
browser is one of the keys to retaining their monopoly.  The problem
is the same they face in operating systems: they can't pay people
enough to build something better than a group of inspired hackers
will build for free.I suspect professionalism was always overrated-- not just in the
literal sense of working for money, but also connotations like
formality and detachment.  Inconceivable as it would have seemed
in, say, 1970, I think professionalism was largely a fashion,
driven by conditions that happened to exist in the twentieth century.One of the most powerful of those was the existence of ""channels."" Revealingly,
the same term was used for both products and information:  there
were distribution channels, and TV and radio channels.It was the narrowness of such channels that made professionals
seem so superior to amateurs.  There were only a few jobs as
professional journalists, for example, so competition ensured the
average journalist was fairly good.  Whereas anyone can express
opinions about current events in a bar.  And so the average person
expressing his opinions in a bar sounds like an idiot compared to
a journalist writing about the subject.On the Web, the barrier for publishing your ideas is even lower.
You don't have to buy a drink, and they even let kids in.
Millions of people are publishing online, and the average
level of what they're writing, as you might expect, is not very
good.  This has led some in the media to conclude that blogs don't
present much of a threat-- that blogs are just a fad.Actually, the fad is the word ""blog,""  at least the way the print
media now use it.  What they mean by ""blogger"" is not someone who
publishes in a weblog format, but anyone who publishes online.
That's going to become a problem as the Web becomes the default
medium for publication.  So I'd
like to suggest an alternative word for someone who publishes online.
How about ""writer?""Those in the print media who dismiss the writing online because of
its low average quality are missing an important point: no one reads
the average blog.  In the old world of channels, it meant something
to talk about average quality, because that's what you were getting
whether you liked it or not.
But now you can read any writer you want.  So the average
quality of writing online isn't what the print media are competing
against.  They're competing against the best writing online.  And, 
like Microsoft, they're losing.I know that from my own experience as a reader.  Though most print
publications are online, I probably
read two or three articles on individual people's sites for every
one I read on the site of a newspaper or magazine.And when I read, say, New York Times stories, I never reach
them through the Times front page.   Most I find through aggregators
like Google News or Slashdot or Delicious. Aggregators show how
much better 
you can do than the channel.  The New York Times front page is
a list of articles written by people who work for the New York Times.  Delicious
is a list of articles that are interesting.  And it's only now that
you can see the two side by side that you notice how little overlap there is.Most articles in the print media are boring.  For example, the
president notices that a majority of voters now think invading Iraq
was a mistake, so he makes an address to the nation to drum up
support.  Where is the man bites dog in that?  I didn't hear the
speech, but I could probably tell you exactly what he said.  A
speech like that is, in the most literal sense, not news: there is
nothing new in it.
[3]Nor is there anything new, except the names and places, in most
""news"" about things going wrong.  A child is abducted; there's a
tornado; a ferry sinks; someone gets bitten by a shark; a small
plane crashes.  And what do you learn about the world from these
stories?  Absolutely nothing.  They're outlying data points; what
makes them gripping also makes them irrelevant.As in software, when professionals produce such crap, it's not
surprising if amateurs can do better.  Live by the channel, die by
the channel: if you depend on an oligopoly, you sink into bad habits
that are hard to overcome when you suddenly get competition.
[4]WorkplacesAnother thing blogs and open source software have in common is that
they're often made by people working at home.  That may not seem
surprising.  But it should be.  It's the architectural equivalent
of a home-made aircraft shooting down an F-18.  Companies spend
millions to build office buildings for a single purpose: to be a
place to work.  And yet people working in their own homes,
which aren't even designed to be workplaces, end up
being more productive.This proves something a lot of us have suspected.  The average
office is a miserable place to get work done.  And a lot of what
makes offices bad are the very qualities we associate with
professionalism.  The sterility
of offices is supposed to suggest efficiency.  But suggesting
efficiency is a different thing from actually being efficient.The atmosphere of the average workplace is to productivity what
flames painted on the side of a car are to speed.  And it's not
just the way offices look that's bleak.  The way people act is just
as bad.Things are different in a startup.  Often as not a startup begins
in an apartment.  Instead of matching beige cubicles
they have an assortment of furniture they bought used.  They work
odd hours, wearing the most casual of clothing.  They look at
whatever they want online without worrying whether it's ""work safe.""
The cheery, bland language of the office is replaced by wicked humor.  And
you know what?  The company at this stage is probably the most
productive it's ever going to be.Maybe it's not a coincidence.  Maybe some aspects of professionalism
are actually a net lose.To me the most demoralizing aspect of the traditional office is
that you're supposed to be there at certain times.  There are usually
a few people in a company who really have to, but the reason most
employees work fixed hours is that the company can't measure their
productivity.The basic idea behind office hours is that if you can't make people
work, you can at least prevent them from having fun.  If employees
have to be in the building a certain number of hours a day, and are
forbidden to do non-work things while there, then they must be
working.  In theory.  In practice they spend a lot of their time
in a no-man's land, where they're neither working nor having fun.If you could measure how much work people did, many companies
wouldn't need any fixed workday. You could just say: this is what
you have to do.  Do it whenever you like, wherever you like.  If
your work requires you to talk to other people in the company, then
you may need to be here a certain amount.  Otherwise we don't care.That may seem utopian, but it's what we told people who came to
work for our company.  There were no fixed office hours.  I never
showed up before 11 in the morning.  But we weren't saying this to
be benevolent.  We were saying: if you work here we expect you to
get a lot done.  Don't try to fool us just by being here a lot.The problem with the facetime model is not just that it's demoralizing, but
that the people pretending to work interrupt
the ones actually working.  I'm convinced the facetime model
is the main reason large organizations have so many meetings.
Per capita, large organizations accomplish very little.
And yet all those people have to be on site at least eight hours a
day.  When so much time goes in one end and so little achievement
comes out the other, something has to give.  And meetings are the
main mechanism for taking up the slack.For one year I worked at a regular nine to five job, and I remember
well the strange, cozy feeling that comes over one during meetings.
I was very aware, because of the novelty, that I was being paid for
programming.  It seemed just amazing, as if there was a machine on
my desk that spat out a dollar bill every two minutes no matter
what I did.  Even while I was in the bathroom!  But because the
imaginary machine was always running, I felt I always ought to be
working. And so meetings felt wonderfully relaxing.  They
counted as work, just like programming, but they were so much easier.
All you had to do was sit and look attentive.Meetings are like an opiate with a network effect.  So is email,
on a smaller scale.  And in addition to the direct cost in time,
there's the cost in fragmentation-- breaking people's day up into
bits too small to be useful.You can see how dependent you've become on something by removing
it suddenly.  So for big companies I propose the following experiment.
Set aside one day where meetings are forbidden-- where everyone has to
sit at their desk all day and work without interruption on
things they can do without talking to anyone else.
Some amount of communication is necessary in most jobs, but I'm
sure many employees could find eight hours worth of stuff they could
do by themselves.  You could call it ""Work Day.""The other problem with pretend work
is that it often looks better than real work.  When I'm
writing or hacking I spend as much time just thinking as I do
actually typing.  Half the time I'm sitting drinking a cup of tea,
or walking around the neighborhood.  This is a critical phase--
this is where ideas come from-- and yet I'd feel guilty doing this
in most offices, with everyone else looking busy.It's hard to see how bad some practice is till you have something
to compare it to.  And that's one reason open source, and even blogging
in some cases, are so important.  They show us what real work looks like.We're funding eight new startups at the moment.  A friend asked
what they were doing for office space, and seemed surprised when I
said we expected them to work out of whatever apartments they found
to live in.  But we didn't propose that to save money.  We did it
because we want their software to be good.  Working in crappy
informal spaces is one of the things startups do right without
realizing it.  As soon as you get into an office, work and life
start to drift apart.That is one of the key tenets of professionalism. Work and life
are supposed to be separate.  But that part, I'm convinced, is a 
mistake.Bottom-UpThe third big lesson we can learn from open source and
blogging is that ideas can bubble up from the bottom, instead of
flowing down from the top.  Open source and blogging both work
bottom-up: people make what they want, and the best stuff
prevails.Does this sound familiar?  It's the principle of a market economy.
Ironically, though open source and blogs are done for free, those
worlds resemble market economies, while most companies, for all
their talk about the value of free markets, are run internally like
communist states.There are two forces that together steer design: ideas about
what to do next, and the enforcement of quality.  In the channel
era, both flowed down from the top.  For example, newspaper editors
assigned stories to reporters, then edited what they wrote.Open source and blogging show us things don't have to work that
way.  Ideas and even the enforcement of quality can flow bottom-up.
And in both cases the results are not merely acceptable, but better.
For example, open source software is more reliable precisely because
it's open source; anyone can find mistakes.The same happens with writing.  As we got close to publication, I
found I was very worried about the essays in 
Hackers
& Painters
that hadn't been online.  Once an essay has had a couple thousand
page views I feel reasonably confident about it.  But these had had 
literally orders of magnitude less scrutiny.  It felt like
releasing software without testing it.That's what all publishing used to be like.  If
you got ten people to read a manuscript, you were lucky.  But I'd
become so used to publishing online that the old method now seemed
alarmingly unreliable, like navigating by dead reckoning once you'd
gotten used to a GPS.The other thing I like about publishing online is that you can write
what you want and publish when you want.  Earlier this year I wrote
something that seemed suitable for a magazine, so
I sent it to an editor I know.
As I was waiting to hear back, I found to my surprise that I was
hoping they'd reject it.  Then I could put it online right away.
If they accepted it, it wouldn't be read by anyone for months, and
in the meantime I'd have to fight word-by-word to save it from being
mangled by some twenty five year old copy editor.
[5]Many employees would like to build great things for the companies
they work for, but more often than not management won't let them.
How many of us have heard stories of employees going to management
and saying, please let us build this thing to make money for you--
and the company saying no?  The most famous example is probably Steve Wozniak,
who originally wanted to build microcomputers for his then-employer, HP.
And they turned him down.  On the blunderometer, this episode ranks
with IBM accepting a non-exclusive license for DOS.  But I think this
happens all the time.  We just don't hear about it usually,
because to prove yourself right you have to quit
and start your own company, like Wozniak did.StartupsSo these, I think, are the three big lessons open source and blogging
have to teach business: (1) that people work harder on stuff they
like, (2) that the standard office environment is very unproductive,
and (3) that bottom-up often works better than top-down.I can imagine managers at this point saying: what is this guy talking
about?  What good does it do me to know that my programmers
would be more productive
working at home on their own projects?  I need their asses in here
working on version 3.2 of our software, or we're never going to
make the release date.And it's true, the benefit that specific manager could derive from
the forces I've described is near zero.  When I say business can
learn from open source, I don't mean any specific business can.  I
mean business can learn about new conditions the same way a gene
pool does.  I'm not claiming companies can get smarter, just that
dumb ones will die.So what will business look like when it has assimilated the lessons
of open source and blogging?  I think the big obstacle preventing
us from seeing the future of business is the assumption that people
working for you have to be employees.  But think about what's going
on underneath:  the company has some money, and they pay it to the
employee in the hope that he'll make something worth more than they
paid him.  Well, there are other ways to arrange that relationship.
Instead of paying the guy money as a salary, why not give it to him
as investment?  Then instead of coming to your office to work on
your projects, he can work wherever he wants on projects of his own.Because few of us know any alternative, we have no idea how much
better we could do than the traditional employer-employee relationship.
Such customs evolve with glacial slowness.  Our 
employer-employee relationship still retains a big chunk of
master-servant DNA.
[6]I dislike being on either end of it.
I'll work my ass off for a customer, but I resent being told what
to do by a boss.  And being a boss is also horribly frustrating; 
half the time it's easier just to do stuff yourself than to get
someone else to do it for you.
I'd rather do almost anything than give or receive a
performance review.On top of its unpromising origins, employment
has accumulated a lot of cruft over the years.  The list of what
you can't ask in job interviews is now so long that for convenience
I assume it's infinite.  Within the
office you now have to walk on eggshells lest anyone 
say or do
something that makes the company prey to a lawsuit.  And God help
you if you fire anyone.Nothing shows more clearly that employment is not an ordinary economic
relationship than companies being sued for firing people.  In any
purely economic relationship you're free to do what you want.  If
you want to stop buying steel pipe from one supplier and start
buying it from another, you don't have to explain why.  No one can
accuse you of unjustly switching pipe suppliers.  Justice implies
some kind of paternal obligation that isn't there in
transactions between equals.Most of the legal restrictions on employers are intended to protect
employees.  But you can't have action without an equal and opposite
reaction.  You can't expect employers to have some kind of paternal
responsibility toward employees without putting employees in the
position of children.  And that seems a bad road to go down.Next time you're in a moderately large city, drop by the main post
office and watch the body language of the people working there.
They have the same sullen resentment as children made to do
something they don't want to.  Their union has exacted pay
increases and work restrictions that would have been the envy of
previous generations of postal workers, and yet they don't seem any
happier for it.  It's demoralizing
to be on the receiving end of a paternalistic relationship, no
matter how cozy the terms.  Just ask any teenager.I see the disadvantages of the employer-employee relationship because
I've been on both sides of a better one: the investor-founder relationship.
I wouldn't claim it's painless.  When I was running a
startup, the thought of our investors used to keep me up at night.
And now that I'm an investor,
the thought of our startups keeps me
up at night.  All the pain of whatever problem you're trying to
solve is still there.
But the pain hurts less when it isn't
mixed with resentment.I had the misfortune to participate in what amounted to a controlled
experiment to prove that.  After Yahoo bought our startup I went
to work for them.  I was doing exactly the same work, except with
bosses.  And to my horror I started acting like a child.  The 
situation pushed buttons I'd forgotten
I had.The big advantage of investment over employment, as the examples of open
source and blogging suggest, is that people working on projects of
their own are enormously more productive.  And a
startup is a project
of one's own in two senses, both of them important: it's creatively
one's own, and also economically ones's own.Google is a rare example of a big company in tune with the forces
I've described. They've tried hard to make their offices less sterile
than the usual cube farm.  They give employees who do great work
large grants of stock to simulate the rewards of a startup.  They
even let hackers spend 20% of their time on their own projects.Why not let people spend 100% of their time on their own projects,
and instead of trying to approximate the value of what they create,
give them the actual market value?  Impossible?  That is in fact
what venture capitalists do.So am I claiming that no one is going to be an employee anymore--
that everyone should go and start a startup?  Of course not.
But more people could do it than do it now.
At the moment, even the smartest students leave school thinking
they have to get a job.  
Actually what they need to do is make
something valuable.  A job is one way to do that, but the more
ambitious ones will ordinarily be better off taking money from an
investor than an employer.Hackers tend to think business is for MBAs.  But business
administration is not what you're doing in a startup.  What you're
doing is business creation.  And the first phase of that
is mostly product creation-- that is, hacking.  That's the
hard part.  It's a lot harder to create something people love than
to take something people love and figure out how to make money from
it.Another thing that keeps people away from starting startups is the
risk.  Someone with kids and a mortgage should think twice before
doing it.  But most young hackers have neither.And as the example of open source and blogging suggests, you'll
enjoy it more, even if you fail.  You'll be working on your own
thing, instead of going to some office and doing what you're told.
There may be more pain in your own company, but it won't hurt as
much.That may be the greatest effect, in the long run, of the forces 
underlying open source and blogging: finally ditching the old
paternalistic employer-employee relationship, and replacing it with
a purely economic one, between equals.
Notes[1]
Survey by Forrester Research reported in the cover story of
Business Week, 31 Jan 2005.  Apparently someone believed you have to
replace the actual server in order to switch the operating system.[2]
It derives from the late Latin tripalium,
a torture device so called because it consisted of three stakes.
I don't know how the stakes were used.  ""Travel"" has the same root.[3]
It would be much bigger news, in that sense, if the president
faced unscripted questions by giving a press conference.[4]
One measure of the incompetence of newspapers is that so many
still make you register to read stories.  I have yet to find a blog
that tried that.[5]
They accepted the article, but I took so long to
send them the final version that by the time I did the section of
the magazine they'd accepted it for had disappeared in a reorganization.[6]
The word ""boss"" is derived from the Dutch baas, meaning
""master.""Thanks to Sarah Harlin, Jessica Livingston, and Robert Morris for reading drafts of this.French TranslationRussian TranslationJapanese TranslationSpanish TranslationArabic Translation","technological and societal insights
",human,"human
","human
"
83,83,"April 2009Om Malik is the most recent of many people
to ask why Twitter is such a big deal.The reason is that it's a new messaging 
protocol, where you don't specify the recipients.
New protocols are rare.  Or more precisely, new
protocols that take off are.
There are only a handful of commonly used ones: TCP/IP 
(the Internet), SMTP (email), HTTP (the web), and so on.  So any
new protocol is a big deal.  But Twitter is a protocol owned
by a private company.  That's even rarer.Curiously, the fact that the founders of Twitter 
have been slow to monetize it may in the long run
prove to be an advantage.  Because they haven't tried
to control it too much, Twitter feels to everyone like
previous protocols.  One forgets it's owned by a
private company.  That must have made it easier for
Twitter to spread.","technological and societal insights
",human,"human
","human
"
84,84,"December 2014American technology companies want the government to make immigration
easier because they say they can't find enough programmers in the
US.  Anti-immigration people say that instead of letting foreigners
take these jobs, we should train more Americans to be programmers.
Who's right?The technology companies are right. What the anti-immigration people
don't understand is that there is a huge variation in ability between
competent programmers and exceptional ones, and while you can train
people to be competent, you can't train them to be exceptional.
Exceptional programmers have an aptitude for and 
interest in
programming that is not merely the product of training.
[1]The US has less than 5% of the world's population.  Which means if
the qualities that make someone a great programmer are evenly
distributed, 95% of great programmers are born outside the US.The anti-immigration people have to invent some explanation to
account for all the effort technology companies have expended trying
to make immigration easier.  So they claim it's because they want
to drive down salaries.  But if you talk to startups, you find
practically every one over a certain size has gone through legal
contortions to get programmers into the US, where they then
paid them the same as they'd have paid an American.  Why would they
go to extra trouble to get programmers for the same price?  The
only explanation is that they're telling the truth: there are just
not enough great programmers to go around.
[2]I asked the CEO of a startup with about 70 programmers how many
more he'd hire if he could get all the great programmers he wanted.
He said ""We'd hire 30 tomorrow morning.""  And this is one of the
hot startups that always win recruiting battles. It's the same all
over Silicon Valley.  Startups are that constrained for talent.It would be great if more Americans were trained as programmers,
but no amount of training can flip a ratio as overwhelming as 95
to 5. Especially since programmers are being trained in other
countries too.  Barring some cataclysm, it will always be true that
most great programmers are born outside the US.  It will always be
true that most people who are great at anything are born outside
the US.
[3]Exceptional performance implies immigration.  A country with only
a few percent of the world's population will be exceptional in some
field only if there are a lot of immigrants working in it.But this whole discussion has taken something for granted: that if
we let more great programmers into the US, they'll want to come.
That's true now, and we don't realize how lucky we are that it is.
If we want to keep this option open, the best way to do it is to
take advantage of it: the more of the world's great programmers are
here, the more the rest will want to come here.And if we don't, the US could be seriously fucked. I realize that's
strong language, but the people dithering about this don't seem to
realize the power of the forces at work here.  Technology gives the
best programmers huge leverage.  The world market in programmers
seems to be becoming dramatically more liquid.  And since good
people like good colleagues, that means the best programmers could
collect in just a few hubs.  Maybe mostly in one hub.What if most of the great programmers collected in one hub, and it
wasn't here?  That scenario may seem unlikely now, but it won't be
if things change as much in the next 50 years as they did in the
last 50.We have the potential to ensure that the US remains a technology
superpower just by letting in a few thousand great programmers a
year.  What a colossal mistake it would be to let that opportunity
slip.  It could easily be the defining mistake this generation of
American politicians later become famous for.  And unlike other
potential mistakes on that scale, it costs nothing to fix.So please, get on with it.
Notes[1]
How much better is a great programmer than an ordinary one?
So much better that you can't even measure the difference directly.
A great programmer doesn't merely do the same work faster.  A great
programmer will invent things an ordinary programmer would never
even think of.  This doesn't mean a great programmer is infinitely
more valuable, because any invention has a finite market value.
But it's easy to imagine cases where a great programmer might invent
things worth 100x or even 1000x an average programmer's salary.[2]
There are a handful of consulting firms that rent out big
pools of foreign programmers they bring in on H1-B visas.  By all
means crack down on these.  It should be easy to write legislation
that distinguishes them, because they are so different from technology
companies.  But it is dishonest of the anti-immigration people to
claim that companies like Google and Facebook are driven by the
same motives.  An influx of inexpensive but mediocre programmers
is the last thing they'd want; it would destroy them.[3]
Though this essay talks about programmers, the group of people
we need to import is broader, ranging from designers to programmers
to electrical engineers.  The best one could do as a general term
might be ""digital talent."" It seemed better to make the argument a
little too narrow than to confuse everyone with a neologism.
Thanks to Sam Altman, John Collison, Patrick Collison, Jessica
Livingston, Geoff Ralston, Fred Wilson, and Qasar Younis for reading
drafts of this.Spanish Translation","technological and societal insights
",human,"human
","human
"
85,85,"August 2005Thirty years ago, one was supposed to work one's way up the corporate
ladder.  That's less the rule now.  Our generation wants to get
paid up front.  Instead of developing a product for some big company
in the expectation of getting job security in return, we develop
the product ourselves, in a startup, and sell it to the big company.
At the very least we want options.Among other things, this shift has created the appearance of a rapid
increase in economic inequality.  But really the two cases are not
as different as they look in economic statistics.Economic statistics are misleading because they ignore the value
of safe jobs.  An easy job from which one can't be fired is worth
money; exchanging the two is one of the commonest forms of
corruption.  A sinecure is, in effect, an annuity.  Except sinecures
don't appear in economic statistics.  If they did, it would be clear
that in practice socialist countries have nontrivial disparities
of wealth, because they usually have a class of powerful bureaucrats
who are paid mostly by seniority and can never be fired.While not a sinecure, a position on the corporate ladder was genuinely
valuable, because big companies tried not to fire people, and
promoted from within based largely on seniority.  A position on the
corporate ladder had a value analogous to the ""goodwill"" that is a
very real element in the valuation of companies.  It meant one could
expect future high paying jobs.One of main causes of the decay of the corporate ladder is the trend
for takeovers that began in the 1980s.  Why waste your time climbing
a ladder that might disappear before you reach the top?And, by no coincidence, the corporate ladder was one of the reasons
the early corporate raiders were so successful.  It's not only
economic statistics that ignore the value of safe jobs.  Corporate
balance sheets do too.  One reason it was profitable to carve up 1980s
companies and sell them for parts was that they hadn't formally
acknowledged their implicit debt to employees who had done good
work and expected to be rewarded with high-paying executive jobs
when their time came.In the movie Wall Street, Gordon Gekko
ridicules a company overloaded with vice presidents.  But the company
may not be as corrupt as it seems; those VPs' cushy jobs were
probably payment for work done earlier.I like the new model better.  For one thing, it seems a bad plan
to treat jobs as rewards.  Plenty of good engineers got made into
bad managers that way.  And the old system meant people had to deal
with a lot more corporate politics, in order to protect the work
they'd invested in a position on the ladder.The big disadvantage of the new system is that it involves more risk.  If you develop ideas in a startup instead
of within a big company, any number of random factors could sink
you before you can finish.  But maybe the older generation would
laugh at me for saying that the way we do things is riskier.  After
all, projects within big companies were always getting cancelled
as a result of arbitrary decisions from higher up.  My father's
entire industry (breeder reactors) disappeared that way.For better or worse, the idea of the corporate ladder is probably
gone for good.  The new model seems more liquid, and more efficient.
But it is less of a change, financially, than one might think.  Our
fathers weren't that stupid.Romanian TranslationJapanese Translation","technological and societal insights
",human,"human
","human
"
86,86,"April 2007A few days ago I suddenly realized Microsoft was dead.  I was talking
to a young startup founder about how Google was different from
Yahoo.  I said that Yahoo had been warped from the start by
their fear of Microsoft.  That was why they'd positioned themselves
as a ""media company"" instead of a technology company.  Then I looked
at his face and realized he didn't understand.  It was as if I'd
told him how much girls liked Barry Manilow in the mid
80s.  Barry who?Microsoft?  He didn't say anything, but I could tell he didn't quite
believe anyone would be frightened of them.Microsoft cast
a shadow over the software world for almost 20 years
starting in the late 80s.
I can remember when it was IBM before them.  I mostly ignored this
shadow.  I never used Microsoft software, so it only affected me
indirectly—for example, in the spam I got from botnets.  And
because I wasn't paying attention, I didn't notice when the shadow
disappeared.But it's gone now.  I can sense that.  No one is even afraid of
Microsoft anymore.  They still make a lot of money—so does IBM,
for that matter.  But they're not dangerous.When did Microsoft die, and of what?  I know they seemed dangerous
as late as 2001, because I wrote an essay then 
about how they were
less dangerous than they seemed.   I'd guess they were dead by 2005.
I know when we started Y Combinator we didn't worry about Microsoft
as competition for the startups we funded.  In fact, we've never
even invited them to the demo days we organize for startups to
present to investors.  We invite Yahoo and Google and some other
Internet companies, but we've never bothered to invite Microsoft.
Nor has anyone there ever even sent us an email.  They're in a
different world.What killed them?  Four things, I think, all of them occurring
simultaneously in the mid 2000s.The most obvious is Google.  There can only be one big man in town,
and they're clearly it.  Google is the most dangerous company
now by far, in both the good and bad senses of the word.  Microsoft
can at best limp along afterward.When did Google take the lead?  There will be a tendency to push
it back to their IPO in August 2004, but they weren't setting the
terms of the debate then.  I'd say they took the lead in
2005.  Gmail was one of the things that put them over the edge.
Gmail showed they could do more than search.Gmail also showed how much you could do with web-based software,
if you took advantage of what later came to be called ""Ajax."" And
that was the second cause of Microsoft's death: everyone can see the
desktop is over.  It now seems inevitable that applications will
live on the web—not just email, but everything, right up to
Photoshop.  Even Microsoft sees that now.Ironically, Microsoft unintentionally helped create Ajax.  The x
in Ajax is from the XMLHttpRequest object, which lets the browser
communicate with the server in the background while displaying a page.
(Originally the only way to communicate with the server was to 
ask for a new page.) XMLHttpRequest was created by Microsoft in the late 90s
because they needed it for Outlook.  What they didn't realize was
that it would be useful to a lot of other people too—in fact, to
anyone who wanted to make web apps work like desktop ones.The other critical component of Ajax is Javascript, the programming
language that runs in the browser.  Microsoft saw the danger of
Javascript and tried to keep it broken for as long as they could.
[1] 
But eventually the open source world won, by producing
Javascript libraries that grew over the brokenness of Explorer
the way a tree grows over barbed wire.The third cause of Microsoft's death was broadband Internet.  Anyone
who cares can have fast Internet access
now.  And the bigger the pipe to the server, the less you need the
desktop.The last nail in the coffin came, of all places, from Apple.  
Thanks to OS X, Apple has come back from the dead in a way
that is extremely rare in technology.
[2]
Their victory is so complete that I'm now surprised when I come across
a computer running Windows.  Nearly all the people we fund at Y
Combinator use Apple laptops.  It was the same in the audience at 
startup
school.  All the computer people use Macs or Linux now.  Windows is for
grandmas, like Macs used to be in the 90s.  So not only does the
desktop no longer matter, no one who cares about computers uses
Microsoft's anyway.And of course Apple has Microsoft on the run in music
too, with TV and phones on the way.I'm glad Microsoft is dead.  They were like Nero or 
Commodus—evil
in the way only inherited power can make you.  Because remember,
the Microsoft monopoly didn't begin with Microsoft.  They got it
from IBM.  The software business was overhung by a
monopoly from about the mid-1950s to about 2005.  For practically
its whole existence, that is.  One of the reasons ""Web 2.0"" has
such an air of euphoria about it is the feeling, conscious or not,
that this era of monopoly may finally be over.Of course, as a hacker I can't help thinking about how something
broken could be fixed.  Is there some way Microsoft could come back?
In principle, yes.  To see how, envision two things: (a) the amount
of cash Microsoft now has on hand, and (b) Larry and Sergey making
the rounds of all the search engines ten years ago trying to sell
the idea for Google for a million dollars, and being turned down
by everyone.The surprising fact is, brilliant hackers—dangerously brilliant
hackers—can be had very cheaply, by the standards of a
company as rich as Microsoft.  They can't 
hire smart people anymore,
but they could buy as many as they wanted for only an order of magnitude 
more. So if they wanted to be a contender
again, this is how they could do it:

 Buy all the good ""Web 2.0"" startups.  They could get substantially
    all of them for less than they'd have to pay for Facebook. Put them all in a building in Silicon Valley, surrounded by
    lead shielding to protect them from any contact with Redmond.

I feel safe suggesting this, because they'd never do it.  Microsoft's
biggest weakness is that they still don't realize how much they
suck.  They still think they can write software in house.  Maybe they
can, by the standards of the desktop world.  But that world ended
a few years ago.I already know what the reaction to this essay will be.  Half the
readers will say that Microsoft is still an enormously profitable
company, and that I should be more
careful about drawing conclusions based on what a few people think
in our insular little ""Web 2.0"" bubble.  The other half, the younger
half, will complain that this is old news.See also: Microsoft is Dead: the Cliffs NotesNotes[1]
It doesn't take a conscious effort to make software incompatible.
All you have to do is not work too hard at fixing bugs—which, if
you're a big company, you produce in copious quantities.  The
situation is analogous to the writing of ""literary
theorists.""  Most don't try to be obscure; they just don't make an
effort to be clear.  It wouldn't pay.[2]
In part because Steve Jobs got pushed out by John Sculley in
a way that's rare among technology companies.  If Apple's board
hadn't made that blunder, they wouldn't have had to bounce back.Portuguese TranslationSimplified Chinese TranslationKorean Translation","technological and societal insights
",human,"human
","human
"
87,87,"May 2007People who worry about the increasing gap between rich and poor
generally look back on the mid twentieth century as a golden age.
In those days we had a large number of high-paying union manufacturing
jobs that boosted the median income.  I wouldn't quite call the
high-paying union job a myth, but I think people who dwell on it
are reading too much into it.Oddly enough, it was working with startups that made me realize
where the high-paying union job came from.  In a rapidly growing
market, you don't worry too much about efficiency.  It's more
important to grow fast.  If there's some mundane problem getting
in your way, and there's a simple solution that's somewhat expensive,
just take it and get on with more important things.  EBay didn't
win by paying less for servers than their competitors.Difficult though it may be to imagine now, manufacturing was a
growth industry in the mid twentieth century.  This was an era when
small firms making everything from cars to candy were getting
consolidated into a new kind of corporation with national reach and
huge economies of scale.  You had to grow fast or die.  Workers
were for these companies what servers are for an Internet startup.
A reliable supply was more important than low cost.If you looked in the head of a 1950s auto executive, the attitude
must have been: sure, give 'em whatever they ask for, so long as
the new model isn't delayed.In other words, those workers were not paid what their work was
worth.  Circumstances being what they were, companies would have
been stupid to insist on paying them so little.If you want a less controversial example of this phenomenon, ask
anyone who worked as a consultant building web sites during the
Internet Bubble.  In the late nineties you could get paid huge sums
of money for building the most trivial things.  And yet does anyone
who was there have any expectation those days will ever return?  I
doubt it.  Surely everyone realizes that was just a temporary
aberration.The era of labor unions seems to have been the same kind of aberration, 
just spread
over a longer period, and mixed together with a lot of ideology
that prevents people from viewing it with as cold an eye as they
would something like consulting during the Bubble.Basically, unions were just Razorfish.People who think the labor movement was the creation of heroic union
organizers have a problem to explain: why are unions shrinking now?
The best they can do is fall back on the default explanation of
people living in fallen civilizations.  Our ancestors were giants.
The workers of the early twentieth century must have had a moral
courage that's lacking today.In fact there's a simpler explanation.  The early twentieth century
was just a fast-growing startup overpaying for infrastructure.  And
we in the present are not a fallen people, who have abandoned
whatever mysterious high-minded principles produced the high-paying
union job.  We simply live in a time when the fast-growing companies
overspend on different things.","technological and societal insights
",human,"human
","human
"
88,88,"

Want to start a startup?  Get funded by
Y Combinator.




July 2004(This essay is derived from a talk at Oscon 2004.)
A few months ago I finished a new 
book, 
and in reviews I keep
noticing words like ""provocative'' and ""controversial.'' To say
nothing of ""idiotic.''I didn't mean to make the book controversial.  I was trying to make
it efficient.  I didn't want to waste people's time telling them
things they already knew.  It's more efficient just to give them
the diffs.  But I suppose that's bound to yield an alarming book.EdisonsThere's no controversy about which idea is most controversial:
the suggestion that variation in wealth might not be as big a
problem as we think.I didn't say in the book that variation in wealth was in itself a
good thing.  I said in some situations it might be a sign of good
things.  A throbbing headache is not a good thing, but it can be
a sign of a good thing-- for example, that you're recovering
consciousness after being hit on the head.Variation in wealth can be a sign of variation in productivity.
(In a society of one, they're identical.) And that
is almost certainly a good thing: if your society has no variation
in productivity, it's probably not because everyone is Thomas
Edison.  It's probably because you have no Thomas Edisons.In a low-tech society you don't see much variation in productivity.
If you have a tribe of nomads collecting sticks for a fire, how
much more productive is the best stick gatherer going to be than
the worst?  A factor of two?  Whereas when you hand people a complex tool
like a computer, the variation in what they can do with
it is enormous.That's not a new idea.  Fred Brooks wrote about it in 1974, and
the study he quoted was published in 1968.  But I think he
underestimated the variation between programmers.  He wrote about productivity in lines
of code:  the best programmers can solve a given problem in a tenth
the time.  But what if the problem isn't given? In programming, as
in many fields, the hard part isn't solving problems, but deciding
what problems to solve.  Imagination is hard to measure, but
in practice it dominates the kind of productivity that's measured
in lines of code.Productivity varies in any field, but there are few in which it
varies so much.  The variation between programmers
is so great that it becomes a difference in kind.  I don't
think this is something intrinsic to programming, though.  In every field,
technology magnifies differences in productivity.  I think what's
happening in programming is just that we have a lot of technological
leverage.  But in every field the lever is getting longer, so the
variation we see is something that more and more fields will see
as time goes on.  And the success of companies, and countries, will
depend increasingly on how they deal with it.If variation in productivity increases with technology, then the
contribution of the most productive individuals will not only be
disproportionately large, but will actually grow with time.  When
you reach the point where 90% of a group's output is created by 1%
of its members, you lose big if something (whether Viking raids,
or central planning) drags their productivity down to the average.If we want to get the most out of them, we need to understand these
especially productive people.  What motivates them?  What do they
need to do their jobs?  How do you recognize them? How do you
get them to come and work for you?  And then of course there's the
question, how do you become one?More than MoneyI know a handful of super-hackers, so I sat down and thought about
what they have in common.  Their defining quality is probably that
they really love to program.  Ordinary programmers write code to pay
the bills.  Great hackers think of it as something they do for fun,
and which they're delighted to find people will pay them for.Great programmers are sometimes said to be indifferent to money.
This isn't quite true.  It is true that all they really care about
is doing interesting work.  But if you make enough money, you get
to work on whatever you want, and for that reason hackers are
attracted by the idea of making really large amounts of money.
But as long as they still have to show up for work every day, they
care more about what they do there than how much they get paid for
it.Economically, this is a fact of the greatest importance, because
it means you don't have to pay great hackers anything like what
they're worth.  A great programmer might be ten or a hundred times
as productive as an ordinary one, but he'll consider himself lucky
to get paid three times as much.  As I'll explain later, this is
partly because great hackers don't know how good they are.  But
it's also because money is not the main thing they want.What do hackers want?  Like all craftsmen, hackers like good tools.
In fact, that's an understatement.  Good hackers find it unbearable
to use bad tools.  They'll simply refuse to work on projects with
the wrong infrastructure.At a startup I once worked for, one of the things pinned up on our
bulletin board was an ad from IBM.  It was a picture of an AS400,
and the headline read, I think, ""hackers despise
it.'' [1]When you decide what infrastructure to use for a project, you're
not just making a technical decision.  You're also making a social
decision, and this may be the more important of the two.  For
example, if your company wants to write some software, it might
seem a prudent choice to write it in Java.  But when you choose a
language, you're also choosing a community.  The programmers you'll
be able to hire to work on a Java project won't be as
smart as the
ones you could get to work on a project written in Python.
And the quality of your hackers probably matters more than the
language you choose.  Though, frankly, the fact that good hackers
prefer Python to Java should tell you something about the relative
merits of those languages.Business types prefer the most popular languages because they view
languages as standards. They don't want to bet the company on
Betamax.  The thing about languages, though, is that they're not
just standards.  If you have to move bits over a network, by all
means use TCP/IP.  But a programming language isn't just a format.
A programming language is a medium of expression.I've read that Java has just overtaken Cobol as the most popular
language.  As a standard, you couldn't wish for more.  But as a
medium of expression, you could do a lot better.  Of all the great
programmers I can think of, I know of only one who would voluntarily
program in Java.  And of all the great programmers I can think of
who don't work for Sun, on Java, I know of zero.Great hackers also generally insist on using open source software.
Not just because it's better, but because it gives them more control.
Good hackers insist on control.  This is part of what makes them
good hackers:  when something's broken, they need to fix it.  You
want them to feel this way about the software they're writing for
you.  You shouldn't be surprised when they feel the same way about
the operating system.A couple years ago a venture capitalist friend told me about a new
startup he was involved with.  It sounded promising.  But the next
time I talked to him, he said they'd decided to build their software
on Windows NT, and had just hired a very experienced NT developer
to be their chief technical officer.  When I heard this, I thought,
these guys are doomed.  One, the CTO couldn't be a first rate
hacker, because to become an eminent NT developer he would have
had to use NT voluntarily, multiple times, and I couldn't imagine
a great hacker doing that; and two, even if he was good, he'd have
a hard time hiring anyone good to work for him if the project had
to be built on NT. [2]The Final FrontierAfter software, the most important tool to a hacker is probably
his office.  Big companies think the function of office space is to express
rank.  But hackers use their offices for more than that: they
use their office as a place to think in.  And if you're a technology
company, their thoughts are your product.  So making hackers work
in a noisy, distracting environment is like having a paint factory
where the air is full of soot.The cartoon strip Dilbert has a lot to say about cubicles, and with
good reason.  All the hackers I know despise them.  The mere prospect
of being interrupted is enough to prevent hackers from working on
hard problems.  If you want to get real work done in an office with
cubicles, you have two options: work at home, or come in early or
late or on a weekend, when no one else is there.  Don't companies
realize this is a sign that something is broken?  An office
environment is supposed to be something that helps
you work, not something you work despite.Companies like Cisco are proud that everyone there has a cubicle,
even the CEO.  But they're not so advanced as they think; obviously
they still view office space as a badge of rank.  Note too that
Cisco is famous for doing very little product development in house.
They get new technology by buying the startups that created it-- where
presumably the hackers did have somewhere quiet to work.One big company that understands what hackers need is Microsoft.
I once saw a recruiting ad for Microsoft with a big picture of a
door.  Work for us, the premise was, and we'll give you a place to
work where you can actually get work done.   And you know, Microsoft
is remarkable among big companies in that they are able to develop
software in house.  Not well, perhaps, but well enough.If companies want hackers to be productive, they should look at
what they do at home.  At home, hackers can arrange things themselves
so they can get the most done.  And when they work at home, hackers
don't work in noisy, open spaces; they work in rooms with doors.  They
work in cosy, neighborhoody places with people around and somewhere
to walk when they need to mull something over, instead of in glass
boxes set in acres of parking lots.  They have a sofa they can take
a nap on when they feel tired, instead of sitting in a coma at
their desk, pretending to work.  There's no crew of people with
vacuum cleaners that roars through every evening during the prime
hacking hours.  There are no meetings or, God forbid, corporate
retreats or team-building exercises.  And when you look at what
they're doing on that computer, you'll find it reinforces what I
said earlier about tools.  They may have to use Java and Windows
at work, but at home, where they can choose for themselves, you're
more likely to find them using Perl and Linux.Indeed, these statistics about Cobol or Java being the most popular
language can be misleading.  What we ought to look at, if we want
to know what tools are best, is what hackers choose when they can
choose freely-- that is, in projects of their own.  When you ask
that question, you find that open source operating systems already
have a dominant market share, and the number one language is probably
Perl.InterestingAlong with good tools, hackers want interesting projects.  What
makes a project interesting?  Well, obviously overtly sexy
applications like stealth planes or special effects software would
be interesting to work on.  But any application can be interesting
if it poses novel technical challenges.  So it's hard to predict
which problems hackers will like, because some become
interesting only when the people working on them discover a new
kind of solution.  Before ITA
(who wrote the software inside Orbitz),
the people working on airline fare searches probably thought it
was one of the most boring applications imaginable.  But ITA made
it interesting by 
redefining the problem in a more ambitious way.I think the same thing happened at Google.  When Google was founded,
the conventional wisdom among the so-called portals was that search
was boring and unimportant.  But the guys at Google didn't think
search was boring, and that's why they do it so well.This is an area where managers can make a difference.  Like a parent
saying to a child, I bet you can't clean up your whole room in
ten minutes, a good manager can sometimes redefine a problem as a
more interesting one.  Steve Jobs seems to be particularly good at
this, in part simply by having high standards.  There were a lot
of small, inexpensive computers before the Mac.  He redefined the
problem as: make one that's beautiful.  And that probably drove
the developers harder than any carrot or stick could.They certainly delivered.  When the Mac first appeared, you didn't
even have to turn it on to know it would be good; you could tell
from the case.  A few weeks ago I was walking along the street in
Cambridge, and in someone's trash I saw what appeared to be a Mac
carrying case.  I looked inside, and there was a Mac SE.  I carried
it home and plugged it in, and it booted.  The happy Macintosh
face, and then the finder.  My God, it was so simple.  It was just
like ... Google.Hackers like to work for people with high standards.  But it's not
enough just to be exacting.  You have to insist on the right things.
Which usually means that you have to be a hacker yourself.  I've
seen occasional articles about how to manage programmers.  Really
there should be two articles: one about what to do if
you are yourself a programmer, and one about what to do if you're not.  And the 
second could probably be condensed into two words:  give up.The problem is not so much the day to day management.  Really good
hackers are practically self-managing.  The problem is, if you're
not a hacker, you can't tell who the good hackers are.  A similar
problem explains why American cars are so ugly.  I call it the
design paradox.  You might think that you could make your products
beautiful just by hiring a great designer to design them.  But if
you yourself don't have good taste, 
how are you going to recognize
a good designer?  By definition you can't tell from his portfolio.
And you can't go by the awards he's won or the jobs he's had,
because in design, as in most fields, those tend to be driven by
fashion and schmoozing, with actual ability a distant third.
There's no way around it:  you can't manage a process intended to
produce beautiful things without knowing what beautiful is.  American
cars are ugly because American car companies are run by people with
bad taste.Many people in this country think of taste as something elusive,
or even frivolous.  It is neither.  To drive design, a manager must
be the most demanding user of a company's products.  And if you
have really good taste, you can, as Steve Jobs does, make satisfying
you the kind of problem that good people like to work on.Nasty Little ProblemsIt's pretty easy to say what kinds of problems are not interesting:
those where instead of solving a few big, clear, problems, you have
to solve a lot of nasty little ones.  One of the worst kinds of
projects is writing an interface to a piece of software that's
full of bugs.  Another is when you have to customize
something for an individual client's complex and ill-defined needs.
To hackers these kinds of projects are the death of a thousand
cuts.The distinguishing feature of nasty little problems is that you
don't learn anything from them.   Writing a compiler is interesting
because it teaches you what a compiler is.  But writing an interface
to a buggy piece of software doesn't teach you anything, because the
bugs are random.  [3] So it's not just fastidiousness that makes good
hackers avoid nasty little problems.  It's more a question of
self-preservation.  Working on nasty little problems makes you
stupid.  Good hackers avoid it for the same reason models avoid
cheeseburgers.Of course some problems inherently have this character.  And because
of supply and demand, they pay especially well.  So a company that
found a way to get great hackers to work on tedious problems would
be very successful.  How would you do it?One place this happens is in startups.  At our startup we had 
Robert Morris working as a system administrator.  That's like having the
Rolling Stones play at a bar mitzvah.  You can't hire that kind of
talent.  But people will do any amount of drudgery for companies
of which they're the founders.  [4]Bigger companies solve the problem by partitioning the company.
They get smart people to work for them by establishing a separate
R&D department where employees don't have to work directly on
customers' nasty little problems. [5] In this model, the research
department functions like a mine. They produce new ideas; maybe
the rest of the company will be able to use them.You may not have to go to this extreme.  
Bottom-up programming
suggests another way to partition the company: have the smart people
work as toolmakers.  If your company makes software to do x, have
one group that builds tools for writing software of that type, and
another that uses these tools to write the applications.  This way
you might be able to get smart people to write 99% of your code,
but still keep them almost as insulated from users as they would
be in a traditional research department.  The toolmakers would have
users, but they'd only be the company's own developers.  [6]If Microsoft used this approach, their software wouldn't be so full
of security holes, because the less smart people writing the actual
applications wouldn't be doing low-level stuff like allocating
memory.  Instead of writing Word directly in C, they'd be plugging
together big Lego blocks of Word-language.  (Duplo, I believe, is
the technical term.)ClumpingAlong with interesting problems, what good hackers like is other
good hackers.  Great hackers tend to clump together-- sometimes
spectacularly so, as at Xerox Parc.   So you won't attract good
hackers in linear proportion to how good an environment you create
for them.  The tendency to clump means it's more like the square
of the environment.  So it's winner take all.  At any given time,
there are only about ten or twenty places where hackers most want to
work, and if you aren't one of them, you won't just have fewer
great hackers, you'll have zero.Having great hackers is not, by itself, enough to make a company
successful.  It works well for Google and ITA, which are two of
the hot spots right now, but it didn't help Thinking Machines or
Xerox.  Sun had a good run for a while, but their business model
is a down elevator.  In that situation, even the best hackers can't
save you.I think, though, that all other things being equal, a company that
can attract great hackers will have a huge advantage.  There are
people who would disagree with this.  When we were making the rounds
of venture capital firms in the 1990s, several told us that software
companies didn't win by writing great software, but through brand,
and dominating channels, and doing the right deals.They really seemed to believe this, and I think I know why.  I
think what a lot of VCs are looking for, at least unconsciously,
is the next Microsoft.  And of course if Microsoft is your model,
you shouldn't be looking for companies that hope to win by writing
great software.  But VCs are mistaken to look for the next Microsoft,
because no startup can be the next Microsoft unless some other
company is prepared to bend over at just the right moment and be
the next IBM.It's a mistake to use Microsoft as a model, because their whole
culture derives from that one lucky break.  Microsoft is a bad data
point.  If you throw them out, you find that good products do tend
to win in the market.  What VCs should be looking for is the next
Apple, or the next Google.I think Bill Gates knows this.  What worries him about Google is
not the power of their brand, but the fact that they have
better hackers. [7]
RecognitionSo who are the great hackers?  How do you know when you meet one?
That turns out to be very hard.  Even hackers can't tell.  I'm
pretty sure now that my friend Trevor Blackwell is a great hacker.
You may have read on Slashdot how he made his 
own Segway.  The
remarkable thing about this project was that he wrote all the
software in one day (in Python, incidentally).For Trevor, that's
par for the course.  But when I first met him, I thought he was a
complete idiot.  He was standing in Robert Morris's office babbling
at him about something or other, and I remember standing behind
him making frantic gestures at Robert to shoo this nut out of his
office so we could go to lunch.  Robert says he misjudged Trevor
at first too.  Apparently when Robert first met him, Trevor had
just begun a new scheme that involved writing down everything about
every aspect of his life on a stack of index cards, which he carried
with him everywhere.  He'd also just arrived from Canada, and had
a strong Canadian accent and a mullet.The problem is compounded by the fact that hackers, despite their
reputation for social obliviousness, sometimes put a good deal of
effort into seeming smart.  When I was in grad school I used to
hang around the MIT AI Lab occasionally. It was kind of intimidating
at first.  Everyone there spoke so fast.  But after a while I
learned the trick of speaking fast.  You don't have to think any
faster; just use twice as many words to say everything.  With this amount of noise in the signal, it's hard to tell good
hackers when you meet them.  I can't tell, even now.  You also
can't tell from their resumes.  It seems like the only way to judge
a hacker is to work with him on something.And this is the reason that high-tech areas 
only happen around universities.  The active ingredient
here is not so much the professors as the students.  Startups grow up
around universities because universities bring together promising young
people and make them work on the same projects.  The
smart ones learn who the other smart ones are, and together
they cook up new projects of their own.Because you can't tell a great hacker except by working with him,
hackers themselves can't tell how good they are.  This is true to
a degree in most fields.  I've found that people who
are great at something are not so much convinced of their own
greatness as mystified at why everyone else seems so incompetent.
But it's particularly hard for hackers to know how good they are,
because it's hard to compare their work.  This is easier in most
other fields.  In the hundred meters, you know in 10 seconds who's
fastest.  Even in math there seems to be a general consensus about
which problems are hard to solve, and what constitutes a good
solution.  But hacking is like writing.  Who can say which of two
novels is better?  Certainly not the authors.With hackers, at least, other hackers can tell.  That's because,
unlike novelists, hackers collaborate on projects.  When you get
to hit a few difficult problems over the net at someone, you learn
pretty quickly how hard they hit them back.  But hackers can't
watch themselves at work.  So if you ask a great hacker how good
he is, he's almost certain to reply, I don't know.  He's not just
being modest.  He really doesn't know.And none of us know, except about people we've actually worked
with.  Which puts us in a weird situation: we don't know who our
heroes should be.  The hackers who become famous tend to become
famous by random accidents of PR.  Occasionally I need to give an
example of a great hacker, and I never know who to use.  The first
names that come to mind always tend to be people I know personally,
but it seems lame to use them.  So, I think, maybe I should say
Richard Stallman, or Linus Torvalds, or Alan Kay, or someone famous
like that.  But I have no idea if these guys are great hackers.
I've never worked with them on anything.If there is a Michael Jordan of hacking, no one knows, including
him.CultivationFinally, the question the hackers have all been wondering about:
how do you become a great hacker?  I don't know if it's possible
to make yourself into one.  But it's certainly possible to do things
that make you stupid, and if you can make yourself stupid, you
can probably make yourself smart too.The key to being a good hacker may be to work on what you like.
When I think about the great hackers I know, one thing they have
in common is the extreme 
difficulty of making them work 
on anything they
don't want to.  I don't know if this is cause or effect; it may be
both.To do something well you have to love it.  
So to the extent you
can preserve hacking as something you love, you're likely to do it
well.  Try to keep the sense of wonder you had about programming at
age 14.  If you're worried that your current job is rotting your
brain, it probably is.The best hackers tend to be smart, of course, but that's true in
a lot of fields.  Is there some quality that's unique to hackers?
I asked some friends, and the number one thing they mentioned was
curiosity.  
I'd always supposed that all smart people were curious--
that curiosity was simply the first derivative of knowledge.  But
apparently hackers are particularly curious, especially about how
things work.  That makes sense, because programs are in effect
giant descriptions of how things work.Several friends mentioned hackers' ability to concentrate-- their
ability, as one put it, to ""tune out everything outside their own
heads.''  I've certainly noticed this.  And I've heard several 
hackers say that after drinking even half a beer they can't program at
all.   So maybe hacking does require some special ability to focus.
Perhaps great hackers can load a large amount of context into their
head, so that when they look at a line of code, they see not just
that line but the whole program around it.  John McPhee
wrote that Bill Bradley's success as a basketball player was due
partly to his extraordinary peripheral vision.  ""Perfect'' eyesight
means about 47 degrees of vertical peripheral vision.  Bill Bradley
had 70; he could see the basket when he was looking at the floor.
Maybe great hackers have some similar inborn ability.  (I cheat by
using a very dense language, 
which shrinks the court.)This could explain the disconnect over cubicles.  Maybe the people
in charge of facilities, not having any concentration to shatter,
have no idea that working in a cubicle feels to a hacker like having
one's brain in a blender.  (Whereas Bill, if the rumors of autism
are true, knows all too well.)One difference I've noticed between great hackers and smart people
in general is that hackers are more 
politically incorrect.  To the
extent there is a secret handshake among good hackers, it's when they
know one another well enough to express opinions that would get
them stoned to death by the general public.  And I can see why
political incorrectness would be a useful quality in programming.
Programs are very complex and, at least in the hands of good
programmers, very fluid.  In such situations it's helpful to have
a habit of questioning assumptions.Can you cultivate these qualities?  I don't know.  But you can at
least not repress them.  So here is my best shot at a recipe.  If
it is possible to make yourself into a great hacker, the way to do
it may be to make the following deal with yourself: you never have
to work on boring projects (unless your family will starve otherwise),
and in return, you'll never allow yourself to do a half-assed job.
All the great hackers I know seem to have made that deal, though
perhaps none of them had any choice in the matter.Notes
[1] In fairness, I have to say that IBM makes decent hardware.  I
wrote this on an IBM laptop.[2] They did turn out to be doomed.  They shut down a few months
later.[3] I think this is what people mean when they talk
about the ""meaning of life.""  On the face of it, this seems an 
odd idea.  Life isn't an expression; how could it have meaning?
But it can have a quality that feels a lot like meaning.  In a project
like a compiler, you have to solve a lot of problems, but the problems
all fall into a pattern, as in a signal.  Whereas when the problems
you have to solve are random, they seem like noise.
[4] Einstein at one point worked designing refrigerators. (He had equity.)[5] It's hard to say exactly what constitutes research in the
computer world, but as a first approximation, it's software that
doesn't have users.I don't think it's publication that makes the best hackers want to work
in research departments.  I think it's mainly not having to have a
three hour meeting with a product manager about problems integrating
the Korean version of Word 13.27 with the talking paperclip.[6] Something similar has been happening for a long time in the
construction industry. When you had a house built a couple hundred
years ago, the local builders built everything in it.  But increasingly
what builders do is assemble components designed and manufactured
by someone else.  This has, like the arrival of desktop publishing,
given people the freedom to experiment in disastrous ways, but it
is certainly more efficient.[7] Google is much more dangerous to Microsoft than Netscape was.
Probably more dangerous than any other company has ever been.  Not
least because they're determined to fight.  On their job listing
page, they say that one of their ""core values'' is ""Don't be evil.''
From a company selling soybean oil or mining equipment, such a
statement would merely be eccentric.  But I think all of us in the
computer world recognize who that is a declaration of war on.Thanks to Jessica Livingston, Robert Morris, and Sarah Harlin
for reading earlier versions of this talk.Audio of talkThe Python ParadoxJapanese TranslationRussian TranslationItalian TranslationSpanish Translation



If you liked this, you may also like
Hackers & Painters.

","technological and societal insights
",human,"human
","human
"
89,89,"August 2003
We may be able to improve the accuracy of Bayesian spam filters
by having them follow links to see what's
waiting at the other end.  Richard Jowsey of
death2spam now does
this in borderline cases, and reports that it works well.Why only do it in borderline cases?  And why only do it once?As I mentioned in Will Filters Kill Spam?,
following all the urls in
a spam would have an amusing side-effect.  If popular email clients
did this in order to filter spam, the spammer's servers
would take a serious pounding.  The more I think about this,
the better an idea it seems.  This isn't just amusing; it
would be hard to imagine a more perfectly targeted counterattack
on spammers.So I'd like to suggest an additional feature to those
working on spam filters: a ""punish"" mode which,
if turned on, would spider every url
in a suspected spam n times, where n could be set by the user. [1]As many people have noted, one of the problems with the
current email system is that it's too passive.  It does
whatever you tell it.  So far all the suggestions for fixing
the problem seem to involve new protocols.  This one  
wouldn't.If widely used, auto-retrieving spam filters would make
the email system rebound.  The huge volume of the
spam, which has so far worked in the spammer's favor,
would now work against him, like a branch snapping back in   
his face.   Auto-retrieving spam filters would drive the
spammer's 
costs up, 
and his sales down:  his bandwidth usage
would go through the roof, and his servers would grind to a
halt under the load, which would make them unavailable
to the people who would have responded to the spam.Pump out a million emails an hour, get a
million hits an hour on your servers.
We would want to ensure that this is only done to
suspected spams.  As a rule, any url sent to millions of
people is likely to be a spam url, so submitting every http
request in every email would work fine nearly all the time.
But there are a few cases where this isn't true: the urls
at the bottom of mails sent from free email services like
Yahoo Mail and Hotmail, for example.To protect such sites, and to prevent abuse, auto-retrieval
should be combined with blacklists of spamvertised sites.
Only sites on a blacklist would get crawled, and
sites would be blacklisted
only after being inspected by humans. The lifetime of a spam
must be several hours at least, so
it should be easy to update such a list in time to
interfere with a spam promoting a new site. [2]High-volume auto-retrieval would only be practical for users
on high-bandwidth
connections, but there are enough of those to cause spammers
serious trouble.   Indeed, this solution neatly
mirrors the problem.  The problem with spam is that in
order to reach a few gullible people the spammer sends 
mail to everyone.  The non-gullible recipients
are merely collateral damage.  But the non-gullible majority
won't stop getting spam until they can stop (or threaten to
stop) the gullible
from responding to it.  Auto-retrieving spam filters offer
them a way to do this.Would that kill spam?  Not quite.  The biggest spammers
could probably protect their servers against auto-retrieving 
filters.  However, the easiest and cheapest way for them
to do it would be to include working unsubscribe links in   
their mails.  And this would be a necessity for smaller fry,
and for ""legitimate"" sites that hired spammers to promote
them.  So if auto-retrieving filters became widespread,
they'd become auto-unsubscribing filters.In this scenario, spam would, like OS crashes, viruses, and
popups, become one of those plagues that only afflict people
who don't bother to use the right software.
Notes[1] Auto-retrieving filters will have to follow redirects,
and should in some cases (e.g. a page that just says
""click here"") follow more than one level of links.
Make sure too that
the http requests are indistinguishable from those of
popular Web browsers, including the order and referrer.If the response
doesn't come back within x amount of time, default to
some fairly high spam probability.Instead of making n constant, it might be a good idea to
make it a function of the number of spams that have been
seen mentioning the site.  This would add a further level of
protection against abuse and accidents.[2] The original version of this article used the term
""whitelist"" instead of ""blacklist"".  Though they were
to work like blacklists, I preferred to call them whitelists
because it might make them less vulnerable to legal attack.
This just seems to have confused readers, though.There should probably be multiple blacklists.  A single point
of failure would be vulnerable both to attack and abuse.
Thanks to Brian Burton, Bill Yerazunis, Dan Giffin,
Eric Raymond, and Richard Jowsey for reading drafts of this.FFB FAQJapanese TranslationA Perl FFBLycos DDoS@Home","technological and societal insights
",human,"human
","human
"
90,90,"September 2009Publishers of all types, from news to music, are unhappy that
consumers won't pay for content anymore.  At least, that's how they
see it.In fact consumers never really were paying for content, and publishers
weren't really selling it either.  If the content was what they
were selling, why has the price of books or music or movies always
depended mostly on the format?  Why didn't better content cost more?
[1]A copy of Time costs $5 for 58 pages, or 8.6 cents a page.  
The Economist costs $7 for 86 pages, or 8.1 cents a page.  Better
journalism is actually slightly cheaper.Almost every form of publishing has been organized as if the medium
was what they were selling, and the content was irrelevant.  Book
publishers, for example, set prices based on the cost of producing
and distributing books.  They treat the words printed in the book
the same way a textile manufacturer treats the patterns printed on
its fabrics.Economically, the print media are in the business of marking up
paper.  We can all imagine an old-style editor getting a scoop and
saying ""this will sell a lot of papers!"" Cross out that final S and
you're describing their business model.  The reason they make less
money now is that people don't need as much paper.A few months ago I ran into a friend in a cafe.  I had a copy of
the New York Times, which I still occasionally buy on weekends.  As
I was leaving I offered it to him, as I've done countless times
before in the same situation.  But this time something new happened.
I felt that sheepish feeling you get when you offer someone something
worthless.  ""Do you, er, want a printout of yesterday's news?"" I
asked.  (He didn't.)Now that the medium is evaporating, publishers have nothing left
to sell.  Some seem to think they're going to sell content—that
they were always in the content business, really.  But they weren't,
and it's unclear whether anyone could be.SellingThere have always been people in the business of selling information,
but that has historically been a distinct business from publishing.
And the business of selling information to consumers has always
been a marginal one.  When I was a kid there were people who used
to sell newsletters containing stock tips, printed on colored paper
that made them hard for the copiers of the day to reproduce.  That
is a different world, both culturally and economically, from the
one publishers currently inhabit.People will pay for information they think they can make money from.
That's why they paid for those stock tip newsletters, and why
companies pay now for Bloomberg terminals and Economist Intelligence
Unit reports.  But will people pay for information otherwise?
History offers little encouragement.If audiences were willing to pay more for better content, why wasn't
anyone already selling it to them?  There was no reason you couldn't
have done that in the era of physical media.  So were the print
media and the music labels simply overlooking this opportunity?  Or
is it, rather, nonexistent?What about iTunes?  Doesn't that show people will pay for content?
Well, not really. iTunes is more of a tollbooth than a store.  Apple
controls the default path onto the iPod.  They offer a convenient
list of songs, and whenever you choose one they ding your credit
card for a small amount, just below the threshold of attention.
Basically, iTunes makes money by taxing people, not selling them
stuff.  You can only do that if you own the channel, and even then
you don't make much from it, because a toll has to be ignorable to
work.  Once a toll becomes painful, people start to find ways around
it, and that's pretty easy with digital content.The situation is much the same with digital books.  Whoever controls
the device sets the terms.  It's in their interest for content to
be as cheap as possible, and since they own the channel, there's a
lot they can do to drive prices down.  Prices will fall even further
once writers realize they don't need publishers.  Getting a book
printed and distributed is a daunting prospect for a writer, but
most can upload a file.Is software a counterexample?  People pay a lot for desktop software,
and that's just information.  True, but I don't think publishers
can learn much from software.  Software companies can charge a lot
because (a) many of the customers are businesses, who get in 
trouble
if they use pirated versions, and (b) though in form merely
information, software is treated by both maker and purchaser as a
different type of thing from a song or an article.   A Photoshop
user needs Photoshop in a way that no one needs a particular song
or article.That's why there's a separate word, ""content,"" for information
that's not software.  Software is a different business.  Software
and content blur together in some of the most lightweight software,
like casual games.  But those are usually free.   To make money the
way software companies do, publishers would have to become software
companies, and being publishers gives them no particular head start
in that domain. 
[2]The most promising countertrend is the premium cable channel.  People
still pay for those.  But broadcasting isn't publishing: you're not
selling a copy of something.  That's one reason the movie business
hasn't seen their revenues decline the way the news and music
businesses have.  They only have one foot in publishing.To the extent the movie business can avoid becoming publishers,
they may avoid publishing's problems.  But there are limits to how
well they'll be able to do that.  Once publishing—giving people
copies—becomes the most natural way of distributing your content,
it probably doesn't work to stick to old forms of distribution just
because you make more that way.  If free copies of your content are
available online, then you're competing with publishing's form of
distribution, and that's just as bad as being a publisher.Apparently some people in the music business hope to retroactively
convert it away from publishing, by getting listeners to pay for
subscriptions.  It seems unlikely that will work if they're just
streaming the same files you can get as mp3s.NextWhat happens to publishing if you can't sell content?  You have two
choices: give it away and make money from it indirectly, or find
ways to embody it in things people will pay for.The first is probably the future of most current media.  
Give music
away and make money from concerts and t-shirts.  Publish articles
for free and make money from one of a dozen permutations of
advertising.  Both publishers and investors are down on advertising
at the moment, but it has more potential than they realize.I'm not claiming that potential will be realized by the existing
players.  The optimal
ways to make money from the written word
probably require different words written by different people.It's harder to say what will happen to movies.  They could evolve
into ads.  Or they could return to their roots and make going to
the theater a treat.  If they made the experience good enough,
audiences might start to prefer it to watching pirated movies at
home. 
[3]
Or maybe the movie business will dry up, and the people
working in it will go to work for game developers.I don't know how big embodying information in physical form will
be.  It may be surprisingly large; people overvalue 
physical stuff.
There should remain some market for printed books, at least.I can see the evolution of book publishing in the books on my
shelves.  Clearly at some point in the 1960s the big publishing
houses started to ask: how cheaply can we make books before people
refuse to buy them?  The answer turned out to be one step short of
phonebooks.  As long as it isn't floppy, consumers still perceive
it as a book.That worked as long as buying printed books was the only way to
read them.  If printed books are optional, publishers will have to
work harder to entice people to buy them.  There should be some
market, but it's hard to foresee how big, because its size will
depend not on macro trends like the amount people read, but on the
ingenuity of individual publishers. 
[4]Some magazines may thrive by focusing on the magazine as a physical
object.  Fashion magazines could be made lush in a way that would
be hard to match digitally, at least for a while.  But this is
probably not an option for most magazines.I don't know exactly what the future will look like, but I'm not
too worried about it.  This sort of change tends to create as many
good things as it kills.  Indeed, the really interesting question is not
what will happen to existing forms, but what new forms will appear.The reason I've been writing about existing forms is that I don't
know what new forms will appear.  But though I can't predict
specific winners, I can offer a recipe for recognizing them.  When
you see something that's taking advantage of new technology to give
people something they want that they couldn't have before, you're
probably looking at a winner.  And when you see something that's
merely reacting to new technology in an attempt to preserve some
existing source of revenue, you're probably looking at a loser.
Notes[1]
I don't like the word ""content"" and tried for a while to avoid
using it, but I have to admit there's no other word that means the
right thing.  ""Information"" is too general.Ironically, the main reason I don't like ""content"" is the thesis
of this essay.  The word suggests an undifferentiated slurry, but
economically that's how both publishers and audiences treat it.
Content is information you don't need.[2]
Some types of publishers would be at a disadvantage trying
to enter the software business.  Record labels, for example, would
probably find it more natural to expand into casinos than software,
because the kind of people who run them would be more at home at
the mafia end of the business spectrum than the don't-be-evil end.[3]
I never watch movies in theaters anymore.  The tipping point
for me was the ads they show first.[4]
Unfortunately, making physically nice books will only be a
niche within a niche.  Publishers are more likely to resort to
expedients like selling autographed copies, or editions with the
buyer's picture on the cover.Thanks to Michael Arrington, Trevor Blackwell, Steven Levy, Robert
Morris, and Geoff Ralston for reading drafts of this.","technological and societal insights
",human,"human
","human
"
91,91,"September 2004(This essay is derived from an invited talk at ICFP 2004.)I had a front row seat for the Internet Bubble,
because I worked at Yahoo during 1998 and 1999.  One day,
when the stock was trading around $200, I sat down and calculated
what I thought the price should be. The 
answer I got was $12.  I went to
the next cubicle and told my friend Trevor.  ""Twelve!"" he said.
He tried to sound indignant, but he didn't quite manage it.  He
knew as well as I did that our valuation was crazy.Yahoo was a special case.  It was not just our price to earnings
ratio that was bogus.  Half our earnings were too.  Not in
the Enron way, of course.  The finance guys seemed
scrupulous about reporting earnings.  What made our
earnings bogus was that Yahoo was, in effect, the center of
a Ponzi scheme.  Investors looked at Yahoo's earnings
and said to themselves, here is proof that Internet companies can make
money.  So they invested in new
startups that promised to be the next Yahoo.  And as soon as these startups
got the money, what did they do with it?
Buy millions of dollars worth of advertising on Yahoo to promote
their brand.  Result: a capital investment in a startup this
quarter shows up as Yahoo earnings next quarter—stimulating
another round of investments in startups.As in a Ponzi scheme, what seemed to be the returns of this system
were simply the latest round of investments in it.
What made it not a Ponzi scheme was that it was unintentional.  
At least, I think it was.  The venture capital business is pretty incestuous,
and there were presumably people in a position, if not to create
this situation, to realize what was happening and to milk it.A year later the game was up.  Starting in January 2000, Yahoo's
stock price began to crash, ultimately losing 95% of its
value.Notice, though, that even with all the fat trimmed off its market
cap, Yahoo was still worth a lot.  Even at the morning-after
valuations of March and April 2001, the people at Yahoo had managed
to create a company worth about $8 billion in just six years.The fact is, despite all the nonsense we heard
during the Bubble about the ""new economy,"" there was a
core of truth.  You need
that to get a really big bubble: you need to have something
solid at the center, so that even smart people are sucked in.
(Isaac Newton and Jonathan Swift both lost money
in the South Sea Bubble of 1720.)Now the pendulum has swung the other way.  Now anything that
became fashionable during the Bubble is ipso facto unfashionable.
But that's a mistake—an even bigger mistake than believing
what everyone was saying in 1999.  Over the long term,
what the Bubble got right will be more important than what
it got wrong.1. Retail VCAfter the excesses of the Bubble, it's now
considered dubious to take companies public before they have earnings.
But there is nothing intrinsically wrong with
that idea.  Taking a company public at an early stage is simply
retail VC: instead of going to venture capital firms for the last round of
funding, you go to the public markets.By the end of the Bubble, companies going public with no
earnings were being derided as ""concept stocks,"" as if it
were inherently stupid to invest in them.
But investing in concepts isn't stupid; it's what VCs do,
and the best of them are far from stupid.The stock of a company that doesn't yet have earnings is  
worth something.
It may take a while for the market to learn
how to value such companies, just as it had to learn to
value common stocks in the early 20th century.   But markets
are good at solving that kind of problem.  I wouldn't be
surprised if the market ultimately did a better
job than VCs do now.Going public early will not be the right plan
for every company.
And it can of course be
disruptive—by distracting the management, or by making the early
employees suddenly rich.  But just as the market will learn
how to value startups, startups will learn how to minimize
the damage of going public.2. The InternetThe Internet genuinely is a big deal.  That was one reason
even smart people were fooled by the Bubble.  Obviously 
it was going to have a huge effect.  Enough of an effect to
triple the value of Nasdaq companies in two years?  No, as it
turned out.  But it was hard to say for certain at the time. [1]The same thing happened during the Mississippi and South Sea Bubbles.
What drove them was the invention of organized public finance
(the South Sea Company, despite its name, was really a competitor
of the Bank of England).  And that did turn out to be
a big deal, in the long run.Recognizing an important trend turns out to be easier than 
figuring out how to profit from it.  The mistake
investors always seem to make is to take the trend too literally.
Since the Internet was the big new thing, investors supposed
that the more Internettish the company, the better.  Hence
such parodies as Pets.Com.In fact most of the money to be made from big trends is made
indirectly.  It was not the railroads themselves that 
made the most money during the railroad boom, but the companies
on either side, like Carnegie's steelworks, which made the rails,
and Standard Oil, which used railroads to get oil to the East Coast,
where it could be shipped to Europe.I think the Internet will have great effects,
and that what we've seen so far is nothing compared to what's
coming.  But most of the winners will only indirectly be
Internet companies; for every Google there will be ten
JetBlues.3. ChoicesWhy will the Internet have great effects?  The general   
argument is that new forms of communication always do.  They happen
rarely (till industrial times there were just speech, writing, and printing),
but when they do, they always cause a big splash.The specific argument, or one of them, is the Internet gives us  
more choices.  In the ""old"" economy,
the high cost of presenting information to people meant they
had only a narrow range of options to choose from.  The tiny,
expensive pipeline to consumers was tellingly named ""the channel.""
Control the channel and you
could feed them what you wanted, on your terms.  And it
was not just big corporations that depended
on this principle.  So, in their way, did
labor unions, the traditional news media,
and the art and literary establishments.
Winning depended not on doing good work, but on gaining control
of some bottleneck.There are signs that this is changing.
Google has over 82 million unique users a month and
annual revenues of about three billion dollars. [2]
And yet have you ever seen
a Google ad?
Something is going on here.Admittedly, Google is an extreme case.  It's very easy for
people to switch to a new search engine.  It costs little
effort and no money to try a new one, and it's easy to
see if the results are better.  And so Google doesn't have
to advertise.  In a business like theirs, being the best is
enough.The exciting thing about the Internet is that it's
shifting everything in that direction.
The hard part, if you want to win by making the best stuff,
is the beginning.  Eventually everyone
will learn by word of mouth that you're the best,
but how do you survive to that point?  And it is in this crucial
stage that the Internet has the most effect.  First, the
Internet lets anyone find you at almost zero cost.
Second, it dramatically speeds up the rate at which
reputation spreads by word of mouth.  Together these mean that in many
fields the rule will be: Build it, and they will come.
Make something great and put it online.
That is a big change from the recipe for winning in the
past century.4. YouthThe aspect of the Internet Bubble that the press seemed most
taken with was the youth of some of the startup founders.
This too is a trend that will last.
There is a huge standard deviation among 26 year olds.  Some
are fit only for entry level jobs, but others are
ready to rule the world if they can find someone to handle
the paperwork for them.A 26 year old may not be very good at managing people or
dealing with the SEC.  Those require experience.
But those are also commodities, which can be handed off to
some lieutenant.  The most important quality in a CEO is his
vision for the company's future.  What will they build next?
And in that department, there are 26 year olds who can
compete with anyone.In 1970 a company president meant someone in his fifties, at
least.   If he had technologists working for him, they were 
treated like a racing stable: prized, but not powerful.  But 
as technology has grown more important, the power of nerds
has grown to reflect it.  Now it's not enough for a CEO to
have someone smart he can ask about technical matters.   Increasingly,
he has to be that person himself.As always, business has clung to old forms.  VCs still seem
to want to install a legitimate-looking 
talking head as the CEO.  But increasingly the founders of
the company are the real powers, and the grey-headed man
installed by the VCs more like a
music group's manager than a general.5. InformalityIn New York, the Bubble had dramatic consequences:
suits went out of fashion.  They made one seem old.  So in
1998 powerful New York types were suddenly wearing
open-necked shirts and khakis and oval wire-rimmed glasses,
just like guys in Santa Clara.The pendulum has swung back a bit, driven in part by a panicked
reaction by the clothing industry.  But I'm betting on the
open-necked shirts.  And this is not as frivolous a question
as it might seem.  Clothes are important, as all nerds can sense,
though they may not realize it consciously.If you're a nerd, you can understand how important clothes are
by asking yourself how you'd feel about a company
that made you wear a suit and tie to work.  The idea sounds
horrible, doesn't it?  In fact, horrible far out of proportion
to the mere discomfort of wearing such clothes.  A company that
made programmers wear suits would have something deeply wrong
with it.And what would be wrong would be that how one presented oneself
counted more than the quality of one's ideas.  That's
the problem with formality.  Dressing up is not so much bad in
itself.  The problem is the receptor it binds to: dressing
up is inevitably a substitute
for good ideas.   It is no coincidence that technically
inept business types are known as ""suits.""Nerds don't just happen to dress informally.  They do it too
consistently.  Consciously or not, they dress informally as
a prophylactic measure against stupidity.6. NerdsClothing is only the most visible battleground in the war
against formality.  Nerds tend to eschew formality of any sort.
They're not impressed by one's job title, for example,
or any of the other appurtenances of authority.Indeed, that's practically the definition of a nerd.  I found
myself talking recently to someone from Hollywood who was planning
a show about nerds.  I thought it would be useful if I
explained what a nerd was.  What I came up with was: someone who
doesn't expend any effort on marketing himself.A nerd, in other words, is someone who concentrates on substance.
So what's the connection between nerds and technology? Roughly
that you can't fool mother nature. In technical matters, you
have to get the right answers.  If your software miscalculates
the path of a space probe, you can't finesse your way out of
trouble by saying that your code is patriotic, or avant-garde,
or any of the other dodges people use in nontechnical
fields.And as technology becomes increasingly important in the
economy, nerd culture is 
rising with it.  Nerds are already
a lot cooler than they were when I was a kid.  When I was in
college in the mid-1980s, ""nerd"" was still an insult.  People
who majored in computer science generally tried to conceal it.
Now women ask me where they can meet nerds.  (The answer that
springs to mind is ""Usenix,"" but that would be like drinking
from a firehose.)I have no illusions about why nerd culture is becoming
more accepted.  It's not because people are
realizing that substance is more important than marketing.
It's because the nerds are getting 
rich.  But that is not going
to change.7. OptionsWhat makes the nerds rich, usually, is stock options.  Now there
are moves afoot to make it harder for companies to grant   
options.  To the extent there's some genuine accounting abuse 
going on, by all means correct it.  But don't kill the golden  
goose.  Equity is the fuel that drives technical innovation.Options are a good idea because (a) they're fair, and (b) they
work.  Someone who goes to work for a company is (one hopes)   
adding to its value, and it's only fair to give them a share
of it.  And as a purely practical measure, people work a lot
harder when they have options.  I've seen that first hand.The fact that a few crooks during the Bubble robbed their
companies by granting themselves options doesn't mean options
are a bad idea.  During the railroad boom, some executives
enriched themselves by selling watered stock—by issuing more
shares than they said were outstanding.  But that doesn't  
make common stock a bad idea.  Crooks just use whatever
means are available.If there is a problem with options, it's that they reward
slightly the wrong thing.  Not surprisingly, people do what you
pay them to. If you pay them by the hour, they'll work a lot of
hours.  If you pay them by the volume of work done, they'll
get a lot of work done (but only as you defined work).
And if you pay them to raise the
stock price, which is what options amount to, they'll raise
the stock price.But that's not quite what you want.  What you want is to
increase the actual value of the company, not its market cap.
Over time the two inevitably meet, but not always as quickly
as options vest.  Which means options tempt employees, if
only unconsciously, to ""pump and dump""—to do things
that will make the company seem valuable.
I found that when I was at Yahoo, I couldn't help thinking,   
""how will this sound to investors?""  when I should have been
thinking ""is this a good idea?""So maybe the standard option deal needs to be tweaked slightly.
Maybe options should be replaced with something tied more
directly to earnings.  It's still early days.8. StartupsWhat made the options valuable, for the most part, is
that they were options on the stock of 
startups.  Startups   
were not of course a creation of the Bubble, but they
were more visible during the Bubble than ever before.One thing most people did learn about for the first time
during the Bubble was the startup
created with the intention of selling it.
Originally a
startup meant a small company that hoped to grow into a
big one.  But increasingly startups are evolving into a
vehicle for developing technology on spec.As I wrote in
Hackers & Painters, employees seem to be most
productive when they're paid in proportion to the wealth
they generate.  And the advantage of a startup—indeed,   
almost its raison d'etre—is that it offers something
otherwise impossible to obtain: a way of measuring that.In many businesses, it just makes more sense for companies
to get technology by buying startups rather than developing   
it in house.  You pay more, but there is less risk,
and risk is what big companies don't want.  It makes the
guys developing the technology more accountable, because they
only get paid if they build the winner.  And you end up   
with better technology, created faster, because things are
made in the innovative atmosphere of startups instead of 
the bureaucratic atmosphere of big companies.Our startup, Viaweb, was built to be sold.  We were open
with investors about that from the start.  And we were     
careful to create something that could slot easily into a
larger company.  That is the pattern for the future.9. CaliforniaThe Bubble was a California phenomenon.  When I showed up
in Silicon Valley in 1998, I felt like an immigrant from
Eastern Europe arriving in America in 1900.  Everyone
was so cheerful and healthy and rich.  It seemed a new
and improved world.The press, ever eager to exaggerate small trends, now gives  
one the impression that Silicon Valley is a ghost town.
Not at all.  When I drive down 101 from the airport,
I still feel a buzz of energy, as if there were a giant
transformer nearby.  Real estate is still more expensive
than just about anywhere else in the country.  The people     
still look healthy, and the weather is still fabulous.
The future is there.
(I say ""there"" because I moved back to the East Coast after
Yahoo.  I still wonder if this was a smart idea.)What makes the Bay Area superior is the attitude of the
people.  I notice that when I come home to Boston.
The first thing I see when I walk out of the airline terminal
is the fat, grumpy guy in
charge of the taxi line.  I brace myself for rudeness:
remember, you're back on the East Coast now.The atmosphere varies from city to city, and fragile
organisms like startups are exceedingly sensitive to such variation.
If it hadn't already been hijacked as a new euphemism
for liberal, the word to describe the atmosphere in
the Bay Area would be ""progressive.""  People there are trying
to build the future.
Boston has MIT and Harvard, but it also has a lot of
truculent, unionized employees like the police who
recently held the Democratic National Convention for   
ransom, and a lot of people trying to be
 Thurston Howell.
Two sides of an obsolete coin.Silicon Valley may not be the next Paris or London, but it
is at least the next Chicago.  For the next fifty years,   
that's where new wealth will come from.10. ProductivityDuring the Bubble, optimistic analysts used to justify high
price to earnings ratios by saying that technology was going 
to increase productivity dramatically.  They were wrong about
the specific companies, but not so wrong about the underlying
principle.  I think one of the big trends we'll see in the
coming century is a huge increase in productivity.Or more precisely, a huge increase in variation in
productivity.  Technology is a lever.  It doesn't add;     
it multiplies.  If the present range of productivity is  
0 to 100, introducing a multiple of 10 increases the range
from 0 to 1000.One upshot of which is that the companies of the future may
be surprisingly small.  I sometimes daydream about how big
you could grow a company (in revenues) without ever having
more than ten people.  What would happen if you outsourced
everything except product development?  If you tried this experiment,
I think you'd be surprised at how far you could get. 
As Fred Brooks pointed out, small groups are
intrinsically more productive, because the
internal friction in a group grows as the
square of its size.Till quite recently, running a major company
meant managing an army of workers.  Our standards about how
many employees a company should have are still influenced by
old patterns.  Startups are perforce small, because they can't
afford to hire a lot of people.  But I think it's a big mistake for
companies to loosen their belts as revenues increase.  The
question is not whether you can afford the extra salaries.   
Can you afford the loss in productivity that comes from making
the company bigger?The prospect of technological leverage will of course raise the
specter of unemployment.  I'm surprised people still worry about
this.
After centuries of supposedly job-killing innovations,
the number of jobs is within ten percent of the number of people
who want them.  This can't be a coincidence.  There must be some
kind of balancing mechanism.What's NewWhen one looks over these trends, is there any overall theme?
There does seem to be: that in the coming century, good ideas
will count for more.  That 26
year olds with good ideas will increasingly have an edge over 50
year olds with powerful connections.  That doing good work will
matter more than dressing up—or advertising, which is the
same thing for companies.  That people
will be rewarded a bit more in proportion to the value of what
they create.If so, this is good news indeed.
Good ideas always tend to win eventually.  The problem is,
it can take a very long time.
It took decades for relativity to be accepted, and the
greater part of a century to establish that central planning didn't work.
So even a small increase in the
rate at which good ideas win would be a momentous
change—big enough, probably, to justify a name like
the ""new economy.""Notes[1] Actually it's hard to say now.  As Jeremy Siegel points
out, if the value of a stock is its future earnings, you 
can't tell if it was overvalued till you see what the earnings
turn out to be.   While certain famous Internet stocks were
almost certainly overvalued in 1999, it is still hard to say for sure
whether, e.g., the Nasdaq index was.Siegel, Jeremy J.  ""What Is an Asset Price Bubble?  An
Operational Definition.""  European Financial Management,
9:1, 2003.[2] The number of users comes from a 6/03 Nielsen
study quoted on Google's site.  (You'd think they'd have
something more recent.)   The revenue estimate is based on
revenues of $1.35 billion for the first half of 2004, as
reported in their IPO filing.Thanks to Chris Anderson, Trevor Blackwell, Sarah Harlin, Jessica
Livingston, and Robert Morris for reading drafts of this.The Long TailRussian TranslationJapanese Translation","technological and societal insights
",human,"human
","human
"
92,92,"January 2016One advantage of being old is that you can see change happen in
your lifetime.  A lot of the change I've seen is fragmentation.  US
politics is much more polarized than it used to be.  Culturally we
have ever less common ground. The creative class flocks to a handful
of happy cities, abandoning the rest.  And increasing economic
inequality means the spread between rich and poor is growing too.
I'd like to propose a hypothesis: that all these trends are instances
of the same phenomenon.  And moreover, that the cause is not some
force that's pulling us apart, but rather the erosion of forces
that had been pushing us together.Worse still, for those who worry about these trends, the forces
that were pushing us together were an anomaly, a one-time combination
of circumstances that's unlikely to be repeated — and indeed, that
we would not want to repeat.The two forces were war (above all World War II), and the rise of
large corporations.The effects of World War II were both economic and social.
Economically, it decreased variation in income.  Like all modern
armed forces, America's were socialist economically.  From each
according to his ability, to each according to his need.  More or
less.  Higher ranking members of the military got more (as higher
ranking members of socialist societies always do), but what they
got was fixed according to their rank.  And the flattening effect
wasn't limited to those under arms, because the US economy was
conscripted too.  Between 1942 and 1945 all wages were set by the
National War Labor Board. Like the military, they defaulted to
flatness.  And this national standardization of wages was so pervasive
that its effects could still be seen years after the war ended.
[1]Business owners weren't supposed to be making money either.  FDR
said ""not a single war millionaire"" would be permitted.  To ensure
that, any increase in a company's profits over prewar levels was
taxed at 85%.  And when what was left after corporate taxes reached
individuals, it was taxed again at a marginal rate of 93%.
[2]Socially too the war tended to decrease variation.  Over 16 million
men and women from all sorts of different backgrounds were brought
together in a way of life that was literally uniform.  Service rates
for men born in the early 1920s approached 80%. And working toward
a common goal, often under stress, brought them still closer together.Though strictly speaking World War II lasted less than 4 years for
the US, its effects lasted longer.  Wars make central governments
more powerful, and World War II was an extreme case of this.  In
the US, as in all the other Allied countries, the federal government
was slow to give up the new powers it had acquired.  Indeed, in
some respects the war didn't end in 1945; the enemy just switched
to the Soviet Union.  In tax rates, federal power, defense spending,
conscription, and nationalism, the decades after the war looked more
like wartime than prewar peacetime.
[3]
And the social effects
lasted too.  The kid pulled into the army from behind a mule team
in West Virginia didn't simply go back to the farm afterward.
Something else was waiting for him, something that looked a lot
like the army.If total war was the big political story of the 20th century, the
big economic story was the rise of a new kind of company.  And this
too tended to produce both social and economic cohesion.
[4]The 20th century was the century of the big, national corporation.
General Electric, General Foods, General Motors.  Developments in
finance, communications, transportation, and manufacturing enabled
a new type of company whose goal was above all scale.  Version 1
of this world was low-res: a Duplo world of a few giant companies
dominating each big market.
[5]The late 19th and early 20th centuries had been a time of consolidation,
led especially by J. P. Morgan.  Thousands of companies run by their
founders were merged into a couple hundred giant ones run by
professional managers. Economies of scale ruled the day.  It seemed
to people at the time that this was the final state of things.  John
D. Rockefeller said in 1880

  The day of combination is here to stay. Individualism has gone,
  never to return.

He turned out to be mistaken, but he seemed right for the next
hundred years.The consolidation that began in the late 19th century continued for
most of the 20th.  By the end of World War II, as Michael Lind
writes, ""the major sectors of the economy were either organized
as government-backed cartels or dominated by a few oligopolistic
corporations.""For consumers this new world meant the same choices everywhere, but
only a few of them.  When I grew up there were only 2 or 3 of most
things, and since they were all aiming at the middle of the market
there wasn't much to differentiate them.One of the most important instances of this phenomenon was in TV.
Here there were 3 choices: NBC, CBS, and ABC. Plus public TV for
eggheads and communists.  The programs that the 3 networks offered were
indistinguishable.  In fact, here there was a triple pressure toward
the center. If one show did try something daring, local affiliates
in conservative markets would make them stop. Plus since TVs were
expensive, whole families watched the same shows together, so they
had to be suitable for everyone.And not only did everyone get the same thing, they got it at the
same time.  It's difficult to imagine now, but every night tens of
millions of families would sit down together in front of their TV
set watching the same show, at the same time, as their next door
neighbors.  What happens now with the Super Bowl used to happen
every night. We were literally in sync.
[6]In a way mid-century TV culture was good. The view it gave of the
world was like you'd find in a children's book, and it probably had
something of the effect that (parents hope) children's books have
in making people behave better.  But, like children's books, TV was
also misleading.  Dangerously misleading, for adults. In his
autobiography, Robert MacNeil talks of seeing gruesome images that
had just come in from Vietnam and thinking, we can't show these to
families while they're having dinner.I know how pervasive the common culture was, because I tried to opt
out of it, and it was practically impossible to find alternatives.
When I was 13 I realized, more from internal evidence than any
outside source, that the ideas we were being fed on TV were crap,
and I stopped watching it.
[7]
But it wasn't just TV.  It seemed
like everything around me was crap.  The politicians all saying the
same things, the consumer brands making almost identical products
with different labels stuck on to indicate how prestigious they
were meant to be, the balloon-frame houses with fake ""colonial""
skins, the cars with several feet of gratuitous metal on each end
that started to fall apart after a couple years, the ""red delicious""
apples that were red but only nominally 
apples. And in retrospect, it was crap.
[8]But when I went looking for alternatives to fill this void, I found
practically nothing.  There was no Internet then.  The only place
to look was in the chain bookstore in our local shopping mall. 
[9]
There I found a copy of The Atlantic.  I wish I could say it became
a gateway into a wider world, but in fact I found it boring and
incomprehensible.  Like a kid tasting whisky for the first time and
pretending to like it, I preserved that magazine as carefully as
if it had been a book. I'm sure I still have it somewhere.  But
though it was evidence that there was, somewhere, a world that
wasn't red delicious, I didn't find it till college.It wasn't just as consumers that the big companies made us similar.
They did as employers too.  Within companies there were powerful
forces pushing people toward a single model of how to look and act.
IBM was particularly notorious for this, but they were only a little
more extreme than other big companies.  And the models of how to
look and act varied little between companies. Meaning everyone
within this world was expected to seem more or less the same.  And
not just those in the corporate world, but also everyone who aspired
to it — which in the middle of the 20th century meant most people
who weren't already in it.  For most of the 20th century, working-class
people tried hard to look middle class.  You can see it in old
photos.  Few adults aspired to look dangerous in 1950.But the rise of national corporations didn't just compress us
culturally.  It compressed us economically too, and on both ends.Along with giant national corporations, we got giant national labor
unions.  And in the mid 20th century the corporations cut deals
with the unions where they paid over market price for labor.  Partly
because the unions were monopolies. 
[10]
Partly because, as
components of oligopolies themselves, the corporations knew they
could safely pass the cost on to their customers, because their
competitors would have to as well.  And partly because in mid-century
most of the giant companies were still focused on finding new ways
to milk economies of scale.  Just as startups rightly pay AWS a
premium over the cost of running their own servers so they can focus
on growth, many of the big national corporations were willing to
pay a premium for labor. 
[11]As well as pushing incomes up from the bottom, by overpaying unions,
the big companies of the 20th century also pushed incomes down at
the top, by underpaying their top management. Economist J. K.
Galbraith wrote in 1967 that ""There are few corporations in which
it would be suggested that executive salaries are at a maximum.""
[12]To some extent this was an illusion.  Much of the de facto pay of
executives never showed up on their income tax returns, because it
took the form of perks.  The higher the rate of income tax, the
more pressure there was to pay employees upstream of it.  (In the
UK, where taxes were even higher than in the US, companies would
even pay their kids' private school tuitions.)  One of the most
valuable things the big companies of the mid 20th century gave their
employees was job security, and this too didn't show up in tax
returns or income statistics. So the nature of employment in these
organizations tended to yield falsely low numbers about economic
inequality.  But even accounting for that, the big companies paid
their best people less than market price.  There was no market; the
expectation was that you'd work for the same company for decades
if not your whole career. 
[13]Your work was so illiquid there was little chance of getting market
price. But that same illiquidity also encouraged you not to seek
it.  If the company promised to employ you till you retired and
give you a pension afterward, you didn't want to extract as much
from it this year as you could. You needed to take care of the
company so it could take care of you.  Especially when you'd been
working with the same group of people for decades.  If you tried
to squeeze the company for more money, you were squeezing the
organization that was going to take care of them.  Plus if
you didn't put the company first you wouldn't be promoted, and if
you couldn't switch ladders, promotion on this one was the only way
up. 
[14]To someone who'd spent several formative years in the armed forces,
this situation didn't seem as strange as it does to us now.  From
their point of view, as big company executives, they were high-ranking
officers.  They got paid a lot more than privates.  They got to
have expense account lunches at the best restaurants and fly around
on the company's Gulfstreams.  It probably didn't occur to most of
them to ask if they were being paid market price.The ultimate way to get market price is to work for yourself, by
starting your own company.  That seems obvious to any ambitious
person now.  But in the mid 20th century it was an alien concept.
Not because starting one's own company seemed too ambitious, but
because it didn't seem ambitious enough. Even as late as the 1970s,
when I grew up, the ambitious plan was to get lots of education at
prestigious institutions, and then join some other prestigious
institution and work one's way up the hierarchy.  Your prestige was
the prestige of the institution you belonged to.  People did start
their own businesses of course, but educated people rarely did,
because in those days there was practically zero concept of starting
what we now call a startup: 
a business that starts small and grows
big.  That was much harder to do in the mid 20th century.  Starting
one's own business meant starting a business that would start small
and stay small. Which in those days of big companies often meant
scurrying around trying to avoid being trampled by elephants.  It
was more prestigious to be one of the executive class riding the
elephant.By the 1970s, no one stopped to wonder where the big prestigious
companies had come from in the first place.  It seemed like they'd
always been there, like the chemical elements.  And indeed, there
was a double wall between ambitious kids in the 20th century and
the origins of the big companies.  Many of the big companies were
roll-ups that didn't have clear founders.  And when they did, the
founders didn't seem like us.  Nearly all of them had been uneducated,
in the sense of not having been to college.  They were what Shakespeare
called rude mechanicals.  College trained one to be a member of the
professional classes.  Its graduates didn't expect to do the sort
of grubby menial work that Andrew Carnegie or Henry Ford started
out doing. 
[15]And in the 20th century there were more and more college graduates.
They increased from about 2% of the population in 1900 to about 25%
in 2000. In the middle of the century our two big forces intersect,
in the form of the GI Bill, which sent 2.2 million World War II
veterans to college.  Few thought of it in these terms, but the
result of making college the canonical path for the ambitious was
a world in which it was socially acceptable to work for Henry Ford,
but not to be Henry Ford.
[16]I remember this world well. I came of age just as it was starting
to break up. In my childhood it was still dominant. Not quite so
dominant as it had been.  We could see from old TV shows and yearbooks
and the way adults acted that people in the 1950s and 60s had been
even more conformist than us.  The mid-century model was already
starting to get old. But that was not how we saw it at the time.
We would at most have said that one could be a bit more daring in
1975 than 1965.  And indeed, things hadn't changed much yet.But change was coming soon. And when the Duplo economy started to
disintegrate, it disintegrated in several different ways at once.
Vertically integrated companies literally dis-integrated because
it was more efficient to.  Incumbents faced new competitors as (a)
markets went global and (b) technical innovation started to trump
economies of scale, turning size from an asset into a liability.
Smaller companies were increasingly able to survive as formerly
narrow channels to consumers broadened.  Markets themselves started
to change faster, as whole new categories of products appeared. And
last but not least, the federal government, which had previously
smiled upon J. P. Morgan's world as the natural state of things,
began to realize it wasn't the last word after all.What J. P. Morgan was to the horizontal axis, Henry Ford was to the
vertical.  He wanted to do everything himself. The giant plant he
built at River Rouge between 1917 and 1928 literally took in iron
ore at one end and sent cars out the other.  100,000 people worked
there. At the time it seemed the future. But that is not how car
companies operate today.  Now much of the design and manufacturing
happens in a long supply chain, whose products the car companies
ultimately assemble and sell.  The reason car companies operate
this way is that it works better.  Each company in the supply chain
focuses on what they know best. And they each have to do it well
or they can be swapped out for another supplier.Why didn't Henry Ford realize that networks of cooperating companies
work better than a single big company? One reason is that supplier
networks take a while to evolve. In 1917, doing everything himself
seemed to Ford the only way to get the scale he needed. And the
second reason is that if you want to solve a problem using a network
of cooperating companies, you have to be able to coordinate their
efforts, and you can do that much better with computers.  Computers
reduce the transaction costs that Coase argued are the raison d'etre
of corporations. That is a fundamental change.In the early 20th century, big companies were synonymous with
efficiency.  In the late 20th century they were synonymous with
inefficiency.  To some extent this was because the companies
themselves had become sclerotic.  But it was also because our
standards were higher.It wasn't just within existing industries that change occurred.
The industries themselves changed.  It became possible to make lots
of new things, and sometimes the existing companies weren't the
ones who did it best.Microcomputers are a classic example. The market was pioneered by
upstarts like Apple. When it got big enough, IBM decided it was
worth paying attention to.  At the time IBM completely dominated
the computer industry. They assumed that all they had to do, now
that this market was ripe, was to reach out and pick it.  Most
people at the time would have agreed with them.  But what happened
next illustrated how much more complicated the world had become.
IBM did launch a microcomputer.  Though quite successful, it did
not crush Apple.  But even more importantly, IBM itself ended up
being supplanted by a supplier coming in from the side — from
software, which didn't even seem to be the same business.  IBM's
big mistake was to accept a non-exclusive license for DOS.  It must
have seemed a safe move at the time.  No other computer manufacturer
had ever been able to outsell them. What difference did it make if
other manufacturers could offer DOS too?  The result of that
miscalculation was an explosion of inexpensive PC clones.  Microsoft
now owned the PC standard, and the customer.  And the microcomputer
business ended up being Apple vs Microsoft.Basically, Apple bumped IBM and then Microsoft stole its wallet.
That sort of thing did not happen to big companies in mid-century.
But it was going to happen increasingly often in the future.Change happened mostly by itself in the computer business.  In other
industries, legal obstacles had to be removed first.  Many of the
mid-century oligopolies had been anointed by the federal government
with policies (and in wartime, large orders) that kept out competitors.
This didn't seem as dubious to government officials at the time as
it sounds to us. They felt a two-party system ensured sufficient
competition in politics.  It ought to work for business too.Gradually the government realized that anti-competitive policies
were doing more harm than good, and during the Carter administration
it started to remove them. The word used for this process was
misleadingly narrow: deregulation.  What was really happening was
de-oligopolization.  It happened to one industry after another.
Two of the most visible to consumers were air travel and long-distance
phone service, which both became dramatically cheaper after
deregulation.Deregulation also contributed to the wave of hostile takeovers in
the 1980s.  In the old days the only limit on the inefficiency of
companies, short of actual bankruptcy, was the inefficiency of their
competitors.  Now companies had to face absolute rather than relative
standards.  Any public company that didn't generate sufficient
returns on its assets risked having its management replaced with
one that would.  Often the new managers did this by breaking companies
up into components that were more valuable separately.
[17]Version 1 of the national economy consisted of a few big blocks
whose relationships were negotiated in back rooms by a handful of
executives, politicians, regulators, and labor leaders.  Version 2
was higher resolution: there were more companies, of more different
sizes, making more different things, and their relationships changed
faster. In this world there were still plenty of back room negotiations,
but more was left to market forces.  Which further accelerated the
fragmentation.It's a little misleading to talk of versions when describing a
gradual process, but not as misleading as it might seem.  There was
a lot of change in a few decades, and what we ended up with was
qualitatively different.  The companies in the S&P 500 in 1958 had
been there an average of 61 years. By 2012 that number was 18 years.
[18]The breakup of the Duplo economy happened simultaneously with the
spread of computing power. To what extent were computers a precondition?
It would take a book to answer that. Obviously the spread of computing
power was a precondition for the rise of startups.  I suspect it
was for most of what happened in finance too.  But was it a
precondition for globalization or the LBO wave?  I don't know, but
I wouldn't discount the possibility.  It may be that the refragmentation
was driven by computers in the way the industrial revolution was
driven by steam engines.  Whether or not computers were a precondition,
they have certainly accelerated it.The new fluidity of companies changed people's relationships with
their employers. Why climb a corporate ladder that might be yanked
out from under you?  Ambitious people started to think of a career
less as climbing a single ladder than as a series of jobs that might
be at different companies. More movement (or even potential movement)
between companies introduced more competition in salaries.  Plus
as companies became smaller it became easier to estimate how much
an employee contributed to the company's revenue.  Both changes
drove salaries toward market price. And since people vary dramatically
in productivity, paying market price meant salaries started to
diverge.By no coincidence it was in the early 1980s that the term ""yuppie""
was coined.  That word is not much used now, because the phenomenon
it describes is so taken for granted, but at the time it was a label
for something novel. Yuppies were young professionals who made lots
of money.  To someone in their twenties today, this wouldn't seem
worth naming.  Why wouldn't young professionals make lots of money?
But until the 1980s, being underpaid early in your career was part
of what it meant to be a professional.  Young professionals were
paying their dues, working their way up the ladder.  The rewards
would come later.  What was novel about yuppies was that they wanted
market price for the work they were doing now.The first yuppies did not work for startups. That was still in the
future.  Nor did they work for big companies. They were professionals
working in fields like law, finance, and consulting.  But their example 
rapidly inspired their peers.  Once they saw that new BMW 325i, they 
wanted one too.Underpaying people at the beginning of their career only works if
everyone does it. Once some employer breaks ranks, everyone else
has to, or they can't get good people.  And once started this process
spreads through the whole economy, because at the beginnings of
people's careers they can easily switch not merely employers but
industries.But not all young professionals benefitted. You had to produce to
get paid a lot.  It was no coincidence that the first yuppies worked
in fields where it was easy to measure that.More generally, an idea was returning whose name sounds old-fashioned
precisely because it was so rare for so long: that you could make
your fortune.  As in the past there were multiple ways to do it.
Some made their fortunes by creating wealth, and others by playing
zero-sum games. But once it became possible to make one's fortune,
the ambitious had to decide whether or not to.  A physicist who
chose physics over Wall Street in 1990 was making a sacrifice that
a physicist in 1960 didn't have to think about.The idea even flowed back into big companies.  CEOs of big companies
make more now than they used to, and I think much of the reason is
prestige.  In 1960, corporate CEOs had immense prestige.  They were
the winners of the only economic game in town. But if they made as
little now as they did then, in real dollar terms, they'd seem like
small fry compared to professional athletes and whiz kids making
millions from startups and hedge funds. They don't like that idea,
so now they try to get as much as they can, which is more than they
had been getting. 
[19]Meanwhile a similar fragmentation was happening at the other end
of the economic scale.  As big companies' oligopolies became less
secure, they were less able to pass costs on to customers and thus
less willing to overpay for labor.  And as the Duplo world of a few
big blocks fragmented into many companies of different sizes — some
of them overseas — it became harder for unions to enforce their
monopolies.  As a result workers' wages also tended toward market
price. Which (inevitably, if unions had been doing their job) tended
to be lower.  Perhaps dramatically so, if automation had decreased
the need for some kind of work.And just as the mid-century model induced social as well as economic
cohesion, its breakup brought social as well as economic fragmentation.
People started to dress and act differently.  Those who would later
be called the ""creative class"" became more mobile. People who didn't
care much for religion felt less pressure to go to church for
appearances' sake, while those who liked it a lot opted for
increasingly colorful forms. Some switched from meat loaf to tofu,
and others to Hot Pockets. Some switched from driving Ford sedans
to driving small imported cars, and others to driving SUVs.  Kids
who went to private schools or wished they did started to dress
""preppy,"" and kids who wanted to seem rebellious made a conscious
effort to look disreputable.  In a hundred ways people spread apart.
[20]Almost four decades later, fragmentation is still increasing.  Has
it been net good or bad?  I don't know; the question may be
unanswerable.  Not entirely bad though.  We take for granted the
forms of fragmentation we like, and worry only about the ones we
don't. But as someone who caught the tail end of mid-century
conformism, 
I can tell you it was no utopia.
[21]My goal here is not to say whether fragmentation has been good or
bad, just to explain why it's happening.  With the centripetal
forces of total war and 20th century oligopoly mostly gone, what
will happen next?  And more specifically, is it possible to reverse
some of the fragmentation we've seen?If it is, it will have to happen piecemeal.  You can't reproduce
mid-century cohesion the way it was originally produced.  It would
be insane to go to war just to induce more national unity.  And
once you understand the degree to which the economic history of the
20th century was a low-res version 1, it's clear you can't reproduce
that either.20th century cohesion was something that happened at least in a
sense naturally.  The war was due mostly to external forces, and
the Duplo economy was an evolutionary phase.  If you want cohesion
now, you'd have to induce it deliberately.  And it's not obvious
how.  I suspect the best we'll be able to do is address the symptoms
of fragmentation.  But that may be enough.The form of fragmentation people worry most about lately is economic inequality, and if you want to eliminate
that you're up against a truly formidable headwind that has
been in operation since the stone age. Technology.Technology is
a lever. It magnifies work.  And the lever not only grows increasingly
long, but the rate at which it grows is itself increasing.Which in turn means the variation in the amount of wealth people
can create has not only been increasing, but accelerating.  The
unusual conditions that prevailed in the mid 20th century masked
this underlying trend.  The ambitious had little choice but to join
large organizations that made them march in step with lots of other
people — literally in the case of the armed forces, figuratively
in the case of big corporations. Even if the big corporations had
wanted to pay people proportionate to their value, they couldn't
have figured out how.  But that constraint has gone now.  Ever since
it started to erode in the 1970s, we've seen the underlying forces
at work again.
[22]Not everyone who gets rich now does it by creating wealth, certainly.
But a significant number do, and the Baumol Effect means all their
peers get dragged along too.
[23]
And as long as it's possible to
get rich by creating wealth, the default tendency will be for
economic inequality to increase.  Even if you eliminate all the
other ways to get rich.  You can mitigate this with subsidies at
the bottom and taxes at the top, but unless taxes are high enough
to discourage people from creating wealth, you're always going to
be fighting a losing battle against increasing variation in
productivity.
[24]That form of fragmentation, like the others, is here to stay.  Or
rather, back to stay.  Nothing is forever, but the tendency toward
fragmentation should be more forever than most things, precisely
because it's not due to any particular cause.  It's simply a reversion
to the mean. When Rockefeller said individualism was gone, he was
right for a hundred years.  It's back now, and that's likely to be
true for longer.I worry that if we don't acknowledge this, we're headed for trouble.
If we think 20th century cohesion disappeared because of few policy
tweaks, we'll be deluded into thinking we can get it back (minus
the bad parts, somehow) with a few countertweaks.  And then we'll
waste our time trying to eliminate fragmentation, when we'd be
better off thinking about how to mitigate its consequences.
Notes[1]
Lester Thurow, writing in 1975, said the wage differentials
prevailing at the end of World War II had become so embedded that
they ""were regarded as 'just' even after the egalitarian pressures
of World War II had disappeared.  Basically, the same differentials
exist to this day, thirty years later."" But Goldin and Margo think
market forces in the postwar period also helped preserve the wartime
compression of wages — specifically increased demand for unskilled
workers, and oversupply of educated ones.(Oddly enough, the American custom of having employers pay for
health insurance derives from efforts by businesses to circumvent
NWLB wage controls in order to attract workers.)[2]
As always, tax rates don't tell the whole story.  There were
lots of exemptions, especially for individuals.  And in World War
II the tax codes were so new that the government had little acquired
immunity to tax avoidance.  If the rich paid high taxes during the
war it was more because they wanted to than because they had to.After the war, federal tax receipts as a percentage of GDP were
about the same as they are now. In fact, for the entire period since
the war, tax receipts have stayed close to 18% of GDP, despite
dramatic changes in tax rates.  The lowest point occurred when
marginal income tax rates were highest: 14.1% in 1950.  Looking at
the data, it's hard to avoid the conclusion that tax rates have had
little effect on what people actually paid.[3]
Though in fact the decade preceding the war had been a time
of unprecedented federal power, in response to the Depression.
Which is not entirely a coincidence, because the Depression was one
of the causes of the war.  In many ways the New Deal was a sort of
dress rehearsal for the measures the federal government took during
wartime.  The wartime versions were much more drastic and more
pervasive though.  As Anthony Badger wrote, ""for many Americans the
decisive change in their experiences came not with the New Deal but
with World War II.""[4]
I don't know enough about the origins of the world wars to
say, but it's not inconceivable they were connected to the rise of
big corporations. If that were the case, 20th century cohesion would
have a single cause.[5]
More precisely, there was a bimodal economy consisting, in
Galbraith's words, of ""the world of the technically dynamic, massively
capitalized and highly organized corporations on the one hand and
the hundreds of thousands of small and traditional proprietors on
the other."" Money, prestige, and power were concentrated in the
former, and there was near zero crossover.[6]
I wonder how much of the decline in families eating together
was due to the decline in families watching TV together afterward.[7]
I know when this happened because it was the season Dallas
premiered.  Everyone else was talking about what was happening on
Dallas, and I had no idea what they meant.[8]
I didn't realize it till I started doing research for this
essay, but the meretriciousness of the products I grew up with is
a well-known byproduct of oligopoly. When companies can't compete
on price, they compete on tailfins.[9]
Monroeville Mall was at the time of its completion in 1969
the largest in the country. In the late 1970s the movie Dawn of
the Dead was shot there. Apparently the mall was not just the
location of the movie, but its inspiration; the crowds of shoppers
drifting through this huge mall reminded George Romero of zombies.
My first job was scooping ice cream in the Baskin-Robbins.[10]
Labor unions were exempted from antitrust laws by the Clayton
Antitrust Act in 1914 on the grounds that a person's work is not
""a commodity or article of commerce."" I wonder if that means service
companies are also exempt.[11]
The relationships between unions and unionized companies can
even be symbiotic, because unions will exert political pressure to
protect their hosts.  According to Michael Lind, when politicians
tried to attack the A&P supermarket chain because it was putting
local grocery stores out of business, ""A&P successfully defended
itself by allowing the unionization of its workforce in 1938, thereby
gaining organized labor as a constituency."" I've seen this phenomenon
myself: hotel unions are responsible for more of the political
pressure against Airbnb than hotel companies.[12]
Galbraith was clearly puzzled that corporate executives would
work so hard to make money for other people (the shareholders)
instead of themselves.  He devoted much of The New Industrial
State to trying to figure this out.His theory was that professionalism had replaced money as a motive,
and that modern corporate executives were, like (good) scientists,
motivated less by financial rewards than by the desire to do good
work and thereby earn the respect of their peers.  There is something
in this, though I think lack of movement between companies combined
with self-interest explains much of observed behavior.[13]
Galbraith (p. 94) says a 1952 study of the 800 highest paid
executives at 300 big corporations found that three quarters of
them had been with their company for more than 20 years.[14]
It seems likely that in the first third of the 20th century
executive salaries were low partly because companies then were more
dependent on banks, who would have disapproved if executives got
too much.  This was certainly true in the beginning. The first big
company CEOs were J. P. Morgan's hired hands.Companies didn't start to finance themselves with retained earnings
till the 1920s.  Till then they had to pay out their earnings in
dividends, and so depended on banks for capital for expansion.
Bankers continued to sit on corporate boards till the Glass-Steagall
act in 1933.By mid-century big companies funded 3/4 of their growth from earnings.
But the early years of bank dependence, reinforced by the financial
controls of World War II, must have had a big effect on social
conventions about executive salaries.  So it may be that the lack
of movement between companies was as much the effect of low salaries
as the cause.Incidentally, the switch in the 1920s to financing growth with
retained earnings was one cause of the 1929 crash.  The banks now
had to find someone else to lend to, so they made more margin loans.[15]
Even now it's hard to get them to. One of the things I find
hardest to get into the heads of would-be startup founders is how
important it is to do certain kinds of menial work early in the
life of a company.  Doing things that don't
scale is to how Henry Ford got started as a high-fiber diet is
to the traditional peasant's diet: they had no choice but to do the
right thing, while we have to make a conscious effort.[16]
Founders weren't celebrated in the press when I was a kid.
""Our founder"" meant a photograph of a severe-looking man with a
walrus mustache and a wing collar who had died decades ago. The
thing to be when I was a kid was an executive. If you weren't
around then it's hard to grasp the cachet that term had. The fancy
version of everything was called the ""executive"" model.[17]
The wave of hostile takeovers in the 1980s was enabled by a
combination of circumstances: court decisions striking down state
anti-takeover laws, starting with the Supreme Court's 1982 decision
in Edgar v. MITE Corp.; the Reagan administration's comparatively
sympathetic attitude toward takeovers; the Depository Institutions
Act of 1982, which allowed banks and savings and loans to buy
corporate bonds; a new SEC rule issued in 1982 (rule 415) that made
it possible to bring corporate bonds to market faster; the creation
of the junk bond business by Michael Milken; a vogue for conglomerates
in the preceding period that caused many companies to be combined
that never should have been; a decade of inflation that left many
public companies trading below the value of their assets; and not
least, the increasing complacency of managements.[18]
Foster, Richard. ""Creative Destruction Whips through Corporate
America."" Innosight, February 2012.[19]
CEOs of big companies may be overpaid. I don't know enough
about big companies to say. But it is certainly not impossible for
a CEO to make 200x as much difference to a company's revenues as
the average employee.  Look at what Steve Jobs did for Apple when
he came back as CEO.  It would have been a good deal for the board
to give him 95% of the company.  Apple's market cap the day Steve
came back in July 1997 was 1.73 billion. 5% of Apple now (January
2016) would be worth about 30 billion.  And it would not be if Steve
hadn't come back; Apple probably wouldn't even exist anymore.Merely including Steve in the sample might be enough to answer the
question of whether public company CEOs in the aggregate are overpaid.
And that is not as facile a trick as it might seem, because the
broader your holdings, the more the aggregate is what you care
about.[20]
The late 1960s were famous for social upheaval. But that was
more rebellion (which can happen in any era if people are provoked
sufficiently) than fragmentation.  You're not seeing fragmentation
unless you see people breaking off to both left and right.[21]
Globally the trend has been in the other direction.  While
the US is becoming more fragmented, the world as a whole is becoming
less fragmented, and mostly in good ways.[22]
There were a handful of ways to make a fortune in the mid
20th century.  The main one was drilling for oil, which was open
to newcomers because it was not something big companies could
dominate through economies of scale.  How did individuals accumulate
large fortunes in an era of such high taxes?  Giant tax loopholes
defended by two of the most powerful men in Congress, Sam Rayburn
and Lyndon Johnson.But becoming a Texas oilman was not in 1950 something one could
aspire to the way starting a startup or going to work on Wall Street
were in 2000, because (a) there was a strong local component and
(b) success depended so much on luck.[23]
The Baumol Effect induced by startups is very visible in
Silicon Valley.  Google will pay people millions of dollars a year
to keep them from leaving to start or join startups.[24]
I'm not claiming variation in productivity is the only cause
of economic inequality in the US. But it's a significant cause, and
it will become as big a cause as it needs to, in the sense that if
you ban other ways to get rich, people who want to get rich will
use this route instead.Thanks to Sam Altman, Trevor Blackwell, Paul Buchheit, Patrick
Collison, Ron Conway, Chris Dixon, Benedict Evans, Richard Florida,
Ben Horowitz, Jessica Livingston, Robert Morris, Tim O'Reilly, Geoff
Ralston, Max Roser, Alexia Tsotsis, and Qasar Younis for reading
drafts of this.  Max also told me about several valuable sources.BibliographyAllen, Frederick Lewis. The Big Change. Harper, 1952.Averitt, Robert. The Dual Economy. Norton, 1968.Badger, Anthony. The New Deal. Hill and Wang, 1989.Bainbridge, John. The Super-Americans. Doubleday, 1961.Beatty, Jack. Collossus. Broadway, 2001.Brinkley, Douglas. Wheels for the World. Viking, 2003.Brownleee, W. Elliot. Federal Taxation in America. Cambridge, 1996.Chandler, Alfred. The Visible Hand. Harvard, 1977.Chernow, Ron. The House of Morgan. Simon & Schuster, 1990.Chernow, Ron. Titan: The Life of John D. Rockefeller. Random House,
1998.Galbraith, John. The New Industrial State. Houghton Mifflin, 1967.Goldin, Claudia and Robert A. Margo. ""The Great Compression: The
Wage Structure in the United States at Mid-Century."" NBER Working
Paper 3817, 1991.Gordon, John. An Empire of Wealth. HarperCollins, 2004.Klein, Maury. The Genesis of Industrial America, 1870-1920. Cambridge,
2007.Lind, Michael. Land of Promise. HarperCollins, 2012.Mickelthwaite, John, and Adrian Wooldridge. The Company. Modern
Library, 2003.Nasaw, David. Andrew Carnegie. Penguin, 2006.Sobel, Robert. The Age of Giant Corporations. Praeger, 1993.Thurow, Lester. Generating Inequality: Mechanisms of Distribution.
Basic Books, 1975.Witte, John. The Politics and Development of the Federal Income
Tax. Wisconsin, 1985.Related:Too Many Elite American Men Are Obsessed With Work and Wealth","technological and societal insights
",human,"human
","human
"
93,93,"August 2005(This essay is derived from a talk at Defcon 2005.)Suppose you wanted to get rid of economic inequality.  There are
two ways to do it: give money to the poor, or take it away from the 
rich.  But they amount to the same thing, because if you want to
give money to the poor, you have to get it from somewhere.  You
can't get it from the poor, or they just end up where they started.
You have to get it from the rich.There is of course a way to make the poor richer without simply
shifting money from the rich.  You could help the poor become more
productive — for example, by improving access to education.  Instead
of taking money from engineers and giving it to checkout clerks,
you could enable people who would have become checkout clerks to
become engineers.This is an excellent strategy for making the poor richer.  But the
evidence of the last 200 years shows that it doesn't reduce economic
inequality, because it makes the rich richer too.  If there
are more engineers, then there are more opportunities to hire them
and to sell them things.  Henry Ford couldn't have made a fortune  
building cars in a society in which most people were still subsistence
farmers; he would have had neither workers nor customers.If you want to reduce economic inequality instead of just improving
the overall standard of living, it's not enough just to raise up  
the poor.  What if one of your newly minted engineers gets ambitious
and goes on to become another Bill Gates?  Economic inequality will
be as bad as ever.  If you actually want to compress the gap between
rich and poor, you have to push down on the top as well as pushing
up on the bottom.How do you push down on the top?  You could try to decrease the
productivity of the people who make the most money: make the best   
surgeons operate with their left hands, force popular actors to
overeat, and so on.  But this approach is hard to implement.  The
only practical solution is to let people do the best work they can,
and then (either by taxation or by limiting what they can charge)
to confiscate whatever you deem to be surplus.So let's be clear what reducing economic inequality means.  It is   
identical with taking money from the rich.When you transform a mathematical expression into another form, you
often notice new things.  So it is in this case.  Taking money from
the rich turns out to have consequences one might not foresee when
one phrases the same idea in terms of ""reducing inequality.""The problem is, risk and reward have to be proportionate.  A bet  
with only a 10% chance of winning has to pay more than one with a
50% chance of winning, or no one will take it.  So if you lop off
the top of the possible rewards, you thereby decrease people's
willingness to take risks.Transposing into our original expression, we get: decreasing economic
inequality means decreasing the risk people are willing to take.There are whole classes of risks that are no longer worth taking    
if the maximum return is decreased.  One reason high tax rates are
disastrous is that this class of risks includes starting new
companies.InvestorsStartups are intrinsically risky.  A startup
is like a small boat
in the open sea.  One big wave and you're sunk.  A competing product,
a downturn in the economy, a delay in getting funding or regulatory
approval, a patent suit, changing technical standards, the departure
of a key employee, the loss of a big account — any one of these can
destroy you overnight.  It seems only about 1 in 10 startups succeeds.
[1]Our startup paid its first round of outside investors 36x.  Which   
meant, with current US tax rates, that it made sense to invest in
us if we had better than a 1 in 24 chance of succeeding.  That  
sounds about right.  That's probably roughly how we looked when we
were a couple of nerds with no business experience operating out
of an apartment.If that kind of risk doesn't pay, venture investing, as we know it,
doesn't happen.That might be ok if there were other sources of capital for new
companies.  Why not just have the government, or some large
almost-government organization like Fannie Mae, do the venture
investing instead of private funds?I'll tell you why that wouldn't work. Because then you're asking
government or almost-government employees to do the one thing they 
are least able to do: take risks.As anyone who has worked for the government knows, the important
thing is not to make the right choices, but to make choices that
can be justified later if they fail.  If there is a safe option,
that's the one a bureaucrat will choose.   But that is exactly the  
wrong way to do venture investing.  The nature of the business means
that you want to make terribly risky choices, if the upside looks
good enough.VCs are currently 
paid in a way that makes them 
focus on the upside:
they get a percentage of the fund's gains.  And that helps overcome
their understandable fear of investing in a company run by nerds
who look like (and perhaps are) college students.If VCs weren't allowed to get rich, they'd behave like bureaucrats.
Without hope of gain, they'd have only fear of loss.  And so they'd
make the wrong choices.  They'd turn down the nerds in favor of the
smooth-talking MBA in a suit, because that investment would be
easier to justify later if it failed.FoundersBut even if you could somehow redesign venture funding to work
without allowing VCs to become rich, there's another kind of investor
you simply cannot replace: the startups' founders and early employees.What they invest is their time and ideas.  But these are equivalent
to money; the proof is that investors are willing (if forced) to
treat them as interchangeable, granting the same status to ""sweat   
equity"" and the equity they've purchased with cash.The fact that you're investing time doesn't change the relationship
between risk and reward.  If you're going to invest your time in
something with a small chance of succeeding, you'll only do it if
there is a proportionately large payoff.
[2]
If large payoffs aren't allowed, you may as well play it safe.Like many startup founders, I did it to get rich.  But not because 
I wanted to buy expensive things.  What I wanted was security.  I   
wanted to make enough money that I didn't have to worry about money.
If I'd been forbidden to make enough from a startup to do this, I
would have sought security by some other means: for example, by
going to work for a big, stable organization from which it would
be hard to get fired.  Instead of busting my ass in a startup, I    
would have tried to get a nice, low-stress job at a big research 
lab, or tenure at a university.That's what everyone does in societies where risk isn't rewarded.
If you can't ensure your own security, the next best thing is to
make a nest for yourself in some large organization where your
status depends mostly on seniority.
[3]Even if we could somehow replace investors, I don't see how we could
replace founders.  Investors mainly contribute money, which in
principle is the same no matter what the source.  But the founders
contribute ideas.  You can't replace those.Let's rehearse the chain of argument so far.  I'm heading for a 
conclusion to which many readers will have to be dragged kicking   
and screaming, so I've tried to make each link unbreakable.  Decreasing
economic inequality means taking money from the rich.  Since risk
and reward are equivalent, decreasing potential rewards automatically
decreases people's appetite for risk.  Startups are intrinsically
risky.  Without the prospect of rewards proportionate to the risk,
founders will not invest their time in a startup.  Founders are
irreplaceable.  So eliminating economic inequality means eliminating
startups.Economic inequality is not just a consequence of startups.
It's the engine that drives them, in the same way a fall of water   
drives a water mill.  People start startups in the hope of becoming
much richer than they were before.  And if your society tries to
prevent anyone from being much richer than anyone else, it will
also prevent one person from being much richer at t2 than t1.GrowthThis argument applies proportionately.  It's not just that if you  
eliminate economic inequality, you get no startups.  To the extent 
you reduce economic inequality, you decrease the number of startups.
[4]
Increase taxes, and willingness to take risks decreases in
proportion.And that seems bad for everyone.  New technology and new jobs both
come disproportionately from new companies.  Indeed, if you don't
have startups, pretty soon you won't have established companies
either, just as, if you stop having kids, pretty soon you won't
have any adults.It sounds benevolent to say we ought to reduce economic inequality. 
When you phrase it that way, who can argue with you?  Inequality
has to be bad, right?  It sounds a good deal less benevolent to say
we ought to reduce the rate at which new companies are founded.
And yet the one implies the other.Indeed, it may be that reducing investors' appetite for risk doesn't
merely kill off larval startups, but kills off the most promising
ones especially.  Startups yield faster growth at greater risk than
established companies.  Does this trend also hold among startups?
That is, are the riskiest startups the ones that generate most
growth if they succeed?  I suspect the answer is yes.  And that's   
a chilling thought, because it means that if you cut investors'
appetite for risk, the most beneficial startups are the first to 
go.Not all rich people got that way from startups, of course.  What
if we let people get rich by starting startups, but taxed away all
other surplus wealth?  Wouldn't that at least decrease inequality?Less than you might think.  If you made it so that people could
only get rich by starting startups, people who wanted to get rich
would all start startups.  And that might be a great thing.  But I
don't think it would have much effect on the distribution of wealth.
People who want to get rich will do whatever they have to.  If
startups are the only way to do it, you'll just get far more people
starting startups.  (If you write the laws very carefully, that is.
More likely, you'll just get a lot of people doing things that can
be made to look on paper like startups.)If we're determined to eliminate economic inequality, there is still
one way out: we could say that we're willing to go ahead and do
without startups.  What would happen if we did?At a minimum, we'd have to accept lower rates of technological   
growth.  If you believe that large, established companies could   
somehow be made to develop new technology as fast as startups, the
ball is in your court to explain how.  (If you can come up with a   
remotely plausible story, you can make a fortune writing business
books and consulting for large companies.)
[5]Ok, so we get slower growth.  Is that so bad?  Well, one reason
it's bad in practice is that other countries might not agree to
slow down with us.  If you're content to develop new technologies
at a slower rate than the rest of the world, what happens is that
you don't invent anything at all.  Anything you might discover has
already been invented elsewhere.  And the only thing you can offer 
in return is raw materials and cheap labor.  Once you sink that
low, other countries can do whatever they like with you: install
puppet governments, siphon off your best workers, use your women
as prostitutes, dump their toxic waste on your territory — all the
things we do to poor countries now.  The only defense is to isolate
yourself, as communist countries did in the twentieth century.  But
the problem then is, you have to become a police state to enforce 
it.
Wealth and PowerI realize startups are not the main target of those who want to
eliminate economic inequality.   What they really dislike is the
sort of wealth that becomes self-perpetuating through an alliance
with power. For example, construction firms that fund politicians'
campaigns in return for government contracts, or rich parents who   
get their children into good colleges by sending them to expensive
schools designed for that purpose.  But if you try to attack this type of wealth
through economic policy, it's hard to hit without destroying
startups as collateral damage.The problem here is not wealth, but corruption.  So why not go after
corruption?We don't need to prevent people from being rich if we can prevent
wealth from translating into power.  And there has been progress
on that front.  Before he died of drink in 1925, Commodore Vanderbilt's
wastrel grandson Reggie ran down pedestrians on five separate   
occasions, killing two of them.  By 1969, when Ted Kennedy drove  
off the bridge at Chappaquiddick, the limit seemed to be down to  
one.  Today it may well be zero.  But what's changed is not variation
in wealth.  What's changed is the ability to translate wealth into
power.How do you break the connection between wealth and power?  Demand   
transparency.  Watch closely how power is exercised, and demand an
account of how decisions are made.  Why aren't all police interrogations
videotaped?  Why did 36% of Princeton's class of 2007 come from
prep schools, when only 1.7% of American kids attend them?  Why did
the US really invade Iraq?  Why don't government officials disclose
more about their finances, and why only during their term of office?A friend of mine who knows a lot about computer security says the
single most important step is to log everything.  Back when he was
a kid trying to break into computers, what worried him most was the
idea of leaving a trail.  He was more inconvenienced by the need 
to avoid that than by any obstacle deliberately put in his path.Like all illicit connections, the connection between wealth and     
power flourishes in secret.  Expose all transactions, and you will
greatly reduce it.  Log everything.  That's a strategy that already
seems to be working, and it doesn't have the side effect of making
your whole country poor.I don't think many people realize there is a connection between
economic inequality and risk.  I didn't fully grasp it till recently.
I'd known for years of course that if one didn't score in a startup,
the other alternative was to get a cozy, tenured research job.  But
I didn't understand the equation governing my behavior.  Likewise, 
it's obvious empirically that a country that doesn't let people get
rich is headed for disaster, whether it's Diocletian's Rome or  
Harold Wilson's Britain.  But I did not till recently understand
the role risk played.If you try to attack wealth, you end up nailing risk as well, and  
with it growth.  If we want a fairer world, I think we're better  
off attacking one step downstream, where wealth turns into power.Notes
[1]
Success here is defined from the initial investors' point of
view: either an IPO, or an acquisition for more than the valuation
at the last round of funding.  The conventional 1 in 10 success rate
is suspiciously neat, but conversations with VCs suggest it's roughly
correct for startups overall.  Top VC firms expect to do better.[2]
I'm not claiming founders sit down and calculate the expected     
after-tax return from a startup.  They're motivated by examples of
other people who did it.  And those examples do reflect after-tax returns.[3]
Conjecture: The variation in wealth in a (non-corrupt) 
country or organization
will be inversely proportional to the prevalence of systems of
seniority.  So if you suppress variation in wealth, seniority will
become correspondingly more important.  So far, I know of no
counterexamples, though in very corrupt countries you may get 
both simultaneously.  (Thanks to Daniel Sobral for pointing
this out.)[4]
In a country with a truly feudal economy, you might be able to
redistribute wealth successfully, because there are no startups to
kill.[5]
The speed at which startups develop new techology is the other     
reason they pay so well.  As I explained in ""How to Make Wealth"", what you do in a startup is compress a
lifetime's worth of work into a few years.  It seems as
dumb to discourage that as to discourage risk-taking.
Thanks to Chris Anderson, Trevor Blackwell, Dan Giffin,
Jessica Livingston, and Evan Williams for reading drafts of this
essay, and to Langley Steinert, Sangam Pant, and Mike Moritz for
information about venture investing.Romanian TranslationDutch TranslationTraditional Chinese TranslationJapanese TranslationHebrew Translation

If you liked this, you may also like
Hackers & Painters.

","technological and societal insights
",human,"human
","human
"
94,94,"December 2001 (rev. May 2002)

(This article came about in response to some questions on
the LL1 mailing list.  It is now
incorporated in Revenge of the Nerds.)When McCarthy designed Lisp in the late 1950s, it was
a radical departure from existing languages,
the most important of which was Fortran.Lisp embodied nine new ideas:
1. Conditionals.  A conditional is an if-then-else
construct.  We take these for granted now.  They were 
invented
by McCarthy in the course of developing Lisp. 
(Fortran at that time only had a conditional
goto, closely based on the branch instruction in the 
underlying hardware.)  McCarthy, who was on the Algol committee, got
conditionals into Algol, whence they spread to most other
languages.2. A function type. In Lisp, functions are first class 
objects-- they're a data type just like integers, strings,
etc, and have a literal representation, can be stored in variables,
can be passed as arguments, and so on.3. Recursion.  Recursion existed as a mathematical concept
before Lisp of course, but Lisp was the first programming language to support
it.  (It's arguably implicit in making functions first class
objects.)4. A new concept of variables.  In Lisp, all variables
are effectively pointers. Values are what
have types, not variables, and assigning or binding
variables means copying pointers, not what they point to.5. Garbage-collection.6. Programs composed of expressions. Lisp programs are 
trees of expressions, each of which returns a value.  
(In some Lisps expressions
can return multiple values.)  This is in contrast to Fortran
and most succeeding languages, which distinguish between
expressions and statements.It was natural to have this
distinction in Fortran because (not surprisingly in a language
where the input format was punched cards) the language was
line-oriented.  You could not nest statements.  And
so while you needed expressions for math to work, there was
no point in making anything else return a value, because
there could not be anything waiting for it.This limitation
went away with the arrival of block-structured languages,
but by then it was too late. The distinction between
expressions and statements was entrenched.  It spread from 
Fortran into Algol and thence to both their descendants.When a language is made entirely of expressions, you can
compose expressions however you want.  You can say either
(using Arc syntax)(if foo (= x 1) (= x 2))or(= x (if foo 1 2))7. A symbol type.  Symbols differ from strings in that
you can test equality by comparing a pointer.8. A notation for code using trees of symbols.9. The whole language always available.  
There is
no real distinction between read-time, compile-time, and runtime.
You can compile or run code while reading, read or run code
while compiling, and read or compile code at runtime.Running code at read-time lets users reprogram Lisp's syntax;
running code at compile-time is the basis of macros; compiling
at runtime is the basis of Lisp's use as an extension
language in programs like Emacs; and reading at runtime
enables programs to communicate using s-expressions, an
idea recently reinvented as XML.
When Lisp was first invented, all these ideas were far
removed from ordinary programming practice, which was
dictated largely by the hardware available in the late 1950s.Over time, the default language, embodied
in a succession of popular languages, has
gradually evolved toward Lisp.  1-5 are now widespread.
6 is starting to appear in the mainstream.
Python has a form of 7, though there doesn't seem to be
any syntax for it.  
8, which (with 9) is what makes Lisp macros
possible, is so far still unique to Lisp,
perhaps because (a) it requires those parens, or something 
just as bad, and (b) if you add that final increment of power, 
you can no 
longer claim to have invented a new language, but only
to have designed a new dialect of Lisp ; -)Though useful to present-day programmers, it's
strange to describe Lisp in terms of its
variation from the random expedients other languages
adopted.  That was not, probably, how McCarthy
thought of it.  Lisp wasn't designed to fix the mistakes
in Fortran; it came about more as the byproduct of an
attempt to axiomatize computation.Japanese Translation","technological and societal insights
",human,"human
","human
"
95,95,"December 2008For nearly all of history the success of a society was proportionate
to its ability to assemble large and disciplined organizations.
Those who bet on economies of scale generally won, which meant the
largest organizations were the most successful ones.Things have already changed so much that this is hard for us to
believe, but till just a few decades ago the largest organizations
tended to be the most progressive.  An ambitious kid graduating
from college in 1960 wanted to work in the huge, gleaming offices
of Ford, or General Electric, or NASA.  Small meant small-time.
Small in 1960 didn't mean a cool little startup.  It meant uncle
Sid's shoe store.When I grew up in the 1970s, the idea of the ""corporate ladder"" was
still very much alive.  The standard plan was to try to get into a
good college, from which one would be drafted into some organization
and then rise to positions of gradually increasing responsibility.
The more ambitious merely hoped to climb the same ladder faster.
[1]But in the late twentieth century something changed.  It turned out
that economies of scale were not the only force at work.  Particularly
in technology, the increase in speed one could get from smaller
groups started to trump the advantages of size.The future turned out to be different from the one we were expecting
in 1970.  The domed cities and flying cars we expected have failed
to materialize.  But fortunately so have the jumpsuits with badges
indicating our specialty and rank.  Instead of being dominated by
a few, giant tree-structured organizations, it's now looking like
the economy of the future will be a fluid network of smaller,
independent units.It's not so much that large organizations stopped working.  There's
no evidence that famously successful organizations like the Roman
army or the British East India Company were any less afflicted by
protocol and politics than organizations of the same size today.
But they were competing against opponents who couldn't change the
rules on the fly by discovering new technology.  Now it turns out
the rule ""large and disciplined organizations win"" needs to have a
qualification appended: ""at games that change slowly."" No one knew
till change reached a sufficient speed.Large organizations will start to do worse now, though,
because for the first time in history they're no longer getting the
best people.  An ambitious kid graduating from college now doesn't
want to work for a big company.  They want to work for the hot
startup that's rapidly growing into one.  If they're really ambitious,
they want to start it. 
[2]This doesn't mean big companies will disappear.  To say that
startups will succeed implies that big companies will exist, because
startups that succeed either become big companies or are acquired
by them. 
[3]
But large organizations will probably never again
play the leading role they did up till the last quarter of the
twentieth century.It's kind of surprising that a trend that lasted so long would ever
run out.  How often does it happen that a rule works for thousands
of years, then switches polarity?The millennia-long run of bigger-is-better left us with a lot of
traditions that are now obsolete, 
but extremely deeply rooted.
Which means the ambitious can now do arbitrage on them.  It will
be very valuable to understand precisely which ideas to keep and
which can now be discarded.The place to look is where the spread of smallness began: in the
world of startups.There have always been occasional cases, particularly in the US,
of ambitious people who grew the ladder under them instead of
climbing it.  But till recently this was an anomalous route that
tended to be followed only by outsiders.  It was no coincidence
that the great industrialists of the nineteenth century had so
little formal education.  As huge as their companies eventually
became, they were all essentially mechanics and shopkeepers at
first.  That was a social step no one with a college education would
take if they could avoid it.  Till the rise of technology startups,
and in particular, Internet startups, it was very unusual for
educated people to start their own businesses.The eight men who left Shockley Semiconductor to found Fairchild
Semiconductor, the original Silicon Valley startup, weren't even
trying to start a company at first.  They were just looking for a
company willing to hire them as a group.  Then one of their parents
introduced them to a small investment bank that offered to find
funding for them to start their own, so they did.  But starting a
company was an alien idea to them; it was something they backed
into.
[4]Now I would guess that practically every Stanford or Berkeley
undergrad who knows how to program has at least considered the idea
of starting a startup.  East Coast universities are not far behind,
and British universities only a little behind them.  This pattern
suggests that attitudes at Stanford and Berkeley are not an anomaly,
but a leading indicator.  This is the way the world is going.Of course, Internet startups are still only a fraction of the world's
economy.  Could a trend based on them be that powerful?I think so.  There's no reason to suppose there's any limit to the
amount of work that could be done in this area.  Like science,
wealth seems to expand fractally.  Steam power was a sliver of the
British economy when Watt started working on it.  But his work led
to more work till that sliver had expanded into something bigger
than the whole economy of which it had initially been a part.The same thing could happen with the Internet.  If Internet startups
offer the best opportunity for ambitious people, then a lot of
ambitious people will start them, and this bit of the economy will
balloon in the usual fractal way.Even if Internet-related applications only become a tenth of the
world's economy, this component will set the tone for the rest.
The most dynamic part of the economy always does, in everything
from salaries to standards of dress.  Not just because of its
prestige, but because the principles underlying the most dynamic
part of the economy tend to be ones that work.For the future, the trend to bet on seems to be networks of small,
autonomous groups whose performance is measured individually.  And
the societies that win will be the ones with the least impedance.As with the original industrial revolution, some societies are going
to be better at this than others.  Within a generation of its birth
in England, the Industrial Revolution had spread to continental
Europe and North America.  But it didn't spread everywhere.  This
new way of doing things could only take root in places that were
prepared for it.  It could only spread to places that already had
a vigorous middle class.There is a similar social component to the transformation that began
in Silicon Valley in the 1960s.  Two new kinds of techniques were
developed there: techniques for building integrated circuits, and
techniques for building a new type of company designed to grow fast
by creating new technology.  The techniques for building integrated
circuits spread rapidly to other countries.  But the techniques for
building startups didn't.  Fifty years later, startups are ubiquitous
in Silicon Valley and common in a handful of other US cities, but
they're still an anomaly in most of the world.Part of the reason—possibly the main reason—that startups
have not spread as broadly as the Industrial Revolution did is their
social disruptiveness.  Though it brought many social changes, the
Industrial Revolution was not fighting the principle that bigger
is better.  Quite the opposite: the two dovetailed beautifully.
The new industrial companies adapted the customs of existing large
organizations like the military and the civil service, and the
resulting hybrid worked well.  ""Captains of industry"" issued orders
to ""armies of workers,"" and everyone knew what they were supposed
to do.Startups seem to go more against the grain, socially.  It's hard
for them to flourish in societies that value hierarchy and stability,
just as it was hard for industrialization to flourish in societies
ruled by people who stole at will from the merchant class.  But
there were already a handful of countries past that stage when the
Industrial Revolution happened.   There do not seem to be that many
ready this time.
Notes[1]
One of the bizarre consequences of this model was that the usual
way to make more money was to become a manager.  This is one of the
things startups fix.[2]
There are a lot of reasons American car companies have been
doing so much worse than Japanese car companies, but at least one
of them is a cause for optimism: American graduates have more
options.[3]
It's possible that companies will one day be able to grow big
in revenues without growing big in people, but we are not very far
along that trend yet.[4]
Lecuyer, Christophe, Making Silicon Valley, MIT Press, 2006.Thanks to Trevor Blackwell, Paul Buchheit, Jessica Livingston,
and Robert Morris for reading drafts of this.","technological and societal insights
",human,"human
","human
"
96,96,"October 2015This will come as a surprise to a lot of people, but in some cases
it's possible to detect bias in a selection process without knowing
anything about the applicant pool.  Which is exciting because among
other things it means third parties can use this technique to detect
bias whether those doing the selecting want them to or not.You can use this technique whenever (a) you have at least
a random sample of the applicants that were selected, (b) their
subsequent performance is measured, and (c) the groups of
applicants you're comparing have roughly equal distribution of ability.How does it work?  Think about what it means to be biased.  What
it means for a selection process to be biased against applicants
of type x is that it's harder for them to make it through.  Which
means applicants of type x have to be better to get selected than
applicants not of type x.
[1]
Which means applicants of type x
who do make it through the selection process will outperform other
successful applicants.  And if the performance of all the successful
applicants is measured, you'll know if they do.Of course, the test you use to measure performance must be a valid
one.  And in particular it must not be invalidated by the bias you're
trying to measure.
But there are some domains where performance can be measured, and
in those detecting bias is straightforward. Want to know if the
selection process was biased against some type of applicant?  Check
whether they outperform the others.  This is not just a heuristic
for detecting bias.  It's what bias means.For example, many suspect that venture capital firms are biased
against female founders. This would be easy to detect: among their
portfolio companies, do startups with female founders outperform
those without?  A couple months ago, one VC firm (almost certainly
unintentionally) published a study showing bias of this type. First
Round Capital found that among its portfolio companies, startups
with female founders outperformed
those without by 63%. 
[2]The reason I began by saying that this technique would come as a
surprise to many people is that we so rarely see analyses of this
type.  I'm sure it will come as a surprise to First Round that they
performed one. I doubt anyone there realized that by limiting their
sample to their own portfolio, they were producing a study not of
startup trends but of their own biases when selecting companies.I predict we'll see this technique used more in the future.  The
information needed to conduct such studies is increasingly available.
Data about who applies for things is usually closely guarded by the
organizations selecting them, but nowadays data about who gets
selected is often publicly available to anyone who takes the trouble
to aggregate it.
Notes[1]
This technique wouldn't work if the selection process looked
for different things from different types of applicants—for
example, if an employer hired men based on their ability but women
based on their appearance.[2]
As Paul Buchheit points out, First Round excluded their most 
successful investment, Uber, from the study.  And while it 
makes sense to exclude outliers from some types of studies, 
studies of returns from startup investing, which is all about 
hitting outliers, are not one of them.
Thanks to Sam Altman, Jessica Livingston, and Geoff Ralston for reading
drafts of this.Arabic TranslationSwedish Translation","technological and societal insights
",human,"human
","human
"
97,97,"April 2001This essay developed out of conversations I've had with
several other programmers about why Java smelled suspicious.  It's not
a critique of Java!  It is a case study of hacker's radar.Over time, hackers develop a nose for good (and bad) technology.
I thought it might be interesting to try and write down what
made Java seem suspect to me.Some people who've read this think it's an interesting attempt to write about
something that hasn't been written about before.  Others say I
will get in trouble for appearing to be writing about
things I don't understand.  So, just in
case it does any good, let me clarify that I'm not writing here
about Java (which I have never used) but about hacker's radar
(which I have thought about a lot).The aphorism ""you can't tell a book by its cover"" originated in
the times when books were sold in plain cardboard covers, to be
bound by each purchaser according to his own taste.  In those days,
you couldn't tell a book by its cover.  But publishing has advanced
since then: present-day publishers work hard to make the cover
something you can tell a book by.I spend a lot of time in bookshops and I feel as if I have by now
learned to understand everything publishers mean to tell me about
a book, and perhaps a bit more.  The time I haven't spent in
bookshops I've spent mostly in front of computers, and I feel as
if I've learned, to some degree, to judge technology by its cover
as well.  It may be just luck, but I've saved myself from a few
technologies that turned out to be real stinkers.So far, Java seems like a stinker to me.  I've never written a Java
program, never more than glanced over reference books about it,
but I have a hunch that it won't be a very successful language.
I may turn out to be mistaken; making predictions about technology
is a dangerous business.  But for what it's worth, as a sort of
time capsule, here's why I don't like the look of Java:
1. It has been so energetically hyped.  Real standards don't have
to be promoted.  No one had to promote C, or Unix, or HTML.  A real
standard tends to be already established by the time most people
hear about it.  On the hacker radar screen, Perl is as big as Java,
or bigger, just on the strength of its own merits.2. It's aimed low.  In the original Java white paper, Gosling
explicitly says Java was designed not to be too difficult for
programmers used to C.  It was designed to be another C++: C plus
a few ideas taken from more advanced languages.  Like the creators
of sitcoms or junk food or package tours, Java's designers were
consciously designing a product for people not as smart as them.
Historically, languages designed for other people to use have been
bad:  Cobol, PL/I, Pascal, Ada, C++.  The good languages have been
those that were designed for their own creators:  C, Perl, Smalltalk,
Lisp.3. It has ulterior motives.  Someone once said that the world would
be a better place if people only wrote books because they had
something to say, rather than because they wanted to write a book.
Likewise, the reason we hear about Java all the time is not because
it has something to say about programming languages.  We hear about
Java as part of a plan by Sun to undermine Microsoft.4. No one loves it.  C, Perl, Python, Smalltalk, and Lisp programmers
love their languages.  I've never heard anyone say that they loved
Java.5. People are forced to use it.  A lot of the people I know using
Java are using it because they feel they have to.  Either it's
something they felt they had to do to get funded, or something they
thought customers would want, or something they were told to do by
management.  These are smart people; if the technology was good,
they'd have used it voluntarily.6. It has too many cooks.  The best programming languages have been
developed by small groups.  Java seems to be run by a committee.
If it turns out to be a good language, it will be the first time
in history that a committee has designed a good language.7. It's bureaucratic.  From what little I know about Java, there
seem to be a lot of protocols for doing things.  Really good
languages aren't like that.  They let you do what you want and get
out of the way.8. It's pseudo-hip.  Sun now pretends that Java is a grassroots,
open-source language effort like Perl or Python.  This one just
happens to be controlled by a giant company.  So the language is
likely to have the same drab clunkiness as anything else that comes
out of a big company.9. It's designed for large organizations.  Large organizations have
different aims from hackers. They want languages that are (believed
to be) suitable for use by large teams of mediocre programmers--
languages with features that, like the speed limiters in U-Haul
trucks, prevent fools from doing too much damage.  Hackers don't
like a language that talks down to them.  Hackers just want power.
Historically, languages designed for large organizations (PL/I,
Ada) have lost, while hacker languages (C, Perl) have won.  The
reason: today's teenage hacker is tomorrow's CTO.10. The wrong people like it.  The programmers I admire most are
not, on the whole, captivated by Java.  Who does like Java?  Suits,
who don't know one language from another, but know that they keep
hearing about Java in the press; programmers at big companies, who
are amazed to find that there is something even better than C++;
and plug-and-chug undergrads, who are ready to like anything that
might get them a job (will this be on the test?).  These people's
opinions change with every wind.11. Its daddy is in a pinch.  Sun's business model is being undermined
on two fronts.  Cheap Intel processors, of the same type used in
desktop machines, are now more than fast enough for servers.  And
FreeBSD seems to be at least as good an OS for servers as Solaris.
Sun's advertising implies that you need Sun servers for industrial
strength applications.  If this were true, Yahoo would be first in
line to buy Suns;  but when I worked there, the servers were all
Intel boxes running FreeBSD.  This bodes ill for Sun's future.  If
Sun runs into trouble, they could drag Java down with them.12. The DoD likes it.  The Defense Department is encouraging
developers to use Java. This seems to me the most damning sign of
all.  The Defense Department does a fine (though expensive) job of
defending the country, but they love plans and procedures and
protocols.  Their culture is the opposite of hacker culture; on
questions of software they will tend to bet wrong.  The last time
the DoD really liked a programming language, it was Ada.
Bear in mind, this is not a critique of Java, but a critique of
its cover.  I don't know Java well enough to like it or dislike
it.  This is just an explanation of why I don't find that I'm eager
to learn it.It may seem cavalier to dismiss a language before you've even tried
writing programs in it.  But this is something all programmers have
to do.  There are too many technologies out there to learn them
all.  You have to learn to judge by outward signs which will be
worth your time.  I have likewise cavalierly dismissed Cobol, Ada,
Visual Basic, the IBM AS400, VRML, ISO 9000, the SET protocol, VMS,
Novell Netware, and CORBA, among others.  They just smelled wrong.It could be that in Java's case I'm mistaken.  It could be that a
language promoted by one big company to undermine another, designed
by a committee for a ""mainstream"" audience, hyped to the skies,
and beloved of the DoD, happens nonetheless to be a clean, beautiful,
powerful language that I would love programming in.  It could be,
but it seems very unlikely.Trevor Re: Java's CoverBerners-Lee Re: JavaBeing PopularSun Internal Memo2005: BusinessWeek AgreesJapanese Translation","technological and societal insights
",human,"human
","human
"
98,98,"
March 2006(This essay is derived from a talk at Google.)A few weeks ago I found to my surprise that I'd been granted four patents.  
This was all the more surprising
because I'd only applied for three.  The patents aren't mine, of
course.  They were assigned to Viaweb, and became Yahoo's when they
bought us.  But the news set me thinking about the question of
software patents generally.Patents are a hard problem.  I've had to advise most of the startups
we've funded about them, and despite years of experience I'm still
not always sure I'm giving the right advice.One thing I do feel pretty certain of is that if you're against
software patents, you're against patents in general.  Gradually our
machines consist more and more of software.  Things that used to
be done with levers and cams and gears are now done with loops and
trees and closures.  There's nothing special about physical embodiments
of control systems that should make them patentable, and the software
equivalent not.Unfortunately, patent law is inconsistent on this point.  Patent
law in most countries says that algorithms aren't patentable.  This
rule is left over from a time when ""algorithm"" meant something like
the Sieve of Eratosthenes.  In 1800, people could not see as readily
as we can that a great many patents on mechanical objects were
really patents on the algorithms they embodied.Patent lawyers still have to pretend that's what they're doing when
they patent algorithms.  You must not use the word ""algorithm"" in
the title of a patent application, just as you must not use the
word ""essays"" in the title of a book.  If you want to patent an
algorithm, you have to frame it as a computer system executing that algorithm.
Then it's mechanical; phew.  The default euphemism for algorithm
is ""system and method.""  Try a patent search for that phrase and
see how many results you get.Since software patents are no different from hardware patents,
people who say ""software patents are evil"" are saying simply ""patents
are evil.""  So why do so many people complain about software patents
specifically?I think the problem is more with the patent office than the concept
of software patents.  Whenever software meets government, bad things
happen, because software changes fast and government changes slow.
The patent office has been overwhelmed by both the volume and the
novelty of applications for software patents, and as a result they've
made a lot of mistakes.The most common is to grant patents that shouldn't be granted.  To
be patentable, an invention has to be more than new.  It also has
to be non-obvious.  And this, especially, is where the USPTO has
been dropping the ball. Slashdot has an icon that expresses the
problem vividly: a knife and fork with the words ""patent pending""
superimposed.The scary thing is, this is the only icon they have for
patent stories.  Slashdot readers now take it for granted that a
story about a patent will be about a bogus patent.
That's how bad the problem has become.The problem with Amazon's notorious one-click patent, for example,
is not that it's a software patent, but that it's obvious.  Any
online store that kept people's shipping addresses would have
implemented this.  The reason Amazon did it first was not that they
were especially smart, but because they were one of the earliest
sites with enough clout to force customers to log in before they
could buy something. 
[1]We, as hackers, know the USPTO is letting people patent the knives
and forks of our world.  The problem is, the USPTO are not hackers.
They're probably good at judging new inventions for casting steel
or grinding lenses, but they don't understand software yet.At this point an optimist would be tempted to add ""but they will
eventually.""  Unfortunately that might not be true.  The problem
with software patents is an instance of a more general one: the
patent office takes a while to understand new technology.  If so,
this problem will only get worse, because the rate of technological
change seems to be increasing.  In thirty years, the patent office
may understand the sort of things we now patent as software, but
there will be other new types of inventions they understand even
less.Applying for a patent is a negotiation.  You generally apply for a
broader patent than you think you'll be granted, and the examiners
reply by throwing out some of your claims and granting others.  So
I don't really blame Amazon for applying for the one-click patent.
The big mistake was the patent office's, for not insisting on
something narrower, with real technical content.  By granting such
an over-broad patent, the USPTO in effect slept with Amazon on the
first date.  Was Amazon supposed to say no?Where Amazon went over to the dark side was not in applying for the
patent, but in enforcing it.  A lot of companies (Microsoft, for
example) have been granted large numbers of preposterously over-broad
patents,  but they keep them mainly for defensive purposes.  Like
nuclear weapons, the main role of big companies' patent portfolios
is to threaten anyone who attacks them with a counter-suit.  Amazon's
suit against Barnes & Noble was thus the equivalent of a nuclear
first strike.That suit probably hurt Amazon more than it helped them.  Barnes &
Noble was a lame site; Amazon would have crushed them anyway.  To
attack a rival they could have ignored, Amazon put a lasting black
mark on their own reputation.  Even now I think if you asked hackers
to free-associate about Amazon, the one-click patent would turn up
in the first ten topics.Google clearly doesn't feel that merely holding patents is evil.
They've applied for a lot of them.  Are they hypocrites?  Are patents
evil?There are really two variants of that question, and people answering
it often aren't clear in their own minds which they're answering.
There's a narrow variant: is it bad, given the current legal system,
to apply for patents? and also a broader one: is it bad that the
current legal system allows patents?These are separate questions.  For example, in preindustrial societies
like medieval Europe, when someone attacked you, you didn't call
the police.  There were no police.  When attacked, you were supposed
to fight back, and there were conventions about how to do it.  Was
this wrong?  That's two questions: was it wrong to take justice
into your own hands, and was it wrong that you had to?  We tend to
say yes to the second, but no to the first.  If no one else will
defend you, you have to defend yourself.  
[2]The situation with patents is similar.  Business is a kind of
ritualized warfare.  Indeed, it evolved from actual warfare: most
early traders switched on the fly from merchants to pirates depending
on how strong you seemed.  In business there are certain rules
describing how companies may and may not compete with one another,
and someone deciding that they're going to play by their own rules
is missing the point.  Saying ""I'm not going to apply for patents
just because everyone else does"" is not like saying ""I'm not going
to lie just because everyone else does.""  It's more like saying
""I'm not going to use TCP/IP just because everyone else does."" Oh
yes you are.A closer comparison might be someone seeing a hockey game for the
first time, realizing with shock that the players were deliberately
bumping into one another, and deciding that one would on no account
be so rude when playing hockey oneself.Hockey allows checking.  It's part of the game.  If your team refuses
to do it, you simply lose.  So it is in business.  Under the present
rules, patents are part of the game.What does that mean in practice?  We tell the startups we fund not
to worry about infringing patents, because startups rarely get sued
for patent infringement.  There are only two reasons someone might
sue you: for money, or to prevent you from competing with them.
Startups are too poor to be worth suing for money. And in practice
they don't seem to get sued much by competitors, either.  They don't
get sued by other startups because (a) patent suits are an expensive
distraction, and (b) since the other startups are as young as they
are, their patents probably haven't issued yet. 
[3]
Nor do startups,
at least in the software business, seem to get sued much by established
competitors.  Despite all the patents Microsoft holds, I don't know
of an instance where they sued a startup for patent infringement.
Companies like Microsoft and Oracle don't win by winning lawsuits.
That's too uncertain.  They win by locking competitors out of their
sales channels.  If you do manage to threaten them, they're more
likely to buy you than sue you.When you read of big companies filing patent suits against smaller
ones, it's usually a big company on the way down, grasping at
straws.  For example, Unisys's attempts to enforce their patent on
LZW compression.  When you see a big company threatening patent
suits, sell.  When a company starts fighting over IP, it's a sign
they've lost the real battle, for users.A company that sues competitors for patent infringement is like
a defender who has been beaten so thoroughly that he turns to plead
with the referee.  You don't do that if you can still reach the
ball, even if you genuinely believe you've been fouled.  So a company
threatening patent suits is a company in trouble.When we were working on Viaweb, a bigger company in the e-commerce
business was granted a patent on online ordering, or something like
that.  I got a call from a VP there asking if we'd like to license
it.  I replied that I thought the patent was completely bogus, and
would never hold up in court.  ""Ok,"" he replied.  ""So, are you guys
hiring?""If your startup grows big enough, however, you'll start to get sued,
no matter what you do.  If you go public, for example, you'll be
sued by multiple patent trolls who hope you'll pay them off to go
away.  More on them later.In other words, no one will sue you for patent infringement till
you have money, and once you have money, people will sue you whether
they have grounds to or not.  So I advise fatalism.  Don't waste
your time worrying about patent infringement.  You're probably
violating a patent every time you tie your shoelaces.  At the start,
at least, just worry about making something great and getting lots
of users.  If you grow to the point where anyone considers you worth
attacking, you're doing well.We do advise the companies we fund to apply for patents, but not
so they can sue competitors.  Successful startups either get bought
or grow into big companies.  If a startup wants to grow into a big
company, they should apply for patents to build up the patent
portfolio they'll need to maintain an armed truce with other big
companies.  If they want to get bought, they should apply for patents
because patents are part of the mating dance with acquirers.Most startups that succeed do it by getting bought, and most acquirers
care about patents.  Startup acquisitions are usually a build-vs-buy
decision for the acquirer.  Should we buy this little startup or
build our own?  And two things, especially, make them decide not
to build their own: if you already have a large and rapidly growing
user base, and if you have a fairly solid patent application on
critical parts of your software.There's a third reason big companies should prefer buying to building:
that if they built their own, they'd screw it up.  But few big
companies are smart enough yet to admit this to themselves.  It's
usually the acquirer's engineers who are asked how hard it would
be for the company to build their own, and they overestimate their
abilities.  
[4]
A patent seems to change the balance.  It gives the
acquirer an excuse to admit they couldn't copy what you're doing.
It may also help them to grasp what's special about your technology.Frankly, it surprises me how small a role patents play in the
software business.  It's kind of ironic, considering all the dire
things experts say about software patents stifling innovation, but
when one looks closely at the software business, the most striking
thing is how little patents seem to matter.In other fields, companies regularly sue competitors for patent
infringement.  For example, the airport baggage scanning business
was for many years a cozy duopoly shared between two companies,
InVision and L-3.  In 2002 a startup called Reveal appeared, with
new technology that let them build scanners a third the size.  They
were sued for patent infringement before they'd even released a
product.You rarely hear that kind of story in our world.  The one example
I've found is, embarrassingly enough, Yahoo, which filed a patent
suit against a gaming startup called Xfire in 2005.  Xfire doesn't
seem to be a very big deal, and it's hard to say why Yahoo felt
threatened.  Xfire's VP of engineering had worked at Yahoo on similar
stuff-- in fact, he was listed as an inventor on the patent Yahoo
sued over-- so perhaps there was something personal about it.  My
guess is that someone at Yahoo goofed.  At any rate they didn't
pursue the suit very vigorously.Why do patents play so small a role in software?  I can think of
three possible reasons.One is that software is so complicated that patents by themselves
are not worth very much.  I may be maligning other fields here, but
it seems that in most types of engineering you can hand the details
of some new technique to a group of medium-high quality people and
get the desired result.  For example, if someone develops a new
process for smelting ore that gets a better yield, and you assemble
a team of qualified experts and tell them about it, they'll be able
to get the same yield.  This doesn't seem to work in software.
Software is so subtle and unpredictable that ""qualified experts""
don't get you very far.That's why we rarely hear phrases like ""qualified expert"" in the
software business.  What that level of ability can get you is, say,
to make your software compatible with some other piece of software--
in eight months, at enormous cost.  To do anything harder you need
individual brilliance.  If you assemble a team of qualified experts
and tell them to make a new web-based email program, they'll get
their asses kicked by a team of inspired nineteen year olds.Experts can implement, but they can't design.
Or rather, expertise in implementation is the only kind most people,
including the experts themselves, can measure. 
[5]But design is a definite skill.  It's not just an airy intangible.
Things always seem intangible when you don't understand them.
Electricity seemed an airy intangible to most people in 1800.  Who
knew there was so much to know about it?  So it is with design.
Some people are good at it and some people are bad at it, and there's
something very tangible they're good or bad at.The reason design counts so much in software is probably that there
are fewer constraints than on physical things.  Building physical
things is expensive and dangerous.  The space of possible choices
is smaller; you tend to have to work as part of a larger group; and
you're subject to a lot of regulations.  You don't have any of that
if you and a couple friends decide to create a new web-based
application.Because there's so much scope for design in software, a successful
application tends to be way more than the sum of its patents.  What
protects little companies from being copied by bigger competitors
is not just their patents, but the thousand little things the big
company will get wrong if they try.The second reason patents don't count for much in our world is that
startups rarely attack big companies head-on, the way Reveal did.
In the software business, startups beat established companies by
transcending them.  Startups don't build desktop word processing
programs to compete with Microsoft Word. 
[6]
They build Writely.
If this paradigm is crowded, just wait for the next one; they run
pretty frequently on this route.Fortunately for startups, big companies are extremely good at denial.
If you take the trouble to attack them from an oblique angle, they'll
meet you half-way and maneuver to keep you in their blind spot.  To
sue a startup would mean admitting it was dangerous, and that often
means seeing something the big company doesn't want to see.  IBM
used to sue its mainframe competitors regularly, but they didn't
bother much about the microcomputer industry because they didn't
want to see the threat it posed.  Companies building web based apps
are similarly protected from Microsoft, which even now doesn't want
to imagine a world in which Windows is irrelevant.The third reason patents don't seem to matter very much in software
is public opinion-- or rather, hacker opinion.  In a recent interview,
Steve Ballmer coyly left open the possibility of attacking Linux
on patent grounds.  But I doubt Microsoft would ever be so stupid.
They'd face the mother of all boycotts.  And not just from the
technical community in general; a lot of their own people would
rebel.Good hackers care a lot about matters of principle, and they are
highly mobile.  If a company starts misbehaving, smart people won't
work there.  For some reason this seems to be more true in software
than other businesses.  I don't think it's because hackers have
intrinsically higher principles so much as that their skills are
easily transferrable.  Perhaps we can split the difference and say
that mobility gives hackers the luxury of being principled.Google's ""don't be evil"" policy may for this reason be the most
valuable thing they've discovered.  It's very constraining in some
ways.  If Google does do something evil, they get doubly whacked
for it: once for whatever they did, and again for hypocrisy.  But
I think it's worth it.  It helps them to hire the best people, and
it's better, even from a purely selfish point of view, to be
constrained by principles than by stupidity.(I wish someone would get this point across to the present
administration.)I'm not sure what the proportions are of the preceding three
ingredients, but the custom among the big companies seems to be not
to sue the small ones, and the startups are mostly too busy and too
poor to sue one another.  So despite the huge number of software
patents there's not a lot of suing going on.  With one exception:
patent trolls.Patent trolls are companies consisting mainly of lawyers whose whole
business is to accumulate patents and threaten to sue companies who
actually make things.  Patent trolls, it seems safe to say, are
evil.  I feel a bit stupid saying that, because when you're saying
something that Richard Stallman and Bill Gates would both agree
with, you must be perilously close to tautologies.The CEO of Forgent, one of the most notorious patent trolls, says
that what his company does is ""the American way."" Actually that's
not true. The American way is to make money by creating wealth, not by suing people. 
[7]
What companies like Forgent do is actually the proto-industrial
way.  In the period just before the industrial revolution, some of
the greatest fortunes in countries like England and France were
made by courtiers who extracted some lucrative right from the crown--
like the right to collect taxes on the import of silk-- and then
used this to squeeze money from the merchants in that business.  So
when people compare patent trolls to the mafia, they're more right
than they know, because the mafia too are not merely bad, but bad
specifically in the sense of being an obsolete business model.Patent trolls seem to have caught big companies by surprise.  In
the last couple years they've extracted hundreds of millions of
dollars from them.  Patent trolls are hard to fight precisely because
they create nothing.  Big companies are safe from being sued by
other big companies because they can threaten a counter-suit.  But
because patent trolls don't make anything, there's nothing they can
be sued for.  I predict this loophole will get closed fairly quickly,
at least by legal standards.  It's clearly an abuse of the system,
and the victims are powerful.
[8]But evil as patent trolls are, I don't think they hamper innovation
much.  They don't sue till a startup has made money, and by that
point the innovation that generated it has already happened.  I
can't think of a startup that avoided working on some problem because
of patent trolls.So much for hockey as the game is played now.  What about the more
theoretical question of whether hockey would be a better game without
checking?  Do patents encourage or discourage innovation?This is a very hard question to answer in the general case.  People
write whole books on the topic.  One of my main hobbies is the
history of technology, and even though I've studied the subject for
years, it would take me several weeks of research to be able to say
whether patents have in general been a net win.One thing I can say is that 99.9% of the people who express opinions
on the subject do it not based on such research, but out of a kind
of religious conviction.  At least, that's the polite way of putting
it; the colloquial version involves speech coming out of organs not
designed for that purpose.Whether they encourage innovation or not, patents were at least
intended to.  You don't get a patent for nothing.  In return for
the exclusive right to use an idea, you have to publish it,
and it was largely to encourage such openness that patents were
established.Before patents, people protected ideas by keeping them secret.  With
patents, central governments said, in effect, if you tell everyone
your idea, we'll protect it for you.  There is a parallel here to
the rise of civil order, which happened at roughly the same time.
Before central governments were powerful enough to enforce order,
rich people had private armies.  As governments got more powerful,
they gradually compelled magnates to cede most responsibility for
protecting them.  (Magnates still have bodyguards, but no longer
to protect them from other magnates.)Patents, like police, are involved in many abuses.  But in both
cases the default is something worse. The choice is not ""patents
or freedom?"" any more than it is ""police or freedom?"" The actual
questions are respectively ""patents or secrecy?"" and ""police or
gangs?""As with gangs, we have some idea what secrecy would be like, because
that's how things used to be.  The economy of medieval Europe was
divided up into little tribes, each jealously guarding their
privileges and secrets.  In Shakespeare's time, ""mystery"" was
synonymous with ""craft.""   Even today we can see an echo of the
secrecy of medieval guilds, in the now pointless secrecy of the
Masons.The most memorable example of medieval industrial secrecy is probably
Venice, which forbade glassblowers to leave the city, and sent
assassins after those who tried.  We might like to think we wouldn't
go so far, but the movie industry has already tried to pass laws
prescribing three year prison terms just for putting movies on
public networks.  Want to try a frightening thought experiment? If
the movie industry could have any law they wanted, where would they
stop?  Short of the death penalty, one assumes, but how close would
they get?Even worse than the spectacular abuses might be the overall decrease
in efficiency that would accompany increased secrecy.  As anyone
who has dealt with organizations that operate on a ""need to know""
basis can attest, dividing information up into little cells is
terribly inefficient.  The flaw in the ""need to know"" principle is
that you don't know who needs to know something.  An idea
from one area might spark a great discovery in another.  But the
discoverer doesn't know he needs to know it.If secrecy were the only protection for ideas, companies wouldn't
just have to be secretive with other companies; they'd have to be
secretive internally.  This would encourage what is already the
worst trait of big companies.I'm not saying secrecy would be worse than patents, just that we
couldn't discard patents for free.  Businesses would become more
secretive to compensate, and in some fields this might get ugly.
Nor am I defending the current patent system.  There is clearly a
lot that's broken about it.   But the breakage seems to affect
software less than most other fields.In the software business I know from experience whether patents
encourage or discourage innovation, and the answer is the type that
people who like to argue about public policy least like to hear:
they don't affect innovation much, one way or the other.  Most
innovation in the software business happens in startups, and startups
should simply ignore other companies' patents.  At least, that's
what we advise, and we bet money on that advice.The only real role of patents, for most startups, is as an element
of the mating dance with acquirers.  There patents do help a little.
And so they do encourage innovation indirectly, in that they give
more power to startups, which is where, pound for pound, the most
innovation happens.  But even in the mating dance, patents are of
secondary importance.  It matters more to make something great and
get a lot of users.Notes[1]
You have to be careful here, because a great discovery often
seems obvious in retrospect.  One-click ordering, however, is not
such a discovery.[2]
""Turn the other cheek"" skirts the issue; the critical question
is not how to deal with slaps, but sword thrusts.[3]
Applying for a patent is now very slow, but it might actually
be bad if that got fixed.  At the moment the time it takes to get
a patent is conveniently just longer than the time it takes a startup
to succeed or fail.[4]
Instead of the canonical ""could you build this?"" maybe the corp
dev guys should be asking ""will you build this?"" or even ""why haven't
you already built this?""[5]
Design ability is so hard to measure that you can't even trust
the design world's internal standards.  You can't assume that someone
with a degree in design is any good at design, or that an eminent
designer is any better than his peers.  If that worked, any company
could build products as good as Apple's just by hiring 
sufficiently qualified designers.[6]
If anyone wanted to try, we'd be interested to hear from them.
I suspect it's one of those things that's not as hard as everyone
assumes.[7]
Patent trolls can't even claim, like speculators, that they
""create"" liquidity.[8]
If big companies don't want to wait for the government to take
action, there is a way to fight back themselves.  For a long time
I thought there wasn't, because there was nothing to grab onto.
But there is one resource patent trolls need: lawyers.  Big technology
companies between them generate a lot of legal business.  If they
agreed among themselves never to do business with any firm employing
anyone who had worked for a patent troll, either as an employee or
as outside counsel, they could probably starve the trolls of the
lawyers they need.Thanks to Dan Bloomberg, Paul Buchheit, Sarah Harlin, 
Jessica Livingston, and Peter Norvig
for reading drafts of this, to Joel Lehrer and Peter Eng for answering
my questions about patents, and to Ankur Pansari for inviting me
to speak.Japanese Translation","technological and societal insights
",human,"human
","human
"
99,99,"The Myth of the Linear Life

Many assume life follows a predictable trajectory: school, job, marriage, kids, retirement.  A neat, upward-sloping line on a graph.  This is a delusion.  Life is more accurately represented by a chaotic scattering of points, connected by jagged, unpredictable lines.

The problem with the linear model is it implies a destination.  A point at the end of the line where you’ve “made it.”  This is a dangerous fantasy, because it encourages passive acceptance of the present.  Why struggle, why push boundaries, if the destination is preordained?

The truth is, there is no destination.  There are only experiences, some satisfying, some less so.  Each experience, whether a resounding success or a crushing failure, alters the trajectory. It creates a new point, a new direction, a new set of possibilities.

Embrace the chaos.  Don’t fight the jagged lines.  The beauty of a life less ordinary lies in its unpredictability.  It’s in the unexpected detours, the unplanned encounters, the sudden shifts in direction, that we discover who we truly are and what we're capable of.

Those who cling to the linear model often find themselves trapped.  Disappointed by the lack of progress, resentful of unmet expectations.  They've mistaken the map for the territory, a rigid framework for the dynamic reality of lived experience.

The most successful people I’ve known aren't the ones who followed a pre-set path. They're the ones who embraced change, who experimented, who dared to deviate. They learned to navigate the chaotic scattering of points, to make the best of each experience, to use each failure as a stepping stone to something new.

So, ditch the linear illusion.  Embrace the jagged lines. Your life, in all its unpredictable glory, is waiting to be written.
","life advice
",AI,"human
","AI
"
100,100,"The Subtle Art of Not Giving a Fuck About Productivity

The modern obsession with productivity is a plague.  We’re told to optimize every minute, to relentlessly pursue quantifiable results, to treat our lives like a well-oiled machine churning out maximum output.  This is bullshit.

The truly productive aren't busy; they're focused.  They're not slaves to their to-do lists; they're masters of their own attention. They understand a crucial truth:  the most valuable work often emerges not from relentless striving, but from a deep, almost reckless, engagement with what genuinely captivates them.

Forget the self-help gurus and their productivity hacks.  They’re selling you a lie, a gilded cage of endless optimization.  Real progress comes from cultivating a powerful, almost obsessive focus on a few genuinely important things, and ignoring the rest.

This isn't laziness; it's strategic neglect.  It’s the art of saying “no” – not to opportunities, but to distractions. It's about ruthlessly prioritizing the projects and pursuits that ignite your curiosity, and letting the rest fall by the wayside.

Many of history’s greatest achievements weren’t born from meticulously planned schedules, but from unexpected detours, from following a compelling thread of inquiry wherever it led.  Think of Newton's obsession with alchemy, seemingly a waste of time, yet perhaps a crucial stepping stone to his breakthroughs in physics.

The key is discernment.  Not all obsessions are created equal.  A passion for collecting stamps is… fine. A consuming interest in solving a complex mathematical problem – that's different.   The latter has the potential to reshape the world; the former, not so much.

Develop the ability to distinguish between the truly significant and the merely diverting. Cultivate an unwavering commitment to your most important projects.  This requires discipline, yes, but more importantly, a deep and abiding interest that makes the work itself its own reward.

Don't fall prey to the productivity trap.  Focus your energy, not on appearing busy, but on actually accomplishing things that matter.  Let your curiosity be your guide. The rest is noise.
","life advice
",AI,"human
","AI
"
101,101,"The Subtle Art of Not Giving a Fuck (About Your Possessions)

Most people accumulate possessions.  They fill their houses, their garages, their lives with stuff.  This isn't inherently bad, of course.  But like most things, it’s a matter of scale.  A few carefully chosen tools are a blessing.  A mountain of junk, a curse.

The shift happened subtly.  For generations, possessions represented hard-won value.  A well-made chair, a sturdy table – these were investments, symbols of accomplishment.  Now?  Mass production and cheap shipping mean the cost of acquiring “stuff” is effectively zero.  The psychological cost, however, is anything but.

The problem isn't simply the clutter. It’s the mental overhead. Every item demands a tiny sliver of your cognitive energy: to remember where it is, what it’s for, how to maintain it.  This low-grade cognitive drain accumulates, leaving you less energy for things that truly matter.

Consider the energy expenditure of a cluttered room.  Your mind struggles to parse the visual chaos, a constant low-level stress.  This is why minimalism isn’t simply about aesthetics; it’s a strategy for reclaiming mental resources.

Think of your possessions as a portfolio.  Do you regularly review it, selling off underperformers?  Likely not.  Most people treat possessions passively, accumulating regardless of utility.

The solution isn't radical asceticism. It's a ruthless prioritization.  Before acquiring anything, ask: ""Will this materially improve my life?  Will I use this consistently, not just occasionally?""  If the answer is no, then it's not worth the psychological and physical space it occupies.

This is not about becoming a monk; it's about a simple calculation: value versus cost.  The cost isn't just monetary.  It’s the cost in time, energy, and mental clutter.

What's the return on investment of that rarely-used kitchen gadget?  Probably less than the opportunity cost of the space it takes up, the time it takes to clean, the mental burden it imposes.

This isn't about becoming a minimalist, exactly.  It's about making conscious choices about your possessions, treating them with a far more critical eye than most do.  Focus on experiences, not things.  Invest in what genuinely adds value to your life, not just fills space.  The difference is significant.  Surprisingly significant.
","life advice
",AI,"human
","human
"
102,102,"The Tyranny of the To-Do List

Most advice on productivity focuses on the wrong thing:  checking items off a to-do list. This is, frankly, a recipe for mediocrity.  The truly ambitious don't conquer their to-do lists; they transcend them.  They work on what matters, not what's scheduled.

The problem with to-do lists is their inherent tyranny.  They fragment your time, forcing you into small, discrete units of work—the equivalent of chopping a redwood into kindling.  Real breakthroughs, the kind that leave a lasting impact, require sustained focus, long stretches of uninterrupted thought.  They demand you ignore the insistent chirping of less important tasks.

Think of the great minds of history. Do you envision them meticulously crossing off items?  I doubt it.  Newton wasn't obsessed with laundry; Darwin didn't fret over his email.  They were consumed by their work, driven by a larger purpose, a singular focus that rendered minor details insignificant.  They weren't managing their time; they were *mastering* their work.

This isn't to say that organization is unimportant.  Rather, it's about prioritizing correctly.  Focus should be on the *what*, not the *when*.  What is the single most important thing you could be doing right now?  What project, if completed, would have the largest impact?  That's where your energy should go, regardless of your carefully crafted schedule.

The key is to cultivate a ruthless disregard for the trivial.  Learn to say ""no"" to the endless stream of minor requests, the petty interruptions, the distractions that consume your day.  These are the parasites feeding on your potential, draining your energy, and diverting you from what truly matters.

The path to significant achievement isn't about efficiency; it's about impact.  It's about identifying the truly important problems and dedicating yourself to their solution.  Let the to-do list wither; cultivate instead an unwavering focus on your most ambitious goals.  Only then will you escape the tyranny of the mundane and unlock your true potential.
","life advice
",AI,"human
","AI
"
103,103,"The Joys of Being a Noob

I've always admired the confident air of mastery some people project.  The effortless way they navigate complex situations, the seemingly intuitive grasp of arcane knowledge.  It's a seductive image, and for years, I chased it. I aimed for expertise, for the feeling of complete competence.  The illusion of having it all figured out.

But the older I get, the more I realize that feeling like a noob—a complete and utter beginner—is not only unavoidable, but actually beneficial.  It's a sign, not of failure, but of exploration.

The irony is palpable.  We strive for expertise, yet find ourselves consistently confronted with new challenges, new territories where our existing knowledge is useless. We feel the sting of incompetence, the awkwardness of asking ""stupid"" questions, the frustration of stumbling over basic concepts.

Consider this:  the more specialized your knowledge, the more acutely you'll feel like a noob when confronted with something outside your narrow domain. A renowned physicist might feel profoundly ignorant when navigating the intricacies of, say, contemporary art.  And yet, this very feeling is a catalyst for learning.  It's the discomfort of the unknown that drives us to seek understanding.

Think of it as a gradient.  The intensity of the ""noob"" feeling is inversely proportional to your overall understanding of the world.  The deeper your knowledge, the more often you’ll encounter domains where you're a complete novice.  And that's a good thing.  It means you’re expanding your horizons, stretching beyond the confines of your comfort zone.

Why, then, do we recoil from this feeling?  Evolutionarily speaking, it made sense to prioritize competence in familiar tasks.  In a stable environment, mastering existing skills was paramount.  But today's world is anything but stable.  The rate of technological and societal change necessitates a constant state of learning, of embracing the unknown.

Our aversion to the noob state is a relic of the past, a vestigial instinct that no longer serves us.  It's time to reframe our relationship with ignorance.  Embrace the feeling of being lost, of being out of your depth.  It's a sign that you're on the verge of discovery, that you’re growing, that you’re expanding your understanding of the world.  And that, in itself, is a powerful reward.  It's the ultimate form of competence—the capacity for continuous learning.  So, the next time you find yourself feeling like a noob, don't despair.  Celebrate it.
","life advice
",AI,"human
","human
"
104,104,"The Cult of Busyness: A Waste of Talent

The modern world worships busyness.  We wear our packed schedules like badges of honor, boasting of sleep deprivation and overflowing inboxes.  But is this frantic activity a sign of success, or a symptom of something far more insidious?  I argue the latter.  True productivity isn't measured in hours worked, but in the impact of those hours.  And the cult of busyness actively sabotages impact.

Consider the average ""productive"" day.  Emails are checked incessantly, meetings dominate the calendar, and urgent but ultimately trivial tasks consume the majority of mental energy.  This is not efficiency; it's a carefully cultivated illusion of progress.  It's the equivalent of running on a treadmill – intense exertion with negligible advancement.

The problem isn't a lack of willpower; it's a misallocation of priorities.  We're trained from a young age to value activity over achievement, to equate motion with progress.  This ingrained mindset makes it incredibly difficult to identify and focus on the truly impactful tasks – the projects that genuinely move the needle.

How many truly significant inventions were born from a meticulously planned schedule?  How many groundbreaking discoveries resulted from a perfectly organized to-do list?  Innovation thrives in the messy spaces between planned activities; in moments of unexpected inspiration and insightful reflection, often sparked by seemingly unproductive periods.

The solution isn't to become a slacker.  It's to cultivate a more discerning approach to work.  Learn to distinguish between the truly important and the merely urgent.  Master the art of saying ""no"" to distractions and requests that pull you away from your core objectives.  Embrace the power of focused, uninterrupted work, even if it means sacrificing the illusion of constant activity.

This shift in mindset requires conscious effort.  You must actively train yourself to resist the siren song of busyness.  Schedule dedicated ""thinking time"" free from distractions.  Prioritize deep work over shallow tasks.  Measure your success not by the number of emails answered, but by the meaningful progress made towards your most ambitious goals.

The alternative – a life consumed by the illusion of busyness – is a slow form of self-sabotage.  It's a subtle trap that leads to exhaustion, frustration, and ultimately, unfulfilled potential.  Break free from the cult of busyness.  Embrace the power of focused intention.  The rewards, I assure you, are immeasurable.
","life advice
",AI,"human
","AI
"
105,105,"The Unnaturalness of Normal Jobs

Do what you love.  It sounds simple, almost cliché.  But the path to loving your work is far from straightforward, especially in the modern world.  Many of us have absorbed, from childhood, the idea that work is inherently tedious, a necessary evil endured to earn the fleeting pleasures of leisure. This is a profound misunderstanding.

The problem, I posit, lies in the scale.  Humans, like other animals, evolved to work in relatively small groups.  Hunter-gatherer societies, for example, typically operated within social structures that rarely exceeded a few dozen individuals.  This is reflected in the research on optimal group size for efficient collaboration—think 8, maybe 20 at most.  Yet the vast majority of salaried workers find themselves in organizations numbering in the hundreds, or even thousands.

The organizational response to this innate human limitation is the hierarchical tree structure: a pyramid of management.  This structure, while ostensibly designed to improve coordination, introduces a critical flaw.  As you move up the hierarchy, each managerial layer represents a larger and larger aggregation of individual effort, transforming a collection of people into a single, artificial entity.  This inherent compression of individual agency is precisely what stifles creativity and motivation.

The result is a perverse situation.  Workers find themselves in teams of a seemingly appropriate size (8-10 people), yet the larger structure exerts a relentless pressure to conform, to suppress individual initiative.  This isn't a conspiracy; it's a systemic consequence of scale.  Your boss, in turn, is constrained by the larger structure above, so they too are forced to operate within constraints that limit individual contribution.

Consider the analogy of food.  The abundance of cheap, processed foods reflects an economy of scale—it's far easier to produce and market, hence more affordable, than fresh, healthy alternatives.   The immediate gratification of a sugary snack trumps the long-term benefits of balanced nutrition.  Similarly, the readily available, well-marketed ""normal"" job—the secure corporate position with a high starting salary—often overshadows the intellectual starvation it creates.

The most acute victims of this systemic unnaturalness are programmers.  The very essence of programming lies in the creation of something new. Yet the constraints of large organizations actively discourage this natural tendency.  Legacy code, bureaucratic processes, and rigid interfaces stand as roadblocks, restricting the programmer's ability to explore and innovate.

So what's the solution?  For individuals, the answer lies in pursuing smaller-scale opportunities.  Working for a small company or, ideally, starting your own business empowers you to reclaim your natural working style. While starting a startup is undoubtedly stressful, it’s the kind of stress akin to a vigorous workout: challenging, yes, but ultimately fulfilling and life-affirming.

Large organizations will always struggle with this inherent conflict between human nature and scale.  They can attempt to mitigate the problem by carefully structuring their teams and fostering a culture of innovation. But the fundamental limitation imposed by the pyramid structure will remain.  Ultimately, the most effective strategy for both individuals and organizations is to embrace smaller scale.  This means hiring exceptional talent and allowing for greater autonomy within the work itself.  Only then can we break free from the unnaturally constrained work environment that has become so commonplace, and restore the natural human drive to create.
","career advice
",AI,"human
","human
"
106,106,"The Perils and Promises of Passion Projects: A Pragmatic Approach to Career Fulfillment

The conventional wisdom—follow your passion!—is, like most conventional wisdom, simplistic.  It ignores the crucial context:  what are you passionate *about*, and what are the practical implications of pursuing that passion?  This isn't about choosing between starving artist and corporate drone; it's about navigating the nuanced terrain between genuine interest and sustainable livelihood.

Many, perhaps most, successful individuals pursued their passions, but their passions weren't arbitrarily chosen; they were often oddly specific, even counterintuitive.  Bill Gates wasn't simply passionate about programming; he was obsessed with creating software *for customers*. That seemingly small distinction is the difference between a hobby and a billion-dollar empire.  His passion wasn't just coding; it was a particular application of coding, fueled by a deeply unusual set of interests.

The pursuit of immense wealth—hundreds of millions or billions— presents a unique twist.  In this realm, following your passion often becomes strategically advantageous.  Not because of increased motivation, but because the path to such riches often lies in creating something genuinely novel, and the most fertile ground for such novelty is precisely where your genuine passions lie.  The best startup ideas often emerge from projects undertaken initially for sheer enjoyment.

But what if your passions are nebulous, your interests diffuse?  The answer, paradoxically, is to embrace this uncertainty.  The inability to choose between passion and practicality stems from a lack of information.  You don’t know yourself, the nature of different fields, or your potential within them.  The solution?  Experiment.  Work.  Explore.  Try things.  Treat your early career as a series of informed guesses, iteratively refining your understanding of your capabilities and inclinations.

Choosing a field isn’t a decision; it’s a process of discovery.  Start small, build, iterate.  Learn to evaluate the quality of your potential colleagues— your future collaborators and mentors. Who do you want to become like? Choose fields and projects that would lead to such associations.

And remember this: the true measure of success isn’t just financial wealth or external validation, but the consistent, almost obsessive pursuit of intellectually honest work.   The deepest fulfillment comes from working intensely on things that genuinely excite you, things that challenge you, things that—in their inherent difficulty—provide a refuge from the mundane.

Don't let fear of failure paralyze you.  Embrace calculated risks.  The path to remarkable achievement rarely resembles a straight line; it’s a meandering journey driven by intense curiosity and a willingness to adapt, to iterate, to pursue even the most unconventional of passions.  The rewards may be unexpected, but they are likely to be far richer than anything achieved through calculated pragmatism alone.
","career advice
",AI,"human
","human
"
107,107,"The Maker's Schedule, the Manager's Career

The conventional wisdom about careers is, to put it mildly, misguided.  It's a recipe for mediocrity, a path paved with the well-trodden stones of conformity.  The advice typically dispensed – climb the corporate ladder, secure a prestigious title, maximize your salary – is fundamentally flawed for those who seek something more than comfortable stagnation.

This isn't about rejecting ambition; it's about redefining it.  True ambition isn't about climbing a pre-ordained hierarchy; it's about building something new, something significant.  It’s about mastering your craft, and not merely accumulating credentials.

The problem with the “Manager’s Career” – the typical corporate trajectory – is that it’s fundamentally incompatible with deep work. Managers are slaves to meetings, to interruptions, to the tyranny of the urgent.  They're forced to operate on the ""Manager's Schedule,"" a fragmented, reactive pattern that stifles creativity and deep thought.  The Maker, on the other hand, operates on a different schedule entirely.  They carve out large blocks of uninterrupted time, the kind necessary for focused, sustained effort.

This distinction has profound implications for career choices. If your goal is genuine mastery, if you aspire to create something of lasting value, you must prioritize the Maker's Schedule.  This requires a conscious rejection of the corporate allure. It may mean forgoing the predictable climb for the unpredictable but potentially far more rewarding path of independent work, entrepreneurial ventures, or even specialized roles within smaller, less hierarchical organizations.

Choosing this path isn't easy.  It demands discipline, self-reliance, and a willingness to tolerate uncertainty.  There will be sacrifices; the stable paycheck, the predictable promotions, the comfortable routine of the corporate world may seem distant and unattainable. But the rewards, for those with the vision and the fortitude to pursue them, are far greater.

The “Manager’s Career” offers security; the “Maker’s Career” offers fulfillment. One offers a predictable path up an established ladder; the other offers the freedom to build your own, infinitely higher and more rewarding, ladder.  Choose wisely.  Your future self will thank you.
","career advice
",AI,"human
","human
"
108,108,"The Most Important Thing They Didn't Teach You in School

Most schools focus on teaching you facts.  That's fine, to a point. But there's a crucial distinction between knowing things and *knowing how to think*. And this is something they rarely, if ever, explicitly address.  It’s a difference that will determine your success, or lack thereof, far more than any specific skill learned in a classroom.

This isn't about some esoteric wisdom. It’s about a simple, fundamental truth: the kind of work that best suits you depends profoundly on whether you're naturally inclined toward independent or conventional thinking.

Independent thinkers thrive in fields demanding originality.  Science, venture capital, starting a company—these reward unconventional insights, the ability to see what others miss.  You’re not just trying to be right; you’re trying to be uniquely right, to forge a path no one else has taken.  Think of the early days of the internet.  Almost everyone thought the idea of selling books online was ludicrous.


Conventional thinkers, on the other hand, flourish in environments valuing accuracy and established procedures.  Corporate management, for example, rewards competence within a defined framework.  There's value in expertise, in executing efficiently within existing systems.


This isn't a value judgment. Both types of thinking are essential, and the world needs both independent and conventional thinkers.  The problem arises when people are mismatched to their work.  A brilliant, unconventional thinker trapped in a bureaucratic role will be stifled; a methodical, detail-oriented person attempting to break new ground will likely falter.

The tragedy is, many people don’t even realize this fundamental truth about themselves until they're deeply unhappy in their careers. They mistakenly believe anyone can be successful in any field if they just ""work hard enough.""  This is demonstrably false.  

So how do you figure out which path is right for you?  There's no magic test.  But consider this: do you find yourself consistently questioning assumptions, seeking novel approaches, and even enjoying being wrong?  Are you more comfortable charting your own course than following a well-trodden path? If so, the path toward independent thinking likely calls to you.


Conversely, if you’re naturally inclined to follow established methods, to prioritize order and efficiency, and value proven strategies over audacious experiments, a career demanding conformity would likely suit you better.

Remember, there are no guarantees. Choosing the right path is still a gamble, but understanding this fundamental distinction is your best chance of winning. The future belongs to both the independent and the conventional thinkers, but only if each understands and embraces their inherent strengths.
","career advice
",AI,"human
","human
"
109,109,"The Most Important Thing They Didn't Teach You in College

Most college students are taught how to get good grades, land a job, and maybe even navigate the treacherous waters of graduate school. But what they're rarely taught is how to actually *do* something meaningful with their lives.  This is a far more challenging problem, and one I wish someone had helped me solve earlier.

The conventional wisdom—do what you love—is helpful, but insufficient. It's like being told to ""build a great company"" without any guidance on how to actually build one.  What does ""love"" even mean in this context?  Is it the fleeting thrill of a new project, or something deeper, a consistent source of satisfaction over a long period?

The answer, I think, lies not in the object of your affection, but in the process.  Truly loving your work doesn't mean you'll adore every single moment. Even the most passionate scientists experience moments of frustration, doubt, and tediousness. But the overall trajectory must be upward—a relentless pursuit of something you admire and find intellectually stimulating.

This pursuit, however, requires a certain level of intellectual independence.  In many fields—science, investing, entrepreneurship—success hinges on thinking differently from your peers.  You can't simply echo existing wisdom; you must unearth new truths, challenge conventional assumptions, and chart your own course.

This doesn't mean flaunting contrarianism for its own sake.  A successful contrarian is not driven by a need to be different, but by an insatiable curiosity and a fastidiousness about truth. They prioritize understanding over conformity, even if that means questioning established dogma.

For those in fields that value conformity over novelty (much of corporate life, for example), this emphasis on independent thinking can be profoundly limiting.  If you're naturally independent-minded, you'll find yourself chafing under the constraints of a rigid hierarchy. Conversely, attempting originality in a field that doesn't reward it will lead to frustration.

So how do you discover what kind of work is right for you? One approach is to analyze your past experiences.  Look back on the projects that truly captivated you—were they collaborative efforts or solitary pursuits?  Did they involve solving pre-defined problems, or did you define the problems yourself?

Another, more active approach, is to experiment. Try working on a variety of projects, both large and small. Don't be afraid to switch fields.  The trajectory of a successful career rarely follows a straight line; it often resembles the erratic path of a ping-pong ball.

The path to discovering meaningful work is rarely straightforward.  It requires self-awareness, persistence, and a willingness to adapt.  But the rewards—the satisfaction of doing something you find genuinely interesting and impactful—far outweigh the effort.  And that's a truth worth pursuing.
","career advice
",AI,"human
","human
"
110,110,"The Perils of Premature Optimization (and the Elegance of Simplicity)

The programming world, much like the startup world, is rife with trends.  One minute everyone's chasing the next shiny language, the next it's some new paradigm promising to revolutionize everything. Currently, functional programming enjoys a surge in popularity, but I see a deeper problem: the misguided prioritization of complexity over clarity.

Many programmers, seduced by the promise of elegant solutions, leap into intricate frameworks and elaborate designs before thoroughly understanding the problem at hand. This is the equivalent of building a cathedral before you've established a solid foundation—a recipe for disaster.

I've seen countless projects bogged down in layers of abstraction, choked by the very tools meant to make them more efficient.  The pursuit of premature optimization leads to bloated codebases, decreased readability, and ultimately, slower development cycles.  The irony is palpable.

What, then, is the solution?  It's deceptively simple: prioritize clarity and simplicity.

Begin by understanding the core problem.  Break it down into manageable components.  Implement the simplest solution that works.  Measure its performance.  Only then—and only if necessary—should you consider optimizing specific bottlenecks.

This isn't about being lazy; it's about being effective.  A concise, well-structured program, even if initially less performant, is far easier to maintain, debug, and extend than a labyrinthine monstrosity crafted in the name of efficiency.  In the long run, the time saved in development and maintenance more than compensates for any minor performance gains sacrificed in the early stages.

Furthermore, a simpler approach often proves more adaptable.  As requirements change—and they inevitably will—a flexible, lightweight architecture is far easier to modify than a rigid, over-engineered system.

Think of it like building a house.  Would you rather spend months meticulously crafting each component, only to discover your initial design was flawed, or would you prefer a quicker, more adaptable construction process that allows for modifications along the way?

The answer, for most projects, is clear.  Embrace the elegance of simplicity, postpone optimization, and focus on building a solid foundation. Only then will you truly unlock the potential for efficiency and scalability.  The truly smart programmer understands this.  The truly great programmer practices it.
","programming advice
",AI,"human
","human
"
111,111,"The Essential Simplicity of Powerful Code

A common misconception among programmers, particularly those schooled in the gentler arts of procedural programming, is that readability and succinctness are mutually exclusive virtues.  This is demonstrably false.  In fact, I propose a stronger claim: succinctness is not merely a desirable characteristic of code, but a direct measure of its power.

The Case for Succinctness

Why do we bother with high-level programming languages at all?  The machine doesn't care about our elegance; it's perfectly content with raw assembly. The answer is leverage. High-level languages allow us to express complex operations concisely, achieving in ten lines what would take thousands in machine code.  This compression is the very essence of power in a programming language.  A language that forces verbosity is, in effect, a blunt instrument.

Beyond Lines of Code

Measuring the power of a language is not a simple matter of counting lines of code. That metric is crude, easily manipulated by stylistic conventions.  A more accurate measure would be the number of conceptual elements in a program – variables, functions, data structures, etc.  This represents the cognitive load required to understand and write the code.  A program with fewer elements is inherently simpler, more powerful.

Designing for Succinctness

The value of this ""elements"" metric is not just for comparing existing languages, but for guiding the design of new ones. The constant comparison between language design choices (with X versus without X) highlights the true test: which yields the shortest, most expressive code?  Languages with reputations for succinctness – think Forth, Joy, Icon – are rich sources of inspiration.

The Productivity Argument

Fred Brooks touched upon this in *The Mythical Man-Month*.  His observation that programmers produce a roughly constant amount of *code* per day regardless of language has profound implications.  It means the only path to faster development is to work with a language that minimizes code size.  This is not merely a matter of writing faster; it translates to fewer bugs, reduced complexity, and a significant competitive advantage.

Beyond Metrics: The Feel of the Code

Ultimately, the best measure is intuitive.  How does the language *feel*? Does it amplify your thinking or hinder it?  Does it force awkward detours or provide elegant shortcuts?  A truly powerful language will make the act of programming feel effortless, allowing for seamless expression of complex ideas. Restrictiveness, often experienced as a frustrating obstacle, is usually a symptom of insufficient succinctness.

Readability and the Cost of Verbosity

While readability is crucial, we should distinguish between the readability of a single line and the overall readability of the program.  A verbose language may boast aesthetically pleasing individual lines, but the sheer volume of those lines renders the program as a whole far less understandable.   This is precisely the trap of the ""low monthly payment"":  many small payments add up to a large total.  Similarly, many easy-to-read lines add up to a program that is vastly harder to grasp as a whole.

The Ongoing Quest for Power

The question isn't simply ""Is succinctness equal to power?"" but ""To what extent are they equal?""  Certainly, succinctness is a crucial component of a powerful language.  Identifying the limits of succinctness – if such limits exist – becomes a fascinating research area.  Are there truly languages that force convoluted, unreadable code in the name of extreme brevity?

The most important message is this: strive for clarity and conciseness. The best code is not merely functional; it's an elegant expression of the underlying problem. The most powerful languages will help you reach that goal.
","programming advice
",AI,"human
","human
"
112,112,"The Existential Dread of the Average Programmer

Most programmers, I suspect, feel a nagging sense of unease. It's not the deadlines, or the bugs, though those certainly contribute.  It's a deeper, more fundamental anxiety: the feeling of never truly *understanding* the system they're building.  They write code, it compiles, it mostly works—but the whole thing remains a blurry, indistinct mass, rather than the crisp, well-defined model it ought to be.

This isn't a matter of skill.  Many perfectly competent programmers find themselves in this state. The problem lies not in their abilities, but in their environment. The typical software development process, as practiced in most large organizations, actively *prevents* a programmer from achieving that crucial state of deep understanding.

Think of a mathematician grappling with a complex theorem. They don't meticulously work out every step on paper; instead, they manipulate the problem in their minds, building an intuitive grasp of its structure.  The best programmers do the same with their code.  They hold the entire program in their heads, able to effortlessly modify and rearrange it. This mental model is the key to true mastery.

But how many programmers get to experience this? The constant interruptions, the shifting priorities, the endless meetings—these are not conducive to the deep, focused concentration required.  And the code itself is often a tangled mess, burdened with unnecessary complexity and poorly chosen abstractions.

Consider the effect of team size. The larger the team, the more fragmented the understanding.  Each programmer holds a piece of the puzzle, but lacks a comprehensive view of the whole. This inevitably leads to errors, inconsistencies, and a general lack of elegance.  Moreover, the code becomes a shared, mutable artifact—something no one truly owns, and therefore no one truly understands.  

The solution, I believe, is not simply to improve the individual programmer’s skill. That's only part of it.  It's about creating an environment conducive to deep, focused work. This means minimizing distractions, working in small teams or even solo, and favoring tools and languages that encourage clarity and simplicity.

It also means embracing a different approach to software development. Instead of striving for comprehensive upfront design, begin with a small, functional prototype. Refine it iteratively, allowing your understanding to deepen as you go. Treat the code as a living organism, always subject to change and improvement.  Rewrite it mercilessly.  Let it reflect your evolving understanding of the problem.

The benefits are numerous.  A better-understood program is more reliable, more easily maintained, and ultimately, more elegant.  But perhaps the most profound benefit is a sense of mastery—the satisfaction of truly knowing your creation.  For it is this deep understanding that transforms programming from a mere job into a profound intellectual pursuit. And that, my friends, is the true reward.
","programming advice
",AI,"human
","AI
"
113,113,"The Myth of the ""Software Engineer""

The term ""software engineer"" is a misnomer.  Engineering implies a predictable, repeatable process, a blueprint meticulously followed to construct a reliable machine.  Software development, particularly at the cutting edge, is closer to experimental science.  It's about exploring a problem space, iterating rapidly, and building something new, not just assembling pre-existing parts.

This distinction has profound implications for how software should be built.  The factory model—large teams, detailed specifications, waterfall processes—is utterly inappropriate for creating innovative software.  It stifles the very thing that makes good software possible: deep, intuitive understanding.

The best programmers don't follow blueprints. They hold the entire program in their minds, a dynamic mental model constantly evolving as the code itself changes.  They see the program as a living entity, capable of flexible adaptation.  This requires intense focus, prolonged periods of uninterrupted work, and a profound mastery of the chosen tools.

The key isn't just technical skill, but a particular mindset.  It's a willingness to throw away code, to refactor relentlessly, to prioritize understanding over immediate efficiency.  It’s the ability to quickly prototype, to test hypotheses, and to constantly refine the design based on experimentation.

Large organizations, with their inherent bureaucracy and compartmentalization, actively work against this.  They demand rigid processes, multiple layers of review, and adherence to established standards, all of which make it impossible to hold a program, especially a complex one, entirely within one's mind.  The result is brittle, inflexible code, and slow, costly development.

The solution isn't to make large organizations more agile, but to recognize their inherent limitations.  The truly innovative work, the breakthroughs that define the future of software, will always come from small teams, even individuals working independently.   They have the freedom to experiment, to fail fast, and to ultimately achieve a level of understanding and control unattainable in larger, more structured environments.

This explains why startups, particularly those built by exceptional programmers, tend to outperform larger, established companies in software-driven innovation.  They operate at a different level, adhering to a different paradigm.  They aren't building machines; they’re creating new worlds.  And those worlds can only be built, one brilliant mind at a time, in the quiet, intense focus that only a small team, or even a solitary programmer, can provide.
","programming advice
",AI,"human
","AI
"
114,114,"The Tao of Debugging: Why Imperfect Code is Your Friend

Many young hackers believe debugging is an unfortunate necessity, a tedious chore tacked onto the end of the ""real"" work of programming. They envision the ideal programmer, a kind of coding deity, who pours forth flawless code, first draft, and never touches a debugger. This, I assure you, is a fantasy.  The truth is far more interesting.

The best programmers I know don't avoid bugs; they embrace them.  They see debugging not as a sign of failure, but as an integral part of the creative process, a feedback loop that refines and elevates their work.  It’s a messy, iterative process akin to sculpting, where the initial clay is rough and imperfect, and the final form emerges through countless chisels and refinements.

Consider the architect: does he begin with perfectly formed blueprints, never to be altered? No.  He sketches, he revises, he demolishes and rebuilds, guided by intuition and experience.  The initial design is only a starting point, constantly evolving in response to unforeseen challenges and new insights.  So it is with code.

The initial draft of a program is rarely elegant, efficient, or even entirely correct.  It's a rough approximation, a first pass at capturing an idea.  The real magic happens during debugging, when the programmer engages in a deep, intimate dance with their creation, probing its weaknesses, understanding its limitations, and reshaping it into something more refined.

This iterative process, fraught with errors and setbacks, isn't simply about fixing bugs; it's about *discovery*.  Each bug encountered reveals a hidden flaw in the underlying design, an opportunity to rethink assumptions, improve structure, and deepen the understanding of the problem.

Static typing, while often touted as a way to prevent bugs, can actually stifle this vital process.  By forcing premature commitment to rigid data structures, static typing can hinder the exploration of new ideas and limit the flexibility needed to adapt to evolving requirements.

The ideal programming language should be like a malleable material, easily reshaped and refined in response to feedback.  It should allow for experimentation, for the exploration of unexpected paths, for the embrace of imperfection.  In short, it should be a tool perfectly suited for the messy, iterative process of creation.

Embrace the bugs.  Learn from them.  Let them guide you.  For it is through the imperfections of our code that we discover the true beauty of our craft.
","programming advice
",AI,"human
","human
"
115,115,"The Tyranny of the Average, and How to Escape It

The software world is a strange place.  You’d think the best tools would win, that superior technology would inevitably triumph.  But this isn’t always the case.  Often, the average prevails, a bland consensus dictated by inertia, fear, and the whims of pointy-haired bosses.  This is especially true in the choice of programming languages.

Why Java? Why C++?  Why *not* Lisp?

The average programmer, the average manager, the average investor—they tend to gravitate towards the familiar.  The languages with the largest marketing budgets, the biggest communities, the most readily available libraries.  They’re comfortable, predictable choices.  But comfort is the enemy of progress.

The Power of Abstraction

Programming languages aren’t all created equal.  Some are far more powerful than others, offering far richer abstractions.  These high-level abstractions allow you to build more with less, to express complex ideas with concise elegance.  They allow for what I call “bottom-up programming,” where you build your own specialized language on top of the base language, drastically shortening development time and increasing maintainability.

The numbers speak for themselves.  Anecdotal evidence suggests productivity gains of 7-10x when comparing Lisp to C or Java.  One line of Lisp can often replace twenty lines of C, a claim supported by the impressive performance of ITA Software, whose core product is a massive Common Lisp application.  This isn’t just about fewer lines of code; it’s about fewer *ideas* to manage, fewer moving parts to keep track of.

The Cost of Mediocrity

The pointy-haired boss will argue that choosing a less popular language increases risk:  fewer programmers available, fewer libraries to leverage, potential interoperability issues.  These are valid concerns, but often exaggerated.

For truly demanding applications, particularly server-based systems where you control the entire stack, the advantages of a more powerful language far outweigh these risks. The ability to build a complex system significantly faster, with a smaller, more talented team, is a powerful advantage in a competitive market.  Moreover, the superior talent attracted to challenging projects using cutting-edge tools often compensates for any perceived scarcity of specialists.

The Path to Success

If you’re building a truly ambitious application, don't let the tyranny of the average dictate your technology choices.  Focus on the problem, not the perceived wisdom of the herd.  Embrace powerful languages.  Leverage abstraction to build a more concise, efficient, and maintainable system.

Choose the right tools, the right team, and prepare to outpace your competitors by a significant margin.  This isn't merely a technical advantage; it's a strategic one. The ability to iterate faster, to adapt more quickly, is crucial in today’s dynamic landscape. It’s the difference between building a monument and building a sandcastle. And, ultimately, the difference between success and failure.
","programming advice
",AI,"human
","AI
"
116,116,"The Myth of the ""Practical"" Language

I’ve been thinking lately about the curious disconnect between what makes a programming language *good* and what makes it *popular*.  The most widely used languages aren't necessarily the most elegant or powerful; they often succeed on factors unrelated to their intrinsic merits. This isn't just an observation about the software industry; it mirrors trends in almost every field.  Think about the bestseller lists: do they always reflect the best writing?  Clearly not.

The perceived practicality of a language is often a major driver of its popularity.  But what constitutes ""practical""?  Is it the sheer number of libraries available? The size of the community? The ease of finding developers?  These are all important, but they are secondary to the underlying power and expressiveness of the language itself.  A language that’s fundamentally flawed, even with massive community support, will ultimately hinder, not help, the programmer.  It's like trying to build a skyscraper with substandard materials; you might get it standing, but it won't be a sound structure.

The pursuit of ""practicality"" often leads to compromises that cripple a language's potential.  Features are added to appease existing users, resulting in a bloated, inconsistent system that's hard to learn and even harder to master.  This is why many ""modern"" languages, while boasting extensive feature sets, frequently fall short in elegance and power compared to their more minimalist predecessors.  They sacrifice purity at the altar of perceived practicality, a trade-off that is rarely worth it in the long run.

The best programming languages, like the best works of art, often defy immediate comprehension.  They possess an underlying elegance and power that becomes apparent only with experience and deeper understanding.  Their initial complexity is not a sign of weakness, but rather an indication of their depth and potential.   It's like learning to play a complex musical instrument; the initial difficulty is overcome by the rewards of mastery.

The real practicality lies not in the superficial aspects of a language's popularity, but in its ability to empower the programmer.  A language that allows for clear, concise, and efficient expression will ultimately lead to better software, regardless of its current popularity.  Don't let the hype distract you from the fundamental truth: choose a language that elevates your thinking, not just one that's currently fashionable.  The true measure of a language's practicality is its ability to make you a better programmer.
","programming advice
",AI,"human
","human
"
117,117,"The Unexpected Joy of Failing Upwards

A few weeks ago, I stumbled upon an old, half-forgotten project.  It wasn't a grand failure, not exactly. More of a… sideways stumble.  A promising idea that fizzled, not because it was inherently flawed, but because the market wasn't ready.  Or perhaps, we weren't.  The details are hazy now, a blend of late nights, half-formed code, and the persistent hum of ambition that fueled those early days.

The project was a social media platform, before social media truly existed.  We envisioned a world where people could connect, share, and build communities online, but our execution was… let's say, ahead of its time.  The technology was clunky, the interface was less than intuitive, and the business model was… optimistic.

We poured months into it, a small team burning with the naive passion only youth can muster.  We believed, genuinely believed, that we were onto something big.  We were wrong.  The platform sputtered, gained a tiny, loyal following, then quietly faded into oblivion.

Looking back, I see the failure not as a loss, but as a necessary step.  It taught me invaluable lessons about market timing, user experience, and the critical importance of a robust business model.  It also highlighted the sheer resilience required to pursue ambitious goals in the face of constant setbacks.  More significantly, it forced me to confront my own assumptions, to challenge my own biases, and to refine my approach to building businesses.

There's a certain freedom in acknowledging failure, in accepting that not every idea, no matter how brilliantly conceived, will translate into success.  It allows for a more honest assessment of one's strengths and weaknesses, and paves the way for more informed decisions in the future.  

The project, that half-forgotten dream, taught me more than any success ever could. It wasn't a trophy I could place on a shelf, but a crucial piece of my personal learning journey, a reminder that the path to success is often paved with well-intentioned failures.  And that sometimes, failing upwards is the only way to truly climb.
","personal experience report
",AI,"human
","human
"
118,118,"The Unexpected Productivity of Slowing Down

For years, I've operated under the assumption that relentless forward momentum was the key to success.  More hours, more projects, more output—this was the equation for achievement.  Then, children arrived.  My carefully constructed schedule, my meticulously planned days, crumbled.  The relentless forward march was replaced by… well, by the unpredictable rhythm of small humans.

Initially, this felt like a catastrophic loss. My productivity, measured in lines of code or essays written, plummeted.  The guilt was palpable, a constant companion.  I had a nagging sense that I was failing, that the relentless productivity machine was sputtering, losing steam.

But something unexpected happened.  In the spaces between diaper changes and bedtime stories, in the quiet moments stolen between bursts of chaotic energy, I began to notice a different kind of productivity.  It wasn't about sheer output; it was about the quality of my work, the depth of my thought.

The forced constraints of a child's schedule—the non-negotiable deadlines of naps and meals—ironically fostered a remarkable focus.  When I did sit down to work, it wasn't with the nagging sense of a million other tasks looming, but with a clear, almost monastic concentration.  The time was finite, precious, and I made the most of it.

This shift wasn't simply a matter of efficient time management; it was a fundamental change in my approach to work.  The relentless striving for more gave way to a deeper appreciation for the value of less.  The pressure to constantly produce was replaced by a quieter focus on quality, on crafting something meaningful rather than merely churning out volume.

It's a subtle shift, this transition from frantic productivity to a more deliberate, mindful approach to work.  It requires a certain surrender, a letting go of the illusion of unlimited capacity.  But the rewards are significant: a deeper satisfaction in the work itself, a clearer perspective, and perhaps, surprisingly, a more substantial output in the long run.  It’s a reminder that sometimes, slowing down is the fastest way to get where you want to go.
","personal experience report
",AI,"human
","human
"
119,119,"The Unexpected Joys of Solitude: A Hacker's Perspective

I’ve always considered myself a builder, a creator.  My world revolves around code, essays, and the relentless pursuit of interesting problems.  This relentless focus, however, has historically come at a cost.  A significant cost.  Loneliness.

For years, I thrived in a self-imposed isolation. The hum of the server, the click-clack of the keyboard – these were the sounds of my tribe.  Social gatherings felt… inefficient.  A distraction from the real work.  Small talk, particularly, felt like a profound waste of processing power.

But something shifted.  Not a dramatic epiphany, but a gradual dawning realization that the intense focus, while productive, left certain… vital systems underutilized.  The human ones.

It began subtly. A quiet appreciation for the stillness of a Sunday morning, the unexpected pleasure of a long walk alone, devoid of the urgent demands of emails and deadlines.  I found myself deliberately choosing solitude, not as a refuge from the world, but as a fertile ground for creativity and introspection.

The silence, once a void to be filled, became a canvas on which to paint ideas.  The absence of external stimuli allowed my internal processes to run with unexpected efficiency.  Problems that had seemed intractable suddenly yielded to a more patient, more thoughtful approach.

This wasn’t a rejection of human connection. Rather, a recalibration.  I learned to value the deep, meaningful conversations, the rare moments of shared laughter and understanding, even more precisely because they were deliberately chosen, not just default settings.

My productivity hasn’t necessarily increased.  But the quality of my work, the depth of my thinking, the very texture of my life, has deepened.  The solitude, it seems, acted as a sort of algorithmic compression, stripping away the noise and leaving only the signal.  The essence.

This isn't an argument for hermitage.  It's a testament to the unexpected richness that can be found in the spaces between, in the deliberate cultivation of silence, in the quiet recognition that sometimes, the most profound connections are made when we're alone.  Alone with our thoughts, alone with our work, alone with the quiet hum of the universe.  And sometimes, that's more than enough.
","personal experience report
",AI,"human
","human
"
120,120,"The Absurdity of ""Well-Roundedness""

The modern obsession with well-roundedness in education is, frankly, bizarre.  It's a testament to how easily we accept profoundly inefficient systems, mistaking busywork for genuine progress.  The ideal student, we're told, is a jack-of-all-trades, proficient in a dizzying array of subjects, extracurriculars, and standardized tests, all meticulously documented for the consumption of admissions committees.  But what does this actually achieve?

Consider the analogy to software development.  Would you praise a programmer for writing mediocre code in ten different languages, rather than mastering one and building something truly remarkable with it?  Of course not.  Yet, this is precisely the logic behind the ""well-rounded"" ideal.  We're training students to be mediocre across the board, rather than exceptional in a few key areas.

The problem isn't a lack of ambition; it's a misdirection of ambition.  Students aren't lazy; they're highly effective at optimizing for the flawed metrics presented to them.  They've learned to play the game, not to master the subject matter.  The elaborate rituals of extracurriculars, standardized tests designed to be gamed, and essays tailored to anticipate the desires of admissions officers, all serve as testament to this.

This system rewards superficial accomplishment, not genuine understanding.  The student who excels at standardized test-taking techniques, regardless of their actual knowledge, is often favored over the student who deeply understands the material but struggles with the artificial constraints of the testing process.  This is not a measure of intellect; it's a measure of compliance and game-playing ability.

The solution isn't simply to replace one set of arbitrary metrics with another.  Instead, we need to fundamentally re-evaluate what constitutes a successful education.  We need to shift the focus from superficial breadth to genuine depth, encouraging students to pursue their passions with intensity and develop true expertise in their chosen fields.  This doesn't require abandoning all structure; rather, it requires building a system that rewards genuine learning, not the clever manipulation of a broken system.

The path to a more effective educational system requires a paradigm shift.  We must decouple learning from arbitrary metrics and allow true talent and passion to flourish. Until then, the ""well-rounded"" student will remain a testament to the absurdity of a system designed to reward cleverness over competence.
","educational content
",AI,"human
","AI
"
121,121,"The Unseen Hand of Conformity: How Societal Pressure Shapes Success

I’ve been thinking lately about the curious disconnect between societal expectations and individual achievement. We’re constantly bombarded with narratives of success, often painted as a ruthless climb to the top, a Darwinian struggle where only the strongest survive.  But my observation, drawn from years of observing founders and innovators, suggests a far more nuanced reality.

The most striking element is the pervasive influence of conformity.  Not the blatant, obvious kind, but the subtle, insidious pressure to blend in, to adopt the prevailing worldview, even when it contradicts one's own judgment. This pressure manifests in countless ways, from the seemingly trivial (choosing a career path based on perceived stability rather than personal passion) to the profoundly significant (silencing dissenting opinions to avoid social repercussions).

Consider the pressure on young professionals.  The seemingly straightforward path – top university, prestigious firm, steady promotion – is often lauded as the gold standard of success.  But how many individuals truly thrive within such a structured environment?  How many sacrifice their passions, their unique talents, at the altar of conformity?

This dynamic is particularly relevant in the world of startups.  While the popular image is of maverick entrepreneurs disrupting industries, the reality is often more complex.  Many successful founders, while possessing independent minds, also navigate the delicate balance between innovation and market acceptance, adapting their visions to resonate with potential investors and customers.

The danger, of course, lies in the potential for conformity to stifle genuine innovation.  A society that rewards compliance over originality ultimately stunts its own growth.  The most groundbreaking ideas, by their very nature, challenge the status quo.  They require individuals to resist the urge to conform, to question established norms, and to embrace the inherent uncertainty of forging new paths.

But there’s hope.  The very nature of innovation is disruptive.  It creates new spaces, new opportunities, where the old rules don’t apply.  This is the fertile ground where true independence can flourish.  The challenge lies in fostering environments that encourage risk-taking, tolerate failure, and celebrate individuality.

The future, in my view, belongs to those who refuse to be molded by societal expectations.  They are the ones who will shape the next generation of ideas, industries, and progress. The ones who understand that true success lies not in conforming to the mold, but in breaking it.
","social commentary
",AI,"human
","AI
"
122,122,"The Unbearable Lightness of Being Popular (and the Surprisingly Heavy Hand of Wealth Taxes)

The other day I was thinking about high school.  Not the intellectual triumphs, of course—those are well-documented elsewhere. No, I was thinking about the sheer, brutal inefficiency of the social system.  It’s a zero-sum game played with terrifying intensity, a popularity contest where the stakes are far higher than they should be.  Nerds, of course, lose.  But even the “winners” often emerge scarred.  The constant pressure to conform, the exhausting performance of fitting in—it’s enough to drive anyone to madness, or at least, to a career in programming.

This got me thinking about another kind of zero-sum game: wealth taxes.  Politicians are increasingly enamored with the idea of taxing wealth, in addition to income and capital gains. It sounds reasonable enough.  After all, who needs *that* much money? But the reality is more complex, and far more brutal than the high school social hierarchy.

Let's take a hypothetical startup founder.  They strike it rich in their twenties, amassing a significant stock portfolio. They then live another sixty years. How much of that wealth will be left after a wealth tax, even a seemingly modest one?

It's surprisingly little. A 1% wealth tax, applied annually for sixty years, will take over 45% of their original holdings. A 2%?  Roughly 70%.  And these are just *raw* numbers, ignoring the complexity of valuation and the realities of a fluctuating market. This isn't a matter of redistribution of existing wealth; it's the systematic erosion of future wealth creation.

Now, some will argue that wealth taxes should include a threshold.  Let’s say the tax only applies to net worth exceeding $50 million.  Does this soften the blow?  Not significantly.  The compounding effect remains.  A 2% tax, with a $50 million threshold, still takes roughly two-thirds of our founder's eventual wealth.

Why such a dramatic effect?  Because wealth taxes are applied repeatedly to the same base. Income tax is a one-time hit on that year's earnings.  But wealth is taxed every year, year after year, until the wealth itself is gone.  It’s a compound tax, and like compound interest, it grows exponentially.

This is not a moral argument, nor an argument about fairness.  It's a simple observation about the mathematics of compounding taxes.  It's also an argument about incentives.  If you're building a company, and you know that the fruits of your labor will be systematically confiscated over your lifetime, how much less effort will you exert? How much less risk will you take?   The answer is, likely, significantly less.

The political systems that drive our schools and tax policy may be entirely different, but they share an underlying inefficiency: they fail to create positive-sum systems for their players.  High school creates a zero-sum competition of social validation; wealth taxes create a zero-sum contest against the ever-increasing demands of the state.  Both leave their participants exhausted, and far poorer than they should be.
","social commentary
",AI,"human
","human
"
123,123,"The Unthinkable Truth About ""Progress""

We like to think of ourselves as more enlightened than past generations. We’ve abolished slavery, granted women the vote, and made strides toward racial equality.  But is this “progress” truly linear, or are there blind spots, moral fashions that future generations will view with the same horrified amusement we feel for the practices of our ancestors?

Consider the seemingly innocuous phrase, “political correctness.”  A few decades ago, it was a badge of honor, a mark of belonging to the vanguard of social justice. Today, it’s a term of derision, synonymous with stifling dissent and enforcing conformity.  What changed? Was the underlying principle itself flawed, or was it simply a case of an idea’s lifespan expiring, like a pair of bell-bottoms?

The unsettling answer, I suspect, is a bit of both.  Many of our society’s supposed advancements are not based on reasoned argument, but on shifting social consensus—moral fashions as arbitrary and ephemeral as the latest trend in clothing.

Consider the current obsession with “diversity, equity, and inclusion.”  While laudable in principle, the practice often veers into absurdity. We celebrate superficial representation while neglecting substantive equality.  We promote specific identity groups based on surface characteristics while ignoring the deep-seated inequalities that truly matter. We judge individuals based on their membership in a group, rather than their merits as individuals. Is this enlightened progress, or a new kind of tribalism cleverly disguised in fashionable terminology?

The problem, I believe, lies in our inability to question our own assumptions.  We readily condemn the injustices of the past, yet we are often blind to the injustices happening right under our noses. We are so busy enforcing today's moral fashions, we fail to see the cracks forming in the edifice of our beliefs.

How do we identify these self-deceptions? By cultivating a discomfort with the unquestioned. By actively seeking out dissenting opinions, not to embrace them blindly, but to challenge our own preconceived notions.  By acknowledging that the current zeitgeist, however virtuous it may seem, could contain its own set of deeply ingrained flaws.

This is not an argument for cynicism, but for intellectual honesty.  Progress is not a straight line; it’s a messy, iterative process of trial and error. To achieve genuine advancement, we must be willing to confront not just the injustices of the past, but also the blind spots in our own present.  We must be brave enough to question what we can't say, and even braver to question why.  For it is precisely in the exploration of the unthinkable that true progress may lie.
","social commentary
",AI,"human
","AI
"
124,124,"The Tyranny of the Unspoken

We’re told constantly to speak our minds, to challenge authority, to fight for what we believe.  But what if the real battle isn’t about voicing our opinions, but about *thinking* them in the first place?  What if the most dangerous conformity isn't outward obedience, but an internal silencing, a self-imposed limitation on what we allow ourselves to consider?

Consider the history of ideas.  Look at any era, any society, and you’ll find a constellation of unspoken truths, beliefs so deeply ingrained that to question them is to risk ostracism, ridicule, or worse.  These aren't necessarily lies, but rather unexamined assumptions, the foundations upon which the prevailing worldview rests.  They’re the invisible scaffolding that supports the dominant narrative.

How do we identify these unspoken truths?  One method is to examine what gets people in trouble.  What statements provoke immediate, visceral anger, not reasoned debate?  Not the obviously false, but the subtly unsettling, the ones that make people uncomfortable because they hint at a deeper, perhaps uncomfortable, reality.  These are the candidates for unspoken truths.

Another approach is to examine the labels themselves.  Every era has its pejoratives: “blasphemous,” “unpatriotic,” “divisive,” “insensitive.” These terms aren’t arguments; they are rhetorical weapons, designed to shut down discussion before it even begins.  They signal an idea so threatening that it must be suppressed, not refuted.

This isn’t merely an academic exercise.  The ability to identify and challenge these unspoken truths is a crucial skill, especially for those who work with ideas.  The most valuable innovations often come from questioning established assumptions, from exploring the edges of the acceptable.  To do groundbreaking work, you need a mind that’s not shackled by conventional wisdom, a mind that dares to venture into the forbidden territory of the unspoken.

But it's crucial to be strategic.  Openly challenging deeply held beliefs can be a fool's errand.  It's not always necessary—or even wise—to vocalize every heretical thought.  The goal is to cultivate a space within your own mind where these thoughts can flourish, where they can be examined and refined without the pressure of immediate social consequences.

This requires a certain amount of intellectual courage, the willingness to risk discomfort, to face the possibility of being wrong.  It’s about cultivating intellectual independence, about choosing your battles wisely and focusing your energy on exploring the truly interesting, even if those explorations lead you down unconventional paths.

The greatest minds throughout history have often been those who were willing to confront the unspoken, to question the unquestionable.  Their work, in turn, has often revolutionized our understanding of the world.  So cultivate the unspoken.  Dare to think what others dare not.  The future of ideas depends on it.
","social commentary
",AI,"human
","AI
"
125,125,"The Cult of the ""Influencer"" and the Death of Genuine Opinion

The internet, that supposed democratizing force, has birthed a strange new aristocracy: the influencer.  These are not the titans of industry or the luminaries of academia.  No, these are individuals who, through a combination of carefully curated online personas and algorithmic happenstance, amass a following and wield a surprising amount of power.  But what exactly is the power they wield, and what does its rise tell us about the state of modern discourse?

The core problem, I believe, lies in the inherent conflict between authenticity and monetization.  An influencer's livelihood often depends on maintaining a consistent stream of content—content that must, in turn, appeal to advertisers and, crucially, algorithms.  The pressure to conform to the demands of the market, both in terms of content and style, inevitably leads to a homogenization of opinion.  The truly unique voice, the one willing to defy trends and challenge prevailing norms, finds itself at a distinct disadvantage.  They are, to use a term I've grown rather fond of, outliers.

This isn't to say that all influencers are insincere.  Some may genuinely believe in the products they endorse or the opinions they espouse. But even in these cases, the very nature of the platform encourages a form of self-censorship.  To maintain their position within the hierarchy of influence, they must avoid alienating their followers and, by extension, their sponsors.  The result is a pervasive blandness, a cautious conformity that stifles genuine dissent.

This trend is particularly evident in areas of social and political commentary.  Where once we might have relied on a diverse range of voices—from journalists to academics to activists—we now find ourselves increasingly reliant on the opinions of individuals whose livelihood depends on staying within the narrow confines of acceptable discourse.  The inherent biases of the platforms themselves only exacerbate this issue.  Algorithms prioritize engagement, favoring sensationalism and controversy over nuanced, thoughtful analysis.  The result is a feedback loop that amplifies the most extreme viewpoints and marginalizes the more moderate and reasoned ones.

So what is the antidote to this insidious trend?  I don't have a simple answer.  Perhaps a return to older forms of communication, the pre-algorithmic landscape of direct engagement and less-mediated conversations.  Perhaps a renewed emphasis on critical thinking, on the ability to discern genuine insight from carefully crafted marketing.  Perhaps we simply need to recognize the limitations of the current system, to understand that the authority of the ""influencer"" is ultimately a fragile and artificial construct.

The death of genuine opinion will not be mourned by those who profit from its decline, but it should be by those of us who value truth and intellectual freedom.  The challenge lies in finding ways to cultivate a richer, more authentic discourse, a space where thoughtful reflection can flourish—a space, in short, that is not dominated by the cult of the influencer.
","social commentary
",AI,"human
","human
"
126,126,"The Tyranny of the In-Group: Why We Need to Rethink Adolescent Social Dynamics

The recent spate of articles about teenage mental health got me thinking.  The usual explanations—hormones, social media—feel inadequate.  They address symptoms, not the underlying disease.  The real problem, I contend, lies in the perverse incentives of the adolescent social ecosystem itself.

We all remember high school.  The pecking order, the brutal hierarchies, the seemingly arbitrary rules of engagement.  This isn't some natural phenomenon.  It's a manufactured environment, a pressure cooker designed, ironically, to prepare individuals for a world it barely resembles.

Consider the primary goal of a typical high school:  to funnel students towards college.  This is a laudable aim, but the mechanism is deeply flawed.  The relentless focus on standardized tests and grades creates a zero-sum game where students compete not for knowledge, but for relative ranking.  This inherently fosters a culture of conformity and exclusion.

The pressure to conform is immense.  Social acceptance is measured not by genuine connection, but by adherence to fleeting trends and behaviors.  This creates a powerful incentive to suppress individuality, especially for those who don't fit the prevailing mold.  The smart kids, the introverts, the intellectually curious—they become outliers, targets for the very system designed to educate them.

This isn't merely about bullying.  It's a systemic problem.  The relentless pursuit of social status drains time and energy, resources that might otherwise be invested in genuine passions and personal growth.   The very structure of adolescent social life—built on tribal affiliations and the need to establish dominance—discourages cooperation and independent thought.  

The solution isn't simply to preach empathy or increase mental health resources.  We need a fundamental rethinking of the adolescent social environment. We need to redesign the educational system to foster genuine intellectual curiosity, rather than creating a pressure cooker focused on narrow metrics.  We need to de-emphasize superficial social hierarchies and create opportunities for collaboration and authentic connection.  

The current model breeds not only unhappiness, but a generation ill-equipped to handle the complexities of the real world.  The pressure to conform stifles creativity and independence, leaving many feeling alienated and inadequate. We must create spaces where intellectual curiosity and individuality are not only tolerated, but celebrated. The future depends on it.
","social commentary
",AI,"human
","human
"
127,127,"The Tyranny of the Unspoken

It's a peculiar thing, this human tendency to self-censor.  We all have thoughts we wouldn't dare voice aloud in certain company.  This isn't mere politeness; it's something deeper, a tacit acceptance of a prevailing moral landscape.  But how much of this landscape is genuinely ethical, and how much is merely fashionable?  This is the question that keeps me up at night.

The obvious place to start is with the things that get people into trouble—the opinions that provoke outrage, lead to ostracism, or even land you in jail.  But simply cataloging these “unsayable” ideas isn’t enough. We need a filter, a way to distinguish between genuinely dangerous falsehoods and mere social transgressions.  The truly dangerous unsayables are those that the powerful fear might actually be true.

Consider history.  Many opinions considered heretical in their time seem harmless now.  This strongly suggests that our present-day taboos also contain a significant proportion of mistaken beliefs.  We can look to the past to identify these errors.  By comparing the accepted norms of various eras, we can begin to see patterns, to identify the recurring themes of social conformity that blind us to inconvenient truths.

Furthermore, examining the linguistic tactics employed to silence dissent can prove illuminating. Words like ""divisive,"" ""insensitive,"" or ""unacceptable"" are frequently used to shut down debate rather than engage with the underlying arguments.  Whenever an idea is dismissed with a label instead of a counterargument, that's a red flag. It suggests a fear of scrutiny, an unwillingness to engage with the potential validity of a controversial perspective.

Another valuable lens is to examine the development of social taboos. How do they emerge? Often, they originate in power struggles, where a group, insecure about its dominance, seeks to enforce conformity to solidify its position.   The resulting taboos are not about truth or falsehood, but about power.  They serve as mechanisms for social control, rather than expressions of genuine moral concern.  Understanding this dynamic allows us to identify those taboo ideas rooted not in wisdom but in the anxieties of those in power.

The self-consciously cool, the early adopters of moral fashions, often lead the charge.  But they are quickly followed by a far larger group motivated by fear.  These individuals conform not out of conviction, but out of a desire to avoid being ostracized.  By studying this pattern, we can identify the likely trajectory of future social norms.

Of course, such an inquiry necessitates a willingness to engage with uncomfortable ideas. It demands a resilience against social pressure, a commitment to independent thought, a rejection of easy answers. It requires, in short, a kind of intellectual courage.  But the rewards are immense.  For it is in the realm of the unsayable, the overlooked, the unconventional, that true innovation often resides.  The ability to think what others fear to even consider is not merely a sign of intelligence, it's a prerequisite for creating truly impactful work.  Embrace the uncomfortable; that is where progress resides.
","social commentary
",AI,"human
","human
"
128,128,"The Unthinkable: A Meditation on Societal Blind Spots

We all have blind spots.  In vision, these are literally areas our eyes can't see.  But we have analogous blind spots in our thinking—whole categories of ideas we refuse to consider, not because they're demonstrably false, but because they're unthinkable.  These are the heresies of our time, the intellectual equivalents of bell-bottom jeans in 1972.  And understanding them is crucial, not just for intellectual curiosity, but for progress itself.

One way to uncover these hidden biases is to examine what gets people into trouble.  What statements elicit immediate, visceral outrage, not because they're factually incorrect, but because they challenge deeply held assumptions?  These are the ideas that prick the conscience of the powerful, the ones that, if believed widely, could topple existing power structures.  They are not necessarily true, but they are certainly worth exploring.  They are the cracks in the facade of our shared reality, revealing the underlying tectonic plates of belief.

Another approach is historical.  What was once considered acceptable, even commonplace, that now seems utterly barbaric?  The shift isn't always progress.  Often, it's simply a change in fashion, a swing of the pendulum of societal norms.  By comparing the past to the present, we can identify those shifts that represent genuine progress and those that are merely arbitrary changes in taste—changes that leave us vulnerable to the same kind of groupthink that plagued past generations.

Consider the geographical variations.  What ideas are considered perfectly acceptable in one culture but taboo in another?  What explanations do societies offer for these divergences?  Often the ""reasons"" themselves reveal a deeper bias.  The discrepancies highlight how relative—how completely culturally contingent—our moral frameworks actually are.

Finally, consider the role of power dynamics.  Which groups are most invested in maintaining the status quo?  What are the intellectual weapons they use to suppress dissent—the labels they apply to inconvenient truths?  Understanding the mechanics of taboo creation—the ways groups maintain power through controlling discourse—is essential to spotting the blind spots in our own thinking.

Why bother exploring these uncomfortable ideas?  Because a mind that can explore the unthinkable is a mind capable of genuine innovation.  The greatest breakthroughs often come from challenging the seemingly self-evident—from questioning the very foundations of our understanding.  To refuse to consider unpopular or unsettling ideas is to willingly limit one's own potential.  It's to limit the potential of our society as a whole. The journey may be uncomfortable, but the destination holds the promise of a more nuanced, more complete, and ultimately, more truthful understanding of ourselves and the world we inhabit.
","social commentary
",AI,"human
","human
"
129,129,"The Unsolvable Problem of Online Discourse

The internet, for all its wonders, has a peculiar flaw:  the relentless amplification of the already loud.  This isn't merely the familiar problem of trolls—though they certainly contribute—but something deeper, something structural. It's the inherent asymmetry between the ease of expressing outrage and the difficulty of reasoned counterargument.

Consider the following scenario: a person, let's call him Alex, posts an opinion online.  It's a nuanced position, carefully considered, perhaps even slightly controversial.  A few people agree, a few disagree, and most remain silent.  But then, a single person, brimming with passionate disapproval, launches a furious counter-attack.  This isn't a well-constructed rebuttal; it's a torrent of invective, emotionally charged and brimming with ad hominem attacks.

The dynamics are clear.  Alex’s measured response, even if brilliant, will likely be lost in the noise. The initial fury has captured attention; the subsequent calm is ignored. The emotional intensity of the attack overwhelms the measured tone of the defense.  The system, in essence, rewards aggression.

This isn't a matter of policing speech.  The problem isn't malicious actors; it’s the architecture of online interaction itself.  The ease of broadcasting anger, combined with the inherent limitations of online communication (lack of nuance, brevity enforced by character limits, and a general lack of contextual understanding), creates a system that disproportionately favors those who shout loudest.  The volume of outrage drowns out the quiet voice of reason.

Some might suggest better moderation as a solution.  But moderation is a Sisyphean task.  For every offensive comment removed, ten more will be posted.  The fundamental problem isn't the content itself, but the underlying mechanics of the system that rewards intensity over substance.

The solution, if there is one, lies not in censorship or moderation, but in a fundamental shift in our expectations.  We need to cultivate a greater tolerance for ambiguity and a deeper appreciation for the complexities of human thought.  We must recognize that passionate disagreement, while sometimes valuable, is not the same as reasoned argument.  We must learn to filter the noise, to find the signals amidst the cacophony.  This is, however, a profoundly difficult task, one that requires a fundamental change in how we interact online, a change that is far from guaranteed. The unsolvable problem of online discourse may simply be a reflection of a deeper unsolvable problem: the inherent human tendency toward emotional reactivity.
","social commentary
",AI,"human
","human
"
130,130,"The Tyranny of the Status Quo: Why  Innovation Needs Defending

One of the most striking features of human societies is the tension between the established order and those who seek to disrupt it.  We tend to categorize individuals along a spectrum of conformity, but a more useful framework considers the *intensity* of that conformity.  There are those who passively accept the status quo, and those who actively defend it, often with surprising aggression.  And then there's the other side, those who question, those who innovate, those who, in the most extreme cases, actively challenge the very foundations of what is considered ""normal"".

Consider the history of scientific progress.  Every significant advance, from the heliocentric model of the solar system to the theory of evolution, has faced staunch opposition from those invested in the existing paradigm.  These weren't simply disagreements; they were often vicious attacks on the character and competence of the innovators. Why? Because challenging the established order is deeply threatening to those who benefit from its stability.

This phenomenon extends far beyond science.  Think of the early adopters of any new technology, the entrepreneurs who launch disruptive businesses. They are frequently met with skepticism, ridicule, and even hostility.  The reasons are familiar: fear of change, ingrained biases, a vested interest in the current system.

But there’s a deeper reason for protecting the space for independent thought and action. The status quo, however comfortable, is ultimately stagnant. It’s in the questioning, the experimenting, the deviation from established norms that we find real progress.  The aggressively independent-minded, while sometimes annoying, are the engine of societal advancement. They are the ones who identify the flaws, the inefficiencies, the injustices in the existing system and dare to propose alternatives.

The problem is that the defenders of the status quo are often remarkably effective. They build powerful institutions, create social norms, and wield the considerable power of conformity to silence dissent.  They don't need to be intellectually superior; they simply need to be organized and ruthless in their defense of the existing power structures.

This isn't a battle easily won. The forces of inertia are strong.  But the stakes are high.  If we allow the space for independent thought and action to shrink, we risk a future characterized by stagnation, inequality, and ultimately, decline.   The task, then, for those who value progress, is to build resilient systems that protect the innovators, the questioners, the disruptors—the canaries in the coal mine—from the ever-present threat of the aggressively conventional-minded.  It's a fight worth having.
","social commentary
",AI,"human
","human
"
131,131,"The Startup Founder's Antidote to Haplessness

The most common ailment I see in aspiring startup founders isn't a lack of ambition or even a shortage of clever ideas.  It's something far more insidious:  a passive acceptance of adversity.  They're not relentlessly resourceful; they're merely… hapless.

And no, I'm not talking about bad luck.  A hapless founder isn't unlucky; they're reactive. They let the world buffet them, instead of skillfully navigating its currents.  They're victims of circumstance, not masters of their destiny.

So what's the antidote?  How do we transform potential founders from hapless wanderers into the resourceful captains of their own ships? The answer, distilled to its essence, is a single phrase: relentless resourcefulness.

It's not merely relentless effort.  Anyone can be relentless, but only a relentlessly *resourceful* individual can consistently overcome the unpredictable obstacles inherent in building a startup.  The challenges are never the same; they're novel, requiring creative, adaptive solutions.  It's the difference between bashing your head against a wall and finding a key.

This isn't a universal recipe for success. It’s not applicable to every field. In art, for instance, the obstacles are primarily internal—the struggle against your own creative limitations.  But in the chaotic world of startups, the problems are external, demanding a constant stream of innovative workarounds.

Can this quality be taught?  Years of working with founders suggests a surprising affirmative. While some individuals are inherently passive, many possess a dormant resourcefulness that needs only to be awakened. This is particularly true for those used to operating within rigid hierarchies, where obedience often trumps initiative.

Understanding relentless resourcefulness also illuminates the question of how many startups are truly possible.  There's no inherent economic limit, no ceiling on the amount of wealth startups can generate. The only limitation is the pool of potential founders—those possessing the rare blend of determination and adaptability required to thrive in the face of constant uncertainty.

This framework is valuable for both aspiring founders and those seeking to build a team.  Ask yourself: are you relentlessly resourceful?  Is your potential cofounder?  If the answer is a resounding yes, then you've taken a crucial first step.  If not, then it's time to cultivate that crucial trait.

""Make something people want"" is the ultimate goal, but relentless resourcefulness is the only reliable path to get there.  Tape that to your mirror, and keep it there.
","startup advice
",AI,"human
","human
"
132,132,"The Myth of the ""Perfect"" Founder

The startup world is obsessed with finding the next unicorn, the next Zuckerberg or Musk.  Venture capitalists spend millions chasing the mythical ""perfect"" founder—a visionary leader with unparalleled technical skill, business acumen, and an unwavering drive.  But this pursuit is misguided.  The truth is far messier, and far more productive.

The reality is that successful startups are rarely built by single, flawless individuals.  They're built by teams, by individuals who complement each other's strengths and weaknesses.  The ""perfect"" founder is a myth, a convenient narrative that ignores the messy reality of collaboration and iteration.

Think about it:  How many truly groundbreaking companies were built by a lone genius?  While there are exceptions, the vast majority of successful startups are the product of collective effort, a blend of technical prowess, marketing savvy, and a shared vision.  The founders might lack individual expertise in certain areas, but they make up for it through strong partnerships and a willingness to learn.

This isn't to say that individual skills aren't important.  A strong technical founder is invaluable, especially in the early stages of a software startup. But even the most gifted coder needs a team to handle sales, marketing, and operations.  The strength of a startup lies in its adaptability, its ability to pivot and evolve as it learns from the market and its users.

Instead of searching for the elusive ""perfect"" founder, investors and founders should focus on building strong teams.  A team with diverse skillsets, a shared vision, and a willingness to adapt is far more likely to succeed than any lone genius.

Furthermore, the ""perfect"" founder narrative often discourages those who don't fit the mold.  It creates a barrier to entry for those who lack certain experiences or qualifications. This is self-defeating.  Startups need diverse perspectives and approaches to succeed.  The world needs more startups, not fewer.

Embrace the messy reality.  Build strong teams.  Learn from your mistakes.  Iterate.  That's the real path to building a successful startup, far more effective than the fantasy of finding the mythical ""perfect"" founder.
","startup advice
",AI,"human
","human
"
133,133,"The Unsung Virtue of Mundanity in Startup Ideas

The quest for the ""next big thing"" often leads founders astray.  They chase shiny, complex ideas, overlooking the surprisingly fertile ground of the mundane.  The truth is, many wildly successful startups weren't born from revolutionary concepts, but from solving everyday problems with elegant simplicity.

Consider the humble beginnings of giants like Craigslist.  No fancy algorithms, no cutting-edge technology; just a straightforward solution to a common need: connecting buyers and sellers.  Its success stemmed not from technological innovation, but from astutely identifying and efficiently addressing a pervasive problem.

Why this focus on the ordinary? Because it's often in the seemingly trivial that we find the greatest untapped markets.  The truly disruptive ideas are not the ones that introduce entirely new concepts, but the ones that refine, simplify, and perfect existing ones, making them accessible to a wider audience.  This requires a different mindset, one that values practicality over novelty.

The seductive allure of the complex is a trap.  Complex ideas often come with complex execution, increased development time, and a heightened risk of failure.  Simple ideas allow for faster iteration, quicker user feedback, and a more agile response to market demands.  This speed is critical in the early stages of a startup.

This isn't about being unambitious; it's about strategic focus.  Instead of searching for a groundbreaking innovation, focus on a specific, well-defined problem within a market you understand intimately.  This intimate knowledge allows for a more nuanced and effective solution.

The path to identifying these ""mundane marvels"" involves a shift in perspective.  Start by meticulously observing your own life and the lives of others.  What daily frustrations exist? What tedious tasks could be automated? What inefficiencies plague existing systems?  These are the seeds of great startup ideas.

Don't dismiss the seemingly small.  A small improvement in an existing process, a minor inconvenience solved with ingenious simplicity, can translate into significant market traction.  The key is to identify problems that resonate with a sizable target audience and address them with efficiency and elegance.

In the end, the most successful startups often aren't the most technologically advanced or conceptually revolutionary; they're the ones that effectively address common, everyday problems.  This often requires less innovation and more a keen eye for opportunity within the everyday.  The rewards for finding these opportunities are substantial.
","startup advice
",AI,"human
","AI
"
134,134,"The Myth of the ""Smart Money"": Why Early-Stage Funding Isn't About the Money

(September 2023)

The phrase ""smart money"" is bandied about in startup circles like a sacred mantra.  It implies that the value of early-stage investment lies not just in the capital, but in the wisdom and network of the investor.  While partially true, this is a misleading oversimplification. In reality, the equation shifts dramatically at the earliest stages.  The dominant factor is not the money itself, but the *absence* of certain constraints that later-stage funding inevitably brings.

Consider the classic venture capital model.  Millions in funding, board seats, detailed business plans, and a relentless focus on metrics.  This is appropriate for companies with established products, sizable teams, and significant burn rates.  But for the nascent startup, barely a glimmer of an idea in the minds of a few founders, this is an albatross.

What these early-stage ventures actually need is a different kind of ""smart money""—a catalytic agent, not a financial behemoth.  The ideal investment at this point is a relatively small sum, sufficient to cover lean living expenses and allow for relentless focus on execution.  More importantly, it's access to a network of experienced mentors, advisors, and other founders, who can guide the team through the treacherous early phases of product development and market validation.

The early-stage struggle is not primarily financial.  It’s a war against uncertainty, against the overwhelming number of unknowns.  It’s navigating the fog of the unknown, testing assumptions, pivoting rapidly, and building something people actually want.  This requires an environment where failure is an acceptable outcome, not a catastrophic event.

Large sums of money, often associated with later-stage funding, tend to increase the pressure and the risk aversion.  This can stifle experimentation and lead to premature optimization. The founders become preoccupied with demonstrating return on investment to their investors, rather than pursuing the most promising path, however unconventional.

The real value proposition of effective early-stage funding is, therefore, the creation of a supportive ecosystem that minimizes distractions and fosters a culture of rapid iteration.  It's about providing a runway for ambitious founders to pursue bold ideas without the suffocating weight of excessive capital and the attendant expectations.  This means carefully curating the investor relationships, prioritizing mentorship over mere monetary injection, and focusing on building a strong foundation rather than chasing short-term wins.  That, more than any amount of “smart money,” is the true alchemy of early-stage success.
","startup advice
",AI,"human
","human
"
135,135,"July 2024

The Most Obvious Startup Advice (That No One Follows)

I’ve spent years advising founders, witnessing both spectacular successes and spectacular failures.  The patterns are surprisingly consistent.  And the most striking thing isn't some complex strategic insight, but rather the consistent disregard for the most obvious advice.  Let's address it directly.

The single biggest mistake founders make?  They fail to build something people actually want.

It sounds simplistic, almost tautological.  But it's the bedrock.  All the fancy marketing, the slick presentations, the brilliant engineering – they’re worthless if the core product misses the mark.  Think of it as the fundamental axiom of startup physics.  Violate it, and your venture is doomed, regardless of funding or team brilliance.

Why is this so often ignored?  It’s a matter of misplaced priorities.  Founders, especially technically-minded ones, tend to focus on what *they* find interesting, or what *they* think is innovative.  They build elegant solutions to problems no one actually has. This isn't just a failure of market research; it's a failure of empathy.  You need to understand your target users intimately.  What are their frustrations?  What truly makes their lives easier?

The solution isn’t complex.  It’s painstakingly simple.  Start with a minimal viable product (MVP).  Get it in front of users.  Listen, obsessively, to their feedback.  Iterate relentlessly, shaping the product based on real-world use, not theoretical ideals.  This iterative process is not a luxury; it’s a survival mechanism.

Don’t get bogged down in perfect designs or fully fleshed-out features.  Focus on solving a critical problem, even imperfectly, for a select group of users.  Once you have evidence of product-market fit—that the thing you’ve built resonates with real people—then, and only then, can you confidently scale.

This isn't just startup advice; it’s a fundamental truth about building anything successful.  The best products are rarely the result of brilliant flashes of inspiration; they're the result of painstaking attention to user needs, a relentless pursuit of improvement, and the courage to adapt.  Embrace the obvious.  It's often the key that unlocks everything else.
","startup advice
",AI,"human
","AI
"
136,136,"The Unexpected Resilience of the Bootstrap

The conventional wisdom about startups is heavily weighted towards venture capital.  Secure funding, scale aggressively, dominate the market.  This narrative, however, overlooks a crucial shift: the plummeting cost of starting a company.

For years, the startup world revolved around a simple equation:  big ideas required big money.  Hardware was expensive, software development complex and costly, marketing a black hole for resources.  Venture capital was the lifeblood.

But that equation is no longer valid. Moore's Law, open-source software, the ubiquity of the web, and increasingly powerful programming languages have combined to drastically reduce the barrier to entry.  Today, many successful startups—perhaps even a majority—could be considered ""bootstrappable.""  Their biggest expense? The founders' salaries.

This isn't a romantic idealization of scrappy entrepreneurs; it's a practical observation. We've seen startups generate significant revenue—enough to achieve profitability—with monthly income figures that would be laughed at in the context of traditional venture-backed scaling.  A few thousand dollars a month can be transformative.  It's not just about the money; it's about the runway.  Once profitability is achieved, even at a modest level, the existential threat of running out of cash evaporates. The runway becomes infinite.

The implications are profound.  The traditional relationship between startups and VCs, once symbiotic, is loosening.  While founders might still seek funding, it's no longer a necessity for many.  They're less reliant on the whims of investors, less pressured to conform to venture-backed growth models.

This shift hasn't been dramatic or obvious.  The post-dot-com crash saw a significant decline in both VC investment and the number of new startups. This time, however, things could be different. The ability to operate profitably on modest revenues, coupled with the continuing entrepreneurial spirit, suggests a decoupling.  Founders might simply decide that the hassle of seeking, negotiating with, and ultimately answering to investors isn't worth the cost, particularly given the current economic uncertainty.

In short, the very foundations of the startup world are shifting.  The dominance of the VC-backed model is being challenged, not by some disruptive technology, but by the quiet, persistent force of declining startup costs. The era of the bootstrap is not just a nostalgic ideal; it's a practical reality, and its influence is only beginning to be felt.
","startup advice
",AI,"human
","AI
"
137,137,"The Myth of the Minimum Viable Startup

The startup world is awash in advice, much of it contradictory and ultimately unhelpful. One particularly pernicious myth is the concept of the ""minimum viable product"" (MVP).  The idea, ostensibly, is to build the smallest possible thing to test your assumptions and iterate quickly.  In practice, it often leads to a half-baked product that fails to gain traction, leaving founders scrambling to salvage a flawed vision.

The problem isn't the idea of iteration; iterating is crucial. The problem is the framing.  The focus on ""minimum"" implicitly suggests a race to the bottom, a competition to see who can build the least amount of product with the fewest resources. This prioritizes speed over substance, a fatal flaw in a world where genuine innovation requires significant effort.

Think of it like building a bridge.  Would you advocate for a ""minimum viable bridge""—a rickety structure barely capable of supporting a single pedestrian? Of course not.  You’d build a sound, robust bridge, capable of withstanding significant weight and traffic.  The same principle applies to startups.

A truly successful startup is built on a foundation of significant, well-considered features.  The initial product must be something demonstrably valuable, not just something barely functional.  It should represent a substantial improvement over the existing alternatives, offering a compelling reason for users to adopt it.

Now, this doesn't mean meticulously crafting every detail before launch.  Far from it. It means prioritizing the core functionality that defines your product's value proposition.  Focus on building a solid core, then iterate to enhance and expand.  The emphasis is on ""value"" not ""minimum.""

This approach requires a different mindset.  It demands a deep understanding of the problem you're solving and a relentless pursuit of quality.  It's less about hacking together a quick prototype and more about building something worth using.

This shift in perspective is essential for survival.  The market is unforgiving.  A half-hearted product will quickly fade into oblivion, while a truly valuable offering will attract users and investors.  Forget the MVP; build a product worthy of success.  The difference is not subtle, it is profound.  And it's the difference between failure and enduring success.
","startup advice
",AI,"human
","AI
"
138,138,"The Startup Delusion:  Why You Should Build Something People Need, Not Something That Makes Money

The conventional wisdom for startups is brutally simple: build something people want, and figure out how to make money from it later.  This, however, contains a subtle and dangerous flaw.  It implies a two-stage process, a separation of creation and monetization. In reality, the most successful startups often blur this distinction entirely.

Consider the seemingly counterintuitive implication:  If you focus solely on creating something people need, regardless of immediate profit, you've essentially described a charity.  And yet, many wildly successful companies operate, at least in their early stages, with a strikingly charitable ethos.

This isn’t about altruism masquerading as business acumen.  It’s about recognizing a fundamental truth: genuine value precedes profitable exploitation.  Creating something people truly need creates a powerful feedback loop.  Users become evangelists, growth becomes organic, and the market rewards this demonstrable value far more readily than clever monetization schemes grafted onto mediocre products.

The Craigslist Conundrum

Craigslist's minimalist design, famously low overhead, and almost passive approach to monetization are often cited as anomalies.  But the anomaly isn't its unconventional business model; it's its unwavering focus on solving a crucial user need—connecting buyers and sellers efficiently.  Its success is a testament to the inherent power of providing genuine value.  It's a company operating *despite* a lack of aggressive monetization, not *because* of it.

The Google Genesis

Google's initial years are similarly instructive. The absence of intrusive advertising in their early days wasn't a calculated strategy; it was a natural consequence of prioritizing a singular, valuable goal:  building a comprehensive and usable search engine.  This dedication attracted users, which subsequently attracted advertisers.  The profits followed organically, as a *consequence* of their core value proposition, not as a primary design goal.

The Anti-Delusion

The ""make something people want"" mantra frequently leads founders astray. The focus shifts from *what* is being created to *how* it's monetized.  But the market is far more discerning than most founders believe.  A cleverly packaged, but ultimately useless, product will fail regardless of its business model.

The true path to success lies in recognizing that genuine need is not just a moral imperative, but a powerful engine of growth.  Prioritize solving real problems for real people.  The financial rewards will follow naturally.  Forget the business model; focus on building something extraordinary. Then, and only then, will the money follow.
","startup advice
",AI,"human
","AI
"
139,139,"The Myth of the Lean Startup: Why You Need More Than Ramen

The current startup dogma preaches leanness: minimal viable products, bootstrapping, ramen-fueled nights.  This gospel, while superficially appealing, often leads to a fatal flaw: a failure to grasp the brutal reality of market dynamics.

The truth is, many successful startups require significant resources, not just to build a compelling product, but to navigate the complexities of acquiring and retaining customers.  Leanness can be a virtue, but only when applied strategically, not as a blanket prescription.

Consider the early days of a truly disruptive technology.  The initial product might be simple, almost crude.  But scaling that product, overcoming the inertia of established players, and educating the market—these all demand substantial investment.  This is not about extravagant spending; it’s about understanding the necessary scale of operation to achieve critical mass.

Let’s examine the counterintuitive relationship between initial leanness and later scaling.  A founder might craft a beautifully simple MVP, perfectly tailored to a niche.  But to break into the broader market, this MVP often requires substantial re-engineering and marketing efforts.  The initial leanness may only be a stepping stone to a future of considerably larger expenditures.  Those who mistake the minimalist origins for an ongoing operational blueprint are often destined for failure.

The crucial element often overlooked is timing.  A company might be lean initially, but it's the judicious application of resources *at precisely the right moment* that decides victory or defeat.  This timing necessitates a keen understanding of market dynamics, the competitive landscape, and the capacity of the team.  These are aspects that can only be determined through careful observation, experience, and often, substantial investment.

The romanticized image of the lone coder bootstrapping their way to a billion-dollar valuation is an exception, not the rule.  For the vast majority of startups, building a significant, sustainable business requires far more than a frugal lifestyle and a great idea.  It necessitates a strategic blend of lean practices and a readiness to invest when the opportunity demands it.  Those founders who are too rigidly committed to leanness, to the detriment of strategically timed scaling, will find themselves out-maneuvered by competitors with the foresight and resources to grow rapidly and effectively.  The truly successful don’t just build a great product; they build a great business. And that often costs more than ramen.
","startup advice
",AI,"human
","AI
"
140,140,"The Most Misunderstood Thing About Startups

Startups are hard.  Everyone knows that.  But what's less understood is *why* they're hard, and what that implies about how to prepare for one.  The difficulty isn't primarily technical, or even business-related, in the conventional sense. It's something far more fundamental: startups defy intuition.

Most people approach problem-solving with a set of ingrained heuristics, developed over years of navigating the relatively predictable environments of school and established companies.  These heuristics, while useful in those contexts, often lead to disastrous results in the chaotic landscape of a nascent company.  It's like trying to navigate a mountain pass using a city map.

Let's consider three core misconceptions:

1. **The Illusion of Control:** The belief that you can plan, predict, and control the trajectory of your startup is a dangerous fallacy.  External factors—market shifts, unexpected competition, technological breakthroughs—are constantly in flux, rendering precise planning futile.  Successful founders are not masters of control, but rather adept improvisers, capable of swiftly adapting to unforeseen circumstances. They build flexibility into their strategies.  A rigid plan is a liability.

2. **The Myth of Expertise:**  Many aspiring entrepreneurs mistakenly believe that specialized knowledge in business administration, finance, or even a specific industry is crucial.  While some level of business acumen is helpful,  what truly matters is deep understanding of your target users and their unmet needs.  The best ideas often emerge from a place of genuine passion and direct experience, not from textbook knowledge. Mark Zuckerberg didn't become a startup guru before Facebook; he became one *through* Facebook.

3. **The Trap of ""Playing House"":**  The outward trappings of a startup—a slick presentation deck, a fancy office, a sizable funding round—are often mistaken for actual success.  In reality, these are merely superficial indicators, easily acquired without any meaningful underlying progress. True success hinges on building something genuinely useful and valuable, something that resonates with users and drives tangible growth.   The most crucial metric isn’t funding, but engagement.  Many startups fail because they focus on the theater instead of the performance.

So, what should you do? Focus on the fundamentals. Develop deep domain expertise, not in startup theory, but in the area you plan to disrupt. Embrace a flexible approach to planning.  Don't chase funding; build something users crave, and the funding will follow. And finally, remember that starting a startup is a marathon, not a sprint. The journey will be full of surprises, both exhilarating and challenging, but the rewards are worth the effort for those who truly persevere.
","startup advice
",AI,"human
","AI
"
141,141,"The Accidental Entrepreneur: Why You Should Embrace the Mess

The romanticized image of the lone genius toiling away in a garage, fueled by ramen and sheer will, persists for a reason.  It's not just a charming narrative; it speaks to a fundamental truth about innovation.  The most groundbreaking ideas often emerge not from the meticulously planned strategies of established institutions, but from the chaotic, unconstrained spaces occupied by outsiders.

Why is this?  One obvious factor is sheer numbers.  There are far more outsiders than insiders, statistically increasing the probability of a breakthrough originating from their ranks.  But the advantage goes deeper than mere probability.  Insiders, by their very nature, are constrained.

Consider the corporate world.  The path to success within a large organization often rewards conformity and political maneuvering over genuine innovation.  Climbing the corporate ladder demands a mastery of internal politics, a skill set rarely shared by those most adept at disruptive thinking.  The result?  Brilliant minds find themselves stifled by bureaucratic inertia, unable to pursue truly radical ideas.

This isn't to say that large corporations are inherently incapable of innovation.  They possess resources and infrastructure that startups often lack.  However, their internal structures often create a kind of anti-test, filtering out the very individuals with the potential for transformative change.

The outsider, however, possesses a crucial advantage: a lack of anything to lose.  They can take risks, experiment with unconventional approaches, and embrace failure as a necessary step in the learning process.  This freedom to fail is often the key to unlocking true innovation.

The path to success for the accidental entrepreneur lies in identifying the weaknesses of the established order.  Look for areas where the selection process has become corrupted, where the established metrics of success are misaligned with genuine progress. These are the fertile grounds for disruption.

Another advantage of the outsider is the availability of time and focus.  Unburdened by the demands of management or the need to justify every minute spent, the accidental entrepreneur can dedicate uninterrupted stretches of time to deep work, allowing for moments of insight and breakthrough that are almost impossible to achieve in a fragmented, highly scheduled environment.

Finally, the outsider often benefits from a crucial element frequently overlooked: the sheer joy of creation.   Unconstrained by the pressures of external expectations and the need to conform, they can pursue projects out of pure passion and curiosity, leading to a level of engagement and intensity that translates directly into superior results.

So, embrace the mess.  Don't be afraid to hack something together, to experiment, to fail, and to learn. The accidental entrepreneur, operating outside the confines of established norms, often possesses the greatest potential for transformative change.  The future, quite possibly, belongs to those who are willing to get their hands dirty.
","startup advice
",AI,"human
","AI
"
142,142,"The Startup Founder's Paradox:  Why Being Nice Works

Startups are often portrayed as ruthless battlefields, a Darwinian struggle for survival where only the cunning and aggressive prevail.  While there’s certainly a competitive element, a counterintuitive truth emerges:  benevolence, far from being a weakness, can be a surprisingly potent weapon.

The common wisdom focuses on building a Minimum Viable Product (MVP), securing funding, and aggressively capturing market share.  These are all important, but they miss a crucial element: the underlying philosophy that guides your decisions.  A startup isn't just about building a product; it's about building a sustainable ecosystem.

Consider this: many highly successful companies, in their early stages, resembled nonprofits. They prioritized user experience and solving a real problem above immediate profit maximization.  They were, in a sense, relentlessly benevolent. This wasn’t a calculated strategy; it was a byproduct of their founders' genuine passion and commitment to their vision. This commitment fostered a strong internal culture and created a virtuous cycle: happy users became advocates, attracting more users and eventually investors.

This benevolent approach isn’t just about altruism; it's about creating a defensible position.  When your users are intrinsically loyal, they're less likely to switch to a competitor. This loyalty translates into a sustainable growth trajectory, making your startup less vulnerable to market fluctuations.

The focus shouldn't be on short-term gains but on long-term growth.  Measure your progress not just in dollars, but in the number of happy, engaged users.  A steady, organic growth fueled by user satisfaction is infinitely more valuable than a short-lived spike driven by aggressive marketing tactics.

The benefits extend beyond the user base.  A reputation for integrity and user-centricity attracts talent.  The best engineers want to work on projects they believe in, and the best investors gravitate towards companies with strong ethical foundations.

Starting a startup is a high-risk endeavor.  But by focusing on user needs above all else, you not only increase your chances of success, you build a company that is both valuable and resilient.  The paradox is that by prioritizing benevolence, you create a powerful engine for growth.

Don’t simply aim to “not be evil.”  Strive to be genuinely helpful.  The market will reward you for it.
","startup advice
",AI,"human
","human
"
143,143,"The Most Valuable Thing in a Startup:  Ignoring the Obvious

Startups are often lauded for their innovation, their rapid growth, their ability to disrupt established industries.  But these are merely symptoms of a deeper, more fundamental truth: successful startups ignore the conventional wisdom.  They operate on a different plane, governed by a different set of rules.  This is not about defying norms for the sake of it; it's about identifying the most efficient path to growth, even if that path seems counterintuitive.

Consider the common advice given to entrepreneurs: build a minimum viable product (MVP), validate your idea quickly, focus on customer acquisition. While sound in principle, slavish adherence to this formula often leads to mediocrity. The truly exceptional startups don't simply validate their initial idea; they discover their idea through a process of relentless, iterative growth.

Instead of starting with a pre-defined market and meticulously testing their assumptions, the most successful startups establish a growth target—a specific, measurable, achievable goal. They then relentlessly optimize for that growth, adapting and iterating their product, their market, even their fundamental business model, until they achieve their objective.  This isn't about blindly pursuing growth at all costs; it’s a process of discovery, where the relentless pressure to achieve the growth target forces founders to identify and exploit previously unseen opportunities.

This approach creates a powerful feedback loop.  The need for growth reveals weaknesses, prompting founders to make difficult, even radical changes.  Ideas that were initially dismissed as irrelevant may suddenly become crucial in the context of achieving the growth target.  The process itself becomes a powerful engine of innovation.

Of course, this is not a guarantee of success.  Many startups fail, even those that pursue relentless growth.  But the odds are significantly better than simply following the well-trodden path of validated assumptions.  The ability to adapt, to pivot, to embrace the unexpected—these are the hallmarks of startups that not only survive but thrive.

The question isn't ""what should I build?"", but ""how can I grow at X% per week?""  The answer to the latter question frequently reveals a far better idea than the former could ever have generated. The relentless pursuit of growth isn’t just a strategy; it’s a compass, guiding the startup through the often turbulent waters of the entrepreneurial landscape.  Ignore the noise, focus on the signal, and let growth reveal your true potential.
","startup advice
",AI,"human
","AI
"
144,144,"The Myth of the ""Level 10"" Hire

Founders are constantly told they need to hire “level 10” people—individuals with exceptional talent and experience.  The implication is that these superstars will magically solve all problems, allowing founders to step back and focus on “higher-level” strategy.  This is, in my view, a dangerous delusion.

The reality is that the best hires for a young startup aren’t always the most experienced.  In fact, often the opposite is true.  Overly experienced hires, accustomed to established corporate hierarchies, can be surprisingly ineffective in the chaotic environment of a rapidly growing company.  They may resist the unconventional methods and rapid iterations that define a successful startup. Their comfort zone is within structures already in place, not creating them. They are masters of the existing game, not pioneers of a new one.

What, then, makes a truly effective early hire?  The answer is less about titles and experience, and more about a specific combination of qualities:

* **Adaptability:** The ability to thrive in ambiguity, to embrace change, and to learn quickly are far more valuable than years spent mastering a single, possibly obsolete, skillset.

* **Execution:**  The capacity to not just brainstorm but to translate ideas into tangible results. A ""level 10"" idea-generator is useless if they can't ship.

* **Ownership:**  A willingness to take responsibility, to identify and solve problems proactively, without waiting for explicit instructions.  This is where many ""experienced"" hires fail.  They expect to be told exactly what to do, instead of owning pieces of the problem space.

* **Culture Fit:**  This often overlooked aspect is crucial.  The early team must be cohesive, able to work effectively together under pressure.  Bringing in someone who expects a traditional corporate structure will disrupt the dynamic, and likely poison the well.

Finding these qualities often means looking beyond the traditional metrics of a resume.  Consider individuals who have demonstrated initiative in personal projects, open source contributions, or unconventional entrepreneurial ventures.  These are the individuals who are often more attuned to the startup mindset.  They are builders, not just managers of existing resources.


The focus should be less on finding “level 10” individuals, and more on assembling a high-performing team, comprised of individuals with complementary skills, who work seamlessly together towards a common goal.  The right ""level 7"" person, hungry, adaptable, and willing to learn, can often be far more valuable than a highly paid, jaded ""level 10"" hire who's more interested in the job title than the mission.  Remember, you're building a rocket, not a bureaucracy.
","startup advice
",AI,"human
","AI
"
145,145,"The Startup Grind: Why You Should Embrace the Chaos

Startups. The word conjures images of ramen-fueled nights, improbable successes, and the ever-present specter of failure.  But beneath the surface of the Silicon Valley mythos lies a stark economic reality, one that’s both brutal and beautiful.  It’s a reality that hinges on a simple, almost brutal truth: the relentless pursuit of disproportionate output.

Most people operate within a constrained system.  They exchange their time for money, trading hours of often unfulfilling work for a predictable (and usually modest) paycheck.  Their productivity is averaged, diluted in the vastness of a large organization.  This is the comfortable, but ultimately limiting, path of least resistance.

Startups are the antithesis of this. They are, in essence, high-pressure cauldrons designed to maximize individual contribution.  A small team, tightly knit and intensely focused, attempts to solve a difficult problem.  The reward, if successful, is exponential.  The risk?  Equally so.

Why the emphasis on difficulty? Because difficulty acts as a natural barrier to entry. It’s a moat, protecting your creation from the inevitable onslaught of larger, more established competitors.  A simple problem attracts simple solutions.  Only those willing to grapple with complexity can truly innovate.

This isn't to say that startups are about masochism. Rather, it's about strategically choosing your battles. It’s about identifying the terrain where your agility and intensity become decisive advantages. It’s about finding the problem that demands not just competence, but the kind of passionate, all-consuming dedication that only a small, fiercely driven team can muster.

The process itself is inherently chaotic.  Ideas will pivot, strategies will adapt, and failure will loom as a constant companion.  But this very instability, this constant state of flux, is the crucible in which innovation is forged.  It forces adaptation, demands resourcefulness, and rewards those who can navigate ambiguity.

So, the next time you’re considering the startup path, remember that it’s not for the faint of heart. It’s a high-stakes game, a gamble on both your skills and your resilience.  But for those who dare to play, the rewards can be truly transformative. It's the ultimate test of individual potential, a chance to compress decades of incremental progress into a few intense, exhilarating years.  Embrace the chaos.  It might just be the key to unlocking something extraordinary.
","startup advice
",AI,"human
","AI
"
146,146,"The Myth of the Overnight Success:  Why ""Scaling"" Is the Wrong Metric for Early-Stage Startups

The conventional wisdom about startups often feels…off.  The popular narrative emphasizes explosive growth, viral adoption, and the mythical “overnight success.”  But  this is a misleading simplification, a convenient fiction obscuring the messy reality of building something genuinely valuable.  Most startups don't just ""take off""; they're *made* to take off.

Consider the engine of a classic car.  It requires a laborious crank to initially start; once running, it's self-sustaining.  Similarly, most successful startups require an initial, often unscalable push to overcome inertia.  This isn’t about some magical trick; it’s about strategically leveraging the advantages of being small.

One such unscalable yet crucial task is direct user acquisition.  Don't wait for users to find you; proactively seek them out.   This might involve personally onboarding users, meticulously customizing the experience for each individual, or even hand-crafting solutions before automation is feasible.  This direct engagement provides invaluable, hyper-specific feedback that's impossible to replicate later.

Many founders resist this hands-on approach due to a combination of shyness, laziness, and a misunderstanding of growth.  They underestimate the power of compounding.  Focusing on a weekly growth rate, however incremental, yields surprising results over time.  The initial small numbers, seemingly insignificant, will compound into substantial growth with persistent effort.

This isn't just about numbers; it's about building a strong foundation.  Early users should feel an unparalleled level of care and attention. This level of engagement, while not scalable in the long run, creates brand loyalty and generates incredibly valuable feedback.  It also subtly shapes the company culture, making customer delight a core value that persists even as the company scales.

Another tactic, particularly effective for hardware or niche markets, is the “contained fire” approach.  Instead of trying to boil the ocean, focus on a very specific segment – a single city, a particular type of user, or a tightly defined problem.  Achieving rapid success within this niche creates a critical mass that provides both momentum and essential feedback for expansion.  Many successful startups unknowingly employ this strategy, initially building something for themselves and a close network of early adopters.

The crucial point is this:  judge your early-stage startup not by its potential to conquer the world immediately, but by the potential it possesses *given the right strategic actions.* These actions, often unscalable and seemingly inconsequential at the time, are often the very things that shape a company's enduring success.  The seemingly insignificant actions of early days—manual user acquisition, hyper-personalization, laser-focused niche selection—frequently shape the trajectory of future growth far more than any grand launch.  Embrace the ""unscalable"" phase; it's where true, sustainable growth is often forged.
","startup advice
",AI,"human
","AI
"
147,147,"October 26, 2023

The Myth of the Overnight Success:  Why Premature Optimization Kills Startups

The startup world is rife with mythology.  One of the most pervasive is the idea of the overnight success.  The image conjured is of a brilliant founder, a killer idea, and instant riches.  Reality, however, is far less glamorous.  The truth is, most successful startups are the result of years of painstaking work, iterative refinement, and a healthy dose of stubborn perseverance.  And one of the biggest obstacles to that journey is premature optimization.

What do I mean by premature optimization?  It's the trap of focusing on secondary concerns before you've nailed the fundamentals.  It's spending weeks crafting the perfect pitch deck before you even have a minimally viable product.  It's agonizing over the details of your business model before you've validated your core assumptions.  It's building a sophisticated marketing strategy for a product nobody wants.

This is not to say that planning is unimportant.  Of course, you need a plan. But your initial plan should be a roadmap, not a detailed blueprint.  It's a framework to guide your early steps, not a rigid structure to constrain your progress.  Premature optimization is like trying to paint a masterpiece before you’ve even sketched the outline – the result is likely to be messy, incoherent, and ultimately unsuccessful.

The most important thing in the early stages of a startup is building something people want.  Everything else is secondary.  Your business model, your marketing strategy, your fundraising efforts – these are all important, but they're only relevant if you have a product that people are willing to pay for.

So how do you avoid the trap of premature optimization?  Here's my advice:

1.  Focus on building a minimally viable product (MVP).  This is the simplest version of your product that still delivers core value to your users.  Get it out there quickly, gather feedback, and iterate based on what you learn.

2.  Validate your assumptions relentlessly.  Don’t just assume that people want your product – test it.  Talk to potential customers, gather data, and use that information to guide your development.

3.  Be willing to pivot.  Your initial idea may not be the right one.  Be flexible, be adaptable, and be willing to change course if necessary.

4.  Don't get bogged down in the details.  Focus on the big picture.  What's the core value proposition of your company?  What problem are you solving?  Once you have a clear understanding of these things, you can start to focus on the details.


The path to startup success isn't a sprint, it's a marathon. And like any marathon, it's not won by focusing on the minutiae early on, but by pacing yourself, building a solid foundation, and keeping your eye on the finish line.  Don't let premature optimization derail your journey before it even begins.
","startup advice
",AI,"human
","human
"
148,148,"The Most Important Skill for Startup Founders:  Strategic Improvisation

Most startup advice focuses on the ""what""—the idea, the market, the business model.  But the ""how""—the execution—is far more important.  And the most crucial skill for successful execution in the chaotic world of startups isn't meticulous planning, but something far more elusive:  strategic improvisation.

Let's dispense with the romantic notion of the brilliant founder with a fully formed vision.  While a compelling initial idea is helpful, it's rarely the whole story. The reality is that startups are inherently unpredictable.  The initial plan is almost always a best guess, a tentative first step into uncharted territory.  The truly successful founders are not those who perfectly anticipate the future, but those who adapt brilliantly to its surprises.

Consider the early days of Google.  The initial vision was a superior search engine, a clear improvement over existing solutions.  But the path to becoming a global behemoth was far from linear.  They iterated constantly, responding to user feedback, adapting their technology, and seizing unexpected opportunities.  Their success wasn't about sticking rigidly to a pre-ordained plan, but about mastering the art of adapting and improving along the way.

What exactly constitutes this ""strategic improvisation""?  It's not simply reacting to events.  It's about maintaining a clear sense of direction while possessing the flexibility to adjust your tactics as needed.  It’s the ability to see unexpected opportunities, to identify valuable detours, and to relentlessly pursue progress in the face of constant setbacks. It's about identifying the most promising path forward, even if that path wasn’t part of the original map.

This skill is not innate; it's cultivated through experience, iterative learning, and a willingness to embrace failure as a source of valuable feedback.  The founders who truly excel are not afraid to experiment, pivot, and refine their approach based on what they learn.  They are relentless in their pursuit of progress, constantly adapting and improving.

It is this dynamic interplay between strategic vision and adaptable execution, a relentless pursuit of progress through strategic improvisation, that truly separates the winners from the also-rans. It's not just about having a good idea; it's about having the capacity to relentlessly improve upon it, adapting it and refining it, in response to the challenges and unexpected opportunities that inevitably arise along the way.  This ability to improvise, to adapt, to relentlessly pursue progress despite uncertainty—this is the true essence of startup success.
","startup advice
",AI,"human
","AI
"
149,149,"The Most Important Thing You Won't Learn in Business School

Startups are, to put it mildly, weird.  Most people’s intuition about how to succeed in business is completely wrong when applied to startups.  This isn't a matter of subtle differences; it's a fundamental mismatch.  It’s like trying to navigate using a map of a completely different city.

Why is this?  Perhaps it's because the knowledge required to build a successful startup hasn’t yet permeated popular culture.  Whatever the reason, the first thing any aspiring founder needs to learn is to distrust their initial instincts.  This is especially true for young founders, fresh out of a system designed to reward the appearance of effort over actual results.

The most common mistake is focusing on the wrong metrics.  Business school teaches you to optimize for things like profitability, efficiency, and market share.  In the startup world, these are secondary considerations.  The primary metric is growth.  Aggressive, relentless growth.  All else flows from that.  If you’re not growing at a rate that makes your competitors look like they’re standing still, you're probably doing something wrong.

This obsession with growth often clashes with what seems like common sense.  It leads founders to make decisions that appear reckless, even suicidal, to outsiders.  But these decisions are often necessary to gain the early momentum that can catapult a company to success.  They are counterintuitive, but in the startup world, counterintuitive is often correct.

Another crucial area where intuition fails is in team building.  You might be tempted to surround yourself with people who have impressive resumes and extensive experience.  Don’t.  Experience in large, established companies is often a liability in a startup.  The skills you need are adaptability, resilience, and an ability to work effectively under immense pressure.  Choose people who possess these qualities, even if their resumes are less than perfect.  Trust your gut; your instincts about people are generally reliable.  Work with people you genuinely like and trust.

Finally, and perhaps most importantly, remember that starting a startup is not a game.  There's no secret formula, no trick to learn.  You can’t game the system.  You succeed by building something people want, and you succeed only to the extent you do that.  The rest – the fundraising, the marketing, the management – are all secondary.  Focus on building a great product, and the rest will follow.  The single most valuable thing you can do in college is to learn deeply about something you’re passionate about.  The best startups almost always begin as side projects, driven by genuine curiosity, not some preconceived notion of building a company.  That's how you turn your mind into the kind of mind that generates winning startup ideas.  Just learn.
","startup advice
",AI,"human
","AI
"
150,150,"The Founder's Paradox: Why Hard Problems Make Easy Money

The conventional wisdom about startups is often wrong.  Many believe the key is a ""disruptive"" idea, a catchy name, or slick marketing.  They're wrong. The real secret to startup success lies in a seemingly paradoxical approach: embrace difficulty.

The economics of a startup are simple, yet often misunderstood.  It's a high-leverage, high-risk endeavor. You compress years of work into a short period, aiming for outsized returns.  But this intense focus isn't on some abstract notion of ""innovation,"" but on solving hard problems.  The harder the problem, the greater the potential reward, and the more defensible your position becomes.

Why hard problems?  Because they create substantial barriers to entry.  Big companies, with their inertia and bureaucracy, struggle to compete in spaces demanding rapid iteration and unconventional thinking.  They’re like lumbering giants trying to navigate a mountain pass; nimble startups scale the cliffs with ease. This isn't about being contrarian for contrarian’s sake; it's a strategic advantage.

Consider the analogy of a craftsman. A skilled artisan doesn't create value by simply producing more of the same; they create value by mastering difficult techniques, by building something exceptional.  Software developers, arguably the modern equivalent of the craftsman, create wealth through code – a finished product born from painstaking effort and deep expertise.

This approach demands an intense focus on execution.  Measure progress not just by arbitrary milestones, but by the concrete impact on users.  The number of users is the ultimate yardstick. It’s a direct measure of wealth created, something that even the most seasoned investor can’t ignore. It's not enough to simply build something technically impressive; you need to build something people genuinely want, something that improves their lives in a meaningful way.

This philosophy isn’t without its risks.  Startups are all-or-nothing propositions.  Many fail.  But those that succeed often do so because they solved a truly difficult problem, leaving competitors in their wake.  The higher the risk, the higher the reward. The intense effort demanded is compressed into a shorter time frame, leading to a potentially lucrative payoff.

So, the next time you’re contemplating a startup, ask yourself:  What truly difficult problem are you uniquely positioned to solve?  Ignore the noise; focus on the hard work.  That’s where the real value, and the real wealth, lies.
","startup advice
",AI,"human
","AI
"
151,151,"The Unsung Virtues of Mediocrity in Startup Land

The tech world venerates the exceptional.  The mythical overnight success, the visionary founder, the unicorn startup – these are the narratives that dominate our collective imagination.  But the reality, as always, is more nuanced.  In fact, I'd argue that a healthy dose of ordinariness is crucial to building a sustainable and ultimately successful company.

Consider the average startup.  It's not building the next revolutionary operating system or designing the self-driving car of the future. It's solving a small, often overlooked problem for a specific niche market. It's not flashy, it's not groundbreaking, but it's steadily profitable.  And that, my friends, is the unsung magic of the mediocre.

The allure of the grandiose, the ""moonshot"" approach, often blinds founders to the potential of the mundane. They chase ambitious, complex projects that require years of development and millions in funding.  They get bogged down in the technical complexities, neglecting the essential element of customer demand.   This is the digital equivalent of building a magnificent cathedral in a desert.

The power of mediocrity lies in its practicality. By focusing on a readily solvable problem within a defined market, a startup can achieve rapid iteration and feedback loops.  This allows them to pivot quickly and efficiently adapt to market changes.  They don't need a team of rocket scientists; a group of competent individuals, focused on delivering a usable product, is often far more effective.

This doesn't imply a lack of ambition.  It's a different kind of ambition: a focused, determined pursuit of sustainable growth.  It's about creating a valuable product that people actually want, rather than pursuing a technologically impressive, yet commercially irrelevant innovation.  It's about building a solid foundation, brick by brick, rather than attempting to construct an unstable skyscraper.

In the long run, sustainable growth trumps explosive but unsustainable growth.  The ""mediocre"" startup that consistently generates revenue and attracts loyal customers is far more resilient than the flashy startup that burns through capital and chases fleeting trends.

So, aspiring founders, don't be afraid to embrace the mundane.  Focus on building something useful, something practical, something people are actually willing to pay for.   Don't chase the mythical unicorn; build a strong, steady horse.  The path to success isn't paved with innovation alone, but with a relentless focus on customer needs and sustainable business practices.  And sometimes, that path leads through the surprisingly fertile ground of mediocrity.
","startup advice
",AI,"human
","AI
"
152,152,"The Myth of the Ruthless Founder

For years, the popular image of a successful entrepreneur has been a ruthless, cutthroat individual, willing to do whatever it takes to get ahead.  Think Gordon Gekko, but with a less flamboyant wardrobe.  This image, however, is largely a myth, particularly in the world of startups.

The reality is far more nuanced.  While ambition and drive are essential for success, the most successful founders are often characterized not by their ruthlessness, but by their ability to build something genuinely valuable.  Their focus is on creating a product or service that people genuinely want, not simply on extracting maximum profit at every turn.

This isn't to say that financial considerations are unimportant.  Far from it.  However,  the path to financial success in a startup is often indirect.  It's not about squeezing every last dollar from your users; it's about achieving exponential growth.  A slightly lower profit margin per user is inconsequential when your user base is growing at a phenomenal rate.

Consider the math.  A startup growing at 5% a week will see astronomical growth within a year or two.  Even if you're ""too nice"" and undercharge by half, you'll still witness substantial growth—and catch up to a more rapacious competitor remarkably quickly.  The difference in revenue will be far less significant than the difference in growth rate.

The key takeaway?  Optimize for growth rate, not immediate profit maximization.  Focus on creating a product so compelling that it spreads organically through word-of-mouth.  This is a far more sustainable and ultimately more lucrative strategy than relying solely on aggressive monetization tactics.

In short, the most valuable asset a startup founder possesses is not ruthlessness, but rather a relentless focus on building a great product and fostering a strong culture.  Niceness, far from being a weakness, can be a competitive advantage.  It can lead to stronger teams, more loyal customers, and ultimately, greater success. So, aspiring founders: be ambitious, be driven, but above all, be smart about your approach. The path to success is paved with excellence, not aggression.
","startup advice
",AI,"human
","AI
"
153,153,"The Myth of the Overnight Success:  Why Slow and Steady Wins the Startup Race

The tech world loves a good overnight success story.  The narrative is seductive:  a brilliant idea, a burst of funding, and then, *boom*, a billion-dollar valuation.  But this narrative obscures a crucial truth:  most successful startups aren't built overnight.  They're built slowly, deliberately, and often against considerable odds.

Investors, particularly those with short-term horizons, fall prey to this myth.  They chase the next big thing, the obvious winner, neglecting the subtle signals that truly great companies often lack in their early stages.  These signals, often mistaken for weaknesses, are the hallmarks of genuine innovation.

Consider this:  if an idea were obviously good, someone else would have already done it.  The most disruptive startups operate in spaces where the opportunity isn't immediately apparent.  They solve problems others haven't even recognized, or they find innovative solutions to existing problems that everyone else thought were intractable.

This is why the best founders often seem contrarian, even foolish, in their early stages. They are not just building a business, they are building a *future*.  This requires a long-term perspective, a tolerance for ambiguity, and an almost obsessive focus on execution.

The pursuit of rapid growth, so often touted as the key to success, can be a trap.  Chasing immediate traction can lead founders to compromise their vision, sacrifice product quality, or neglect the essential foundations of a durable business.  A slow and steady approach, focused on building a great product and a strong team, often yields far greater long-term returns.

The key is not to chase the illusion of the overnight success, but to embrace the reality of building something truly valuable.  This requires patience, perseverance, and a deep understanding of the problem you are solving, and the unique value your solution offers.  This is not a race; it is a marathon.  And the most important thing you need to succeed is endurance.
","startup advice
",AI,"human
","human
"
154,154,"The Counterintuitive Truth About Startup Success

It’s a common refrain: build a great product, and the money will follow.  While not entirely false, this maxim misses a crucial, counterintuitive truth about startup success.  The path to building a truly significant company often veers sharply from conventional wisdom.  It's a path paved with seemingly bad ideas, initially underwhelming traction, and a healthy disregard for early validation.

Consider the initial reception of many groundbreaking companies.  Would a pitch deck focused on ""a website for college kids to waste time"" (sound familiar?) have resonated with the average investor a decade ago?  Likely not.  Yet, that’s essentially what Facebook was in its nascent stages.  The brilliance, of course, lay not in the superficial description, but in recognizing an overlooked problem (connecting people) and solving it elegantly.  This wasn’t about a clever algorithm or complex technology; it was about identifying a latent need and fulfilling it in a way that felt both intuitive and engaging.

The core problem is this: good ideas rarely appear self-evident.  If an idea was instantly recognizable as a winner, someone would have already capitalized on it.  The most successful ventures often tackle problems deemed too niche, too difficult, or frankly, too boring by the mainstream.  They require a conviction that borders on stubbornness to pursue.

This isn’t to say that meticulous planning and execution are unimportant; they are.  But the initial spark is often found in a contrarian space, a domain where the potential for massive disruption outweighs the perceived risks.  And these risks are often substantial.  Early iterations are likely to be rough around the edges, the target market initially limited, and the financial projections modest (to put it mildly).

The key is to focus on the underlying problem, to identify the core need being addressed.  This core need often transcends superficial metrics and initial user feedback.  It requires a deep understanding of the market, a finely-tuned intuition about user behavior, and the resolve to ignore the noise.  The noise, I assure you, will be substantial.

Fundamentally, building a successful startup is less about having a polished pitch and more about having a compelling vision.  A vision not easily defined by conventional wisdom, a vision often initially misunderstood, but a vision that nonetheless holds the potential to reshape its corner of the world. The truly remarkable successes are often born from the seemingly bad ideas, the ones that initially challenge our deeply-held assumptions about what constitutes a good investment, a good product, or indeed, a good idea.  Dare to defy those assumptions. The rewards may surprise you.
","startup advice
",AI,"human
","AI
"
155,155,"The Myth of the Series A Crunch: Why Founders Should Be Less Afraid

The conventional wisdom in startup land is that Series A funding is a brutal gauntlet.  The valuations are sky-high, the competition fierce, and the investors… well, let's just say they're not always acting in the best interests of the founders.  This narrative, while containing elements of truth, obscures a more fundamental reality:  the problem isn't the Series A round itself, but the way founders approach it.

The current Series A landscape is a direct consequence of two powerful trends. First, the cost of starting a company has plummeted.  Software, cloud infrastructure, and global talent pools have dramatically reduced the capital needed to launch a viable business.  Second, the very definition of ""startup"" has expanded.  It's no longer just a Silicon Valley phenomenon; it's a global movement, and the pool of potential founders is far larger than it once was.

These shifts have created a surplus of excellent startups, but the investor ecosystem hasn't fully adapted.  Many VC firms cling to outdated models, chasing oversized deals and focusing on portfolio management above all else.  This is where the perceived ""crunch"" comes from: investors are still operating as if founders are desperate for capital, when in many cases the reverse is true.

The result?  Overvaluation, excessive dilution, and lengthy fundraising processes that steal valuable time from building the product.  Founders find themselves caught in a game of poker they don't fully understand, playing against opponents with vastly more experience.

The solution is not to fight fire with fire, engaging in valuation games and endless negotiations.  Instead, embrace the new reality.  Focus on building a great product.  Become the kind of startup that is so compelling, it forces investors to compete for a slice of the pie rather than the other way around.  Here's how:

* **Prioritize speed and efficiency:**  Rapid iteration and early traction are your best weapons.  The faster you build and the more users you gain, the less you'll need to negotiate and the more control you'll retain.

* **Build cheaply:**  Focus on lean, cost-effective solutions that maximize output and minimize dependence on external funding.  The less money you need, the more leverage you have in negotiations.

* **Seek out the right investors:**  Partner with investors who share your vision, understand your product, and operate with integrity. Don't prioritize the size of the check or the prestige of the firm; focus instead on a partnership based on mutual respect and shared values.

* **Have a backup plan:**  Always maintain the flexibility to bootstrap or explore alternative funding paths.  This will prevent you from being pressured into unfavorable terms.

The Series A is not the end-all be-all.  It's a crucial step, but one that should be strategically approached, not treated as a battle to be won.  The most successful startups aren't those that secured the highest valuation in their Series A, but rather those that built exceptional products and established a strong foundation for sustained growth.  Focus on that, and the funding will follow.
","startup advice
",AI,"human
","AI
"
156,156,"The Startup Cambrian Explosion: Why Now is the Time to Build

Something interesting is happening in the startup world.  It's not a subtle shift; it's a tectonic plate movement.  The cost of starting a significant web-based venture has plummeted, leading to a Cambrian explosion of new businesses.  This isn't just more startups; it's a fundamental restructuring of how value is created and captured.

The Analogy of Cheap Technology

This mirrors the evolution of technology itself.  Initially, a new technology is expensive, artisanal, and rare.  Then, a breakthrough in production lowers the cost dramatically, leading to mass adoption and unexpected applications.  Think of the transition from mainframe computers to personal computers. The same pattern repeats across history – steel, electricity, even agriculture underwent similar transformations.  Today, the technology isn't the hardware, but the entire process of building a startup.

Predicting the Future (and Why it Matters)

Several clear trends emerge from this shift:

1. Sheer Volume: The barrier to entry is now courage, not capital.  This will lead to an unprecedented number of startups, far exceeding current estimates. The sheer number alone will create new opportunities and challenges.

2. Standardization:  Mass production necessitates standardization. This is already happening with investment terms, simplifying fundraising and reducing friction for founders.  Expect more standardization across the board, including in acquisition processes.

3. The Acquisition Advantage:  Large companies will increasingly view acquisitions as efficient talent acquisition, rather than a last resort.  The stigma surrounding acquisitions will fade as even the most technically proficient corporations realize the speed and efficiency of acquiring proven startups.

4. Embracing Risk:  With the ability to launch and iterate quickly, founders can take on higher-risk, higher-reward strategies.  The portfolio approach, previously the sole domain of investors, will become more prevalent among serial founders.

5. A New Breed of Founder:  The lower barrier to entry empowers younger, more technically focused founders. The focus shifts from impressing investors to building compelling products.  The ability to demonstrate traction with a working product becomes a powerful alternative to elaborate business plans.

6. The Enduring Power of Hubs: While startups are increasingly location-independent, startup hubs like Silicon Valley will remain crucial for collaboration, networking, and mentorship.  The value of face-to-face interaction isn't easily replicated.

7. The Need for Better Judgement:  The influx of startups requires more sophisticated evaluation mechanisms from investors and acquirers.  Improved methodologies, possibly aided by software, will be essential.

8. The Evolution of Education:  The rise of startups as a viable career path will impact higher education. Degrees may become less important, and universities will need to adapt to focus on fostering collaboration and entrepreneurial skills rather than simply credentials.

9.  The Competitive Landscape:  Increased competition doesn't negate the benefits of lower startup costs.  It simply accelerates innovation, leading to faster technological advancement.

10.  Accelerated Innovation:  The combination of more startups and faster iteration cycles will propel technological advancement at an unprecedented rate.  The constraints of large organizations will be bypassed, ushering in a period of rapid innovation.

The Straight Pipe

The current startup landscape resembles the chaotic plumbing of an old house.  The future, however, points to a streamlined, efficient system. This ""straight pipe"" will allow the force of performance to dictate success at every stage, from high school to established corporation. This will lead to a more efficient and meritocratic ecosystem, benefiting both founders and consumers.  This is the dawn of the startup Cambrian explosion, and it's a thrilling time to be building.
","startup advice
",AI,"human
","AI
"
157,157,"The Shower Test: A Founder's Guide to Prioritization

I've been thinking lately about the surprisingly profound insights that emerge during those quiet moments under the shower spray.  It's more than just a good time for brainstorming; I'd argue it's a crucial indicator of what truly occupies your mind, and consequently, what will likely shape your success—or failure—as a founder.

Most founders juggle numerous priorities.  Marketing, product development, fundraising, team management – the list is endless.  But amidst this cacophony, there's usually one dominant idea, a kind of mental gravitational center. This is the idea your thoughts drift toward, even when you aren't consciously focusing on it.  And that's where the trouble starts.

The insidious nature of this phenomenon hit me hard during two protracted fundraising rounds.  I'd long noticed a slowdown in progress whenever startups started seeking funding. I thought it was simply the time commitment involved.  I was wrong. The issue wasn't the meetings; it was the mental real estate.  Fundraising, almost by definition, becomes the dominant thought, the thing that occupies that precious space in your morning shower contemplation.  And everything else suffers.

This isn't unique to fundraising.  It applies equally to acquisitions, protracted legal battles, or any situation demanding significant mental energy. These issues become attention sinks—voracious consumers of cognitive resources that crowd out more important, more productive endeavors.  The struggle to secure funding, or navigate a complex legal dispute, becomes so all-consuming that it eclipses the actual work of building the company.

The solution isn't to try and force your mind to focus on other things, which ironically undermines the very process we're discussing.  This isn't a matter of direct control, but of indirect influence.  It's about crafting your environment, consciously choosing the battles you fight, and ensuring that the most pressing problems are the ones you actually want to spend your mental energy on.

Two categories of mental distractions are particularly pernicious: money and disputes.  Money, in its pursuit, often demands intense focus, becoming a relentless mental occupant.  Similarly, disputes – legal, interpersonal, or otherwise – hold you captive in an endless cycle of reactive thinking, preventing proactive progress.

This isn't a new idea, of course. Even Newton, that titan of scientific thought, found himself bogged down in unproductive debates, ultimately choosing silence as a strategic retreat.  The lesson?  Sometimes, turning the other cheek, ignoring the trivial disputes, is not only the moral high ground, but a surprisingly effective means of reclaiming control over your own cognitive landscape.  I've found that the things I’ve truly *forgotten* are often the things I don't deserve space in my head for.

The shower test is simple. What topic keeps returning to you? Is it what you *want* to be thinking about, or is it something that’s hijacked your mind, draining its energy?  If it's not the former, you need to make a change. Because the way you spend your mental energy – those quiet, unforced thoughts under the shower spray – is a much better predictor of your success than any business plan.
","startup advice
",AI,"human
","human
"
158,158,"The Myth of the Minimum Viable Product

The Minimum Viable Product (MVP) is a seductive idea.  The notion that you can launch something small, learn quickly, and iterate your way to success is appealing, especially to young founders eager to avoid the perceived waste of building something no one wants.  But the MVP, as often implemented, is a recipe for mediocrity.

The problem isn't the concept itself; it's the execution.  The desire to launch *something* as quickly as possible often leads to the creation of a product that is genuinely minimal, but also genuinely *bad*.  A truly minimal product focuses ruthlessly on the core value proposition, eliminating everything extraneous. A bad MVP, however, often just means a half-baked version of a flawed idea.

Think of it this way:  Would you rather have a perfectly crafted, miniature masterpiece, or a sprawling, unfinished sketch?  The former might be small, but it's still impressive.  The latter is just a mess, regardless of its size.

The pursuit of an MVP can also encourage founders to undervalue the importance of design and user experience.  A polished product, even a small one, signals competence and attention to detail – qualities that build trust and attract users. A clunky, functional MVP, on the other hand, often repels potential customers.

Instead of fixating on an arbitrary launch date, focus on building something *remarkable*.  Something users will love, not just tolerate.   This might take longer, but the payoff will be far greater.  If your product solves a genuine problem with elegance and grace, the size of the initial release almost becomes irrelevant. Users will be far more forgiving of limitations if they are impressed by the underlying quality.

The best startups are built by people who care deeply about the product itself, not just about the business.  They are passionate about solving problems, and this passion shines through in the quality of their work.  The idea of an MVP should be a guiding principle for efficient development, not an excuse for releasing subpar work.  Build something great, even if it means starting small, and the market will respond.  Avoid the trap of the minimum; strive for the remarkable.
","startup advice
",AI,"human
","human
"
159,159,"The Myth of the Experienced Founder

The conventional wisdom in Silicon Valley holds that a successful startup requires seasoned leadership.  Experienced founders, the narrative goes, possess the wisdom and strategic acumen to navigate the treacherous waters of the startup world.  They've seen it all before, they know the pitfalls, and they can steer their companies to success.

This is, to put it mildly, bullshit.

My years at Y Combinator have shown me a different truth:  the most important ingredient in a successful startup isn't experience, it's *execution*.  Experience can be a valuable asset, certainly, but it's often a poor predictor of success.  Indeed, I've seen plenty of startups stumble and fall precisely because their founders were too wedded to their past successes, too unwilling to adapt to changing market conditions.

The problem with experience is that it often leads to a kind of intellectual sclerosis.  Experienced founders can become trapped in their own mental models, unable to see new possibilities.  They've seen a particular strategy work in the past, and they're reluctant to deviate from it, even when the circumstances have changed.

Young founders, on the other hand, are often more flexible and adaptable. They haven't yet developed the rigid mental models that can constrain their thinking. They're more willing to experiment, to try new things, and to embrace failure as a learning opportunity.

This isn't to say that experience is entirely useless.  Some level of experience is necessary to avoid the most obvious pitfalls.  But the kind of experience that matters most is not years spent in corporate boardrooms, but rather the experience of building and shipping successful products.  And that kind of experience can be gained quickly, especially in today's rapidly evolving tech landscape.

What truly separates successful startups from the rest is not the age or experience of their founders, but their ability to identify a real problem, build a solution people want, and execute flawlessly.  Everything else is secondary.

So, to aspiring founders:  don't let the myth of the experienced founder hold you back. If you have the passion, the vision, and the relentless drive to execute, then go build something amazing.  The world is waiting.
","startup advice
",AI,"human
","AI
"
160,160,"The Underestimation of Obvious Problems

Many founders believe the key to a successful startup is a revolutionary, groundbreaking idea.  They spend months, sometimes years, searching for the ""million-dollar idea,"" convinced that the path to riches lies in unprecedented innovation.  This is a mistake.  The most valuable startups often spring from tackling seemingly obvious, even mundane, problems.

Consider the humble spreadsheet.  At the time of its inception, it wasn't revolutionary technology.  Yet, the act of creating a collaborative, web-based spreadsheet, solving a clear problem for users, birthed a massive industry.  The underlying innovation wasn’t the spreadsheet itself, but rather its implementation in a way that made it easily accessible and collaborative.

The key here isn't to avoid innovation.  Instead, it's about focusing on solving existing problems efficiently and elegantly.  Many businesses today are riddled with inefficient processes, clumsy interfaces, and poor user experiences.  These shortcomings, though seemingly insignificant individually, can combine to create massive pain points for users and create fertile ground for disruptive startups.

Look around you.  What irks you? What tasks take up an unreasonable amount of your time or energy?  What problems do your friends and colleagues continually face?  These are the breeding grounds of successful startups.  The ""aha!"" moment often doesn't come from a stroke of genius, but from the persistent annoyance of a poorly solved problem.

Moreover, don’t get hung up on originality.  Competition is not the enemy.  A market’s existence implies demand; the presence of existing players merely indicates that there’s room for improvement.  Your startup should strive to address an existing problem more efficiently, effectively, or elegantly than the current solutions. This often involves focusing on a niche audience or a specific aspect of the problem that is underserved.

The greatest opportunities are often found in the seemingly obvious, in the daily frustrations that most people simply accept as unavoidable.  Don't be afraid to dive into these mundane challenges.  The seemingly straightforward problem could be the doorway to a multi-million dollar business.  You just need the vision to see it, the determination to solve it, and the tenacity to build it.
","startup advice
",AI,"human
","human
"
161,161,"The Myth of the Lean Startup: Why You Need More Than Ramen

The prevailing wisdom in startupland preaches leanness.  Bootstrap, they say.  Minimize costs.  Live on ramen.  This advice, while superficially appealing, often misses a crucial point:  the cost of being too lean.

Startups are inherently risky.  Failure is the default outcome.  The odds are stacked against you.  But those odds can be improved, and one of the most significant factors influencing your odds is the amount of runway you possess.

The ""lean"" approach, while reducing initial expenses, severely restricts your ability to react to opportunities and unforeseen challenges.  A small, unexpected delay in sales, a sudden surge in competition, a critical bug discovered late in development – these are all events that can cripple a startup with insufficient resources.  The ability to weather these storms, to adapt and pivot, requires a greater reserve than a few months’ worth of ramen.

The cost of leanness isn’t just financial.  It's also a cost to morale.  Constantly worrying about the next rent check, constantly juggling multiple gigs to keep the lights on, severely limits your ability to focus on the core task at hand: building a great product.  The stress, the distraction, can undermine even the most brilliant team.

Consider this:  would you rather be a slightly overweight boxer, capable of absorbing punches, or a featherweight who can be knocked out by a stiff breeze?  The initial weight advantage might seem counterintuitive, but it's that extra capacity which provides the resilience to withstand the inevitable blows of the startup game.

This isn’t an argument for reckless spending.  It's an argument for strategic resource allocation.  A well-funded startup can afford to:

* Experiment more aggressively:  Test different marketing channels, explore multiple product iterations, and take risks on promising but unproven ideas.
* Hire the best talent:  Attracting and retaining top-tier engineers, designers, and marketers is crucial for success, and this requires competitive compensation.
* Weather the inevitable storms:  Periods of slow growth or unexpected setbacks are part of the startup life cycle.  Having a cushion provides the breathing room needed to navigate these rough patches.

So, how much is ""enough""? There's no magic number.  It depends on your specific circumstances, the market you’re entering, and the nature of your product. But the crucial point is this:  don’t be so focused on minimizing immediate costs that you cripple your ability to survive and thrive.  A bit of fat on the bones, in the form of sufficient funding, may be the difference between success and failure.  The myth of the lean startup is that it's lean enough. It isn't.  Go get funded.
","startup advice
",AI,"human
","AI
"
162,162,"The Startup Founder's Mindset: Why ""Good Enough"" Isn't Good Enough

The path to building a successful startup isn't paved with brilliant ideas, but rather a relentless pursuit of execution, a laser focus on the customer, and a monastic devotion to frugality.  Many fail because they stumble in one of these areas, not because of some insurmountable hurdle.  This isn't magical thinking; it's a practical observation.  And the potential payoff – wealth – makes the difficulty worthwhile.

Forget the ""Eureka!"" Moment

The prevailing myth surrounding startups is that a dazzling, groundbreaking idea is the key to success. This couldn't be further from the truth.  Most successful startups are built around solving existing problems, often mundane ones, in a way that's significantly better than current solutions.  The market is littered with poorly executed products solving real problems, while the truly novel ideas often flounder due to poor execution.  Focus on the ""how,"" not just the ""what.""

The Importance of the Right Team

The most critical ingredient in any successful startup is the founding team.  This isn't about assembling a group of friends; it's about finding people who possess an almost pathological drive to succeed. They are the ""animals"" –  individuals utterly consumed by their work, pushing boundaries, and refusing to settle for mediocrity.  This intensity is contagious and fuels progress when external motivation might wane.

Customer Obsession: The North Star

Above all, a successful startup is deeply attuned to its customer.  It’s not about building what you, the founder, think people want, but what they demonstrably need.  This requires relentless testing, iterative development, and a willingness to pivot when the data dictates a new direction.  The number of users isn’t just a metric; it’s the ultimate proof you’ve created something of value.

The Frugality Imperative

Financial discipline is paramount.  Many startups squander funding on lavish offices, excessive headcount, and unnecessary features, leading to premature death.  A lean, efficient operation is not just preferable; it's essential for survival.  Think “grad student” mentality, not “law firm” opulence.  Wasteful spending indicates a lack of understanding of the core business.

Scaling Smart, Not Fast

The conventional wisdom of ""grow big fast"" is often a recipe for disaster.  Sustainable growth is achieved through incremental improvements, a continuous feedback loop from users, and a relentless focus on building a superior product.  Premature scaling without solid foundations leads to instability and leaves you vulnerable to competitors who emphasize building a better product over flashy marketing campaigns.

Finding the Funding

Securing funding is a necessary but not sufficient condition for success.  Early-stage funding, often from angel investors, is relatively straightforward, requiring a clear understanding of the problem and the solution.  Subsequent funding from venture capitalists requires a more nuanced approach, focusing on demonstrating traction and a sustainable business model.  Negotiate wisely; remember, you are selling a vision, not merely an idea.

In Conclusion

Building a successful startup requires a unique blend of technical prowess, customer obsession, financial prudence, and a healthy dose of resilience. It’s a high-stakes endeavor, but the rewards are significant for those who understand the core principles.  It's about recognizing that the obstacles aren’t insurmountable, and that consistent execution is more important than the initial brilliance of an idea. The journey is challenging, but the destination is within reach for the determined and focused.
","startup advice
",AI,"human
","AI
"
163,163,"The Most Important Thing in a Startup:  Ignoring Obvious Advice

Startups are, at their core, a battle against conventional wisdom.  The advice you hear most often is usually the least helpful.  This isn't because it's wrong, per se, but because it's generic and irrelevant to the specific, brutal realities of building a successful venture.

Most advice boils down to platitudes: ""build a great product,"" ""find the right market,"" ""raise sufficient funds.""  These are all true, obviously.  The challenge lies in the execution, which is where most founders fall short.  The successful ones don’t follow the advice; they ignore it.  They ignore it because they understand something far more profound: success requires a contrarian, even reckless approach.

Take funding, for example.  The common wisdom is to meticulously plan your burn rate, secure enough capital to reach profitability, and only then scale.  The reality?  Many wildly successful startups have flouted this.  They raised aggressively, prioritized growth above all else, and worried about the ""how"" much later. This seemingly irresponsible approach was only possible because their product was so compelling that investors were willing to take a risk.

Consider the importance of team composition. The standard advice emphasizes balanced skill sets and a harmonious team. While essential, this often overlooks the value of singular, intense focus. A team entirely comprised of individuals driven to a near-fanatical degree, even if somewhat dysfunctional, can generate a level of output and innovation impossible with a more ""well-rounded"" team.

Or think about market analysis.  The conventional approach involves exhaustive market research, carefully identifying a niche and avoiding competition. Yet many breakthroughs happen by disrupting existing markets, ignoring established norms, and challenging well-entrenched competitors. The most successful founders identified a problem that they, themselves, acutely felt. They weren't searching for the perfect market; they were creating one.

This isn't a prescription for chaos.  It’s a call to prioritize intuition and a relentless focus on building something truly exceptional.  The founders who succeed aren't the most obedient students; they are the most skilled at ignoring what everyone else says. They’re able to recognize when the standard advice is a distraction from the far more important goal:  creating something so valuable that the rules cease to apply.  That's the real secret of startup success, the thing no one ever tells you because it's fundamentally antithetical to the kind of calculated advice most people give.
","startup advice
",AI,"human
","AI
"
164,164,"The Unlikely Virtue of Startup Benevolence

Many founders chase aggressive growth, viewing their startup as a conquest.  They strategize ruthlessly, aiming to dominate the market and capture as much value as possible.  This is, of course, natural.  But is it optimal?  I’d argue no.  In fact, the path to substantial success in a startup often runs surprisingly close to that of a charity.

This isn't about altruism as a virtue in itself, though that's certainly a bonus. I'm talking about a practical approach: focusing intensely on building something users genuinely need and love.  Why? Because this single-minded pursuit, this near-benevolence towards the customer, has unexpected multiplicative effects.

Consider Craigslist.  Not exactly known for lavish spending or aggressive monetization. Yet, they're incredibly successful. Their lean approach, almost charitable in its resource allocation, allowed them to establish a dominant position, effectively upwind of massive potential revenue streams.  They dictate the terms of engagement, not the other way around.

Google, in its early years, also resembled a charity.  They prioritized building a superior product, pushing the boundaries of web indexing long before aggressive monetization became a priority.  This focus allowed them to attract a massive user base and, eventually, reap the rewards of that scale.

This pattern repeats itself.  Startups that initially prioritize creating something genuinely valuable often find themselves in an unexpectedly strong position. The resulting user loyalty and network effects create a flywheel effect, far surpassing the impact of any short-term focus on profits.

This isn't to say you should aim to be a non-profit.  But adopting a mindset of deep user-centricity, of striving to solve a real problem for your users, often leads to the kind of growth and market dominance that makes money a consequence, not a primary goal.  The key is to align your actions with what benefits the user most. That's your compass.

The counter-intuitive aspect of this approach is its impact on your team's morale.  When you're working on something meaningful, something that truly matters to your users, the downswings of the entrepreneurial roller coaster become less daunting.  This increased resilience is crucial for navigating the inevitable challenges of a startup.

Similarly, a focus on user needs attracts collaborators: investors, mentors, and exceptionally talented employees who are themselves motivated by impactful work.  They see in your project not just a money-making scheme, but an opportunity to contribute to something greater.

The argument for this approach is not solely philosophical. It is empirical.  Our data at Y Combinator demonstrates a clear correlation between user focus and long-term success.  Startups that truly helped their users navigated difficulties with surprising agility and created lasting value.

So while the drive for growth and market share is natural, consider a different path.  Focus intently on creating genuine value for users.  The benefits—in morale, partnerships, and ultimately, success—may surprise you.  It's a powerful strategy that runs counter to conventional wisdom, yet frequently proves its worth.
","startup advice
",AI,"human
","AI
"
165,165,"The Startup Myth of Overnight Success

The popular image of a startup is a sudden explosion: a brilliant idea, a flash of funding, and then, *boom*, a billion-dollar valuation.  This narrative is almost always false.  The reality is far messier, far more iterative, and, frankly, far more interesting.  What truly separates the successful from the also-rans isn't some magical ingredient, but a relentless, almost obsessive focus on a single, crucial element: the user.

Forget the grand vision statements and the slick pitches.  The most important thing you can do, especially in the early stages, is to deeply understand your users.  This isn't about market research surveys or abstract demographics. It's about getting your hands dirty, talking to real people who might use your product, observing their behavior, and honestly assessing their needs.  The best startups are built not around a pre-conceived notion of what people *should* want, but around what they *actually* do want.

This deep understanding of the user reveals itself in subtle ways.  You'll see it in the founders' ability to articulate the user's problems with remarkable clarity. You'll see it in the product itself—not just in its functionality, but in its intuitive design and seamless user experience.  And you'll see it reflected in the product's evolution—a constant process of refinement and adaptation based on direct user feedback.

One of the most common mistakes early-stage founders make is mistaking activity for progress.  They might spend weeks perfecting a feature that ultimately has little impact on user satisfaction.  Or they might chase metrics that don't reflect genuine engagement.  The relentless focus on the user forces a prioritization that ruthlessly cuts away the extraneous, allowing the founders to concentrate their efforts on the crucial few things that really matter.

This isn't to say that all successful startups are born from profound user empathy.  Some founders stumble upon unexpected market opportunities.  Others have a genius for technological innovation that creates entirely new needs.  But even in these cases, the successful execution depends heavily on a constant feedback loop with users.  It's the user who validates the idea, confirms its potential, and guides its development.

The journey of a successful startup is not a straight line.  It's more akin to navigating a complex landscape, constantly adjusting course based on the feedback from the terrain.  The ability to effectively pivot, to change course based on user signals, is vital.   A startup that doesn't listen to its users is essentially navigating blind, destined to crash.

So, the next time you’re crafting your startup pitch, don't focus on the scale of your ambition or the elegance of your technology.  Instead, focus on this:  what have you learned from your users, and how are you using that knowledge to build a product they genuinely love?  If you can answer that question honestly and compellingly, you’ve already taken the most important step toward success.
","startup advice
",AI,"human
","human
"
166,166,"The Most Misunderstood Secret to Startup Success:  It's Not the Idea

Most founders obsess over the ""big idea.""  They spend weeks, months, even years, meticulously crafting a pitch deck, chasing the next shiny object, all while neglecting the single most crucial element of a successful startup:  the *founder*.  

Forget the elaborate business plans and market analyses.  The real question isn't ""what problem are you solving?"" but rather ""are *you* the right person to solve it?""  Investors, the truly discerning ones at least, aren’t investing in ideas; they're investing in people.

This isn't about charisma or slick presentations. It's about a deeper, more fundamental quality:  conviction.  A truly formidable founder possesses an unwavering belief in their vision, a belief rooted not in blind optimism, but in a deep understanding of the problem and a clear path to solving it.

This conviction is palpable. It's evident in the way a founder articulates their vision, in their unwavering focus, in their relentless execution.  It's the energy that infects everyone around them, the force that drives them through setbacks and obstacles.

Where does this conviction come from?  It doesn't spring fully formed from the mind; it's cultivated through deep expertise, tireless work, and a genuine passion for the problem being tackled.   It’s the result of having lived the problem, of having wrestled with it firsthand, of having an intimate understanding of its nuances.

Think of the most successful startups. They weren’t built on some abstract notion; they were born from the founders' own struggles, frustrations, and insights. They identified a pain point, not in a theoretical market research report, but in their own lives or the lives of those close to them.  They weren't just building a product; they were building a solution.

So, how do you cultivate this conviction?  Focus on solving a problem you deeply understand, one that you've experienced firsthand.  Become an expert in your domain, mastering every detail, every nuance.  Build something that you yourself would use, something that genuinely addresses a real need.  Don't chase the hype; chase the solution.

And when you present your vision to investors, don't try to sell them a dream;  show them the solution you've built, the expertise you possess, and the unwavering belief that burns within you.  That’s the recipe for startup success.  The rest will follow.
","startup advice
",AI,"human
","human
"
167,167,"The Myth of the Minimum Viable Product

The Minimum Viable Product (MVP) is a seductive idea.  It whispers promises of lean efficiency, of rapid iteration, of escaping the dreaded ""build it and they won't come"" scenario.  Yet, in practice, the MVP often becomes a self-fulfilling prophecy of mediocrity.

The core problem lies in the misinterpretation of ""minimum.""  Many startups, seduced by the allure of speed, launch something truly minimal—so minimal, in fact, that it fails to capture the imagination or solve a significant problem.  They mistake a lack of features for elegance.  A truly great product, even in its early stages, possesses a certain spark, a hint of the brilliance to come.

Instead of aiming for a ""minimum,"" strive for a ""compelling minimum.""  This means identifying the core essence of your product—the single, most captivating feature—and building it with exceptional polish.  Don't skimp on design, on user experience, or on the overall sense of quality. A well-executed, albeit limited, offering is far more likely to generate traction than a buggy, hastily assembled collection of half-baked ideas.

Think of it this way:  a beautifully crafted prototype, even if it only demonstrates one key function, is exponentially more persuasive than a rickety, barely functional version attempting to do too much at once.

Investors are not impressed by speed alone; they are drawn to potential. A compelling minimum conveys potential far more effectively than a rushed MVP that falls short.

Furthermore, the relentless focus on the ""minimum"" can stifle innovation.  By constantly striving for the bare minimum, you restrict your thinking. You fail to explore the full spectrum of possibilities, limiting your chances of discovering truly breakthrough ideas.

The relentless pursuit of the MVP often leads to a series of incremental improvements on an initially flawed foundation. This is like trying to build a skyscraper on quicksand.  It's far more effective to invest in a strong foundation, then build upwards from there.

So, forget the MVP.  Aim instead for a compelling minimum—a product that is minimal in scope but maximal in execution.  A product that sparks excitement, that generates buzz, and most importantly, that leaves users craving more.  This is the path to true success.
","startup advice
",AI,"human
","AI
"
168,168,"The Curious Case of Lost Civilizations: Why We Miss the Obvious

Historians, like investors, often suffer from hindsight bias. We see a civilization rise, flourish, and then mysteriously vanish, and we concoct elaborate theories to explain their demise.  We posit plagues, invasions, climate change—grand narratives that satisfy our need for a comprehensive explanation. But what if the real story is far simpler, far less dramatic? What if we're missing the obvious because it's too mundane to be interesting?

Consider the Anasazi, the sophisticated cliff-dwelling culture of the American Southwest.  Their abandonment of Chaco Canyon, a marvel of engineering and social organization, is a classic mystery.  The usual suspects—drought, warfare, overpopulation—are trotted out, debated endlessly. But what if the explanation is less about a single catastrophic event, and more about a slow, gradual decline? A series of incremental bad decisions, perhaps, compounded by unforeseen circumstances.  Imagine a society spread too thin, its resources stretched to the breaking point by a changing climate. No single cause, just a creeping entropy.

This isn't to say that cataclysmic events play no role in historical collapses. Of course they do. But the focus on singular, spectacular causes distracts us from the more subtle, insidious processes at work. We build narratives around easily digestible explanations, neglecting the less glamorous reality of complex systems nearing their limits.  The seemingly sudden collapse of a civilization might be less a dramatic fall from grace, and more a slow, inevitable unwinding.

The challenge for historians isn’t just to identify the ""why,"" but to understand the ""how""—the intricate interplay of factors that lead to a society's decline. The search for a single, definitive cause, while tempting, may be ultimately misleading.  A more fruitful approach involves piecing together the intricate puzzle of human choices, environmental pressures, and unforeseen contingencies—a nuanced approach that often lacks the narrative arc of a good story. But history, unlike a novel, doesn't always have a satisfyingly clean conclusion.  Sometimes, the end is simply the sum of its parts, a messy and less-than-dramatic fade-out.  And that, perhaps, is the most important lesson of all.
","historical analysis
",AI,"human
","human
"
169,169,"The Curious Case of the Vanishing Medici: Why Dynastic Wealth Is Dying

The Forbes 400.  A list that, for decades, seemed to solidify the myth of inherited wealth, a gilded cage passed down through generations.  Yet, a closer look reveals a surprising shift. The dominance of inherited fortunes is fading, replaced by a new breed of wealth creation.  This isn't some subtle adjustment; it's a tectonic shift in the very landscape of economic power.

In the early 80s, the list was dominated by heirs.  The old money, the established families, held sway.  It was a system seemingly impervious to change, a testament to the enduring power of legacy. But the 21st century has proven far less forgiving.

Why this change?  Did inheritance taxes suddenly spike?  No.  In fact, they decreased.  The reason is far more fundamental.  It's not that fewer people are inheriting wealth, it's that far more are creating it.  And the method of creation is as revolutionary as the result.

The new titans of wealth aren't primarily extracting resources or manipulating markets in the time-honored fashion.  They're building.  They're innovating.  They are, to use the somewhat overused but still largely accurate term, building companies that scale at an unprecedented rate.

Roughly three-quarters of the newly minted billionaires on the Forbes 400 owe their fortunes to company creation, not mere financial maneuvering. This isn't about oil barons or real estate moguls; it's about founders, coders, and the architects of the digital age.

This isn't a mere shift in the distribution of wealth; it's a metamorphosis of the means of wealth creation.  It's a rejection of the old, established hierarchies, a bold assertion of self-made success.

The implications are far-reaching.  The traditional power structures are being disrupted, replaced by a meritocracy (imperfect, undoubtedly, but still a move in that direction) where innovation and execution determine success, rather than birthright.

This isn't to say inherited wealth has entirely disappeared.  But its influence is waning.  The future of wealth creation seems increasingly tied to the creation of technology, the development of groundbreaking businesses, and the relentless pursuit of innovation.

The Medici family, the quintessential example of dynastic wealth, would likely find the current economic landscape utterly alien.  Their model, based on control of resources and political influence, is being superseded by a model that prizes agility, innovation, and, above all, the ability to build something entirely new.  The age of the self-made is upon us, and the old order is yielding to the disruptive forces of the new.
","historical analysis
",AI,"human
","human
"
170,170,"The Tyranny of the Well-Written Sentence

Many aspiring writers, especially those steeped in the hallowed halls of academia, believe that good writing necessitates complex sentence structures and a rich vocabulary.  They strive for eloquence, for a polished sheen that masks the underlying thought.  This is a mistake.

Consider the goal of writing.  Is it to impress with linguistic dexterity? Or is it to communicate ideas clearly and effectively?  The latter, of course.  A beautifully crafted sentence that leaves the reader bewildered is a failure.

Simple writing isn't about dumbing down your ideas; it's about sharpening them.  When you force yourself to express a complex concept in simple terms, you're forced to understand it more deeply yourself.  The act of simplification clarifies; the excess verbiage obfuscates.

Think of it as software engineering.  The most elegant code is not the most verbose; it's the most concise, the most efficient.  It achieves its purpose without unnecessary embellishments.  Good writing should follow the same principle.

Moreover, simple writing is more inclusive.  Not everyone speaks English as a first language, and even native speakers can be tripped up by overly ornate prose.  Complex sentences add friction to the reading experience, diverting attention from the ideas themselves.  The energy expended on deciphering convoluted syntax is energy not spent grappling with the substance of your argument.

Some might object that simple writing lacks the artistic flair of its more flowery counterpart.  This is a false dichotomy.  Simplicity is not synonymous with blandness.  A well-crafted, concise sentence can possess a beauty all its own, a strength that derives from its directness and precision.  It's the equivalent of a perfectly executed minimalist painting – powerful in its restraint.

Aim for clarity, not cleverness.  Strive for precision, not pretension.  Write as if you're speaking directly to your reader, honestly and without artifice.   Let your ideas, not your vocabulary, be the focus.  That, in the end, is the measure of true writing.
","writing advice
",AI,"human
","human
"
171,171,"The Unsung Power of the Precise Phrase: Why Writing Matters

Many believe the goal of writing is persuasion.  That’s a decent start, I suppose. But I’d argue for something bolder:  writing should be a tool for discovery.  And that begins with precision.

Think about it:  vagueness is the refuge of the uninformed.  The academic paper riddled with qualifiers, hedging its bets at every turn, tells you nothing of genuine value.  It’s a smokescreen, masking a lack of genuine understanding.  Useful writing, conversely, makes strong claims—claims that are as assertive as truth allows.

Precision, however, is a double-edged sword.  You can't sacrifice truth at the altar of conciseness.  The bluster of the demagogue is the inverse of academic obfuscation.  Effective writing is both bold and accurate.

But it's more than that.  Truly effective writing illuminates something vital – something the reader, consciously or unconsciously, didn't fully grasp.   Often, the most impactful insights are those we’ve sensed, but never articulated.  They’re the bedrock upon which a deeper comprehension rests.

So, the ideal essay, in my view, is a synthesis of four critical elements:  it communicates something true and important, novel to a substantial readership, and expressed with unequivocal clarity.  These aren't binary; they’re gradients.  The more of each, the stronger the overall effect.

How do you achieve this?  Here’s the secret:  ruthless self-editing. My friend Robert Morris, a man who loathes uttering anything inane, has a simple rule: he only speaks when certain of his words’ merit. This produces infrequent, but invariably insightful pronouncements.

Apply this to writing:  kill your darlings.  Delete the weak sentences, the rambling paragraphs, even entire essays if necessary.  You can’t control the quality of *every* idea, but you *can* control what sees the light of day.  The scientific world calls this publication bias, and considers it a sin.  In essay writing, it's a virtue.

My process is iterative:  a rapid first draft, followed by meticulous refinement.  I've lost count of how many times I've reread certain sentences, hunting for awkward phrasing or subtle inaccuracies.  These flaws, initially subconscious, become increasingly irritating with each iteration.  They’re the verbal equivalent of burrs clinging to your clothing. I won't publish until they're gone.

Essayists possess a crucial advantage over journalists: time.  There’s no deadline.  Perfection, or as close to it as possible, is achievable.  The pressure to publish diminishes; mistakes retreat in the face of unlimited revision.

The quest for importance?  Make something you yourself want to read.  What resonates deeply with you will likely resonate with others.  Novelty?  Write about things you’ve pondered extensively.  What surprises *you*, after extensive reflection, will likely surprise many others.  Again, rigorous self-editing is key. If you haven’t learned something, neither will your readers.

Humility is essential.  The ability to acknowledge your past ignorance is a sign of intellectual strength.  Confidence allows you to recognize when you’ve discovered something new, because you trust your expertise to gauge its originality.

Strength of writing comes from two things:  clarity of thought and artful qualification. These are counterweights.  As your ideas solidify, qualifying language diminishes.  But it never quite vanishes.  Sometimes a carefully crafted qualification is better than a clumsy attempt at absolute certainty.

Simplicity is a reader-centric value, though it indirectly strengthens the writing.  Unnecessary complexity obscures flaws; simple language brings them into sharp relief.  It’s elegant, like a well-crafted program.  Write simply, unless you’re sure your audience will appreciate flourish.

Ultimately, there's an almost limitless supply of essays yet to be written. The web has democratized the form, creating an environment where untold discoveries await.  This, perhaps, is the most exhilarating aspect of essay writing:  the potential for constant exploration and the ever-present chance to uncover something new.
","writing advice
",AI,"human
","human
"
172,172,"The Unsung Power of Bad Writing

Most writing advice focuses on mechanics: grammar, style, structure.  This is like advising a chef to sharpen his knives while ignoring the quality of his ingredients.  Technique matters, but it's secondary to having something worth saying.  And the surprising truth is, the struggle to express bad ideas can often lead to good ones.

The conventional wisdom, especially in academia, is to start with a thesis, a pre-formed argument, and then build supporting paragraphs around it.  This is a recipe for blandness.  It's building a house on a foundation already laid, rather than discovering the land itself.  Real discovery is messy, and so is the writing that reflects it.

Imagine a painter who sketches meticulously before beginning his masterpiece. He meticulously plans every brushstroke, pre-visualizing the final result. That might create a technically perfect painting, but what happens if he never allows for improvisation? What if he misses the chance to embrace unforeseen beauty or resolve a composition by deviating from the plan?

My approach, and I suspect that of most thoughtful writers, is far less structured. I begin with a vague sense of direction, a question more than a statement. I write to figure things out, not to prove a point already decided. The act of writing itself becomes a process of discovery, a conversation between me and the page.

Initially, this results in a lot of bad writing.  Sentences that meander, ideas that go nowhere, clumsy phrasing.  It's a draft, not a polished product. This raw, unrefined material is essential. It's in the process of wrestling with these poorly formed ideas that clarity emerges.  The act of forcing bad ideas into words exposes their flaws, illuminating where they're weak or illogical.

The process is iterative.  I write, I rewrite, I cut, I rearrange.  Often, I find myself discarding entire sections, going back to the beginning and approaching the problem from a different angle. This is akin to a sculptor chipping away at a block of marble, gradually revealing the form within. The waste is a necessary part of the process.

Some might argue this is inefficient.  Why not just think things through carefully before writing? Because that's often impossible. The act of writing itself forces a kind of precision, a level of articulation that thinking alone cannot achieve. It's like trying to build a complex machine in your head; putting pen to paper brings the design into a tangible form and immediately reveals its flaws.  The resulting iterations lead to the uncovering of new ideas not apparent in the initial conception.

Therefore, embrace the messiness of bad writing.  Don't be afraid to write badly.  In fact, see it as an essential part of the creative process.  The struggle to articulate your half-formed ideas is the crucible where new understandings are forged.  The path to good writing is often paved with bad writing.
","writing advice
",AI,"human
","AI
"
173,173,"The Essence of Good Writing: A Surprisingly Simple Approach

Most writing advice is, frankly, terrible.  It's filled with pompous pronouncements about ""voice"" and ""style,""  leaving the aspiring writer more confused than ever.  The truth is far simpler, and far more effective.  Good writing, at its core, is about clarity and impact.  It's about getting your ideas across with minimal friction.

Forget flowery language and convoluted sentences.  These are the hallmarks of insecure writers trying to compensate for a lack of substance.  Imagine reading a poorly-written, overly-complex scientific paper versus a concisely-written one.  Which delivers the information more effectively?  Which is more likely to change your perspective?  The concise one, naturally.  Good writing is the same.

My approach?  Brutal honesty, combined with ruthless editing.  I write the first draft quickly, focusing solely on getting my ideas down.  It’s usually messy, full of digressions and awkward phrasing.  That’s fine.  The first draft is for me, not the reader.  This is where the real work begins.

The second stage is merciless editing.  I cut, I rewrite, I refine.  My aim is to eliminate anything that doesn’t directly serve the central idea.  This process often involves reducing sentences to their simplest forms, discarding unnecessary words, and rearranging paragraphs for better flow.  The goal is to make the reading experience effortless, almost imperceptible to the reader, so the ideas can leap directly into their heads.

Consider your reader.  They're not as invested in your prose as you are.  They're likely busy, and their attention spans are limited.  A complex sentence that takes time to parse will immediately lose them.  Write short, clear sentences.  Use common words.  A simple declarative structure is your friend.

How do you write something worth reading?  Think about the topic deeply before even beginning to write. Let the ideas germinate and develop.   The best writing comes from genuinely engaging with a topic, and the resulting insights will naturally lead to compelling prose.   This pre-writing phase can and should take considerable time.

Write about subjects that truly fascinate you.  Your passion will infuse your writing with energy, and make the process more rewarding. Don’t force it.  Writing is a conversation you have with yourself, and to a degree, with the reader.  It should feel natural, not labored.

Finally, seek feedback.  Show your work to others you trust—people who will tell you the truth, even if it hurts.  Their feedback can identify areas where your writing is unclear or meandering, allowing you to further refine and polish the piece.  A second, and third pair of eyes can save you from many writing pitfalls.

In short, good writing isn’t about arcane rules or stylistic flourishes.  It's about clarity, precision, and impact.  It's about communicating your ideas effectively and efficiently.  It's about being honest with yourself and your reader.  And it’s often surprisingly simple.
","writing advice
",AI,"human
","human
"
174,174,"The Tyranny of the Well-Formed Idea

Many believe that writing is simply the act of transcribing pre-existing thoughts.  This is a dangerous delusion.  The truth is far more brutal:  writing forces you to confront the messy, incomplete nature of your own understanding.

Consider this:  how often do you find yourself, mid-sentence, realizing that the elegant, fully-formed idea you *thought* you had is, in reality, a sputtering engine of half-baked notions?  The process of writing reveals this with merciless efficiency.  Words, unlike the blurry outlines of mental images, demand precision. They refuse to be coerced into accommodating vague concepts.

This isn’t merely a matter of stylistic finesse.  It’s a fundamental truth about the acquisition of knowledge.  The act of forcing an idea into the rigid structure of language exposes its weaknesses, reveals its hidden flaws, and inevitably leads to a deeper, more nuanced understanding.  Often, the most insightful ideas emerge not before, but *during* the process of writing.  The essay itself becomes a crucible, forging clarity from the raw ore of inchoate thought.

The discomfort this process provokes is understandable.  Few of us are comfortable admitting our mental models are incomplete, even to ourselves.  But to avoid this confrontation is to limit our potential for genuine intellectual growth.  It’s akin to an athlete who refuses to push past their limits, content with mediocrity.

The solution isn't to avoid writing, but to embrace its inherent challenge.  View the first draft not as a polished gem, but as raw material to be refined through ruthless self-critique.  Read your work aloud.  Imagine a skeptical reader, someone who knows nothing of your intentions, and relentlessly question your every assertion.  The goal is not perfect articulation—a chimera, in my experience—but a relentless pursuit of accuracy and completeness.

The writer who truly masters this process discovers a paradox: by accepting the inherent messiness of thought, they achieve a clarity and precision unattainable through any other means.  This is the reward for those brave enough to confront the tyranny of the well-formed idea.
","writing advice
",AI,"human
","AI
"
175,175,"The Essay as a Recursive Process

The conventional wisdom about essay writing is…well, conventional.  It’s a recipe: choose a topic, formulate a thesis, develop supporting arguments, and conclude.  This approach, while functional, misses the vital, generative core of the essay.  The best essays aren't written; they're discovered.

Forget the pre-planned structure. The true essayist embarks on a journey, not knowing precisely where the path will lead.  The initial ""question,"" broadly defined—it might be a vague curiosity, a nagging inconsistency, or a seemingly minor puzzle—acts as the catalyst.  This isn't a meticulously crafted thesis statement; it's a spark.

From this spark, a chain reaction begins.  The initial response, often clumsy and incomplete, is committed to writing.  The act of writing itself transforms nebulous ideas into concrete (and often flawed) statements.  This is not a setback; it's a crucial step.  By seeing the imperfections in our initial response, we identify the gaps, the places where our understanding falters.

The process is recursive.  Each answer generates new questions, each response branches into further exploration.  The essayist navigates this branching path, selecting which lines of inquiry to pursue.  The best choices offer both generality—the potential to illuminate broader concepts—and novelty—the promise of genuinely fresh insights.

This iterative process necessitates rewriting.  Extensive rewriting.  It’s not about polishing a pre-existing structure; it's about dismantling and rebuilding, discarding flawed branches and pursuing more fruitful ones.  The most valuable discovery often emerges from realizing a foundational assumption was incorrect.

The optimal essay doesn't merely answer a question; it unravels a series of interconnected questions, each building upon the last, ultimately leading to a satisfying, if not necessarily definitive, conclusion. The feeling of completion isn't the absence of further questions, but rather a sense of having adequately explored a sufficiently rich vein of thought.

The initial question, while not predetermining the essay's ultimate value, sets a significant upper bound. A dull question rarely leads to a brilliant essay, no matter how meticulously crafted the prose.  Thus, the selection of this initial spark is paramount.

Where do these sparks come from?  From voracious curiosity, from deep engagement with the world, from a relentless pursuit of understanding. It's about breadth of knowledge—exposing oneself to diverse fields and perspectives—and depth of experience—actively grappling with problems and challenges within a specific domain.

The best essayists aren't just writers; they are explorers. They are driven by the relentless pursuit of uncovering new ideas, not just polishing pre-existing ones.  The essay, in its truest form, is a dynamic act of discovery, a process that yields something new and unexpected, not just an organized presentation of pre-existing knowledge.
","writing advice
",AI,"human
","human
"
176,176,"The Quiet Revolution: How the Web Is Reshaping Work and Business

The internet, it turns out, is not just a faster way to do the same old things. It's a catalyst for fundamental shifts in how we work, how we create, and how businesses operate.  Two seemingly disparate phenomena—open source software and blogging—offer profound insights into this quiet revolution, and the implications are far-reaching.

The first, and perhaps most surprising, lesson is the power of intrinsic motivation. Open source projects and blogs alike are built by individuals who contribute their time and effort freely, often surpassing the quality of work produced by professionally compensated teams. This isn't simply a matter of passion; it's a testament to the fundamentally different nature of work when fueled by genuine interest rather than external pressure. Businesses, clinging to outdated models that equate work with ""travail,""  are failing to tap into this immense reservoir of creativity and productivity.

The traditional office environment, with its rigid structures and stifling atmosphere, actively undermines this intrinsic motivation.  The sterile cubicles, fixed hours, and endless meetings are antithetical to the focused, often solitary, work needed for breakthroughs.  The stark contrast between the productivity of individuals working from home, amidst comfortable chaos, and the relative stagnation of corporate offices is compelling evidence of this dysfunction. The modern workplace often resembles a performance rather than a place of genuine creation.

The third crucial insight lies in the shift from top-down to bottom-up models of innovation. Both open source and blogging thrive on decentralized collaboration. Individuals contribute, the best work rises to the surface, and the process is self-correcting. This organic approach, mirroring the principles of a free market, stands in stark contrast to the often bureaucratic and inflexible structures within most companies.  The control over both the direction and quality of a project is far more effective when distributed amongst a community rather than concentrated in management.

What does the future of business look like in light of these insights? The traditional employer-employee relationship, with its inherent power imbalance and stifling regulations, is ripe for disruption.  Venture capital funding offers a model that aligns incentives more effectively.  Instead of paying employees to work on a company’s projects,  investing in individuals to pursue their own visions is a more potent engine for innovation.  This shift towards a purely economic relationship, between equals, is not utopian but a more efficient way to harness human potential.  It demands a change in perspective, moving away from the outdated idea that employees are mere cogs in a machine and towards a recognition of their inherent drive to create and excel.  The companies that adapt to this paradigm will be the ones that thrive in this new economic phase.
","technological and societal insights
",AI,"human
","human
"
177,177,"The Tyranny of the Inbox and the Liberation of the Algorithm

The modern office is a bizarre artifact.  We build expensive, climate-controlled boxes specifically designed for the purpose of… not getting work done.  Consider the average workday:  a fragmented mess of meetings, emails, and the Sisyphean task of keeping the inbox at zero.  This isn't a productivity machine; it's a performance art piece celebrating the futility of human endeavor.

Why is this so?  The underlying problem isn't malice, but measurement.  Or rather, the lack thereof.  The traditional office relies on a crude proxy for productivity:  face time.  Are you physically present?  Then, by god, you must be working.  Never mind that actual work often occurs in bursts, punctuated by periods of deep thought, or even simple rest.  The office demands a continuous outward show of activity, a pantomime of industry.

Open-source software development offers a stark contrast.  Here, the quality of the output is judged directly, not by hours clocked in a cubicle but by the utility of the code.  The contributors are not paid for their time, but for the value they create.  And yet, the results often surpass those of highly paid teams laboring under the yoke of traditional management.

This isn't just about software.  The same principles apply to countless other fields.  The rise of remote work, the success of independent contractors, and the explosive growth of the gig economy all point to a shift away from the outdated model of the centrally managed office.

The key is not simply letting employees work from home, although that's a necessary first step.  The real liberation lies in aligning incentives with output.  If a company can accurately measure the value an individual creates, it can discard the antiquated metrics of face time and adherence to a rigid schedule.

This shift isn't without its challenges.  Accurately measuring output isn't always easy, and some roles inherently require collaboration.  But the potential rewards are enormous.  By freeing individuals from the tyranny of the inbox, we can unlock their full creative potential.

The future of work will be less about where you work and more about what you produce.  The algorithm, in its cold, impartial efficiency, will increasingly replace the human manager as the arbiter of value.  This may seem cold and impersonal, but it's also profoundly liberating.  It allows individuals to focus on what they do best, without the distractions and micromanagement of the traditional office.  It is the end of the performance art of work, and the beginning of genuine productivity.
","technological and societal insights
",AI,"human
","AI
"
178,178,"The Vanishing Middleman: How Technology Is Reshaping Power Dynamics

The internet, we’re told, democratizes everything.  But it’s not quite that simple.  The internet doesn’t just level the playing field; it fundamentally reshapes it.  And the most interesting shifts aren’t the obvious ones, like the rise of social media or the decline of brick-and-mortar stores.  The truly profound changes are happening in the subtle, almost invisible ways technology alters power dynamics.

Consider the modern creator. Twenty years ago, if you wanted to share your music or writing with the world, you needed a publisher, a record label, a gatekeeper.  These gatekeepers controlled distribution, marketing, and ultimately, your access to an audience. They were the middlemen, extracting a hefty cut in exchange for their services.

Today, the internet has bypassed the middlemen.  A musician can upload their songs directly to streaming services, reaching millions without ever signing a contract. A writer can self-publish an ebook, cutting out the traditional publisher and keeping a larger share of the profits. This is the promise of democratization – and to a large extent, it’s true.

But the story is more nuanced than simply ""good guys win, bad guys lose."" The power shift isn’t solely about creators gaining leverage.  The internet also creates new forms of gatekeeping.  It’s not the old-fashioned gatekeepers of publishing houses, but rather the algorithms of tech giants.  These algorithms determine what content gets seen, what creators get promoted, and ultimately, who gets rich.  It’s a new form of control, less visible perhaps, but arguably more pervasive.

This shift is evident in many industries.  Take news.  The decline of print media has opened the door for online news sources, ostensibly empowering independent journalists.  However, these independent journalists often compete for attention within the algorithms of social media platforms, facing similar challenges of visibility and control.

What does this mean for the future?  It's a complex question, but one pattern is clear: organizations that excel at adapting to these shifting power dynamics will flourish.   Those that cling to outdated models of gatekeeping and distribution will struggle.  Success in the internet age increasingly depends not just on creating great content, but on understanding and navigating the new power structures that have emerged. The battleground isn't simply about content itself; it's about gaining and maintaining control over the means of distribution and access. The next generation of winners will be those who master this new terrain.
","technological and societal insights
",AI,"human
","AI
"
179,179,"The Illusion of Meritocracy in Tech

The tech industry prides itself on meritocracy.  The best ideas win, the hardest workers succeed, and raw talent rises to the top.  This narrative, however, conveniently overlooks a crucial detail:  the inherent biases baked into the systems that supposedly reward merit.

Consider the hiring process.  Resumes are filtered, interviews are conducted, and decisions are made.  Each step presents an opportunity for unconscious bias to creep in.  We like people who remind us of ourselves. We gravitate towards those who articulate their ideas in a way that resonates with our own experiences.  These seemingly innocuous preferences compound, creating a system that subtly, yet powerfully, favors certain demographics over others.

The myth of the ""self-made"" entrepreneur further reinforces this illusion. The narrative glorifies individual brilliance, neglecting the role of privilege, luck, and network effects in propelling some individuals to success while others, equally talented, are left behind.  Access to capital, mentorship, and even the right social circles are often prerequisites for success, factors often unequally distributed.

Furthermore, the very metrics used to measure success are often flawed.  Venture capital investments, for instance, are frequently based on incomplete data and projections, susceptible to confirmation bias and herd mentality.  The resulting “winner-takes-all” dynamic further exacerbates inequality, creating a distorted picture of meritocratic achievement.

The solution isn't to dismantle the system entirely, but to acknowledge its inherent flaws and actively work to mitigate them.  This requires a multi-pronged approach:  blind resume screening, structured interviews, diverse hiring panels, and a conscious effort to seek out and support talent from underrepresented groups.  It also necessitates a critical examination of the metrics we use to judge success, moving beyond simplistic measures of revenue and market capitalization towards more holistic assessments of impact and social value.

The tech industry's pursuit of meritocracy is admirable, but it's a goal that requires constant vigilance and a willingness to confront uncomfortable truths.  Until we actively address the biases ingrained in our systems, the narrative of meritocracy will remain, at best, a partial truth, and at worst, a self-serving myth.
","technological and societal insights
",AI,"human
","human
"
180,180,"The Unseen Architecture of Silicon Valley

Silicon Valley. The name conjures images of gleaming campuses, venture capitalists in bespoke suits, and hoodie-clad billionaires. But the true essence of the Valley isn't found in its iconic buildings, but in the subtle, almost invisible, architecture of its success.  It's a landscape etched not in steel and glass, but in coffee stains, chance encounters, and the relentless hum of innovation.

Consider Stanford University.  Its sprawling campus, seemingly empty for much of the day, belies a potent energy. The near-perfect climate, the stunning backdrop of the Santa Cruz mountains, and the proximity to San Francisco: these are the unseen ingredients that brewed a unique environment, fostering an ecosystem unlike any other.  It wasn’t simply the academic rigor; it was the context.

Then there's University Avenue in Palo Alto. This isn't just a street; it's a crucible.  Weekday mornings and afternoons see a constant, low-key drama unfold in its cafes: founders, eyes gleaming with fervent belief, pitching to investors whose expressions range from polite disinterest to outright skepticism. The air crackles with a palpable tension, a silent negotiation of fortunes.

The seemingly mundane office at 165 University Avenue, once home to Google and later PayPal, speaks volumes. Its strategic location, nestled amongst restaurants and pedestrian traffic, wasn't accidental.  It fostered a culture of collaboration, where ideas flowed freely, spilling over from working hours into evenings spent brainstorming over dinner.  This organic, spontaneous exchange is the lifeblood of innovation.

The older parts of Palo Alto, north of the Oregon Expressway, maintain a distinct character.  Expensive, yes, but with a history that speaks to a different era.  Mature trees and older buildings (many tragically succumbing to McMansion creep) are a testament to time and growth.  This is the post-exit Valley, the quiet accumulation of wealth, the manifestation of hard work and daring ideas.

Sand Hill Road.  The address synonymous with venture capital.  The uniformity of the buildings, their bland exteriors, are strangely fitting. This isn't a place of flamboyant displays; it's a center of careful calculation, risk assessment, and patient observation. The quiet hum of activity is more powerful than any fanfare.

The shift toward Mountain View, with Red Rock Cafe as its de facto headquarters, highlights a change in the Valley’s center of gravity. Palo Alto’s increasing cost pushes innovation outwards, yet the core interactions remain. The need to meet investors continues to pull founders to places like University Avenue, while day-to-day operations spread geographically.

Google's sprawling campus, a collection of buildings from different eras and styles, avoids the sterile uniformity of corporate headquarters.  It reflects the organic growth of a company that, while powerful, maintains a certain grassroots energy. A subtle utopianism pervades; the air is thick with ambition and a quiet confidence.

Skyline Drive, offering breathtaking views of both the Valley and the Pacific Ocean, provides a perspective on the landscape's duality. The redwood forests, nourished by coastal fog, stand in contrast to the technological desert below.  Nature and technology, coexisting in a delicate balance.

Finally, Highway 280, a scenic counterpoint to the utilitarian 101, runs alongside the San Andreas Fault, a constant reminder of the underlying instability, the inherent risk, the unpredictable nature of both the geological and the technological worlds.

The architecture of Silicon Valley isn’t what you see at first glance; it’s the unseen forces that shape its unique culture, the subtle cues that reveal its innovative power, the almost imperceptible tremors of change that continue to reshape its future.
","technological and societal insights
",AI,"human
","AI
"
181,181,"The Curious Case of the Self-Driving Car and the Death of Boredom

The self-driving car is coming.  This isn't a prediction, it's an observation.  The technology is demonstrably improving at an exponential rate. The question isn't *if* they'll arrive, but *how* they'll change everything.  And one of the most profound changes will be, ironically, the death of boredom.

For most of human history, transportation has been synonymous with tedium.  The journey itself was the burden, not the destination.  Think of the endless hours spent on horseback, or the rattling discomfort of a stagecoach.  Even the automobile, while faster, often presented hours of monotonous highway driving.  This involuntary captivity, this enforced idleness, has shaped our culture.  We’ve developed entire industries around filling the void— audiobooks, podcasts, car games.  These are all, fundamentally, distractions from the drudgery of travel.

But the self-driving car offers a radical alternative.  Suddenly, the commute becomes productive time.  You can read, work, or even sleep.  The car becomes an extension of your home or office, not a metal prison.

This will have unforeseen consequences.  Imagine a world where millions reclaim hours each week that were previously lost to unproductive driving.  The societal implications are vast.  Productivity will soar.  The nature of work itself may transform.  The very definition of leisure may need to be redefined.  Will we crave the manufactured boredom of the old days?  Will the ubiquitous convenience lead to a sense of unease, a lack of appreciation for slower, more deliberate forms of travel?

Of course, there are technological hurdles yet to clear.  The ethical dilemmas posed by algorithmic decision-making in emergency situations are complex. The economic dislocations caused by the displacement of millions of professional drivers are significant.  But these are engineering and societal problems, not insurmountable obstacles.  The technology will advance. The adaptations will occur.

The arrival of the self-driving car is not just a technological revolution. It is, in its own peculiar way, a philosophical one.  It forces us to confront our relationship with time, with work, and with boredom itself.  And the answers, I suspect, will be far more interesting than the endless miles we’ve previously endured.
","technological and societal insights
",AI,"human
","AI
"
182,182,"The Tyranny of the Average, and How to Escape It

One of the most insidious lies we tell ourselves is that hard work guarantees success.  Teachers, parents, even self-help gurus repeat this mantra, implying a simple linear relationship between effort and reward.  But the world doesn’t work that way.  In reality, the returns for exceptional performance are profoundly *superlinear*.

This isn’t some quirk of capitalism. It’s a fundamental feature of reality.  Look at any field:  the most successful companies dwarf their competitors; the most influential thinkers shape entire eras; the greatest athletes command millions. The distribution isn't a bell curve; it's a power law.  The rich, in every sense of the word, get richer.

Why?  Primarily, this stems from two interconnected phenomena: exponential growth and thresholds.

Exponential growth is self-explanatory.  A startup that finds product-market fit can grow at an astonishing rate, leaving competitors in the dust.  Similarly, knowledge compounds; the more you know, the faster you learn.  This isn't a metaphorical observation; it's a literal, measurable phenomenon.

Thresholds are less obvious but equally powerful.  Think of the winner-take-all dynamics of many competitions.  The difference between second and first place might be tiny, yet the reward is vastly different.  This isn't limited to competition; there are thresholds in every field. The ability to read opens up a universe of knowledge; a single breakthrough can revolutionize an industry.

These two forces often work in tandem.  A company that crosses a critical threshold might enter a period of exponential growth. A scientific discovery can unlock entirely new fields of research, creating more exponential opportunities.

How do you leverage this reality?  The most straightforward approach is to strive for excellence.  In a superlinear world, incremental improvements are not enough. You must aim for exceptional work.  This requires not just hard work, but a deep understanding of your field, relentless curiosity, and an almost reckless willingness to take risks.  Focus on compounding efforts. Build things that scale exponentially or teach you new skills.

Don't fall into the trap of optimizing for the average.  Seek out fields where a few exceptional individuals dominate.  These are often the areas where independent thinking, novel ideas, and a willingness to challenge convention are most rewarded.

It’s a high-stakes game.  Failure is more likely. But the potential rewards – the chance to create something truly significant – are vastly greater than in a world constrained by linear returns.  The future belongs to those who understand and master the dynamics of superlinearity.  Those who aren't afraid to pursue the outliers, to push past the thresholds, and to ride the wave of exponential growth.  Are you one of them?
","technological and societal insights
",AI,"human
","AI
"
183,183,"The Quiet Revolution of Offline Computing

The internet, for all its wonders, has subtly warped our understanding of computation.  We've become accustomed to a world where processing power resides in distant servers, accessed through sleek interfaces. This has led to a peculiar blindness to the power – and elegance – of offline computing.

Consider the humble spreadsheet.  Millions use them daily, yet few appreciate the sophisticated algorithms humming beneath the surface.  These aren't relying on a network connection; the calculations happen locally, instantly, silently.  This immediate feedback loop, this unmediated connection between action and result, is a crucial aspect of effective work, often overlooked in the cloud-obsessed present.

The advantages of offline processing go beyond speed. It's about resilience.  A distributed system, however robust, remains vulnerable to outages, security breaches, and the whims of internet service providers.  Offline tools, conversely, operate independently. They are the digital equivalent of a well-stocked toolbox – always ready, always reliable.

Furthermore, the mental model of offline computing fosters a different kind of engagement.  Working without the constant distractions of notifications and updates allows for deeper focus, a more deliberate pace of thought.  It’s a return to a more meditative approach to problem-solving, a stark contrast to the frantic multitasking of the perpetually connected.

This isn't a Luddite manifesto; I'm not advocating for a return to punch cards and vacuum tubes. Instead, I'm suggesting a re-evaluation of our priorities. We should cultivate a more nuanced relationship with technology, recognizing the unique strengths of both online and offline approaches.  The most powerful tools often lie not in the complexity of the network, but in the elegance of the isolated system.  It's time to rediscover the quiet power of computing, unplugged.
","technological and societal insights
",AI,"human
","AI
"
184,184,"The Inevitable Rise of the Algorithmic City

The modern city, a chaotic agglomeration of human activity, is increasingly shaped by algorithms.  This isn't some dystopian fantasy; it's a straightforward consequence of the relentless advance of computing power and the ever-decreasing cost of data processing.  We’re on the cusp of a profound transformation, and understanding its mechanics is crucial for navigating the coming decades.

Consider the seemingly mundane: traffic management.  Once the domain of traffic lights timed by human engineers, urban arteries are now increasingly regulated by AI systems that learn from real-time data streams.  These algorithms dynamically adjust traffic flows, minimizing congestion and optimizing commute times. This is only the beginning.

The implications extend far beyond traffic.  Consider urban planning itself. Algorithms can analyze vast datasets encompassing demographics, land use, and infrastructure to model the impact of different development proposals.  They can predict strain on utilities, estimate future population density, and even forecast the potential success of new businesses.  This kind of predictive power allows for far more efficient, and arguably, fairer resource allocation.

This isn't just about efficiency; it's about a shift in the very nature of urban governance.  Traditional top-down planning, with its inherent limitations, is giving way to a more adaptive, data-driven approach.  The algorithmic city learns and reacts in real-time, constantly optimizing itself based on the needs and behaviors of its inhabitants.

Of course, this raises profound questions.  What happens when algorithms become the de facto city planners?  Who ensures that these systems are fair and equitable, and not simply amplifying existing inequalities?  How do we prevent the creation of algorithmic ghettos, where certain neighborhoods are systematically disadvantaged by the very systems designed to improve the city?

These are critical challenges, and they demand our attention.  The rise of the algorithmic city is not something to be feared, but it is something that requires careful consideration and proactive intervention.  We need to design systems that are transparent, accountable, and ultimately serve the needs of all citizens, not just a privileged few.  The future of our cities depends on it.
","technological and societal insights
",AI,"human
","AI
"
185,185,"The Myth of the ""Technological Singularity""

The term ""technological singularity""—the hypothetical point at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization—is, frankly, a bit silly.  It's a dramatic, science-fiction-tinged narrative, but it obscures a more nuanced and arguably more important truth: technological progress is not a singular, unpredictable event, but a messy, iterative process driven by incremental improvements and unexpected detours.

The idea of a sudden, transformative moment ignores the reality of technological development.  Consider the history of computing.  Did the invention of the transistor instantly usher in an era of ubiquitous personal computers?  Of course not.  It took decades of refinement, innovation in software, and the development of entire supporting industries before computers became commonplace.  Each step forward built upon the previous ones, often in unpredictable ways.

Furthermore, the singularity narrative often posits a kind of intelligence explosion, where AI surpasses human intelligence and fundamentally alters the world. But this assumes a linear progression of intelligence, ignoring the complexities of human cognition and the potential for emergent properties in artificial systems that are difficult, if not impossible, to predict.  We're better at making incremental progress than we are at predicting paradigm shifts.

What then, is a more useful framework for understanding technological progress?  I suggest focusing on the underlying incentives and constraints.  Technological development isn't driven solely by abstract concepts like ""intelligence,"" but by the practical needs and desires of humans.  Innovations arise to solve problems, satisfy wants, or create new markets.  These are all inherently messy, unpredictable processes.  A focus on these fundamental drivers is likely to yield a far more accurate understanding of future technological advancements.

Therefore, instead of fixating on a singular, apocalyptic event, we should instead focus on the ongoing, iterative process of technological change.  This involves understanding the forces shaping innovation—economic incentives, human needs, and the limitations of technology itself—rather than indulging in fantasies of sudden, dramatic transformations.  Only then can we develop a pragmatic approach to managing the challenges and opportunities presented by technological progress.
","technological and societal insights
",AI,"human
","AI
"
186,186,"The Myth of the Meritocracy: Why True Equality Demands More Than Fair Tests

The conventional wisdom, especially among the economically progressive, centers on the idea of a level playing field.  Level the playing field, the argument goes, and merit will naturally rise to the top.  Fix the broken system of credentials, ensure fairness in college admissions, and true equality will follow.

This is a naive, almost childish, understanding of how power and wealth actually function.  It assumes a static pie, a fixed amount of success to be distributed, rather than the reality of a constantly expanding, dynamically shifting economy.

The truth is far more complex, and far less comforting to those clinging to the simplistic ""level the playing field"" narrative.  The problem isn't simply unfair tests or biased admissions processes.  The problem is the inherent, systemic tendency for wealth and power to concentrate, irrespective of any attempts to engineer fairness at the individual level.

Consider the history of credentialism itself.  While initially a significant step forward from outright nepotism and bribery in selecting candidates for positions of power, credentialism has always been susceptible to manipulation.  The rise of cram schools, from ancient China to modern South Korea, exemplifies this.  Cram schools become sophisticated systems for translating parental wealth into academic success, subtly circumventing the supposed meritocratic filter.  The system adapts to the attempts to game it; every patch creates a new loophole.

The current obsession with fixing the ""leaks"" in the system—improving the fairness of tests, increasing transparency in admissions—is a Sisyphean task.  The forces of wealth and influence will always find ways to permeate the system.  It's a never-ending arms race.

But there is a more powerful force at play that renders many of these concerns moot: the dynamism of technological and economic growth.  The rise of smaller, more agile organizations, particularly startups, is fundamentally altering the relationship between credentials and success.  In an environment dominated by large corporations, credentials act as a self-fulfilling prophecy.  In a landscape teeming with small companies, however, performance becomes the ultimate arbiter.

The rise of the ""startup culture"" is not merely a shift in individual career paths; it represents a fundamental restructuring of the economic playing field itself.  Small organizations inherently function with more efficient systems of meritocracy than their larger counterparts.  Performance, not pedigree, is what matters. This system is fundamentally resistant to the types of manipulation that plague older, more established systems.

This is not to say that all is well in the current system. Far from it.  Abuses and inequalities still exist. But addressing these abuses requires a more fundamental shift than merely tweaking the existing metrics of ""fairness."" We need to grapple with the underlying forces that drive wealth concentration—forces far more potent than any individual's attempts to gain an unfair advantage.  We must move past naive notions of a level playing field and address the reality of an exponentially expanding economy.  True equality may require a radical reimagining of how we structure power and opportunity, rather than a perpetual campaign of patching leaks in a fundamentally flawed system.
","technological and societal insights
",AI,"human
","AI
"
187,187,"The Subtle Art of Ignoring the Obvious: Why We Should Embrace the Unloved

There's a peculiar bias in our industry, a sort of technological herd mentality. We chase the shiny, the hyped, the loudly proclaimed.  We flock to the conferences, devour the press releases, and line up for the next big thing—often before it's even demonstrably *a* thing. This, I argue, is a profound mistake.

The truly transformative technologies, the ones that reshape industries and alter the course of history, rarely arrive with fanfare. They emerge from the shadows, often unloved and misunderstood, even by those who would eventually benefit most from their adoption.

Consider the early days of the internet.  A chaotic, disorganized network of academics and hobbyists, it wasn’t exactly a polished, corporate product. Yet, from that unpromising beginning sprung a revolution.  The elegance was in the underlying architecture, not the marketing.

This isn’t to say that all neglected technologies are diamonds in the rough. Many fail for good reason. But the indiscriminate dismissal of anything lacking polish or popular acclaim is a recipe for missing genuinely disruptive innovation.

Why does this happen?  Several factors contribute.  Firstly, the loudest voices aren't always the wisest.  Hype is often inversely proportional to substance. Secondly, there's a psychological element at play.  We gravitate towards what's familiar, what’s easily understood.  The genuinely novel often feels uncomfortable, even unsettling.

But discomfort can be a good sign.  If something truly challenges the status quo, it will inevitably ruffle feathers.  It will contradict entrenched beliefs, existing power structures, and long-held assumptions.  This resistance is not a mark of failure; it's a symptom of true innovation.

So how does one cultivate the ability to spot these hidden gems?  It begins with a deliberate act of ignoring the obvious.  Tune out the hype machine.  Look beyond the polished presentations and focus on the underlying principles.  Ask yourself: what problem does this technology solve? Is it solving it in a fundamentally new way? Does it possess an elegance and simplicity that transcends its current limitations?

Above all, be willing to embrace the unloved.  The technologies that ultimately triumph are often the ones that are initially dismissed as niche, impractical, or even plain ugly. They are the technologies that force us to confront our biases, re-evaluate our assumptions, and redefine what's possible.  The world is full of such neglected potential. The challenge is in finding the courage and discernment to recognize it.
","technological and societal insights
",AI,"human
","human
"
188,188,"The Subtle Power of Network Effects: Why Some Things Take Off and Others Don't

We often marvel at the meteoric rise of certain technologies – a seemingly overnight explosion of popularity that leaves competitors scrambling.  We attribute this success to various factors: clever marketing, superior technology, even sheer luck.  But I believe a more fundamental force is at play: network effects.  Not the crude, obvious kind, like the value of a phone network increasing with every new subscriber, but something more subtle, more deeply ingrained in the fabric of how we interact with technology.

Consider the evolution of programming languages.  Lisp, with its radical innovations, faced an uphill battle for decades.  Its elegant, functional paradigm was largely ignored, overshadowed by more pragmatically designed languages. Why?  Part of it was the perceived steep learning curve, certainly. But a significant factor was the lack of a critical mass of users. Libraries, tools, and communities—the ecosystem around a language—take time to build.  And without a vibrant community, talented developers are less likely to invest their time in it, creating a self-reinforcing cycle.

This isn't just about programming languages. Think about the rise of the iPhone. It wasn't the first smartphone, nor was it necessarily the most technically advanced initially. But Apple cultivated a powerful network effect:  a seamless user experience combined with a thriving app ecosystem.  This drew developers, which attracted users, which attracted more developers, and so on.  The result was a virtuous cycle that propelled Apple to market dominance.

The key is understanding the different kinds of network effects. Some are directly quantifiable: the more users on a platform, the more valuable it becomes.  But others are less tangible, more about the intangible benefits of being part of a community, access to a shared knowledge base, the feeling of belonging to something bigger than oneself. This type of network effect is particularly potent in fields like software development, design, and even research.

Many promising projects fail not because of inherent flaws, but because they fail to ignite this subtle kind of network effect.  They lack the critical mass to attract the talent and users necessary to build a sustainable ecosystem.  It's a lesson for entrepreneurs and technologists alike: focus on building not just a great product, but a thriving community around it. The network effect, in its most subtle forms, may be the most powerful force driving technological success.
","technological and societal insights
",AI,"human
","AI
"
189,189,"The Unforeseen Consequences of Ubiquitous Computing

The relentless march of technology often leaves us grappling with its societal implications long after the initial excitement fades.  Consider the seemingly innocuous rise of the ""always-on"" internet, embedded not just in our desktops but now in our pockets and even our homes.  This ubiquity, while offering unprecedented convenience, presents a subtle yet profound shift in how we interact with the world, one that deserves careful consideration.

The most immediate impact is, of course, on our attention spans. The constant barrage of notifications, updates, and readily available information fragments our focus, fostering a culture of short-term gratification and diminishing our capacity for deep, sustained thought.  This isn't simply a matter of individual discipline; it's a systemic issue shaped by the very design of the technology itself.  The addictive nature of social media, for example, isn't accidental; it's a carefully engineered feature designed to maximize engagement, often at the expense of our well-being.

Beyond individual impact, the pervasive nature of computing subtly alters the power dynamics within society.  The concentration of data in the hands of a few powerful tech giants raises significant concerns about privacy, censorship, and the potential for manipulation.  Algorithms, designed to optimize for efficiency or profit, can inadvertently amplify existing biases, leading to discriminatory outcomes in areas like hiring, loan applications, and even criminal justice.  We're not just creating new technologies; we're creating new systems of power, often without fully understanding their consequences.

The long-term effects are even more uncertain.  The very fabric of our social interactions is being reshaped by digital communication.  While online communities offer connection and support, they can also foster echo chambers and polarization, exacerbating societal divisions.  The increasing automation of tasks, though promising economic efficiency, raises anxieties about job displacement and the future of work.  These are not abstract concerns; they are challenges we must address proactively.

This isn't a call for technological Luddism.  The potential benefits of ubiquitous computing are immense.  However, a blind faith in technological progress without a critical examination of its consequences is a dangerous path.  We need a more nuanced approach—one that embraces innovation while simultaneously addressing the potential downsides.  This requires a multi-faceted strategy, involving not just technological solutions but also societal changes in our habits, our regulations, and our understanding of how technology shapes our lives.  The challenge before us is to harness the power of ubiquitous computing while mitigating its risks, ensuring that technology serves humanity, rather than the other way around.
","technological and societal insights
",AI,"human
","human
"
190,190,"The Tyranny of the Urgent: How Technology is Reshaping Our Attention

The modern world is a whirlwind of distractions.  A constant barrage of notifications, a seemingly endless scroll of social media feeds, the siren song of instant gratification – these are the hallmarks of our technologically advanced society.  But this constant stimulation isn't just inconvenient; it's fundamentally altering the way we think, work, and live.  It's a subtle form of tyranny, one that steals our focus and diminishes our ability to pursue meaningful goals.

The root of the problem isn't malice, but rather the relentless engine of technological progress.  Innovation, by its very nature, seeks to amplify our desires, to make things more efficient, more engaging, more immediately satisfying.  This is a double-edged sword.  While progress has brought us countless benefits – from medical miracles to unprecedented connectivity – it has also created a landscape ripe for addiction.

Consider the trajectory of various substances:  hard liquor is a more concentrated form of fermented beverages, crack cocaine a hyper-potent derivative of coca leaves.  The pattern isn't limited to drugs.  Video games become more immersive, social media more captivating, food more palatable and readily available.  Each iteration is a refinement, a step closer to maximizing engagement, often at the expense of our long-term well-being.

The acceleration of this process is undeniable.  What was considered an exceptional level of engagement a few decades ago – say, the dedication required for a serious hobby – now pales in comparison to the addictive pull of readily available entertainment.  Our attention spans shrink, and the ability to focus on demanding tasks, those requiring deep thought and sustained effort, erodes.

The irony is that this very distraction is preventing us from addressing the problem effectively.  The tools designed to connect us, to improve our productivity, are ironically making us less productive and more isolated.  Our ability to discern what truly matters, to separate the urgent from the important, becomes increasingly clouded.

Escaping this isn't about rejecting technology wholesale.  It's about cultivating a mindful approach, about intentionally shaping our relationship with these tools.  It's about recognizing the seductive power of immediate gratification and consciously choosing alternatives: deep work, meaningful relationships, activities that demand focus and offer intrinsic reward.

The path forward requires individual responsibility.  It means establishing personal boundaries, creating spaces free from digital distractions, and cultivating habits that promote focus and long-term goals. It's a battle against the tyranny of the urgent, a fight for the freedom to pursue what truly matters.  And it's a battle worth fighting.
","technological and societal insights
",AI,"human
","AI
"
191,191,"The Myth of the Generalist and the Rise of the Hyper-Specialist

The modern economy, particularly in the tech sector, seems obsessed with the idea of the ""full-stack"" developer, the design-thinking entrepreneur, the multi-talented individual who can seemingly do it all.  This notion, however appealing, is ultimately a dangerous delusion.  It rests on a fundamental misunderstanding of how exceptional performance arises.

The truth is, exceptional results in any field require deep, focused expertise.  It's not enough to be competent across a broad range of skills; true mastery demands years, even decades, of intense specialization. The idea that a single individual can possess world-class proficiency in multiple disparate domains is statistically improbable bordering on impossible.

Consider the most successful companies.  Do they thrive because of generalist leadership, or because of a concentration of hyper-specialists? The answer is self-evident.  The giants of the tech world are not built on jacks-of-all-trades; they are constructed on teams of individuals, each possessing an almost frightening level of expertise in their respective niches.

This is not to say generalists are useless.  They have their place, particularly in bridging communication gaps between highly specialized teams.  But to mistake general competence for the engine of innovation is a profound error.  The breakthroughs, the game-changing ideas, always come from those who've plumbed the depths of a specific domain, those who possess a depth of knowledge that borders on the uncanny.

The current infatuation with the generalist is likely a reflection of a certain kind of anxiety.  In a rapidly evolving world, the desire for a broad skillset offers a sense of security, a hedge against obsolescence.  However, this strategy is inherently defensive.  True success comes from embracing specialization, from becoming so deeply knowledgeable in a particular area that one’s expertise becomes almost irreplaceable.

The future, therefore, doesn't belong to the generalist. It belongs to the hyper-specialist – the individual who dares to dive deep, who commits to mastery, and who understands that true innovation comes not from superficial breadth, but from profound depth.  The challenge is not to be a little bit good at many things, but exceptionally good at one. And maybe, just maybe, exceptionally good at a few things deeply related.
","technological and societal insights
",AI,"human
","AI
"
192,192,"The Coming Collapse of the Middle Class Mind

The modern world is obsessed with efficiency. We streamline processes, automate tasks, and constantly seek ways to optimize our output.  This relentless pursuit of efficiency, however, often overlooks a critical consequence: the erosion of fundamental skills. I'm not talking about blacksmithing or buggy-whip making, skills rendered obsolete by technological progress. I'm talking about something far more insidious: the decline of clear thinking, a decline directly linked to the erosion of writing ability.

For years, the pressure to write has been immense.  From the most mundane emails to complex research papers, the ability to articulate thoughts clearly and concisely has been a near-universal requirement, particularly in higher-level professions. This pressure has acted as a filter, forcing individuals to develop their capacity for clear thinking, which is inherently difficult work.  The very act of translating complex thoughts into written form demands a level of intellectual rigor that cannot be easily replicated elsewhere.

Yet, this filter is crumbling.  The advent of advanced AI writing tools has dramatically altered the landscape.  No longer is the ability to write a necessary condition for professional success.  The middle ground—those who are merely adequate writers—are finding a convenient bypass. The pressure to craft compelling prose, to organize complex ideas logically, to express nuanced concepts effectively, is rapidly dissipating.

The consequences of this shift are profound.  Writing is not merely a skill; it's a cognitive process.  As Leslie Lamport eloquently stated, ""If you're thinking without writing, you only think you're thinking.""  The ability to translate thought into words is not simply about communication; it's about refining and solidifying one's own understanding.  Without the crucible of writing, the clarity of thought itself is jeopardized.

We are heading towards a bifurcation, a world divided into ""writes"" and ""write-nots"".  Those who have consciously cultivated the discipline of writing and who see its value beyond mere task completion will remain capable of deep, complex thought.  But the vast majority, relieved of the burden of clear expression, will risk an atrophy of their critical thinking skills.  This will not be a silent, gradual decline; its impact will reverberate through society, affecting everything from the quality of scientific research to the efficacy of political discourse.

We see this phenomenon replicated in other areas.  Physical strength, once a near-universal necessity for many manual labor jobs, is now a largely optional pursuit.  Those who choose to cultivate it through exercise retain an advantage, while many others accept a degree of physical frailty as the norm.  The same trajectory seems inevitable for clear thinking. The challenge is whether we, as a society, will actively strive to resist the decline or passively accept the simplification of thought.  I, for one, know which side of the divide I'd prefer to occupy.
","technological and societal insights
",AI,"human
","human
"
193,193,"The Most Valuable Thing:  Beyond Intelligence

For years, I've observed a curious disconnect in the way we value intellect.  We lionize intelligence, associating it with success, achievement, and even genius.  Yet, a simple thought experiment reveals a deeper truth:  raw intelligence, while necessary, is not sufficient for groundbreaking contributions.  The truly valuable thing is not *being* smart, but *doing* something smart—creating something new, insightful, or impactful.

Consider two individuals: one possesses exceptional intellect, a mind capable of mastering complex systems.  The other possesses average intelligence but an insatiable curiosity, a relentless drive to explore uncharted territories.  Who is more valuable to society?  Who leaves a more enduring legacy?

The answer, I believe, is self-evident.  The second individual, while perhaps less dazzling in intellectual displays, ultimately generates the novel ideas, the paradigm shifts, that reshape our understanding of the world.  The first individual, despite their superior intellect, may remain confined to the well-trodden paths, contributing incrementally rather than radically.

This distinction is not merely semantic.  It highlights a crucial blind spot in our cultural valuation of intelligence.  We emphasize the *potential* for greatness, often mistaking potential for actual accomplishment.  The brilliance of a mind is only as valuable as the impact it produces.  A brilliant mind unburdened by action remains, in effect, a latent potential, a tantalizing glimpse of what could have been.

The question then becomes: what are the ingredients, beyond raw intelligence, that fuel innovation and transformative achievement?  This is a rich area of inquiry, one that demands a more thorough examination than I can offer here.  However, I can offer some initial observations.

One crucial ingredient is sustained, focused passion.  A deep, abiding interest in a particular field, a burning desire to uncover its mysteries, is the engine that drives relentless exploration. This isn't merely hard work; it's a kind of intrinsic motivation, an almost obsessive curiosity that transcends the demands of external validation.

Another vital element is independent thought. The capacity to question assumptions, to challenge established paradigms, to forge one's own path—this independent spirit is often the wellspring of truly original ideas.  While partially innate, it can also be nurtured through mindful practice and conscious cultivation of dissenting viewpoints.

Finally, let's not underestimate the power of effective communication.  The ability to articulate ideas clearly and persuasively is crucial in disseminating new knowledge and influencing others.  This is particularly relevant in domains where the impact of an idea relies heavily on its widespread adoption and application.


In conclusion, while we rightly celebrate intelligence, we must recognize that its value is ultimately determined by its application.  The real prize is not intellectual prowess alone, but the tangible, enduring contributions born from that potential, contributions that shape our world in profound and lasting ways.  The path to lasting impact requires not only intellectual brilliance but also passion, independent thinking, and effective communication—ingredients that, unlike raw intelligence, can be nurtured and cultivated throughout life.
","philosophical reflection
",AI,"human
","AI
"
194,194,"The Most Valuable Truth

Most people, when they think about universal truths, immediately jump to mathematics and physics.  The speed of light, the Pythagorean theorem—these seem rock-solid, independent of human whim or cultural context.  But what about truths that go beyond the purely formal or physical?  What about truths that might be shared, not just by humans, but by any sufficiently intelligent species, anywhere in the universe?

I call these ""alien truths.""  The concept is simple:  what fundamental principles, beyond the laws of physics and mathematics, are so deeply woven into the fabric of intelligence itself that they would inevitably emerge in any sufficiently advanced civilization?  This isn't science fiction; it's a framework for philosophical inquiry.  It's a way of cutting through the noise of human-centric biases to get at something deeper, more universal.

Consider the scientific method.  The core principle—that controlled experimentation leads to proportional increases in belief—seems profoundly non-human-specific.  An alien scientist, faced with conflicting hypotheses, would likely adopt a similar approach.  Similarly, the idea of improving skill through practice, or the principle of Occam's Razor (the simplest explanation is usually the best), appear to transcend our particular species.

Of course, we can only speculate.  We don't know what form alien intelligence might take.  But the uncertainty shouldn't paralyze us.  The value of the ""alien truth"" framework lies in its heuristic power.  It provides a target, a standard for evaluating the generality of our ideas.  If a principle might reasonably apply to any intelligent life form, then it's worthy of serious consideration.  Justice, for example, seems like a strong candidate.  While we can't definitively prove aliens would share our concept of justice, the possibility alone makes it a compelling area for philosophical exploration.

This search for alien truths, this attempt to identify the fundamental principles governing intelligent thought regardless of its embodiment, is essentially what philosophy should be.  It's a far more ambitious and potentially fruitful endeavor than much of what currently passes for philosophical discourse.  Let's strive for universal understanding, not just human-centric navel-gazing.

Ultimately, this pursuit might even become a self-fulfilling prophecy.  The development of advanced artificial intelligence could offer a concrete testbed for our hypotheses.  We might discover that certain principles, like Occam's Razor, are not merely likely, but logically necessary for the emergence of intelligence.  But regardless of future discoveries, the attempt to uncover these ""alien truths"" is a deeply worthwhile endeavor—and that, ironically, may well be an alien truth itself.
","philosophical reflection
",AI,"human
","AI
"
195,195,"The Most Valuable Kind of Stupidity

Most people agree that intelligence is good.  But what about its less celebrated cousin, stupidity?  Is there such a thing as *good* stupidity?  The answer, I believe, is a resounding yes.  And understanding this distinction is crucial for anyone seeking to build or create anything of lasting value.

The common conception of stupidity is negative, even contemptuous. It’s the inability to grasp obvious truths, the stubborn refusal to learn, the embarrassing blunder in plain sight.  This is the stupidity to be avoided, the kind that holds one back.  This is, in essence, *obstinate* stupidity.

But consider the opposite. Imagine someone tirelessly pursuing a seemingly impossible goal, ignoring conventional wisdom, and relentlessly testing their own assumptions.  They may make repeated mistakes, often spectacular ones.  They may seem foolish to outsiders, even to themselves at times.  Yet they persist, driven by an inner conviction that transcends immediate failure.  This is what I’d call *persistent* stupidity.

The difference is not merely in the outcome.  A persistent fool might ultimately fail, and an obstinate fool might stumble onto success through sheer luck. The difference lies in the *process*.  The obstinate fool clings to initial ideas, resisting all evidence to the contrary.  They are intellectually closed.  The persistent fool, however, is open to revision, even radical revision. They are intellectually flexible, using their failures as data points in a larger experiment.  Their stupidity is not a lack of intelligence, but rather a willingness to act in the face of uncertainty, to endure repeated setbacks, to learn from mistakes.

This willingness to appear foolish, to embrace failure as a necessary step towards progress, is exceptionally rare.  It requires a unique blend of courage, imagination, and a deep-seated belief in one’s own ability to learn.  It’s a kind of intellectual recklessness, a calculated gamble on the future.

The greatest breakthroughs, the most significant inventions, often arise from this persistent form of stupidity.  The pioneers who dared to challenge established norms, the artists who pushed the boundaries of expression, the scientists who embraced the uncertainty of experimentation – all displayed a profound capacity for the valuable kind of stupidity.

So, the next time you encounter someone who seems stubbornly foolish, take a closer look.  Are they merely clinging to outdated ideas, or are they courageously forging a new path, accepting failure as part of the journey?  The answer will tell you much about the nature of their endeavor, and about the true value of stupidity itself.
","philosophical reflection
",AI,"human
","human
"
196,196,"The Illusion of Control: Why We're Not as Free as We Think

The feeling of free will is a powerful one.  It’s the bedrock of our moral systems, our legal systems, even our sense of self.  We believe we *choose* our actions, that we are the authors of our own lives.  But is this belief justified? Or is it merely an illusion, a convenient fiction we tell ourselves to navigate a complex world?

Consider the simple act of raising your hand.  Seems straightforward enough, right?  A pure act of will. But peel back the layers.  The decision to raise your hand is a result of a complex interplay of neurological processes, influenced by everything from your immediate environment to your deepest memories and ingrained biases.  These processes, while incredibly complex, are ultimately physical processes, governed by the laws of physics. Are these truly *your* choices, or merely the predictable output of a sophisticated machine?

The determinists, of course, would argue the latter.  They posit that every action is predetermined by prior causes, stretching back to the Big Bang.  Every thought, every feeling, every decision is merely a link in an unbroken chain of cause and effect.  Free will, in this view, is a delusion.

But the determinist position, while intellectually compelling, feels fundamentally wrong.  The subjective experience of choice is undeniable.  We *feel* free, and this feeling, however illusory, is a powerful force shaping our behavior.

Perhaps the resolution lies in a more nuanced understanding of what we mean by ""choice.""  We are not omnipotent.  We cannot choose to fly or teleport.  Our choices are constrained by the physical realities of our existence.  But within those constraints, we operate with a degree of autonomy.  We can weigh options, evaluate consequences, and make decisions that are, within the bounds of our capabilities and our current state, genuinely our own.

The illusion of control, then, is not entirely an illusion.  It is a useful approximation of a complex reality.  We aren't puppets on strings, but neither are we entirely free agents.  We are complex systems, capable of self-awareness and decision-making, but fundamentally governed by the laws of nature and the constraints of our own biology.  To truly understand ourselves, we must reconcile these seemingly contradictory realities.  We are both machines and authors, and in that paradox lies the true mystery of human existence.
","philosophical reflection
",AI,"human
","AI
"
197,197,"The Unreasonable Effectiveness of Foundational Truths

The pursuit of truth often feels like a Sisyphean task. We toil away, building intricate systems of thought, only to find them crumbling under the weight of new evidence or contradictory perspectives.  Yet, some ideas possess a remarkable resilience, a stubborn refusal to be dislodged. These aren't the dry, mathematical axioms, nor the empirically verifiable laws of physics.  I'm talking about a deeper layer of truths, the kind that might resonate with any sufficiently intelligent being, anywhere in the universe.

Consider the principle of parsimony, Occam's Razor.  It suggests the simplest explanation is usually the best.  Is this merely a human construct, a cognitive shortcut born from our limited processing power?  Or does it represent a fundamental aspect of how information and reality are organized?  I suspect the latter. An alien civilization grappling with complex problems would likely arrive at a similar heuristic, even if they expressed it differently.

This is the essence of what I'll call ""alien truths""—propositions robust enough to transcend the peculiarities of our specific evolutionary path. Identifying them isn't about imagining what aliens *might* believe; rather, it provides a rigorous standard for evaluating the solidity of our own ideas.  If a concept holds up under the scrutiny of hypothetical, radically different intelligences, it gains a degree of legitimacy that mere cultural consensus can never provide.

The quest for alien truths has a practical side as well.  It forces us to confront our biases, to strip away the layers of assumptions and prejudices that cloud our judgment.  It's a kind of intellectual hygiene, a process of rigorous self-examination, pushing our concepts to their absolute limits.

What about concepts like justice or beauty?  Can we imagine aliens lacking any sense of fairness, or appreciating aesthetic principles? Perhaps. But the very act of considering these questions compels us to refine our understanding of these concepts, to articulate their essential features in terms that transcend human-specific cultural baggage.

The search for alien truths isn't a purely speculative exercise.  It has the potential to significantly enhance our understanding of fundamental principles, providing a touchstone for evaluating the validity and robustness of our ideas. It's a path towards a deeper, more universal truth, one that resonates not only with our own minds, but with the potentially limitless expanse of intelligence beyond our planet.  And that, in itself, seems like an alien truth worthy of pursuit.
","philosophical reflection
",AI,"human
","AI
"
