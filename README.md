# Is this AI-Generated? A Comparative Analysis of Zero-Shot and Few-Shot Text Classification with GPT-based Language Models

## Abstract
The rise of AI-generated content presents a growing need for low-resource, scalable detection methods. This project explores the performance of GPT-based language models, leveraging zero-shot and few-shot learning for binary text classification. By creating a dataset of human-written blog posts by Paul Graham and generating AI texts through Google’s gemini-1.5-fast, we aim to establish a reliable baseline for distinguishing between human and AI-generated content. Our findings highlight the advantages of few-shot learning over zero-shot learning, offering a cost-effective and accessible approach to AI text detection.

## Research Questions
1. How do zero-shot and few-shot learning approaches compare in distinguishing human and AI-generated text?
2. How can we leverage thematic and linguistic preprocessing and analysis to ensure the alignment between human- and AI-generated datasets before the classification tasks?

## Main Datasets
- **Human-generated content**: Blog posts by Paul Graham, collected via web scraping.
- **AI-generated content**: Blog posts generated by Google’s gemini-1.5-fast, aligned to human content in themes, style, and tone.
- **Mixed dataset**: Combines human and AI content with labels (`Human` or `AI`) for classification tasks.
  
### Dataset Details:
- Format: CSV
- Approximate Size: 100+ blog posts per dataset
- Enrichment Steps:
  - Thematic coding for categorization
  - Lexicon analysis for linguistic alignment
  - Random sampling for balanced data generation

## A Tentative List of Milestones for the Project
1. **Week 1**: Set up repository structure and scrape Paul Graham’s blog posts.
2. **Week 2**: Perform text preprocessing (normalization, tokenization, lemmatization) and thematic coding of human-generated content.
3. **Week 3**: Generate AI content using gemini-1.5-fast and conduct lexicon analysis to ensure alignment.
4. **Week 4**: Develop and test classification scripts for zero-shot and few-shot learning.
5. **Week 5**: Evaluate classification performance and visualize results using plots.
6. **Week 6**: Finalize the project, prepare documentation, and submit the report.

### Responsibilities:
- **Data Preparation and Analysis**: [Your Name]
- **AI Content Generation and Alignment**: [Your Name]
- **Classification and Evaluation**: [Your Name]
- **Documentation and Visualizations**: [Your Name]

## Documentation
### Repository Structure
- **`data/`**: Contains raw and processed datasets.
- **`notebooks/`**: Jupyter notebooks for data exploration, preprocessing, and analysis.
- **`src/`**: Source code for scraping, preprocessing, content generation, classification, and visualization.
- **`reports/`**: Figures and summary reports of findings.
- **`tests/`**: Unit tests for verifying the correctness of modules.

```bash
AI-content-detection-with-gpt-models/
│
├── data/  # Folder containing datasets for the project
│   ├── raw/
│   │   ├── human_blog_posts.csv
│   │   ├── ai_generated_posts.csv
│   ├── processed/
│   │   ├── combined_dataset_processed_classified.csv
│   │   ├── human_blog_posts_processed.csv
│   │   ├── ai_generated_blog_posts_processed.csv
│   └── README.md
│
├── notebooks/
│   ├── data_preprocessing.ipynb
│   ├── dataset_analysis.ipynb
│   ├── content_generation.ipynb
│   ├── text_classification.ipynb
│   ├── evaluation_metrics.ipynb
│   └── README.md
│
├── src/  # Source code for the project's core functionality
│   ├── scraping/
│   │   ├── scrape_paul_graham.py
│   │   ├── README.md
│   ├── preprocessing/
│   │   ├── normalize_tokenize_lemmatize.py
│   │   ├── thematic_coding.py
│   │   ├── lexicon_analysis.py
│   │   ├── dataset_mixing.py
│   │   ├── README.md
│   ├── content_generation/
│   │   ├── generate_ai_texts.py  # Generates AI content using GPT-based models
│   │   ├── README.md
│   ├── classification/
│   │   ├── zero_shot_classification.py
│   │   ├── few_shot_classification.py
│   │   ├── evaluate_performance.py
│   │   └── README.md
│   └── visualization/  # Modules for creating visualizations
│       ├── plot_thematic_distribution.py 
│       ├── plot_lexicon_analysis.py
│       ├── plot_performance_metrics.py
│       └── README.md
│
├── reports/  # Contains results and visualizations
│   ├── figures/  # Stores generated visualizations and figures
│   │   ├── thematic_distribution.png
│   │   ├── lexicon_analysis.png
│   │   ├── performance_metrics.png
│   └── results_summary.md
│
├── tests/
│   ├── test_scraping.py
│   ├── test_preprocessing.py
│   ├── test_classification.py
│   ├── test_visualization.py
│   └── README.md
│
├── requirements.txt
├── README.md
├── .gitignore
├── LICENSE
└── setup.py
```

## Getting started
### Step 1: clone the repository
1. Clone the repository to your local machine:
```bash
git clone https://github.com/your_username/ai-text-detection.git
cd ai-text-detection
```
### Step 2: Set Up the Environment
2. Set up a virtual environment (optional but recommended):
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
```
3. Install dependencies:
```bash
pip install -r requirements.txt
```
4. Run tests to validate setup:
```bash
python -m unittest discover tests/
```
### Step 3: Data Preparation and AI content generation
1. Scrape Paul Graham’s blog posts:
```bash
python src/scraping/scrape_paul_graham.py
```
2. Enrich human-generated dataset with themes' codes through LLM-based thematic analysis:
```bash
python src/thematic_coding.py
```
3. Generate AI texts aligned with the human content for themes:
```bash
python src/content_generation/generate_ai_texts.py
```
4. Text processing of both datasets before linguistic analysis:
```bash
python src/preprocessing/normalize_tokenize_lemmatize.py
```
5. Lexicon analysis to ensure datasets' linguistic alignment
```bash
python src/preprocessing/lexicon_analysis.py
```
6. Unify the two datasets in balanced and homogeneous way:
```bash
python src/preprocessing/dataset_mixing.py
```
### Step 5: Classification
1. Perform zero-shot classification:
```bash
python src/classification/zero_shot_classification.py
```
2. Perform few-shot classification:
```bash
python src/classification/few_shot_classification.py
```
### Step 6: Visualizations
Generate visualizations for analysis:

1. Plot thematic distribution:
```bash
python src/visualization/plot_thematic_distribution.py
```
2. Plot lexicon analysis:
```bash
python src/visualization/plot_lexicon_analysis.py
```
3. Plot performance metrics:
```bash
python src/visualization/plot_performance_metrics.py
```

## License
This project is licensed under the MIT License.

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

1. The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
2. THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

For more details, see the full license text in the [LICENSE](LICENSE) file.
```
