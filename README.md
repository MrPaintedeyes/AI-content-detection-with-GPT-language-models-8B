# Is this AI-Generated? A Comparative Analysis of Zero-Shot and Few-Shot Text Classification with GPT-based Language Models

## Abstract
The rise of AI-generated content presents a growing need for low-resource, scalable detection methods. This project explores the performance of GPT-based small language models (gemini-1.5-fast 8B), leveraging zero-shot and few-shot learning for binary text classification of human- and AI-generated content. By creating a dataset of human-written blog posts by Paul Graham and generating AI texts through Google’s gemini-1.5-fast aligned for topics, tone and style, we aim to establish baseline performance metrics in distinguishing between human and AI-generated content with in-context learning approaches, hoping to provide a foundation for future research and applications. Our findings highlight the advantages of few-shot learning over zero-shot learning, altough they are both unrealiable for a consistent detection of AI-generated texts. 

## Research questions
1. How do zero-shot and few-shot learning approaches compare in distinguishing human and AI-generated text?
2. How can we leverage topic labeling and lexicon analysis (lemmas' overlap) to ensure the alignment between human- and AI-generated datasets before the classification tasks?

## Main datasets in CSV
- **Human-generated texts ("data/human_blog_posts.csv")**: 226 blog posts by Paul Graham, collected via web scraping, accompanied by their link, author name ("author" column, label: "Paul Graham"), main topic label ("topic_category" column; example of label: "startup advice") and annotated ground truth ("AI_or_human" column, labels "AI" and "human").
- **AI-generated texts ("data/ai_generated_blog_posts.csv")**: 99 blog posts generated by Google’s gemini-1.5-fast, aligned to human content in topics, style, and tone; they are also accompanied by the author name (label: "gemini-1.5-fast"), main topic label, and annotated ground truth.
- **Mixed dataset ("data/ai_human_blog_posts.csv")**: 198 texts; it combines human and AI-generated blog posts; it is also accompanied by the columns "AI_or_human" and "topic_category". It is also enriched with the columns "classification_result_zeroshot" and "classification_result_fewshot" to store classification outputs from the model (labels: "AI", "human").

### Other datasets in JSON
- **Processed human-generated texts ("data/human_blog_posts_processed.json)**: with the same headers, number of entries and values from the csv counterpart, but enriched with the outputs from the linguistic pre-processing module (normalize_tokenize_lemmatize.py), storing cleaned (no stop words, no punctuation), normalized, tokenized (list) and lemmatized texts (list) - added columns: "cleaned_blog_post", "normalized_blog_post", "tokenized_blog_post", and "lemmatized_blog_post".
- **Procesed AI-generated texts ("data/ai_blog_posts_processed.json")**: with the same headers, number of entries and values from the csv counterpart, and enriched as human_blog_posts_processed.json above.
- **Classification results ("data/model_classification_results.json")**: it contains the counts for true positives, true negatives, false positives, false negatives derived from the classification outputs compared with ground truth (for both zero-shot and few-shot approaches, 8 entries in total).
- **Performance metrics ("data/model_performance_metrics.json")**: it contains accuracy, precision, recall, F1-score measurements for both prompting approaches (8 entries in total).
  

## List of milestones for the project
1. **Week 1**: Scrape Paul Graham’s blog posts and implement topic labeling with a small language model.
2. **Week 2**: Generate AI content using gemini-1.5-fast, maintaining topics' proportions of the dataset with human-generated texts.
3. **Week 3**: Perform text preprocessing (normalization, cleaning, tokenization, lemmatization) and conduct lexicon analysis (counting and ranking lemmas in human- and AI-generated texts) to evaluate similarity.
4. **Week 4**: Develop and test classification scripts for zero-shot and few-shot learning.
5. **Week 5**: Evaluate classification performance and visualize results using plots.
6. **Week 6**: Finalize the project, prepare documentation, and submit the report.

## Repository structure and modules functioning

```bash
AI-content-detection-with-gpt-8B-models/
│
├── data/  # folder containing all datasets for the project
│   ├── human_blog_posts.csv
│   ├── ai_generated_blog_posts.csv
│   ├── ai_human_blog_posts.csv
│   ├── human_blog_posts_processed.json
│   ├── ai_generated_blog_posts_processed.json
│   ├── model_classification_results.json
│   ├── model_performance_metrics.json
│   └── README.md
│
├── notebooks/
│
├── src/  # source code for the project's core modules
│   ├── scraping/
│   │   └── web_scraper.py # scrapes posts from Paul Graham blog
│   ├── preprocessing/
│   │   ├── normalize_tokenize_lemmatize.py
│   │   ├── topic_labeler.py
│   │   └── human_ai_data_mixer.py
│   ├── linguistic_analysis/
│   │   ├── lexicon_analysis_visualization.py
│   │   └── topic_analysis_visualization.py
│   ├── content_generation/
│   │   ├── ai_content_generator.py
│   └── classification/
│       ├── zero_shot_classifier.pyg
│       ├── few_shot_classifier.py
│       └── performance_analysis_visualization.py 
│
├── report/ # 4-page double column project report
│   └── unibo_python_project_giovannimariaocchipinti.pdf/
│
├── requirements.txt
├── README.md
├── .gitignore
└── LICENSE
```
- web_scraper.py scrapes posts from Paul Graham blog
- normalize_tokenize_lemmatize.py implements normalization, tokenization and lemmatization of all blog posts
- topic_labeler.py calls a language model to attach topic labels to the human blog posts
- human_ai_data_mixer.py merges the human- and AI-generated datasets
- lexicon_analysis_visualization.py counts and ranks lemmas across all blog posts, plots lemmas distribution
- lexicon_analysis_visualization.py counts topics occurences across all blog posts, then plots topics distribution
- ai_content_generator.py generates synthetic blog posts mirroring Paul Grahams topics, style and tone
- zero_shot_classifier.py implements classification task with zero-shot learning
- few_shot_classifier.py implements the classification task with few-shot learning
- performance_analysis_visualization.py measures TP, TN, FP, FN and performance metrics, plots results

## Getting started
The project's code could take a couple of hours to run entirely (due to the model's multiple API calls). Thus, to access snippets, graphs, results quickly, you can look at the jupyter notebook within the repository following these steps directly *from your terminal*:

1. Clone the repository to your local machine:
```bash
git clone https://github.com/MrPaintedeyes/AI-content-detection-with-GPT-language-models-8B.git
```
2. Navigate to the repository folder:
```bash
cd AI-content-detection-with-GPT-language-models-8B
```
3. Set up a virtual environment:
```bash
python -m venv env
```
4. Activate the virtual environment:
```bash
source venv/bin/activate  # Windows ---> use .\env\Scripts\activate
```
5. Install dependencies:
```bash
pip install -r requirements.txt
```
6. Launch the jupyter notebook with main snippets, graphs and explanations:
```bash
jupyter lab
```

### Steps to run the entire code and replicate end-to-end the research
Following the next step-by-step procedure, you will be able to collect, transform, and analyze the data again. Plus, you will implement the classification tasks, and analyze/visualize the results (just take into account it could take a couple of hours to run entirely).

1. Scrape Paul Graham’s blog posts:
```bash
python src/scraping/web_scraping.py
```
2. Enrich human-generated dataset with topic labels codes through GPT-based topic analysis:
```bash
python src/topic_labeler.py
```
3. Generate AI texts aligned with the human content for topics:
```bash
python src/content_generation/ai_content_generator.py
```
4. Text processing of both datasets before linguistic analysis:
```bash
python src/preprocessing/normalize_tokenize_lemmatize.py
```
5. Linguistic analysis to evaluate topics and lexical alignment of the datasets
```bash
python src/linguistic_analysis/topics_analysis_visualization.py
```
```bash
python src/linguistic_analysis/lexicon_analysis_visualization.py
```
6. Unify the two datasets in balanced and homogeneous way:
```bash
python src/preprocessing/ai_human_data_mixer.py
```
7. Perform zero-shot classification:
```bash
python src/classification/zeroshot_classifier.py
```
8. Perform few-shot classification:
```bash
python src/classification/fewshot_classifier.py
```
9. Analyze classification results and performances:
```bash
python src/classification/performance_analysis_visualization.py
```

## License
This project is licensed under the MIT License.

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

1. The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
2. THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

For more details, see the full license text in the [LICENSE](LICENSE) file.
```
