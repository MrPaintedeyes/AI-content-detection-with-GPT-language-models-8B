# Is this AI-Generated? A Comparative Analysis of Zero-Shot and Few-Shot Text Classification with GPT-based Language Models

## Abstract
The rise of AI-generated content presents a growing need for low-resource, scalable detection methods. This project explores the performance of GPT-based language models, leveraging zero-shot and few-shot learning for binary text classification. By creating a dataset of human-written blog posts by Paul Graham and generating AI texts through Google’s gemini-1.5-fast, we aim to establish a reliable baseline for distinguishing between human and AI-generated content. Our findings highlight the advantages of few-shot learning over zero-shot learning, offering a cost-effective and accessible approach to AI text detection.

## Research Questions
1. How do zero-shot and few-shot learning approaches compare in distinguishing human and AI-generated text?
2. What are the limitations of prompt-based classification without fine-tuning?
3. How effective are thematic and linguistic alignment techniques in dataset preparation for improving classification performance?

## Dataset
- **Human-generated content**: Blog posts by Paul Graham, collected via web scraping.
- **AI-generated content**: Blog posts generated by Google’s gemini-1.5-fast, aligned to human content in themes, style, and tone.
- **Mixed dataset**: Combines human and AI content with labels (`Human` or `AI`) for classification tasks.
  
### Dataset Details:
- Format: CSV
- Approximate Size: 100+ blog posts per dataset
- Enrichment Steps:
  - Thematic coding for categorization
  - Lexicon analysis for linguistic alignment
  - Random sampling for balanced data generation

## A Tentative List of Milestones for the Project
1. **Week 1**: Set up repository structure and scrape Paul Graham’s blog posts.
2. **Week 2**: Perform text preprocessing (normalization, tokenization, lemmatization) and thematic coding of human-generated content.
3. **Week 3**: Generate AI content using gemini-1.5-fast and conduct lexicon analysis to ensure alignment.
4. **Week 4**: Develop and test classification scripts for zero-shot and few-shot learning.
5. **Week 5**: Evaluate classification performance and visualize results using plots.
6. **Week 6**: Finalize the project, prepare documentation, and submit the report.

### Responsibilities:
- **Data Preparation and Analysis**: [Your Name]
- **AI Content Generation and Alignment**: [Your Name]
- **Classification and Evaluation**: [Your Name]
- **Documentation and Visualizations**: [Your Name]

## Documentation
### Repository Structure
- **`data/`**: Contains raw and processed datasets.
- **`notebooks/`**: Jupyter notebooks for data exploration, preprocessing, and analysis.
- **`src/`**: Source code for scraping, preprocessing, content generation, classification, and visualization.
- **`reports/`**: Figures and summary reports of findings.
- **`tests/`**: Unit tests for verifying the correctness of modules.

### How to Reproduce Results
1. Clone this repository:
   ```bash
   git clone https://github.com/your_username/ai-text-detection.git
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
