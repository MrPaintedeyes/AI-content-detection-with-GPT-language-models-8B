{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "a35eeb9f-df70-4ab1-a243-2d2025888eb0",
      "cell_type": "markdown",
      "source": "# Is this AI-Generated? \n## A Comparative Analysis of Zero-Shot and Few-Shot Text Classification with GPT-based Language Models\n\n**Abstract:** \nThe rise of AI-generated content presents a growing need for scalable detection methods. This notebook explores the performance of GPT-based models using zero-shot and few-shot learning for classifying human- and AI-generated content. It provides a step-by-step demonstration of the dataset preparation, analysis, classification, and performance evaluation.\n\n## Environment Setup\nTo ensure smooth execution, ensure all dependencies are installed.",
      "metadata": {}
    },
    {
      "id": "4e119303-4bc4-4a0a-8212-9965596234cb",
      "cell_type": "markdown",
      "source": "## Web scraping module\nThis is the module I used to retrieve Paul Graham's blog posts in a way tailored to its website.\nYou could run the code if you want, just changing the cell setting from \"raw\" to \"code\" (the same counts for all other modules in "raw mode" next). However, you can run the entire modules following the README.md instructions. Just be aware it could take up to a couple of hours.",
      "metadata": {}
    },
    {
      "id": "a7ad06ac-cf40-4c8c-b895-0b10f5be20e5",
      "cell_type": "raw",
      "source": "import urllib\nfrom urllib.request import urlopen\nfrom urllib.error import URLError\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nimport pandas as pd\n\n# fetching an high-level webpage within Paul Graham's website, where there is a complete list of all his blog posts (\"content directory\")\n\ntry:\n    html_content_directory = urlopen(\"https://www.paulgraham.com/articles.html\")\nexcept urllib.request.URLError as e:\n    print(\"there was an URLError, try with a different URL\")\n\n# parsing the webpage requested\n\nsoup_content_directory = BeautifulSoup(html_content_directory, \"lxml\")\n\n# visualizing the parsed webpage to understand its structure and where the blog posts' links are included (uncomment if needed)\n# print(soup_content_directory.prettify())\n\n# initializing an empty list of articles' links\n\narticles = []\n\n# identifying all the blog post links \n# with structure \"https://www.paulgraham.com/\" + the string inside the attribute href of each filtered element <a>\n\nfor a in soup_content_directory.find_all('a'):\n    article_link = \"https://www.paulgraham.com/\" + a['href']\n\n    # skipping all the links not related to the blog posts:\n    # link to the home page, to the rss feed or that don't end with \"html\"\n\n    if \"index.html\" in article_link or \"rss.html\" in article_link or not article_link.endswith(\"html\"):\n        continue\n\n# appending each correctly identified article's link to the list \"articles\"\n        \n    articles.append(article_link)\n\n    #printing the retrieved link to verify that each iteration is working (uncomment if needed)\n    # print(article_link)\n\n# printing all the retrieved links to verify that retrieval worked (uncomment if needed)\n# print(articles)\n\n# looking at the structure of a typical blog post page, by fetching it, parsing it and visualizing it \n# to understand where the blog post text is included within the html structure (uncomment if needed)\n# test_link = articles[0]\n# html_test = urlopen(test_link)\n# soup_test = BeautifulSoup(html_test, \"lxml\")\n# print(soup_test.prettify())\n\nhuman_blog_posts_data = []  # initializing a list of (URL, content body) pairs to convert into a csv later\n\n# actually scraping blog posts' content\n# implementing the fetching, parsing and collection of all blog post texts related to the stored links (iterating over links)\n# knowing that the main text is within the element <body>\n\nfor link in articles:\n    try:\n        html_blog_post = urlopen(link)\n        soup_blog_post = BeautifulSoup(html_blog_post, \"lxml\")\n        blog_post_body = soup_blog_post.body # the content of body is the blog post text mixed with other html elements we don't need\n        blog_post_text = blog_post_body.get_text() # so we apply bs4 method \"get_text()\" to retrieve all the textual content of the body\n        human_blog_posts_data.append((link, blog_post_text)) # adding to the list human_blog_posts_data the pair (URL + text)\n    except urllib.request.URLError as e:\n        print(\"there was an URLError, try with a different URL\")\n\n# creating the dataset where to store data in the target directory \"data\"\nhuman_blog_posts_path = os.path.join(\"data\", \"human_blog_posts.csv\")\n\nos.makedirs(os.path.dirname(human_blog_posts_path), exist_ok=True)\n\n# saving human_blog_posts_data the data into a csv file with columns: URL, blog_post, author, AI_or_human\n\nwith open(human_blog_posts_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n    writer = csv.writer(file) # initializing writer\n    writer.writerow([\"URL\", \"blog_post\", \"author\", \"AI_or_human\"]) # writing the columns names (first row)\n    for URL, blog_post in human_blog_posts_data: # writing the other rows, including each pair (URL + blog post) + author name + \"human\" label in a single row\n        writer.writerow([URL, blog_post , \"Paul Graham\", \"human\"])\n\n# checking that the process went through (uncomment if needed)\nprint(\"human_blog_posts_data have been saved to a csv file, including also the variables 'author' and 'AI_or_human'\")\n\n# exploring the dataset with pandas, in particular searching for empty values\ndf_human = pd.read_csv(human_blog_posts_path)\n\nhuman_blog_posts_path = os.path.join(\"data\", \"human_blog_posts.csv\")\n\ndf_human = pd.read_csv(human_blog_posts_path)\n\n# exploring human_blog_posts.csv structure, entries counts, values' types and emppty values\nprint(df_human.describe)\nprint(df_human.info)\nprint(df_human.isna().sum())\n\n# drop rows with empty values in them\ndf_human.dropna(inplace=True)\n\n# take a look at potential changes in count\nprint(df_human.tail)\n\n# and save the df back to csv without the super annoying index\ndf_human.to_csv(human_blog_posts_path, index=False)",
      "metadata": {}
    },
    {
      "id": "90326fa6-7552-40b8-a7f4-ede70dbb4266",
      "cell_type": "markdown",
      "source": "the previous module stores texts into human_blog_posts.csv, further enriched in the following steps.",
      "metadata": {}
    },
    {
      "id": "5569da91-6495-4278-98ad-bd1b9ff93150",
      "cell_type": "markdown",
      "source": "## Topic labeling the blog posts \nBy iterating over human_blog_posts.csv, column \"blog_post\", with gemini-1.5-fast we were able to register topic labels to the scraped blog posts.  In the snippet, you can take a look also to the used prompt that propose a list of lables to use and provide instructions to the model. The following module implement this process:",
      "metadata": {}
    },
    {
      "id": "8ab407f2-3846-47b6-8175-fa19c8534fae",
      "cell_type": "raw",
      "source": "import os\nimport pandas as pd\nimport google.generativeai as genai\nimport time\n\n# configuring API key, initializing selected generative ai model\ngenai.configure(api_key=\"AIzaSyCkTDJidvCdoo3vSinFNLYutZV43pm_fBI\")\n\nmodel = genai.GenerativeModel(\n    model_name=\"gemini-1.5-flash\",\n)\n\n# indicating the csv dataset containing the blog posts to classify, and transforming it into a dataframe\nhuman_blog_posts_path = os.path.join(\"data\", \"human_blog_posts.csv\")\n\ndf_human_blog_posts = pd.read_csv(human_blog_posts_path)\n\n# initializing an empty list where to store all the thematic labels for the moment\ntopic_categories = []\n\n# iterating over all the values (the blog post texts) within the column \"blog_post\" of the dataframe df_human_blog_posts\n# send the blog post text to the model in order to classify it with respect to its main theme\n# then retrieve the model's response (the thematic label)\nfor post in df_human_blog_posts['blog_post']:\n\n    # configuring a prompt that changes dynamically incorporating the blog post text while iterating over the dataframe\n    prompt = f\"\"\"ROLE: You are a classifier aimed to label written blog posts by Paul Graham with respect to their MAIN TOPIC/PURPOSE. \n    Thus, your goal is to identify the overarching topic/purpose of blog post text, focusing on its primary message. \n    \n    LABELS YOU CAN USE:\n    - life advice\n    - career advice\n    - technological and societal insights\n    - programming advice\n    - startup advice\n    - social commentary\n    - educational content\n    - historical analysis\n    - philosophical reflection\n    - personal experience report\n    - writing advice\n    - other\n    \n    INSTRUCTIONS:\n    1. Provide a structured and consistent output. OUTPUT ONLY THE LABEL REPRESENTING THE BLOG POST’S TOPIC/PURPOSE, \n    2. DO NOT ATTACH ANYTHING TO THE LABEL IN YOUR ANSWER (NO EXPLANATIONS, NO REFLECTIONS, NO INTRODUCTIONS). \n    3. Use only the provided list of labels. Use \"other\" when all the other labels do not apply to the text analyzed.\n\n    \n    RECAP: You are a blog post classifier aimed to output the label best representing the blog post topic/purpose. \n    You just have to output the label describing the topic/purpose in plain text with nothing else attached.\n    You must use only the labels in the provided list. \n    Remember to stick to this output format answering with just the label and nothing else attached.\n    \n    Now, provide the classification for the following blog post: {post}\"\"\"\n    \n    # sending the prompt to the model, while configuring temperature and max_output_tokens\n    response = model.generate_content(prompt,\n    generation_config = genai.GenerationConfig(\n        max_output_tokens=8000,\n        temperature=0,\n    ))\n    \n    model_output = response.text # retrieve the model response in plain text\n\n    topic_categories.append(model_output) # adding the thematic label to the list\n\n    print(f\"topic category: {model_output} was added to the list\") # checking the progression of the labeling task\n\n    time.sleep(15) # adding some seconds of delay to avoid the overload of API requests\n\n# adding a new column to the dataframe with the values from the list \"topic categories\"\ndf_human_blog_posts['topic_category'] = topic_categories\n\n# to verify the overall process went through\nprint(df_human_blog_posts.head())\n\n# save the updated dataset back to the csv file\ndf_human_blog_posts.to_csv(human_blog_posts_path, encoding=\"utf-8\", index=False)\n\n# giving a feedback on the completion of the process\nprint(\"human_blog_posts.csv was enriched with topics' labels and saved back to a csv file\")",
      "metadata": {}
    },
    {
      "id": "452c7743-673f-4ef0-8a1b-1a979f6a52ae",
      "cell_type": "markdown",
      "source": "to see how the resulting enriched dataset looks, run the following:",
      "metadata": {}
    },
    {
      "id": "8737081d-1588-4f97-bd0c-9505c4fa8d71",
      "cell_type": "code",
      "source": "print(df_human)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e87e5ecc-0ab6-405c-867c-fa640b87c276",
      "cell_type": "markdown",
      "source": "## Generating AI-texts mimicking Paul Graham with gemini-1.5-fast and few-shot learning\nThe following is one of the most exciting and personally my favourite part of the project. It's the moment of generating some content! \n\nThe next snippet implement this process leveraging Google's small generative model prompted with instructions about the output format and with random labeled examples per each topic incorporated dinamically into the system prompt.\n\nWe structured the generation rounds iterating over each topic category and setting a number of rounds equal to the number of contents needed to cover that topic equally to the original human dataset.\n\n(Run at own risk, it could take some time)",
      "metadata": {}
    },
    {
      "id": "e063f95c-16ba-43c9-9eb7-5decc0ea2f0c",
      "cell_type": "raw",
      "source": "import pandas as pd\nimport google.generativeai as genai\nimport os\nimport random\nimport csv\nimport time\n\n# indicating the path to the blog post examples and to the dataset with ai-generated texts\n# if ai_generated_blog_posts.csv doesn't exist yet, it is created with columns \"blog_post\", \"author\", \"AI_or_human\", and \"topic_category\"\n\nblog_posts_examples_path = os.path.join(\"data\", \"human_blog_posts.csv\")\n\nai_blog_posts_path = os.path.join(\"data\", \"ai_generated_blog_posts.csv\")\n\nif not os.path.exists(ai_blog_posts_path):\n    with open(ai_blog_posts_path, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n                writer = csv.writer(file)\n                writer.writerow([\"blog_post\", \"author\", \"AI_or_human\", \"topic_category\"])\n\n# the dataset with human blog posts is transformed into a dataframe, to randomly retrieve blog post examples to embed in the prompt\n\ndf_human = pd.read_csv(blog_posts_examples_path)\n\n# after we set the number of synthetic blog posts to craft (100)\nnumber_synthetic_contents = 100\n\n# we can compute the percentage of each topic in the original dataset \n# to dinamically adjust the amount of synthetic content to produce for each category\n# and produce a dataset of ai-generated blog posts that is balanced and representative with respect to human_blog_posts.csv\n\ncategories = set(df_human[\"topic_category\"])\n\n# configuring API key and selecting the generative model\ngenai.configure(api_key=\"AIzaSyCkTDJidvCdoo3vSinFNLYutZV43pm_fBI\")\n\nmodel = genai.GenerativeModel(\n  model_name=\"gemini-1.5-flash\",\n)\n\n# dinamically changing the prompt with different blog posts examples randomly extracted within each topic category\n# this should decrease the influence of specific examples on the quality of the generation\n\nfor category in categories:\n\n    topic_percentage = len(df_human[df_human[\"topic_category\"] == category]) / len(df_human)\n    \n    # calculate the number of synthetic contents to generate for each topic category\n    number_of_contents_per_category = round(topic_percentage * number_synthetic_contents)\n    \n    print(f\"generating: {number_of_contents_per_category} entries for {category}\") # getting feedback on the process' start\n\n    # iterating over each predicted generation round\n    for i in range(1, (number_of_contents_per_category+1)):\n\n        content_list = list(df_human[df_human[\"topic_category\"] == category][\"blog_post\"])\n\n        sample_content_examples = []\n\n        # 3 human blog posts are extracted from the human_blog_posts.csv dataset and listed in sample_content_examples,\n        # but if the number of examples within a topic category is less than 3,\n        # all the examples in that category are listed in sample_content_examples\n        if len(content_list) >= 3:\n            sample_content_examples = random.sample(content_list, 3) # randomly lists 3 blog posts from the human blog posts\n        else:\n            sample_content_examples = content_list\n\n        # setting a prompt that changes dinamically while different content examples are sampled at each generation round\n        prompt = f\"Write a blog post as Paul Graham would do. Look at the following 3 examples within the topic category {category} as a reference for tone of voice, style, structure, lenght and main topic/purpose. Do not plagarize the examples repeating their wording and content, but only look at them as models. Examples: {sample_content_examples}. You should just output the new blog post in plain text, with nothing else attached. Return the blog post as plain text only, and NO JSON, NO extra formatting or NO explanations are needed. Just output the blog post in plain text. Remember to adhere strictly to the output format (blog post only in plain text). Remember to look at the examples as models, but do not plagarize them.\"\n\n        # sending the prompt to the model, while configuring also its temperature to 1 (enhancing creativity and variety) and the max number of tokens\n        response = model.generate_content(\n                prompt, \n                generation_config = genai.GenerationConfig(\n                      max_output_tokens=100000,\n                      temperature=1,\n                )\n        )\n\n        # checking the progression of generation for the current topic category\n        print(f\"writing {i}° piece of content for the category {category}\")\n        \n        # retrieving the model's response as plain text\n        blog_post_text = response.text\n\n        # adding and saving progressively the output in the target dataset \"ai_generated_blog_posts.csv\", respecting columns order\n        with open(ai_blog_posts_path, \"a\", newline=\"\", encoding=\"utf-8\") as file:\n            writer = csv.writer(file)\n            writer.writerow([blog_post_text, \"gemini-1.5-fast\", \"AI\", category])\n        \n        time.sleep(5) # avoid to overcome API requests' limits by adding some seconds of delay between model's calls\n    \n    \n#checking on the completion of the task and on the dataframe 5 first rows\nprint(\"content generation completed, blog posts saved into ai_generated_blog_posts.csv\")\n\n# exploring ai_generated_blog_posts.csv structure, entries' counts, values' types\ndf_ai = pd.read_csv(ai_blog_posts_path)\n\nprint(df_ai.info)\n\n# and saving df_ai back to csv as ai_generated_blog_posts.csv without the super annoying index\ndf_ai.to_csv(ai_blog_posts_path, index=False)",
      "metadata": {}
    },
    {
      "id": "10400156-196f-47e8-beba-e20c3c65c74f",
      "cell_type": "markdown",
      "source": "take a look at the final synthetic dataset running the following cell:",
      "metadata": {}
    },
    {
      "id": "2453d820-4a18-4354-bb5d-bfabf3555f73",
      "cell_type": "code",
      "source": "print(df_ai)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "01026b0a-36ec-4a76-a17c-c3372e4a1611",
      "cell_type": "markdown",
      "source": "## Topics distribution analysis and visualization\nTo double-check that the previous module worked in generating a dataset of synthetic content while preserving topics proportions, we implemented the following script to count topics occurences and visualize their distribution for both datasets. (You can run the following safely without losing your entire day)",
      "metadata": {}
    },
    {
      "id": "870fa916-0d85-40f5-b370-3c0e2c73afd1",
      "cell_type": "code",
      "source": "import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n\n# loading the two datasets as dataframes to handle further data analysis and visualization\nhuman_blog_posts_path = os.path.join(\"data\", \"human_blog_posts.csv\")\nai_blog_posts_path = os.path.join(\"data\", \"ai_generated_blog_posts.csv\")\n\ndf_human = pd.read_csv(human_blog_posts_path)\ndf_ai = pd.read_csv(ai_blog_posts_path)\n\n# iterating over topic category within the dataframes, filtering all the entries with the corresponding topic value\nhuman_topic_categories = set(df_human['topic_category'])\nai_topic_categories = set(df_ai['topic_category'])\n\n# then counting the number of entries presenting that topic value\n# computing the percentage of entries with that topic\n# and filling data into a dict, for both dataframes\nhuman_topic_percentages = {}\nfor topic in human_topic_categories:\n    topic_count = len(df_human[df_human['topic_category'] == topic])\n    topic_percentage = topic_count / len(df_human) * 100\n    human_topic_percentages[topic] = topic_percentage\n\nprint(human_topic_percentages) # checking if it went through and the structure of the dict\n\nai_topic_percentages = {}\nfor topic in ai_topic_categories:\n    topic_count = len(df_ai[df_ai['topic_category'] == topic])\n    topic_percentage = topic_count / len(df_ai) * 100\n    ai_topic_percentages[topic] = topic_percentage\n\nprint(ai_topic_percentages) # checking if it went through and the structure of the dict\n\n# plotting and saving human topics' percentages and ai topics' percentages in two separate figures\nplt.figure(figsize=(10, 6))\nplt.bar(human_topic_percentages.keys(), human_topic_percentages.values(), color='blue')\nplt.title(\"topics' prcentages in the human dataset\")\nplt.xlabel(\"topics\")\nplt.ylabel(\"percentages\")\nplt.xticks(rotation = 45, fontsize = 10)\nplt.tight_layout()\n\n# saving the figures in the plot folder, creating it if it doesn't exist yet\nos.makedirs(\"plot\", exist_ok=True)\n\nhuman_plot_path = os.path.join(\"plot\", \"human_topics_percentages.png\")\nplt.savefig(human_plot_path)\nplt.show() # checking by showing\n\n# repeating the same for ai_generated_blog_posts.csv\n\nplt.figure(figsize=(10, 6))\nplt.bar(ai_topic_percentages.keys(), ai_topic_percentages.values(), color='orange')\nplt.title(\"topics' percentages in the AI dataset\")\nplt.xlabel(\"topics\")\nplt.ylabel(\"percentages\")\nplt.xticks(rotation = 45, fontsize = 10)\nplt.tight_layout()\n\nai_plot_path = os.path.join(\"plot\", \"ai_topics_percentages.png\")\nplt.savefig(ai_plot_path)\nplt.show()\n\n# we can visually confirmed that the topics' percentages are the same within both datasets, \n# and the ai-based generation pipeline respected the human blog posts' quotas provided",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b1dbeaf5-8fe5-4918-97d9-d9d26abd87b2",
      "cell_type": "markdown",
      "source": "## Linguistic pre-processing and lexical analysis\nTo further validate the similarity between the two datasets, we implemented a nlp pipeline to prepare texts and a lexical analysis to analyze and visualize lemmas distribution in the two corpora and to compute the overlap between the most frequent lemmas. You can safely run both modules without wasting too much time. \n\nThe first run the nlp pipeline cleaning, normalizing, tokenizing and lemmatizing texts of the two datasets. \n\nThe second run the lexical analysis and the visualization.",
      "metadata": {}
    },
    {
      "id": "65057da6-b9e0-471b-bc75-cbe965d0feb9",
      "cell_type": "code",
      "source": "import pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport os\nimport string\n\n# tokenization, removing stop words, lemmatization, word count (propedeutic for data analysis and visualization)\n\n# downloading necessary NLTK data\nnltk.download(\"stopwords\")\n\n# file paths to the datasets with human- and ai-generated blog posts\nhuman_blog_posts_path = os.path.join(\"data\", \"human_blog_posts.csv\")\nai_generated_blog_posts_path = os.path.join(\"data\", \"ai_generated_blog_posts.csv\")\n\n# loading the datasets as dataframes\ndf_human = pd.read_csv(human_blog_posts_path)\ndf_ai = pd.read_csv(ai_generated_blog_posts_path)\n\n# normalizing blog posts' text, and adding the normalized texts as the values of a new column \"normalized_blog_post\", in both datasets\ndf_human['normalized_blog_post'] = df_human['blog_post'].str.lower()\ndf_ai['normalized_blog_post'] = df_ai['blog_post'].str.lower()\n\nprint(\"performed normalization on both datasets and saved outputs in a new column 'normalized_blog_post'\") # checking on the completion of the process\n\n# tokenizing normalized blog posts' text, and adding the tokenized texts as the values of a new column \"tokenized_blog_post\", in both datasets\ntokenized_human_posts = []\nfor human_post in list(df_human['blog_post']):\n    tokenized_human_post = nltk.tokenize.word_tokenize(human_post)\n    tokenized_human_posts.append(tokenized_human_post)\ndf_human['tokenized_blog_post'] = tokenized_human_posts\n\ntokenized_ai_posts = []\nfor ai_post in list(df_ai['blog_post']):\n    tokenized_ai_post = nltk.tokenize.word_tokenize(ai_post)\n    tokenized_ai_posts.append(tokenized_ai_post)\ndf_ai['tokenized_blog_post'] = tokenized_ai_posts\n\nprint(\"Performed tokenization on both datasets and saved outputs in a new column 'tokenized_blog_post'\") # checking\n\n\n# cleaning of blog post texts, namely stop word and punctuation remotion, in both datasets\nstop_words = set(stopwords.words(\"english\")) # set of stop words\npunctuation = string.punctuation # list of punctuation characters\n\ncleaned_human_posts = []\nfor tokenized_human_post in list(df_human['tokenized_blog_post']):\n    filtered_tokens = [token for token in tokenized_human_post if token not in stop_words and token not in punctuation]\n    cleaned_human_posts.append(filtered_tokens)\ndf_human['cleaned_blog_post'] = cleaned_human_posts\n\ncleaned_ai_posts = []\nfor tokenized_ai_post in list(df_ai['tokenized_blog_post']):\n    filtered_tokens = [token for token in tokenized_ai_post if token not in stop_words and token not in punctuation]\n    cleaned_ai_posts.append(filtered_tokens)\ndf_ai['cleaned_blog_post'] = cleaned_ai_posts\n\nprint(\"removed stop words from tokenized blog posts, and saved the outputs in a separate column 'blog_post_no_stop_words\") # checking\n\n# lemmatization of cleaned blog posts in both datasets\nlemmatized_human_posts = []\n\nfor cleaned_human_post in df_human['cleaned_blog_post']:\n    lemmatized_human_post = []\n    for token in cleaned_human_post:\n        lemma = nltk.WordNetLemmatizer().lemmatize(token)\n        lemmatized_human_post.append(lemma)\n    lemmatized_human_posts.append(lemmatized_human_post)  # Append here, inside the outer loop\n\ndf_human['lemmatized_blog_post'] = lemmatized_human_posts\n\nlemmatized_ai_posts = []\n\nfor cleaned_ai_post in df_ai['cleaned_blog_post']:\n    lemmatized_ai_post = []\n    for token in cleaned_ai_post:\n        lemma = nltk.WordNetLemmatizer().lemmatize(token)\n        lemmatized_ai_post.append(lemma)\n    lemmatized_ai_posts.append(lemmatized_ai_post)\n\ndf_ai['lemmatized_blog_post'] = lemmatized_ai_posts\n\nprint(\"performed lemmatization on both datasets, and saved the output in a new column 'blog_post_lemmatized'\") # checking\n\n# saving processed datasets to json files to preserve lists of lemmas (df series \"lemmatized_blog_post\")\n\nhuman_blog_posts_json_path = os.path.join(\"data\", \"human_blog_posts_processed.json\")\nai_blog_posts_json_path = os.path.join(\"data\", \"ai_generated_blog_posts_processed.json\")\n\ndf_human.to_json(human_blog_posts_json_path, orient = 'records')\ndf_ai.to_json(ai_blog_posts_json_path, orient = 'records')\n\nprint(\"both datasets were saved, and all the processing (normalization, tokenization, lemmatization) went through\") # checking",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2250247c-6c4c-42c6-9ee5-fb6ca38a9d4b",
      "cell_type": "code",
      "source": "import os\nimport pandas as pd\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Paths to the JSON files\nhuman_blog_posts_path = os.path.join(\"data\", \"human_blog_posts_processed.json\")\nai_generated_blog_posts_path = os.path.join(\"data\", \"ai_generated_blog_posts_processed.json\")\n\n# loading the JSON datasets into pandas DataFrames\ndf_human = pd.read_json(human_blog_posts_path, orient=\"records\")\ndf_ai = pd.read_json(ai_generated_blog_posts_path, orient=\"records\")\n\n# initializing lists to collect lemmas for both datasets\nhuman_corpus_lemmas = []\nai_corpus_lemmas = []\n\n# collecting lemmas from the lemmatized_blog_post column as an unified list of lists (all 'lemmatized_blog_post', which are lists of lemmas)\nfor lemmatized_blog_post in df_human['lemmatized_blog_post']:\n    human_corpus_lemmas.extend(lemmatized_blog_post)\n\nfor lemmatized_blog_post in df_ai['lemmatized_blog_post']:\n    ai_corpus_lemmas.extend(lemmatized_blog_post)\n\n# counting the occurrences of each lemma in the unified lists for both corpora\nhuman_corpus_lemmas_count = Counter(human_corpus_lemmas)\nai_corpus_lemmas_count = Counter(ai_corpus_lemmas)\n\n# computing the 100 most common lemmas in each corpus\nmost_common_human_corpus_lemmas = human_corpus_lemmas_count.most_common(100)\nmost_common_ai_corpus_lemmas = ai_corpus_lemmas_count.most_common(100)\n\n# extracting the sets of lemmas from the most common lists...\nhuman_lemmas_set = set([lemma for lemma, frequency in most_common_human_corpus_lemmas])\nai_lemmas_set = set([lemma for lemma, frequency in most_common_ai_corpus_lemmas])\n\n# ...to find the intersection of the two sets and calculating the percentage of lemmas' overlap\ncommon_lemmas = human_lemmas_set.intersection(ai_lemmas_set)\nlemmas_overlap_percentage = (len(common_lemmas) / 100) * 100\n\n# checking the number and percentage of lemmas in common, and printing the lemmas in common\nprint(f\"Number of lemmas in common: {len(common_lemmas)}\")\nprint(f\"Percentage of lemmas' overlap: {lemmas_overlap_percentage}%\")\nprint(f\"The most common and shared lemmas are the following: {common_lemmas}\")\n\n# creating the \"plot\" folder if it doesn't exist yet, and declaring the saving path for graphs\nos.makedirs(\"plot\", exist_ok=True)\n\nai_lemmas_plot_path = os.path.join(\"plot\", \"top_100_ai_corpus_lemmas.png\")\nhuman_lemmas_plot_path = os.path.join(\"plot\", \"top_100_human_corpus_lemmas.png\")\n\n# separating most common lemmas from their counts by iterating over most_common_[human/AI]_corpus_lemmas\n# to retrieve the lemma, we just need to select the first element (idx 0) of each pair\n# while to retrieve the frequency, we just need to select the second element (idx 1) of each pair\n\nlemmas_human = [pair[0] for pair in most_common_human_corpus_lemmas] # lemmas\nfrequencies_human = [pair[1] for pair in most_common_human_corpus_lemmas] # frequencies\n\nlemmas_ai = [pair[0] for pair in most_common_ai_corpus_lemmas] # lemmas\nfrequencies_ai = [pair[1] for pair in most_common_ai_corpus_lemmas] # frequencies\n\n# plotting most common lemmas for the human and AI corpora to visualize lexical similarity\nplt.figure(figsize=(12, 7))\nplt.bar(range(len(lemmas_human)), frequencies_human, color='green')\nplt.xticks(range(len(lemmas_human)), lemmas_human, rotation=90, fontsize=8)\nplt.title(\"top 100 most common lemmas in human corpus\", fontsize=14)\nplt.xlabel(\"lemmas\", fontsize=12)\nplt.ylabel(\"frequency\", fontsize=12)\nplt.savefig(human_lemmas_plot_path)\nplt.show()\n\n\nplt.figure(figsize=(13, 8))\nplt.bar(range(len(lemmas_ai)), frequencies_ai, color='red')\nplt.xticks(range(len(lemmas_ai)), lemmas_ai, rotation=90, fontsize=8)\nplt.title(\"top 100 most common lemmas in AI corpus\", fontsize=14)\nplt.xlabel(\"lemmas\", fontsize=12)\nplt.ylabel(\"frequency\", fontsize=12)\nplt.savefig(ai_lemmas_plot_path)\nplt.show()\n\n# Observations about overlap in lemmas:\n# topic similarity is evident in certain keywords like startup, founder, company, idea, people, user, work, language, writing etc.\n# as these are completely aligned with Paul Graham's focus as a writer, philosophy major, tecg startup founder/investor and computer scientist.\n# we should consider also the relevance of the personal pronoun \"I\", since the majority of blog posts from Paul Graham are in first person",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a2c0211f-f438-4c70-830d-ec61fb568a2a",
      "cell_type": "markdown",
      "source": "## Mixing the datasets before the classification tasks\nWe are almost there: this is the last preliminary step before implementing the actual research experiment. The next module is in fact needed to prepare the dataset on which we will test the model. \n\nThe dataset is created by merging 99 blog posts from the human dataset with all the AI-generated text (99), creating a balance dataset both for topics and for the category \"AI_or_human\", annotating the ground truth representing the contents.",
      "metadata": {}
    },
    {
      "id": "1c7b91d0-6627-4384-9a36-cf34b317f438",
      "cell_type": "code",
      "source": "import pandas as pd\nimport os\nimport random\nimport csv\n\n# loading human- and ai-generated datasets into dataframes, plus declaring the mixed dataset path...\nhuman_blog_posts_path = os.path.join(\"data\", \"human_blog_posts.csv\")\nai_blog_posts_path = os.path.join(\"data\", \"ai_generated_blog_posts.csv\")\nai_human_blog_posts_path = os.path.join(\"data\", \"ai_human_blog_posts.csv\")\n\ndf_human = pd.read_csv(human_blog_posts_path)\ndf_ai = pd.read_csv(ai_blog_posts_path)\n\n# ...that it's created if it doesn't exist yet\nif not os.path.exists(ai_human_blog_posts_path):\n    with open(ai_human_blog_posts_path, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n                writer = csv.writer(file)\n                writer.writerow([\"blog_post\", \"topic_category\", \"AI_or_human\"]) # writing the header\n\n# to compile the mixed dataset in a way it is balances, we unify all the 100 ai-generated blog posts to 100 random human blog posts,\n# while preserving topics' percentages\n# so we iterate over each topic category, compute the number of needed human blog posts for that category (= to the number of ai-blog-posts), \n# and randomly sample that number of human blog posts from df_human[df_human['topic_category]==category][blog_post]\n\n# Initialize lists to store mixed data as df series (df['series'] = list)\nblog_posts = []\ntopic_categories = []\nauthors = []\nai_or_human = []\n\n# Iterate over each topic category\ncategories = set(df_ai[\"topic_category\"])\n\nfor category in categories:\n    # Compute the number of AI blog posts for the current category\n    num_ai_blog_posts_per_category = len(df_ai[df_ai[\"topic_category\"] == category])\n    \n    print(f\"We should add {num_ai_blog_posts_per_category} human blog posts for the category {category}\")  # Progression check\n\n    # Sample human-generated content for the current category\n    human_contents = df_human[df_human[\"topic_category\"] == category]['blog_post']\n    sampled_human_contents = random.sample(list(human_contents), num_ai_blog_posts_per_category)\n    \n    # add sampled human blog posts to lists, along with the labels for \"topic category\" and \"AI_or_human\"\n    blog_posts.extend(sampled_human_contents)\n    topic_categories.extend([category for element in range(num_ai_blog_posts_per_category)])\n    ai_or_human.extend([\"human\" for element in range(num_ai_blog_posts_per_category)])\n\n# adding all ai-generated blog posts to the previous lists, before populating the corresponding dataframe series of df_mixed\nmixed_blog_posts = blog_posts + list(df_ai['blog_post'])\nmixed_topic_categories = topic_categories + list(df_ai['topic_category'])\nmixed_ai_or_human = ai_or_human + list(df_ai[\"AI_or_human\"])\n\n# populating the dataframe df_mixed, declaring as many dataframe series as the number of mixed lists we created before\n# and saving the new dataframe as a csv file in the target path\n\ndf_mixed = pd.DataFrame({\n    'blog_post': mixed_blog_posts,\n    'topic_category': mixed_topic_categories,\n    'AI_or_human': mixed_ai_or_human})\n\ndf_mixed.to_csv(ai_human_blog_posts_path, index=False, encoding=\"utf-8\")\n\nprint(\"ai_human_blog_posts.csv was successfully created.\") # checking if it went through",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "87ffc6ea-657a-4c40-993a-02744d63e12a",
      "cell_type": "markdown",
      "source": "to examine how the mixed dataset looks like in detail, run the next cell:",
      "metadata": {}
    },
    {
      "id": "7753c78d-2d8e-4363-ba7c-06a5a5eebeb9",
      "cell_type": "code",
      "source": "print(df_mixed)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6424b714-6efe-4661-858d-332308ce2a2d",
      "cell_type": "markdown",
      "source": "## Classification tasks with zero-shot and few-shot learning\nNow it's time to start what everyone was waiting for: the actual research experiment, implementing zero-shot and few-shot AI text detection with gemini-1.5-fast. \n\nWe structured the classification for both approaches by iterating model's calls over the blog_post column of the mixed dataset. Zero-shot learning (1st snippet) doesn't incorporate dinamically random labeled examples of AI-generated and human texts in the system prompt, while few-shot learning (2nd snippet) stores dinamically 6 examples (3 for AI texts, 3 for human texts).\n\nThe following modules implement the classification task for both approaches, and running it could take some time (so take it easy).",
      "metadata": {}
    },
    {
      "id": "526f6942-ab40-4eab-b113-88103708ebfe",
      "cell_type": "raw",
      "source": "import os\nimport time\nimport pandas as pd\nimport google.generativeai as genai\n\n# loading datasets from csv to df\nmixed_blog_posts_path = os.path.join(\"data\", \"ai_human_blog_posts.csv\")\nhuman_blog_posts_path = os.path.join(\"data\", \"human_blog_posts.csv\")\nai_blog_posts_path = os.path.join(\"data\", \"ai_generated_blog_posts.csv\")\n\ndf_mixed = pd.read_csv(mixed_blog_posts_path)\ndf_human = pd.read_csv(human_blog_posts_path)\ndf_ai = pd.read_csv(ai_blog_posts_path)\n\n# configuring API key and selecting generative language model\ngenai.configure(api_key=\"AIzaSyCkTDJidvCdoo3vSinFNLYutZV43pm_fBI\")\n\nmodel = genai.GenerativeModel(\n  model_name=\"gemini-1.5-flash\",\n)\n\n# initializing a list where we store classification results\nclassification_results = []\n\n# iterating over the dataframe rows thanks to .iterrows() to pass each blog post to the model for classification\nfor index, row in df_mixed.iterrows():\n    \n    post = row[\"blog_post\"] # dinamically identify the blog post to classify within each row\n\n    # configuring a static system prompt giving context, output formatting guidelines, and instructions (no blog posts' examples)\n    system_prompt = f\"You are a AI-generated content detector aimed to classify blog post texts either as \\\"human\\\" if you think the blog post is human-generated, or \\\"AI\\\" if you think the blog post is AI-generated.\\n\\nYour outout should contain just the classification result, namely the label representing your choice, with nothing else attached. The output format should be in plain text. \\n\\nLook at this output example to understand better how you should respond:\\n\\nIf the blog post text seems to you AI-generated -> OUTPUT = AI\\nIf the blog post text seems to you human-generated -> OUTPUT = human\\n\\nRemember to strictly adhere to the mentioned output format (just the classification label in plain text and nothing else attached).\"\n    \n    # configuring a dynamic user prompt, incorporating system prompt and the blog post to classify\n    user_prompt = system_prompt + post\n\n    # sending the prompt to the model while configuring temperature to 0 and max number of output tokens\n    response = model.generate_content(\n        user_prompt, \n        generation_config = genai.GenerationConfig(\n        max_output_tokens=100000,\n        temperature=0,\n        ))\n            \n    classification_result = response.text # accessing the model response\n\n    classification_results.append(classification_result) # storing the classification result in the list\n\n    print(f\"classified {index+1}° blog post -> result = {classification_result}\")\n\n    time.sleep(5) # adding some delay between calls to stay under API limits\n\ndf_mixed['classification_result_zeroshot'] = classification_results # storing the results in a proper df column\n\n# saving the updated df_mixed back to a csv file\ndf_mixed.to_csv(mixed_blog_posts_path, index=False, encoding=\"utf-8\")\n\nprint(\"df_mixed successfully saved; all classification results for zeroshot are stored\")\n\nprint(df_mixed) # double-checking if process went through",
      "metadata": {}
    },
    {
      "id": "71f5e9d6-54c3-4cab-946f-72a57995ce11",
      "cell_type": "raw",
      "source": "import os\nimport time\nimport pandas as pd\nimport google.generativeai as genai\n\n# loading datasets from csv to df\nmixed_blog_posts_path = os.path.join(\"data\", \"ai_human_blog_posts.csv\")\nhuman_blog_posts_path = os.path.join(\"data\", \"human_blog_posts.csv\")\nai_blog_posts_path = os.path.join(\"data\", \"ai_generated_blog_posts.csv\")\n\ndf_mixed = pd.read_csv(mixed_blog_posts_path)\ndf_human = pd.read_csv(human_blog_posts_path)\ndf_ai = pd.read_csv(ai_blog_posts_path)\n\n# configuring API key and selecting generative language model\ngenai.configure(api_key=\"AIzaSyCkTDJidvCdoo3vSinFNLYutZV43pm_fBI\")\n\nmodel = genai.GenerativeModel(\n  model_name=\"gemini-1.5-flash\",\n)\n\n# initializing a list where we store classification results\nclassification_results = []\n\n# iterating over the dataframe rows thanks to .iterrows() to pass each blog post to the model for classification\nfor index, row in df_mixed.iterrows():\n    \n    post = row[\"blog_post\"] # dinamically identify the blog post to classify within each row\n\n    # configuring a static system prompt giving context, output formatting guidelines, and instructions (no blog posts' examples)\n    system_prompt = f\"You are a AI-generated content detector aimed to classify blog post texts either as \\\"human\\\" if you think the blog post is human-generated, or \\\"AI\\\" if you think the blog post is AI-generated.\\n\\nYour outout should contain just the classification result, namely the label representing your choice, with nothing else attached. The output format should be in plain text. \\n\\nLook at this output example to understand better how you should respond:\\n\\nIf the blog post text seems to you AI-generated -> OUTPUT = AI\\nIf the blog post text seems to you human-generated -> OUTPUT = human\\n\\nRemember to strictly adhere to the mentioned output format (just the classification label in plain text and nothing else attached).\"\n    \n    # configuring a dynamic user prompt, incorporating system prompt and the blog post to classify\n    user_prompt = system_prompt + post\n\n    # sending the prompt to the model while configuring temperature to 0 and max number of output tokens\n    response = model.generate_content(\n        user_prompt, \n        generation_config = genai.GenerationConfig(\n        max_output_tokens=100000,\n        temperature=0,\n        ))\n            \n    classification_result = response.text # accessing the model response\n\n    classification_results.append(classification_result) # storing the classification result in the list\n\n    print(f\"classified {index+1}° blog post -> result = {classification_result}\")\n\n    time.sleep(5) # adding some delay between calls to stay under API limits\n\ndf_mixed['classification_result_zeroshot'] = classification_results # storing the results in a proper df column\n\n# saving the updated df_mixed back to a csv file\ndf_mixed.to_csv(mixed_blog_posts_path, index=False, encoding=\"utf-8\")\n\nprint(\"df_mixed successfully saved; all classification results for zeroshot are stored\")\n\nprint(df_mixed) # double-checking if process went through",
      "metadata": {}
    },
    {
      "id": "510d2901-d6d7-42d9-ad49-56a814b38474",
      "cell_type": "markdown",
      "source": "Look inside the enriched mixed dataset with classification results",
      "metadata": {}
    },
    {
      "id": "a5417c02-4b83-4e08-9bd5-b466467b1dd3",
      "cell_type": "code",
      "source": "print(df_mixed)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ea6aae48-93ff-4c2f-9d9d-7c4dd4a01d35",
      "cell_type": "markdown",
      "source": "## Classification results and performance metrics\nFinally, we arrived at the end of this AI-powered experiment. \n\nThe following snippet compute true positives, true negatives, false positives and false negatives by comparing the model's prediction with the annotated ground truth, iterating over all the blog posts in the mixed dataset.\n\nThen, it uses these outputs to compute performance metrics, namely: accuracy, precision, recall, and F1-score.\n\nThese operations are performed, of course, for both prompting approaches.\n\nFinally, the code plots data and create a clear visualization of the results obtained. (I know, I must improve in graphs' aesthetic and readibility, my bad. I will). Run the code safely, it takes no time.",
      "metadata": {}
    },
    {
      "id": "4fe5574e-3c34-479b-8414-392a216addac",
      "cell_type": "code",
      "source": "import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport json\n\n# declaring the path to the mixed data and loading the data\nmixed_blog_posts_path = os.path.join(\"data\", \"ai_human_blog_posts.csv\")\n\ndf_mixed = pd.read_csv(mixed_blog_posts_path)\n\n# stripping newlines from columns where values must be equal strings to compute classification results (true positives etc.)\ndf_mixed['classification_result_zeroshot'] = df_mixed['classification_result_zeroshot'].str.strip()\ndf_mixed['classification_result_fewshot'] = df_mixed['classification_result_fewshot'].str.strip()\ndf_mixed['AI_or_human'] = df_mixed['AI_or_human'].str.strip()\n\n# initializing counts for TP, TN, FP, FN for zeroshot and fewshot learning\ntrue_positives_zero = 0\ntrue_negatives_zero = 0\nfalse_positives_zero = 0\nfalse_negatives_zero = 0\n\ntrue_positives_few = 0\ntrue_negatives_few = 0\nfalse_positives_few = 0\nfalse_negatives_few = 0\n\n# iterating through each row in df_mixed to compare the row classification results with the annotated groundtruth (what was the blog post)\nfor index, row in df_mixed.iterrows():\n    classification_zero = row['classification_result_zeroshot']\n    classification_few = row['classification_result_fewshot']\n    ground_truth = row['AI_or_human']\n\n    if classification_zero == 'AI' and ground_truth == 'AI':\n        true_positives_zero += 1\n    if classification_few == 'AI' and ground_truth == 'AI':\n        true_positives_few += 1\n    if classification_zero == 'human' and ground_truth == 'human':\n        true_negatives_zero += 1\n    if classification_few == 'human' and ground_truth == 'human':\n        true_negatives_few += 1\n    if classification_zero == 'AI' and ground_truth == 'human':\n        false_positives_zero += 1\n    if classification_few == 'AI' and ground_truth == 'human':\n        false_positives_few += 1\n    if classification_zero == 'human' and ground_truth == 'AI':\n        false_negatives_zero += 1\n    if classification_few == 'human' and ground_truth == 'AI':\n        false_negatives_few += 1\n\n# computing performance metrics for zeroshot and fewshot learning following the standard formulas\naccuracy_zero = (true_positives_zero + true_negatives_zero) / len(df_mixed)\nprecision_zero = (true_positives_zero / (true_positives_zero + false_positives_zero) if (true_positives_zero + false_positives_zero) > 0 else 0)\nrecall_zero = (true_positives_zero / (true_positives_zero + false_negatives_zero) if (true_positives_zero + false_negatives_zero) > 0 else 0)\nF1_score_zero = (2 * (precision_zero * recall_zero) / (precision_zero + recall_zero) if (precision_zero + recall_zero) > 0 else 0)\n\n# Compute metrics for fewshot\naccuracy_few = (true_positives_few + true_negatives_few) / len(df_mixed)\nprecision_few = (true_positives_few / (true_positives_few + false_positives_few) if (true_positives_few + false_positives_few) > 0 else 0)\nrecall_few = (true_positives_few / (true_positives_few + false_negatives_few) if (true_positives_few + false_negatives_few) > 0 else 0)\nF1_score_few = (2 * (precision_few * recall_few) / (precision_few + recall_few) if (precision_few + recall_few) > 0 else 0)\n\n# storing classification results and performance metrics into dictionaries, comparing both approaches\nclassification_results = {\n    'true positives zero-shot': true_positives_zero,\n    'true positives few-shot': true_positives_few,\n    'true negatives zero-shot': true_negatives_zero,\n    'true negatives few-shot': true_negatives_few,\n    'false positives zero-shot': false_positives_zero,\n    'false positives few-shot': false_positives_few,\n    'false negatives zero-shot': false_negatives_zero,\n    'false negatives few-shot': false_negatives_few\n}\n\nperformance_metrics = {\n    'accuracy zero-shot': accuracy_zero,\n    'accuracy few-shot': accuracy_few,\n    'precision zero-shot': precision_zero,\n    'precision few-shot': precision_few,\n    'recall zero-shot': recall_zero,\n    'recall few-shot': recall_few,\n    'F1-score zero-shot': F1_score_zero,\n    'F1-score few-shot': F1_score_few\n}\n\nprint(classification_results) # looking at results\nprint(performance_metrics)\n\n# store the results into json\nclassification_json_path = os.path.join(\"data\", \"model_classification_results.json\")\nwith open(classification_json_path, 'w') as file:\n    json.dump(classification_results, file)\n\nmetrics_json_path = os.path.join(\"data\", \"model_performance_metrics.json\")\nwith open(metrics_json_path, 'w') as file:\n    json.dump(performance_metrics, file)\n\n# plotting classification results comparing both approaches, and saving the graph to the plot folder\nclassification_colors = ['cyan', 'pink', 'cyan', 'pink', 'cyan', 'pink', 'cyan', 'pink'] # defining colours for bars contrasting for the 2 approaches\nperformance_colors = ['purple', 'gray', 'purple', 'gray', 'purple', 'gray', 'purple', 'gray']\n\nplt.figure(figsize=(10, 6))\nplt.bar(classification_results.keys(), classification_results.values(), color=classification_colors)\nplt.title(\"classification results: zeroshot vs fewshot\")\nplt.xlabel(\"classification results\")\nplt.ylabel(\"absolute counts\")\nplt.xticks(rotation = 45, fontsize = 10)\nplt.tight_layout()\n\nclassification_results_plot_path = os.path.join(\"plot\", \"classification_results_zeroshot_vs_fewshot.png\")\nplt.savefig(classification_results_plot_path)\nplt.show()\n\n# plotting performance metrics comparing both approaches, and saving the graph to the plot folder\nplt.figure(figsize=(10, 6))\nplt.bar(performance_metrics.keys(), performance_metrics.values(), color=performance_colors)\nplt.title(\"performance metrics: zeroshot vs fewshot\")\nplt.xlabel(\"performance metrics\")\nplt.ylabel(\"performance measurement\")\nplt.xticks(rotation = 45, fontsize = 10)\nplt.tight_layout()\n\nperformance_metrics_plot_path = os.path.join(\"plot\", \"performance_metrics_zeroshot_vs_fewshot.png\")\nplt.savefig(performance_metrics_plot_path)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5aa8ae0a-822d-4218-82f6-a1c8f64793b8",
      "cell_type": "markdown",
      "source": "I hope you enjoyed this demo and that it was clear enough! You can dive even further into the project by looking at the entire repository and by reading the 4-page double-column report.",
      "metadata": {}
    }
  ]
}
